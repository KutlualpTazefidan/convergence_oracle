{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'509b1b28aacae6635c78ba106789f9c4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = \"2005-09-01\"\n",
    "date_end = \"2022-12-31\"\n",
    "issn = '1552-4450'\n",
    "\n",
    "api_keys = pd.read_csv('data/apikeys.csv')\n",
    "find_api_key = api_keys[api_keys['api_server']=='springer_eric'].reset_index()\n",
    "global_api_key = find_api_key['api_key'][0]\n",
    "\n",
    "# use this link:    https://dev.springernature.com/adding-constraints\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 17 17:24:43 2020\n",
    "Search in Spring Nature using API by keywords for NIS lab reserch projects.\n",
    "\n",
    "@author oudeng, Graduate School of Human Sciences, Waseda University\n",
    "\n",
    "Python program for searching info from Springer Nature via API by XML.\n",
    "Results includes the title, pdf url and abstract of destination articles. \n",
    "\n",
    "https://dev.springernature.com/signup.  \n",
    "registerred by WasedaID of Ou,DENG, got necessary API.\n",
    "\n",
    "How to use?\n",
    "1) Run this program in Python eviroment, including bs4 and requests lib.\n",
    "2) Input keywords for searching.\n",
    "3) Searching results will in Python console window.\n",
    "4) Read in console window directly if not too many results,\n",
    "   or copy contents to any other more confortable browsers.\n",
    "5) Copy url of artitle you like in results, use broswer to read PDF.\n",
    " \n",
    "Can use for other API?\n",
    "Yes. By modifying base_url, api_key, total and content identification tags.\n",
    "Just confirm the url you try in other API. \n",
    "\n",
    "Can for JSON?\n",
    "No, this programe for XML only.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Use a for-loop to flatten a list of lists\n",
    "def flatten_lists(listToFlatten):\n",
    "    flat_list = list()\n",
    "    for sub_list in listToFlatten:\n",
    "        flat_list += sub_list\n",
    "    return flat_list\n",
    "\n",
    "def getXML(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"Scraping Errors. Oooop!!\"\n",
    "\n",
    "def spList(total): \n",
    "    S=1 \n",
    "    P=50\n",
    "    if total%P != 0:\n",
    "        s = list(range(S, total//P*P, P)) + [total//P*P+1]\n",
    "        p = list([P]*(len(s)-1)) + [total%P]\n",
    "    else:\n",
    "        s = list(range(S, total//P*P, P)) \n",
    "        p = list([P]*len(s))\n",
    "    # print(s)\n",
    "    # print(p)\n",
    "    return s, p\n",
    "\n",
    "def search_function():\n",
    "    base_url = \"http://api.springernature.com/metadata/pam?q=\"\n",
    "    api_key = \"&api_key=\" + global_api_key\n",
    "    #Please register at SpringerNature as the following url for your API key.\n",
    "    # https://www.springernature.com/gp/campaign/librarian-covid-tdm?sap-outbound-id=64AF2B62DCE26C591DAA9263090CDAF763E0CD1F\n",
    "    \n",
    "    #input keywords to search in API, url_1 for confirm 'total' value    \n",
    "    \n",
    "    date_range = \"onlinedatefrom:\" + date_start + \"%20 onlinedateto:\" + date_end\n",
    "    journalISSN = \" issn: \" + issn\n",
    "    \n",
    "    # ================================================================= change keywords\n",
    "    #keywords = input(\"Input '+' in searching keywords:    \")\n",
    "    # =================================================================\n",
    "    \n",
    "    url_1 = base_url + date_range + journalISSN + api_key\n",
    "    xml_1 = getXML(url_1)\n",
    "    soup = bs(xml_1,'html.parser')\n",
    "    total = int(soup.find('total').string)\n",
    "    \n",
    "    print('Search Results Totally =',total)\n",
    "    \n",
    "    s, p = spList(total)\n",
    "    \n",
    "    full_list_of_papers = []\n",
    "    for i in range(len(s)):\n",
    "    # ================================================================= change keywords\n",
    "        url = base_url + date_range + journalISSN + '&s=' + str(s[i]) + '&p=' + str(p[i]) + api_key\n",
    "    # =================================================================\n",
    "        xml = getXML(url)\n",
    "        soup = bs(xml, \"html.parser\")\n",
    "        current_list_of_papers = soup.find_all('pam:message')\n",
    "        full_list_of_papers.append(current_list_of_papers)\n",
    "    \n",
    "            \n",
    "    print('Total=',total)\n",
    "    return flatten_lists(full_list_of_papers)\n",
    "\n",
    "## pull data \n",
    "## get citation number \n",
    "#https://stackoverflow.com/questions/69067691/retrieve-number-of-citations-of-a-scientific-paper-in-a-given-year\n",
    "# pull all the data\n",
    "# get data from allometric \n",
    "\n",
    "def search_to_dataframe(flattened_papers_list):\n",
    "    all_papers = flattened_papers_list\n",
    "    first_paper = all_papers[0]\n",
    "    list_of_fields = list(set([tag.name for tag in first_paper.find_all()]))\n",
    "\n",
    "    ## create a dictionary with a set of keys that is column titles\n",
    "    dict_of_paper_fields = {}\n",
    "    for i in list_of_fields: \n",
    "        appenddict = {i:[]}\n",
    "        dict_of_paper_fields.update(appenddict)\n",
    "\n",
    "    ## find the fields for each paper\n",
    "    for i in all_papers: \n",
    "        for j in list_of_fields: \n",
    "            dict_of_paper_fields[j].append(i.find_all(j))\n",
    "\n",
    "    return pd.DataFrame(dict_of_paper_fields)\n",
    "\n",
    "def csv_of_data(dataframeCreated):\n",
    "    dataframeCreated.to_csv('data/' +issn+ \"__\"+ date_start+\"_\"+date_end+ '.csv',index=False)\n",
    "\n",
    "def main():\n",
    "    list_of_search = search_function()\n",
    "    dataframe_of_search = search_to_dataframe(list_of_search)\n",
    "    csv_of_data(dataframe_of_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results Totally = 3684\n",
      "Total= 3684\n"
     ]
    }
   ],
   "source": [
    "date_start = \"2005-09-01\"\n",
    "date_end = \"2022-12-31\"\n",
    "issn = '1552-4450'\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
