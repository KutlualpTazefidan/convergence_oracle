{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar API\n",
    "100 requests per 5 minutes\n",
    "The API allows up to 100 requests per 5 minutes. \n",
    "\n",
    "To access a higher rate limit, complete the form to request authentication for your project.\n",
    "\n",
    "Max limit for each request is 100, so every 5 min we can gather (100*100)* amount of people\n",
    "\n",
    "Issue/Drawback of semantic scholar: it needs a search term, meaning, the topics/industry/niche needs to be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for retrieving data\n",
    "amount_of_data = 100  # Total amount of data to retrieve\n",
    "year_of_publication = 2015  # Year of publication\n",
    "research_field = ['author:Einstein']  # Research field(s) to query (as a list)\n",
    "initial_offset = 0  # Initial start offset\n",
    "\n",
    "# Gentle fetching\n",
    "sleep_duration = 30  # Sleep duration in seconds between requests\n",
    "max_retries = 3  # Maximum number of retries for failed requests\n",
    "\n",
    "# Define the API endpoint\n",
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "length_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m length_df \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df)  \u001b[39m# Update the total length of unfiltered data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Save each iteration into a separate file - to keep some of the data in case of an error\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# The filename format includes the research field, offset, and is converted to lowercase\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m research_field[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mlower() \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(offset) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Sleep for a specified duration between requests\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_Fetching_Data_From_SemanticScholar_alp.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResearch field\u001b[39m\u001b[39m\"\u001b[39m, research_field[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m\"\u001b[39m, i)\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3773\u001b[0m     path_or_buf,\n\u001b[1;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3789\u001b[0m )\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/pandas/io/formats/csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/pandas/io/common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 737\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    741\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/pandas/io/common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data'"
     ]
    }
   ],
   "source": [
    "# Fetching Data\n",
    "\n",
    "for i in range(int(amount_of_data / 100)):\n",
    "    offset = initial_offset + 100 * i  # Calculate the current offset\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        # Define query parameters\n",
    "        params = {\n",
    "            'query': research_field[0],  # Use the first research field from the list\n",
    "            'year': year_of_publication,\n",
    "            'limit': 100,  # Number of results per request\n",
    "            'offset': offset,  # Offset for pagination\n",
    "            'fields': 'title,authors,abstract,citationCount,year',\n",
    "        }\n",
    "\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "            # Create a DataFrame for unfiltered results\n",
    "            df = pd.DataFrame(data['data'])\n",
    "\n",
    "            break  # Successful request, exit the retry loop\n",
    "        else:\n",
    "            print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(\"Retrying after sleep...\")\n",
    "                time.sleep(sleep_duration)\n",
    "\n",
    "    length_df += len(df)  # Update the total length of unfiltered data\n",
    "\n",
    "    # Save each iteration into a separate file - to keep some of the data in case of an error\n",
    "    # The filename format includes the research field, offset, and is converted to lowercase\n",
    "    df.to_csv('./data/' + research_field[0].replace(\" \", \"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "\n",
    "    # Sleep for a specified duration between requests\n",
    "    print(\"Research field\", research_field[0], \"Iteration: \", i)\n",
    "    time.sleep(sleep_duration)\n",
    "\n",
    "# Print the total length of unfiltered data after all iterations\n",
    "print(\"Length of the unfiltered data\", length_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Query - Using the Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of the code is not required but it provides example how to filter the data before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetching and filtering at the same time if required\n",
    "# # Fetching Data\n",
    "# length_filtered_df = 0\n",
    "# length_unfiltered_df = 0\n",
    "\n",
    "# for i in range(int(amount_of_data/100)):\n",
    "#     offset = initial_offset + 100 * i\n",
    "#     retries = 0\n",
    "    \n",
    "#     while retries < max_retries:\n",
    "#         # Define query parameters\n",
    "#         params = {\n",
    "#             'query': research_field,\n",
    "#             'year': year_of_publication,\n",
    "#             'limit': 100,\n",
    "#             'offset': offset,\n",
    "#             'fields': 'title,authors,abstract,citationCount,year',\n",
    "#         }\n",
    "\n",
    "#         # Make the GET request\n",
    "#         response = requests.get(api_url, params=params)\n",
    "\n",
    "#         # Check if the request was successful\n",
    "#         if response.status_code == 200:\n",
    "#             data = json.loads(response.text)\n",
    "\n",
    "#             # Create a DataFrame for unfiltered results\n",
    "#             df_unfiltered = pd.DataFrame(data['data'])\n",
    "\n",
    "#             # Filter the results based on 'citationCount' > 10\n",
    "#             filtered_results = [paper for paper in data['data'] if paper.get('citationCount', 0) > 10]\n",
    "\n",
    "#             # Create a DataFrame for filtered results\n",
    "#             df_filtered = pd.DataFrame(filtered_results)\n",
    "\n",
    "\n",
    "#             break  # Successful request, exit the retry loop\n",
    "#         else:\n",
    "#             print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "#             retries += 1\n",
    "#             if retries < max_retries:\n",
    "#                 print(\"Retrying after sleep...\")\n",
    "#                 time.sleep(sleep_duration)\n",
    "    \n",
    "#     # length_filtered_df = len(df_filtered)\n",
    "#     length_unfiltered_df += len(df_unfiltered)\n",
    "\n",
    "#     # Save each iteration into separate file - to keep some of the data in case of an error\n",
    "#     df_unfiltered.to_csv('./data/'+research_field.replace(\" \",\"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "#     df_filtered.to_csv('./data/df_filtered' + str(offset) + '.csv', index=False)\n",
    "\n",
    "#     # Sleep for a specified duration between requests\n",
    "#     print(\"Research field\",research_field,\"Iteration: \", i)\n",
    "#     time.sleep(sleep_duration)\n",
    "# print(\"Length of the filtered data\",length_unfiltered_df)\n",
    "# print(\"Length of the unfiltered data\",length_filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
