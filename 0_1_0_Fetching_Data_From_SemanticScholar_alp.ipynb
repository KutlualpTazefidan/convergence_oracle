{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar API\n",
    "100 requests per 5 minutes\n",
    "The API allows up to 100 requests per 5 minutes. \n",
    "\n",
    "To access a higher rate limit, complete the form to request authentication for your project.\n",
    "\n",
    "Max limit for each request is 100, so every 5 min we can gather (100*100)* amount of people\n",
    "\n",
    "Issue/Drawback of semantic scholar: it needs a search term, meaning, the topics/industry/niche needs to be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for retrieving data\n",
    "amount_of_data = 100  # Total amount of data to retrieve\n",
    "start_date = 2019  # Year of publication\n",
    "end_date = 2023  # Year of publication\n",
    "journal = [\"Nat. Chem. Biol\"]\n",
    "# research_field = ['Computer Science'] # Research field(s) to query (as a list)\n",
    "alphabet_letters = ['A','E','I','U','O'] #  list(string.ascii_uppercase) # Research field(s) to query (as a list)\n",
    "initial_offset = 0  # Initial start offset\n",
    "number_of_publications_per_request = 100\n",
    "\n",
    "# Gentle fetching\n",
    "sleep_duration = 1  # Sleep duration in seconds between requests\n",
    "retry_sleep_duration = 1  # Sleep duration in seconds between requests\n",
    "max_retries = 100  # Maximum number of retries for failed requests\n",
    "\n",
    "# Define the API endpoint\n",
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "length_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving single publication for debugging purposes\n",
    "params = {\n",
    "    'query': \"A\",  # Use the first research field from the list\n",
    "    'venue': journal[0],\n",
    "    'year': 2013,\n",
    "    'limit': 1,  # Number of results per request\n",
    "    'offset': 0,  # Offset for pagination\n",
    "    # 'fields': 'externalIds,title,abstract,citationCount,year',\n",
    "    'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,publicationVenue,year,tldr,embedding.specter_v2,citations,references',\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(api_url, params=params,headers={'X-API-KEY':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.text)\n",
    "else:\n",
    "    print(f\"Error: Request failed with status code {response.status_code}\")\n",
    "    print(f\"Error message: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>externalIds_MAG</th>\n",
       "      <th>externalIds_DOI</th>\n",
       "      <th>externalIds_CorpusId</th>\n",
       "      <th>externalIds_PubMed</th>\n",
       "      <th>publicationVenue_id</th>\n",
       "      <th>publicationVenue_name</th>\n",
       "      <th>publicationVenue_type</th>\n",
       "      <th>publicationVenue_alternate_names</th>\n",
       "      <th>publicationVenue_issn</th>\n",
       "      <th>...</th>\n",
       "      <th>s2FieldsOfStudy</th>\n",
       "      <th>tldr_model</th>\n",
       "      <th>tldr_text</th>\n",
       "      <th>publicationTypes</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>embedding_vector</th>\n",
       "      <th>authors</th>\n",
       "      <th>citations</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08e50d1cc59a1d9c1a0d21dfe9667e243aa1094d</td>\n",
       "      <td>1984615306</td>\n",
       "      <td>10.1016/S0140-6736(13)61611-6</td>\n",
       "      <td>41940846</td>\n",
       "      <td>23993280</td>\n",
       "      <td>ba627f0e-abf9-4f01-8146-73118f4ebaf9</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>journal</td>\n",
       "      <td>[Lancet]</td>\n",
       "      <td>0923-7577</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'category': 'Medicine', 'source': 'external'...</td>\n",
       "      <td>tldr@v2.0.0</td>\n",
       "      <td>The findings show the striking and growing cha...</td>\n",
       "      <td>[Review, JournalArticle]</td>\n",
       "      <td>2013-11-09</td>\n",
       "      <td>specter_v2</td>\n",
       "      <td>[0.24848510324954987, 0.40732088685035706, -0....</td>\n",
       "      <td>[{'authorId': '5260255', 'name': 'H. Whiteford...</td>\n",
       "      <td>[{'paperId': '53840ae196ecf4d46c5f02ed535beafe...</td>\n",
       "      <td>[{'paperId': '00f63e8997831bbe70cd2bb949c44dbd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId externalIds_MAG   \n",
       "0  08e50d1cc59a1d9c1a0d21dfe9667e243aa1094d      1984615306  \\\n",
       "\n",
       "                 externalIds_DOI  externalIds_CorpusId externalIds_PubMed   \n",
       "0  10.1016/S0140-6736(13)61611-6              41940846           23993280  \\\n",
       "\n",
       "                    publicationVenue_id publicationVenue_name   \n",
       "0  ba627f0e-abf9-4f01-8146-73118f4ebaf9            The Lancet  \\\n",
       "\n",
       "  publicationVenue_type publicationVenue_alternate_names   \n",
       "0               journal                         [Lancet]  \\\n",
       "\n",
       "  publicationVenue_issn  ...   \n",
       "0             0923-7577  ...  \\\n",
       "\n",
       "                                     s2FieldsOfStudy   tldr_model   \n",
       "0  [{'category': 'Medicine', 'source': 'external'...  tldr@v2.0.0  \\\n",
       "\n",
       "                                           tldr_text   \n",
       "0  The findings show the striking and growing cha...  \\\n",
       "\n",
       "           publicationTypes publicationDate  embedding_model   \n",
       "0  [Review, JournalArticle]      2013-11-09       specter_v2  \\\n",
       "\n",
       "                                    embedding_vector   \n",
       "0  [0.24848510324954987, 0.40732088685035706, -0....  \\\n",
       "\n",
       "                                             authors   \n",
       "0  [{'authorId': '5260255', 'name': 'H. Whiteford...  \\\n",
       "\n",
       "                                           citations   \n",
       "0  [{'paperId': '53840ae196ecf4d46c5f02ed535beafe...  \\\n",
       "\n",
       "                                          references  \n",
       "0  [{'paperId': '00f63e8997831bbe70cd2bb949c44dbd...  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten_dict(d, parent_key='', separator='_'):\n",
    "    flattened = {}\n",
    "    for key, value in d.items():\n",
    "        new_key = parent_key + separator + key if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                sub_new_key = new_key + separator + sub_key\n",
    "                flattened[sub_new_key] = sub_value\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "flattened_data = []  # A list to store flattened dictionaries\n",
    "original_data = pd.DataFrame(data)  # Original DataFrame\n",
    "num_records = len(data['data'])  # Number of records in the 'data' list\n",
    "\n",
    "# Loop through each record in 'data' and flatten it\n",
    "for i in range(num_records):\n",
    "    flattened_data.append(flatten_dict(data['data'][i]))\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "flattened_df = pd.DataFrame(flattened_data)\n",
    "flattened_df.to_csv(\"./temp_data/all_features.csv\",index=False)\n",
    "# Display the first few rows of the flattened DataFrame\n",
    "display(flattened_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the letter:  I  batches:  [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 27]\n",
      "Letter:  I  Batch:  100  Iteration:  0\n",
      "Letter:  I  Batch:  100  Iteration:  1\n",
      "Letter:  I  Batch:  100  Iteration:  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m: alphabet_letters[k],  \u001b[39m# Use the first research field from the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mvenue\u001b[39m\u001b[39m'\u001b[39m: journal[\u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mexternalIds,title,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,publicationVenue,year\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Make the GET request\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(api_url, params\u001b[39m=\u001b[39;49mparams,headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mx-api-key\u001b[39;49m\u001b[39m'\u001b[39;49m:os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mSEMANTICSCHOLAR_API_KEY\u001b[39;49m\u001b[39m\"\u001b[39;49m)})\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# response = requests.get(api_url, params=params)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/linuxuser/spiced-academy/data_science_capstone/convergence_oracle/0_1_0_Fetching_Data_From_SemanticScholar_alp.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    942\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    943\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/spiced-academy/data_science_capstone/convergence_oracle/.venv/lib/python3.11/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fetching Data\n",
    "year_list = list(range(start_date, end_date + 1))\n",
    "retries_retry_exhausted = False\n",
    "year_list.reverse()\n",
    "for j, year in enumerate(year_list):\n",
    "    print(\"Year\",year)\n",
    "    for k,letter in enumerate(alphabet_letters):\n",
    "        # First request to get the number of total papers to limit the loop\n",
    "        params = {\n",
    "            'query': alphabet_letters[k],  # Use the first research field from the list\n",
    "            'venue': journal[0],\n",
    "            'year': year,\n",
    "            'limit': 1,  # Number of results per request\n",
    "            'offset': 0,  # Offset for pagination\n",
    "            'fields': 'title,authors,abstract,citationCount,year',\n",
    "            # 'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,citations,references,publicationVenue,year',\n",
    "\n",
    "        }\n",
    "\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, params=params,headers={'x-api-key':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "        # response = requests.get(api_url, params=params)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "        # calculate the batches required to get every publication for that topic\n",
    "        num_batches, remainder = divmod(data['total'], number_of_publications_per_request)\n",
    "        batches = [number_of_publications_per_request] * num_batches\n",
    "        if remainder > 0:\n",
    "            batches.append(remainder)\n",
    "            \n",
    "        print(\"For the letter: \", letter ,\" batches: \",batches)\n",
    "\n",
    "        for i,batch in enumerate(batches):\n",
    "            offset = initial_offset + number_of_publications_per_request * i  # Calculate the current offset\n",
    "            retries = 0\n",
    "\n",
    "            print(\"Letter: \", alphabet_letters[k],\" Batch: \",str(batches[i]), \" Iteration: \", i)\n",
    "            while retries < max_retries:\n",
    "                # Define query parameters\n",
    "                params = {\n",
    "                    'query': alphabet_letters[k],  # Use the first research field from the list\n",
    "                    'venue': journal[0],\n",
    "                    'year': year,\n",
    "                    'limit': batch,  # Number of results per request\n",
    "                    'offset': offset,  # Offset for pagination\n",
    "                    # 'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,citations,references,publicationVenue,year',\n",
    "                    'fields': 'externalIds,title,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,publicationVenue,year',\n",
    "                }\n",
    "\n",
    "                # Make the GET request\n",
    "                response = requests.get(api_url, params=params,headers={'x-api-key':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "                # response = requests.get(api_url, params=params)\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    data = json.loads(response.text)\n",
    "\n",
    "                    # Create a DataFrame for unfiltered results\n",
    "                    df = pd.DataFrame(data['data'])\n",
    "\n",
    "                    break  # Successful request, exit the retry loop\n",
    "                else:\n",
    "                    print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "                    retries += 1\n",
    "                    if retries < max_retries:\n",
    "                        print(\"Retrying after sleep: \"+str(retry_sleep_duration*1)+\"min\")\n",
    "                        time.sleep(retry_sleep_duration*1)\n",
    "                    if retries == max_retries:\n",
    "                        print(\"Fetch Failed\")\n",
    "                        retries_retry_exhausted = True\n",
    "                        break\n",
    "                if retries_retry_exhausted == True:\n",
    "                    print(\"Fetch Failed\")\n",
    "                    break\n",
    "            if retries_retry_exhausted == True:\n",
    "                print(\"Fetch Failed\")\n",
    "                break\n",
    "            # print(\"total amount\",response.json())\n",
    "            length_df += len(df)  # Update the total length of unfiltered data\n",
    "\n",
    "            # Save each iteration into a separate file - to keep some of the data in case of an error\n",
    "            # The filename format includes the research field, offset, and is converted to lowercase\n",
    "            df.to_csv('./data/'+ str(year) +\"_\"+ journal[0]+ \"_\" + alphabet_letters[k].replace(\" \", \"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "\n",
    "            # Sleep for a specified duration between requests\n",
    "            time.sleep(sleep_duration)\n",
    "    if retries_retry_exhausted == True:\n",
    "        print(\"Fetch Failed\")\n",
    "        break\n",
    "    # Create an empty DataFrame to store the merged data\n",
    "    merged_data = pd.DataFrame()\n",
    "    # Iterate through CSV files in the directory\n",
    "    for filename in os.listdir(\"./data/\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Extract the year from the filename (assuming the filename follows the \"year_\" format)\n",
    "            file_year = int(filename.split(\"_\")[0])\n",
    "\n",
    "            if file_year == year:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                temp_df = pd.read_csv(\"./data/\"+ filename)\n",
    "                merged_data = pd.concat([merged_data, temp_df], ignore_index=True)\n",
    "                # Remove the old CSV file\n",
    "                os.remove(\"./data/\"+ filename)\n",
    "    # Calculate the number of rows dropped\n",
    "    dropped_count = len(merged_data) - len(merged_data.drop_duplicates(subset='paperId', keep='first'))\n",
    "\n",
    "    # Remove rows with duplicate 'externalIdsIDs' values\n",
    "    merged_data = merged_data.drop_duplicates(subset='paperId', keep='first')\n",
    "\n",
    "    # Calculate the number of rows remaining\n",
    "    remaining_count = len(merged_data)\n",
    "\n",
    "    # Print the counts\n",
    "    print(f\"Removed {dropped_count} rows. Rows remaining: {remaining_count}\")\n",
    "    merged_data.to_csv('./data/'+ str(year) +\"_\"+ journal[0]+\"_\"+str(remaining_count)+'no_cit_no_ref_no_auth.csv', index=False)\n",
    "    time.sleep(sleep_duration)\n",
    " \n",
    "\n",
    "\n",
    "# Print the total length of unfiltered data after all iterations\n",
    "print(\"Length of the data\", length_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Query - Using the Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "# https://api.semanticscholar.org/graph/v1/paper/search?query=computer+science&year=2015&fields=title,year,authors,citationCount&limit=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of the code is not required but it provides example how to filter the data before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetching and filtering at the same time if required\n",
    "# # Fetching Data\n",
    "# length_filtered_df = 0\n",
    "# length_unfiltered_df = 0\n",
    "\n",
    "# for i in range(int(amount_of_data/100)):\n",
    "#     offset = initial_offset + 100 * i\n",
    "#     retries = 0\n",
    "    \n",
    "#     while retries < max_retries:\n",
    "#         # Define query parameters\n",
    "#         params = {\n",
    "#             'query': research_field,\n",
    "#             'year': year_of_publication,\n",
    "#             'limit': 100,\n",
    "#             'offset': offset,\n",
    "#             'fields': 'title,authors,abstract,citationCount,year',\n",
    "#         }\n",
    "\n",
    "#         # Make the GET request\n",
    "#         response = requests.get(api_url, params=params)\n",
    "\n",
    "#         # Check if the request was successful\n",
    "#         if response.status_code == 200:\n",
    "#             data = json.loads(response.text)\n",
    "\n",
    "#             # Create a DataFrame for unfiltered results\n",
    "#             df_unfiltered = pd.DataFrame(data['data'])\n",
    "\n",
    "#             # Filter the results based on 'citationCount' > 10\n",
    "#             filtered_results = [paper for paper in data['data'] if paper.get('citationCount', 0) > 10]\n",
    "\n",
    "#             # Create a DataFrame for filtered results\n",
    "#             df_filtered = pd.DataFrame(filtered_results)\n",
    "\n",
    "\n",
    "#             break  # Successful request, exit the retry loop\n",
    "#         else:\n",
    "#             print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "#             retries += 1\n",
    "#             if retries < max_retries:\n",
    "#                 print(\"Retrying after sleep...\")\n",
    "#                 time.sleep(sleep_duration)\n",
    "    \n",
    "#     # length_filtered_df = len(df_filtered)\n",
    "#     length_unfiltered_df += len(df_unfiltered)\n",
    "\n",
    "#     # Save each iteration into separate file - to keep some of the data in case of an error\n",
    "#     df_unfiltered.to_csv('./data/'+research_field.replace(\" \",\"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "#     df_filtered.to_csv('./data/df_filtered' + str(offset) + '.csv', index=False)\n",
    "\n",
    "#     # Sleep for a specified duration between requests\n",
    "#     print(\"Research field\",research_field,\"Iteration: \", i)\n",
    "#     time.sleep(sleep_duration)\n",
    "# print(\"Length of the filtered data\",length_unfiltered_df)\n",
    "# print(\"Length of the unfiltered data\",length_filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
