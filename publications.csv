Title,Abstract,Citations
Dynamic Backtracking,"Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"A Market-Oriented Programming Environment and its Application to
  Distributed Multicommodity Flow Problems","Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms.",See http://www.jair.org/ for any accompanying files
An Empirical Analysis of Search in GSAT,"We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms.",See http://www.jair.org/ for any accompanying files
The Difficulties of Learning Logic Programs with Cut,"As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages.",See http://www.jair.org/ for any accompanying files
Software Agents: Completing Patterns and Constructing User Interfaces,"To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Decidable Reasoning in Terminological Knowledge Representation Systems,"Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted.",See http://www.jair.org/ for any accompanying files
Teleo-Reactive Programs for Agent Control,"A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots.",See http://www.jair.org/ for any accompanying files
"Learning the Past Tense of English Verbs: The Symbolic Pattern
  Associator vs. Connectionist Models","Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"Substructure Discovery Using Minimum Description Length and Background
  Knowledge","The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Bias-Driven Revision of Logical Domain Theories,"The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.",See http://www.jair.org/ for any accompanying files
"Exploring the Decision Forest: An Empirical Investigation of Occam's
  Razor in Decision Tree Induction","We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees.",See http://www.jair.org/ for any accompanying files
"A Semantics and Complete Algorithm for Subsumption in the CLASSIC
  Description Logic","This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC, a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions, the developers have had to use an
algorithm that is incomplete with respect to the standard, model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete, and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs,
which are an abstracted version of the implementation structures used in
CLASSIC, and are of independent interest.",See http://www.jair.org/ for any accompanying files
Applying GSAT to Non-Clausal Formulas,"In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far.",See http://www.jair.org/ for any accompanying files
Random Worlds and Maximum Entropy,"Given a knowledge base KB containing first-order and statistical facts, we
consider a principled method, called the random-worlds method, for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals, then we can consider all
possible worlds, or first-order models, with domain {1,...,N} that satisfy KB,
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only, we can naturally associate an entropy with each world. As N
grows larger, there are many more worlds with higher entropy. Therefore, we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence,
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly, the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general, the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods.",See http://www.jair.org/ for any accompanying files
"Pattern Matching and Discourse Processing in Information Extraction from
  Japanese Text","Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First, sentence level processing locates relevant
pieces of information scattered throughout the text; second, discourse
processing merges coreferential information to generate the output. In the
first step, pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step, partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format, complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance.",See http://www.jair.org/ for any accompanying files
A System for Induction of Oblique Decision Trees,"This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
On Planning while Learning,"This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner.",See http://www.jair.org/ for any accompanying files
Wrap-Up: a Trainable Discourse Module for Information Extraction,"The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text,
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging, lexical disambiguation, and dictionary construction.
Wrap-Up is fully trainable, and not only automatically decides what classifiers
are needed, but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain.",See http://www.jair.org/ for any accompanying files
Operations for Learning with Graphical Models,"This paper is a multidisciplinary review of empirical, statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks, directed graphs representing a Markov chain, and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition, differentiation, and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework: Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas, some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression, techniques for feed-forward networks,
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms.",See http://www.jair.org/ for any accompanying files
Total-Order and Partial-Order Planning: A Comparative Analysis,"For many years, the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper, we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Solving Multiclass Learning Problems via Error-Correcting Output Codes,"Multiclass learning problems involve finding a definition for an unknown
function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k
``classes''). The definition is acquired by studying collections of training
examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning
problems include direct application of multiclass algorithms such as the
decision-tree algorithms C4.5 and CART, application of binary concept learning
algorithms to learn individual binary functions for each of the k classes, and
application of binary concept learning algorithms with distributed output
representations. This paper compares these three approaches to a new technique
in which error-correcting codes are employed as a distributed output
representation. We show that these output representations improve the
generalization performance of both C4.5 and backpropagation on a wide range of
multiclass learning tasks. We also demonstrate that this approach is robust
with respect to changes in the size of the training sample, the assignment of
distributed representations to particular classes, and the application of
overfitting avoidance techniques such as decision-tree pruning. Finally, we
show that---like the other methods---the error-correcting code technique can
provide reliable class probability estimates. Taken together, these results
demonstrate that error-correcting output codes provide a general-purpose method
for improving the performance of inductive learning programs on multiclass
problems.",See http://www.jair.org/ for any accompanying files
A Domain-Independent Algorithm for Plan Adaptation,"The paradigms of transformational planning, case-based planning, and plan
debugging all involve a process known as plan adaptation - modifying or
repairing an old plan so it solves a new problem. In this paper we provide a
domain-independent algorithm for plan adaptation, demonstrate that it is sound,
complete, and systematic, and compare it to other adaptation algorithms in the
literature. Our approach is based on a view of planning as searching a graph of
partial plans. Generative planning starts at the graph's root and moves from
node to node using plan-refinement operators. In planning by adaptation, a
library plan - an arbitrary node in the plan graph - is the starting point for
the search, and the plan-adaptation algorithm can apply both the same
refinement operators available to a generative planner and can also retract
constraints and steps from the plan. Our algorithm's completeness ensures that
the adaptation algorithm will eventually search the entire graph and its
systematicity ensures that it will do so without redundantly searching any
parts of the graph.",See http://www.jair.org/ for any accompanying files
"Truncating Temporal Differences: On the Efficient Implementation of
  TD(lambda) for Reinforcement Learning","Temporal difference (TD) methods constitute a class of methods for learning
predictions in multi-step prediction problems, parameterized by a recency
factor lambda. Currently the most important application of these methods is to
temporal credit assignment in reinforcement learning. Well known reinforcement
learning algorithms, such as AHC or Q-learning, may be viewed as instances of
TD learning. This paper examines the issues of the efficient and general
implementation of TD(lambda) for arbitrary lambda, for use with reinforcement
learning algorithms optimizing the discounted sum of rewards. The traditional
approach, based on eligibility traces, is argued to suffer from both
inefficiency and lack of generality. The TTD (Truncated Temporal Differences)
procedure is proposed as an alternative, that indeed only approximates
TD(lambda), but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is
derived is fairly simple and not new, but probably unexplored so far.
Encouraging experimental results are presented, suggesting that using lambda
&gt 0 with the TTD procedure allows one to obtain a significant learning
speedup at essentially the same cost as usual TD(0) learning.",See http://www.jair.org/ for any accompanying files
"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic
  Decision Tree Induction Algorithm","This paper introduces ICET, a new algorithm for cost-sensitive
classification. ICET uses a genetic algorithm to evolve a population of biases
for a decision tree induction algorithm. The fitness function of the genetic
algorithm is the average cost of classification when using the decision tree,
including both the costs of tests (features, measurements) and the costs of
classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,
which classifies without regard to cost. The five algorithms are evaluated
empirically on five real-world medical datasets. Three sets of experiments are
performed. The first set examines the baseline performance of the five
algorithms on the five datasets and establishes that ICET performs
significantly better than its competitors. The second set tests the robustness
of ICET under a variety of conditions and shows that ICET maintains its
advantage. The third set looks at ICET's search in bias space and discovers a
way to improve the search.",See http://www.jair.org/ for any accompanying files
"Rerepresenting and Restructuring Domain Theories: A Constructive
  Induction Approach","Theory revision integrates inductive learning and background knowledge by
combining training examples with a coarse domain theory to produce a more
accurate theory. There are two challenges that theory revision and other
theory-guided systems face. First, a representation language appropriate for
the initial theory may be inappropriate for an improved theory. While the
original representation may concisely express the initial theory, a more
accurate theory forced to use that same representation may be bulky,
cumbersome, and difficult to reach. Second, a theory structure suitable for a
coarse domain theory may be insufficient for a fine-tuned theory. Systems that
produce only small, local changes to a theory have limited value for
accomplishing complex structural alterations that may be required.
Consequently, advanced theory-guided learning systems require flexible
representation and flexible structure. An analysis of various theory revision
systems and theory-guided learning systems reveals specific strengths and
weaknesses in terms of these two desired properties. Designed to capture the
underlying qualities of each system, a new system uses theory-guided
constructive induction. Experiments in three domains show improvement over
previous theory-guided systems. This leads to a study of the behavior,
limitations, and potential of theory-guided constructive induction.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Using Pivot Consistency to Decompose and Solve Functional CSPs,"Many studies have been carried out in order to increase the search efficiency
of constraint satisfaction problems; among them, some make use of structural
properties of the constraint network; others take into account semantic
properties of the constraints, generally assuming that all the constraints
possess the given property. In this paper, we propose a new decomposition
method benefiting from both semantic properties of functional constraints (not
bijective constraints) and structural properties of the network; furthermore,
not all the constraints need to be functional. We show that under some
conditions, the existence of solutions can be guaranteed. We first characterize
a particular subset of the variables, which we name a root set. We then
introduce pivot consistency, a new local consistency which is a weak form of
path consistency and can be achieved in O(n^2d^2) complexity (instead of
O(n^3d^3) for path consistency), and we present associated properties; in
particular, we show that any consistent instantiation of the root set can be
linearly extended to a solution, which leads to the presentation of the
aforementioned new method for solving by decomposing functional CSPs.",See http://www.jair.org/ for any accompanying files
Adaptive Load Balancing: A Study in Multi-Agent Learning,"We study the process of multi-agent reinforcement learning in the context of
load balancing in a distributed system, without use of either central
coordination or explicit communication. We first define a precise framework in
which to study adaptive load balancing, important features of which are its
stochastic nature and the purely local information available to individual
agents. Given this framework, we show illuminating results on the interplay
between basic adaptive behavior parameters and their effect on system
efficiency. We then investigate the properties of adaptive load balancing in
heterogeneous populations, and address the issue of exploration vs.
exploitation in that context. Finally, we show that naive use of communication
may not improve, and might even harm system efficiency.",See http://www.jair.org/ for any accompanying files
Provably Bounded-Optimal Agents,"Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.",See http://www.jair.org/ for any accompanying files
Pac-Learning Recursive Logic Programs: Efficient Algorithms,"We present algorithms that learn certain classes of function-free recursive
logic programs in polynomial time from equivalence queries. In particular, we
show that a single k-ary recursive constant-depth determinate clause is
learnable. Two-clause programs consisting of one learnable recursive clause and
one constant-depth determinate non-recursive clause are also learnable, if an
additional ``basecase'' oracle is assumed. These results immediately imply the
pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained, it is shown in a companion paper that
they are maximally general, in that generalizing either class in any natural
way leads to a computationally difficult learning problem. Thus, taken together
with its companion paper, this paper establishes a boundary of efficient
learnability for recursive logic programs.",See http://www.jair.org/ for any accompanying files
Pac-learning Recursive Logic Programs: Negative Results,"In a companion paper it was shown that the class of constant-depth
determinate k-ary recursive clauses is efficiently learnable. In this paper we
present negative results showing that any natural generalization of this class
is hard to learn in Valiant's model of pac-learnability. In particular, we show
that the following program classes are cryptographically hard to learn:
programs with an unbounded number of constant-depth linear recursive clauses;
programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of
constant locality. These results immediately imply the non-learnability of any
more general class of programs. We also show that learning a constant-depth
determinate program with either two linear recursive clauses or one linear
recursive clause and one non-recursive clause is as hard as learning boolean
DNF. Together with positive results from the companion paper, these negative
results establish a boundary of efficient learnability for recursive
function-free clauses.",See http://www.jair.org/ for any accompanying files
FLECS: Planning with a Flexible Commitment Strategy,"There has been evidence that least-commitment planners can efficiently handle
planning problems that involve difficult goal interactions. This evidence has
led to the common belief that delayed-commitment is the ""best"" possible
planning strategy. However, we recently found evidence that eager-commitment
planners can handle a variety of planning problems more efficiently, in
particular those with difficult operator choices. Resigned to the futility of
trying to find a universally successful planning strategy, we devised a planner
that can be used to study which domains and problems are best for which
planning strategies. In this article we introduce this new planning algorithm,
FLECS, which uses a FLExible Commitment Strategy with respect to plan-step
orderings. It is able to use any strategy from delayed-commitment to
eager-commitment. The combination of delayed and eager operator-ordering
commitments allows FLECS to take advantage of the benefits of explicitly using
a simulated execution state and reasoning about planning constraints. FLECS can
vary its commitment strategy across different problems and domains, and also
during the course of a single planning problem. FLECS represents a novel
contribution to planning in that it explicitly provides the choice of which
commitment strategy to use while planning. FLECS provides a framework to
investigate the mapping from planning domains and problems to efficient
planning strategies.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"Induction of First-Order Decision Lists: Results on Learning the Past
  Tense of English Verbs","This paper presents a method for inducing logic programs from examples that
learns a new class of concepts called first-order decision lists, defined as
ordered lists of clauses each ending in a cut. The method, called FOIDL, is
based on FOIL (Quinlan, 1990) but employs intensional background knowledge and
avoids the need for explicit negative examples. It is particularly useful for
problems that involve rules with specific exceptions, such as learning the
past-tense of English verbs, a task widely studied in the context of the
symbolic/connectionist debate. FOIDL is able to learn concise, accurate
programs for this problem from significantly fewer examples than previous
methods (both connectionist and symbolic).",See http://www.jair.org/ for any accompanying files
"Building and Refining Abstract Planning Cases by Change of
  Representation Language","ion is one of the most promising approaches to improve the performance of
problem solvers. In several domains abstraction by dropping sentences of a
domain description -- as used in most hierarchical planners -- has proven
useful. In this paper we present examples which illustrate significant
drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we
propose a more general view of abstraction involving the change of
representation language. We have developed a new abstraction methodology and a
related sound and complete learning algorithm that allows the complete change
of representation language of planning cases from concrete to abstract.
However, to achieve a powerful change of the representation language, the
abstract language itself as well as rules which describe admissible ways of
abstracting states must be provided in the domain model. This new abstraction
approach is the core of Paris (Plan Abstraction and Refinement in an Integrated
System), a system in which abstract planning cases are automatically learned
from given concrete cases. An empirical study in the domain of process planning
in mechanical engineering shows significant advantages of the proposed
reasoning from abstract cases over classical hierarchical planning.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Using Qualitative Hypotheses to Identify Inaccurate Data,"Identifying inaccurate data has long been regarded as a significant and
difficult problem in AI. In this paper, we present a new method for identifying
inaccurate data on the basis of qualitative correlations among related data.
First, we introduce the definitions of related data and qualitative
correlations among related data. Then we put forward a new concept called
support coefficient function (SCF). SCF can be used to extract, represent, and
calculate qualitative correlations among related data within a dataset. We
propose an approach to determining dynamic shift intervals of inaccurate data,
and an approach to calculating possibility of identifying inaccurate data,
respectively. Both of the approaches are based on SCF. Finally we present an
algorithm for identifying inaccurate data by using qualitative correlations
among related data as confirmatory or disconfirmatory evidence. We have
developed a practical system for interpreting infrared spectra by applying the
method, and have fully tested the system against several hundred real spectra.
The experimental results show that the method is significantly better than the
conventional methods used in many similar systems.",See http://www.jair.org/ for any accompanying files
An Integrated Framework for Learning and Reasoning,"Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically,
learning being the topic of machine learning and neural networks, and reasoning
falling under classical (or symbolic) AI. However, learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE, that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented,
including classical induction, many important reasoning protocols and two
simple expert systems.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Diffusion of Context and Credit Information in Markovian Models,"This paper studies the problem of ergodicity of transition probability
matrices in Markovian models, such as hidden Markov models (HMMs), and how it
makes very difficult the task of learning to represent long-term context for
sequential data. This phenomenon hurts the forward propagation of long-term
context information, as well as learning a hidden state representation to
represent long-term context, which depends on propagating credit information
backwards in time. Using results from Markov chain theory, we show that this
problem of diffusion of context and credit is reduced when the transition
probabilities approach 0 or 1, i.e., the transition probability matrices are
sparse and the model essentially deterministic. The results found in this paper
apply to learning approaches based on continuous optimization, such as gradient
descent and the Baum-Welch algorithm.",See http://www.jair.org/ for any accompanying files
Improving Connectionist Energy Minimization,"Symmetric networks designed for energy minimization such as Boltzman machines
and Hopfield nets are frequently investigated for use in optimization,
constraint satisfaction and approximation of NP-hard problems. Nevertheless,
finding a global solution (i.e., a global minimum for the energy function) is
not guaranteed and even a local solution may take an exponential number of
steps. We propose an improvement to the standard local activation function used
for such networks. The improved algorithm guarantees that a global minimum is
found in linear time for tree-like subnetworks. The algorithm, called activate,
is uniform and does not assume that the network is tree-like. It can identify
tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid
local minima along these trees. For acyclic networks, the algorithm is
guaranteed to converge to a global minimum from any initial state of the system
(self-stabilization) and remains correct under various types of schedulers. On
the negative side, we show that in the presence of cycles, no uniform algorithm
exists that guarantees optimality even under a sequential asynchronous
scheduler. An asynchronous scheduler can activate only one unit at a time while
a synchronous scheduler can activate any number of units in a single time step.
In addition, no uniform algorithm exists to optimize even acyclic networks when
the scheduler is synchronous. Finally, we show how the algorithm can be
improved using the cycle-cutset scheme. The general algorithm, called
activate-with-cutset, improves over activate and has some performance
guarantees that are related to the size of the network's cycle-cutset.",See http://www.jair.org/ for any accompanying files
"Learning Membership Functions in a Function-Based Object Recognition
  System","Functionality-based recognition systems recognize objects at the category
level by reasoning about how well the objects support the expected function.
Such systems naturally associate a ``measure of goodness'' or ``membership
value'' with a recognized object. This measure of goodness is the result of
combining individual measures, or membership values, from potentially many
primitive evaluations of different properties of the object's shape. A
membership function is used to compute the membership value when evaluating a
primitive of a particular physical property of an object. In previous versions
of a recognition system known as Gruff, the membership function for each of the
primitive evaluations was hand-crafted by the system designer. In this paper,
we provide a learning component for the Gruff system, called Omlet, that
automatically learns membership functions given a set of example objects
labeled with their desired category measure. The learning algorithm is
generally applicable to any problem in which low-level membership values are
combined through an and-or tree structure to give a final overall membership
value.",See http://www.jair.org/ for any accompanying files
Flexibly Instructable Agents,"This paper presents an approach to learning from situated, interactive
tutorial instruction within an ongoing agent. Tutorial instruction is a
flexible (and thus powerful) paradigm for teaching tasks because it allows an
instructor to communicate whatever types of knowledge an agent might need in
whatever situations might arise. To support this flexibility, however, the
agent must be able to learn multiple kinds of knowledge from a broad range of
instructional interactions. Our approach, called situated explanation, achieves
such learning through a combination of analytic and inductive techniques. It
combines a form of explanation-based learning that is situated for each
instruction with a full suite of contextually guided responses to incomplete
explanations. The approach is implemented in an agent called Instructo-Soar
that learns hierarchies of new tasks and other domain knowledge from
interactive natural language instructions. Instructo-Soar meets three key
requirements of flexible instructability that distinguish it from previous
systems: (1) it can take known or unknown commands at any instruction point;
(2) it can handle instructions that apply to either its current situation or to
a hypothetical situation specified in language (as in, for instance,
conditional instructions); and (3) it can learn, from instructions, each class
of knowledge it uses to perform tasks.",See http://www.jair.org/ for any accompanying files
OPUS: An Efficient Admissible Algorithm for Unordered Search,"OPUS is a branch and bound search algorithm that enables efficient admissible
search through spaces for which the order of search operator application is not
significant. The algorithm's search efficiency is demonstrated with respect to
very large machine learning search spaces. The use of admissible search is of
potential value to the machine learning community as it means that the exact
learning biases to be employed for complex learning tasks can be precisely
specified and manipulated. OPUS also has potential for application in other
areas of artificial intelligence, notably, truth maintenance.",See http://www.jair.org/ for any accompanying files
"Vision-Based Road Detection in Automotive Systems: A Real-Time
  Expectation-Driven Approach","The main aim of this work is the development of a vision-based road detection
system fast enough to cope with the difficult real-time constraints imposed by
moving vehicle applications. The hardware platform, a special-purpose massively
parallel system, has been chosen to minimize system production and operational
costs. This paper presents a novel approach to expectation-driven low-level
image segmentation, which can be mapped naturally onto mesh-connected massively
parallel SIMD architectures capable of handling hierarchical data structures.
The input image is assumed to contain a distorted version of a given template;
a multiresolution stretching process is used to reshape the original template
in accordance with the acquired image content, minimizing a potential function.
The distorted template is the process output.",See http://www.jair.org/ for any accompanying files
Generalization of Clauses under Implication,"In the area of inductive learning, generalization is a main operation, and
the usual definition of induction is based on logical implication. Recently
there has been a rising interest in clausal representation of knowledge in
machine learning. Almost all inductive learning systems that perform
generalization of clauses use the relation theta-subsumption instead of
implication. The main reason is that there is a well-known and simple technique
to compute least general generalizations under theta-subsumption, but not under
implication. However generalization under theta-subsumption is inappropriate
for learning recursive clauses, which is a crucial problem since recursion is
the basic program structure of logic programs. We note that implication between
clauses is undecidable, and we therefore introduce a stronger form of
implication, called T-implication, which is decidable between clauses. We show
that for every finite set of clauses there exists a least general
generalization under T-implication. We describe a technique to reduce
generalizations under implication of a clause to generalizations under
theta-subsumption of what we call an expansion of the original clause. Moreover
we show that for every non-tautological clause there exists a T-complete
expansion, which means that every generalization under T-implication of the
clause is reduced to a generalization under theta-subsumption of the expansion.",See http://www.jair.org/ for any accompanying files
Decision-Theoretic Foundations for Causal Reasoning,"We present a definition of cause and effect in terms of decision-theoretic
primitives and thereby provide a principled foundation for causal reasoning.
Our definition departs from the traditional view of causation in that causal
assertions may vary with the set of decisions available. We argue that this
approach provides added clarity to the notion of cause. Also in this paper, we
examine the encoding of causal relationships in directed acyclic graphs. We
describe a special class of influence diagrams, those in canonical form, and
show its relationship to Pearl's representation of cause and effect. Finally,
we show how canonical form facilitates counterfactual reasoning.",See http://www.jair.org/ for any accompanying files
Translating between Horn Representations and their Characteristic Models,"Characteristic models are an alternative, model based, representation for
Horn expressions. It has been shown that these two representations are
incomparable and each has its advantages over the other. It is therefore
natural to ask what is the cost of translating, back and forth, between these
representations. Interestingly, the same translation questions arise in
database theory, where it has applications to the design of relational
databases. This paper studies the computational complexity of these problems.
Our main result is that the two translation problems are equivalent under
polynomial reductions, and that they are equivalent to the corresponding
decision problem. Namely, translating is equivalent to deciding whether a given
set of models is the set of characteristic models for a given Horn expression.
We also relate these problems to the hypergraph transversal problem, a well
known problem which is related to other applications in AI and for which no
polynomial time algorithm is known. It is shown that in general our translation
problems are at least as hard as the hypergraph transversal problem, and in a
special case they are equivalent to it.",See http://www.jair.org/ for any accompanying files
Statistical Feature Combination for the Evaluation of Game Positions,"This article describes an application of three well-known statistical methods
in the field of game-tree search: using a large number of classified Othello
positions, feature weights for evaluation functions with a
game-phase-independent meaning are estimated by means of logistic regression,
Fisher's linear discriminant, and the quadratic discriminant function for
normally distributed features. Thereafter, the playing strengths are compared
by means of tournaments between the resulting versions of a world-class Othello
program. In this application, logistic regression - which is used here for the
first time in the context of game playing - leads to better results than the
other approaches.",See http://www.jair.org/ for any accompanying files
Rule-based Machine Learning Methods for Functional Prediction,"We describe a machine learning method for predicting the value of a
real-valued function, given the values of multiple input variables. The method
induces solutions from samples in the form of ordered disjunctive normal form
(DNF) decision rules. A central objective of the method and representation is
the induction of compact, easily interpretable solutions. This rule-based
decision model can be extended to search efficiently for similar cases prior to
approximating function values. Experimental results on real-world data
demonstrate that the new techniques are competitive with existing machine
learning and statistical methods and can sometimes yield superior regression
performance.",See http://www.jair.org/ for any accompanying files
"The Design and Experimental Analysis of Algorithms for Temporal
  Reasoning","Many applications -- from planning and scheduling to problems in molecular
biology -- rely heavily on a temporal reasoning component. In this paper, we
discuss the design and empirical analysis of algorithms for a temporal
reasoning system based on Allen's influential interval-based framework for
representing temporal information. At the core of the system are algorithms for
determining whether the temporal information is consistent, and, if so, finding
one or more scenarios that are consistent with the temporal information. Two
important algorithms for these tasks are a path consistency algorithm and a
backtracking algorithm. For the path consistency algorithm, we develop
techniques that can result in up to a ten-fold speedup over an already highly
optimized implementation. For the backtracking algorithm, we develop variable
and value ordering heuristics that are shown empirically to dramatically
improve the performance of the algorithm. As well, we show that a previously
suggested reformulation of the backtracking search problem can reduce the time
and space requirements of the backtracking search. Taken together, the
techniques we develop allow a temporal reasoning component to solve problems
that are of practical size.",See http://www.jair.org/ for any accompanying files
"Well-Founded Semantics for Extended Logic Programs with Dynamic
  Preferences","The paper describes an extension of well-founded semantics for logic programs
with two types of negation. In this extension information about preferences
between rules can be expressed in the logical language and derived dynamically.
This is achieved by using a reserved predicate symbol and a naming technique.
Conflicts among rules are resolved whenever possible on the basis of derived
preference information. The well-founded conclusions of prioritized logic
programs can be computed in polynomial time. A legal reasoning example
illustrates the usefulness of the approach.",See http://www.jair.org/ for any accompanying files
Logarithmic-Time Updates and Queries in Probabilistic Networks,"Traditional databases commonly support efficient query and update procedures
that operate in time which is sublinear in the size of the database. Our goal
in this paper is to take a first step toward dynamic reasoning in probabilistic
databases with comparable efficiency. We propose a dynamic data structure that
supports efficient algorithms for updating and querying singly connected
Bayesian networks. In the conventional algorithm, new evidence is absorbed in
O(1) time and queries are processed in time O(N), where N is the size of the
network. We propose an algorithm which, after a preprocessing phase, allows us
to answer queries in time O(log N) at the expense of O(log N) time per evidence
absorption. The usefulness of sub-linear processing time manifests itself in
applications requiring (near) real-time response over large probabilistic
databases. We briefly discuss a potential application of dynamic probabilistic
reasoning in computational biology.",See http://www.jair.org/ for any accompanying files
Quantum Computing and Phase Transitions in Combinatorial Search,"We introduce an algorithm for combinatorial search on quantum computers that
is capable of significantly concentrating amplitude into solutions for some NP
search problems, on average. This is done by exploiting the same aspects of
problem structure as used by classical backtrack methods to avoid unproductive
search choices. This quantum algorithm is much more likely to find solutions
than the simple direct use of quantum parallelism. Furthermore, empirical
evaluation on small problems shows this quantum algorithm displays the same
phase transition behavior, and at the same location, as seen in many previously
studied classical search methods. Specifically, difficult problem instances are
concentrated near the abrupt change from underconstrained to overconstrained
problems.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Mean Field Theory for Sigmoid Belief Networks,"We develop a mean field theory for sigmoid belief networks based on ideas
from statistical mechanics. Our mean field theory provides a tractable
approximation to the true probability distribution in these networks; it also
yields a lower bound on the likelihood of evidence. We demonstrate the utility
of this framework on a benchmark problem in statistical pattern
recognition---the classification of handwritten digits.",See http://www.jair.org/ for any accompanying files
Improved Use of Continuous Attributes in C4.5,"A reported weakness of C4.5 in domains with continuous attributes is
addressed by modifying the formation and evaluation of tests on continuous
attributes. An MDL-inspired penalty is applied to such tests, eliminating some
of them from consideration and altering the relative desirability of all tests.
Empirical trials show that the modifications lead to smaller decision trees
with higher predictive accuracies. Results also confirm that a new version of
C4.5 incorporating these changes is superior to recent approaches that use
global discretization and that construct small trees with multi-interval
splits.",See http://www.jair.org/ for any accompanying files
Active Learning with Statistical Models,"For many types of machine learning algorithms, one can compute the
statistically `optimal' way to select training data. In this paper, we review
how optimal data selection techniques have been used with feedforward neural
networks. We then show how the same principles may be used to select data for
two alternative, statistically-based learning architectures: mixtures of
Gaussians and locally weighted regression. While the techniques for neural
networks are computationally expensive and approximate, the techniques for
mixtures of Gaussians and locally weighted regression are both efficient and
accurate. Empirically, we observe that the optimality criterion sharply
decreases the number of training examples the learner needs in order to achieve
good performance.",See http://www.jair.org/ for any accompanying files
A Divergence Critic for Inductive Proof,"Inductive theorem provers often diverge. This paper describes a simple
critic, a computer program which monitors the construction of inductive proofs
attempting to identify diverging proof attempts. Divergence is recognized by
means of a ``difference matching'' procedure. The critic then proposes lemmas
and generalizations which ``ripple'' these differences away so that the proof
can go through without divergence. The critic enables the theorem prover Spike
to prove many theorems completely automatically from the definitions alone.",See http://www.jair.org/ for any accompanying files
Practical Methods for Proving Termination of General Logic Programs,"Termination of logic programs with negated body atoms (here called general
logic programs) is an important topic. One reason is that many computational
mechanisms used to process negated atoms, like Clark's negation as failure and
Chan's constructive negation, are based on termination conditions. This paper
introduces a methodology for proving termination of general logic programs
w.r.t. the Prolog selection rule. The idea is to distinguish parts of the
program depending on whether or not their termination depends on the selection
rule. To this end, the notions of low-, weakly up-, and up-acceptable program
are introduced. We use these notions to develop a methodology for proving
termination of general logic programs, and show how interesting problems in
non-monotonic reasoning can be formalized and implemented by means of
terminating general logic programs.",See http://www.jair.org/ for any accompanying files
Iterative Optimization and Simplification of Hierarchical Clusterings,"Clustering is often used for discovering structure in data. Clustering
systems differ in the objective function used to evaluate clustering quality
and the control strategy used to search the space of clusterings. Ideally, the
search strategy should consistently construct clusterings of high quality, but
be computationally inexpensive as well. In general, we cannot have it both
ways, but we can partition the search so that a system inexpensively constructs
a `tentative' clustering for initial examination, followed by iterative
optimization, which continues to search in background for improved clusterings.
Given this motivation, we evaluate an inexpensive strategy for creating initial
clusterings, coupled with several control strategies for iterative
optimization, each of which repeatedly modifies an initial clustering in search
of a better one. One of these methods appears novel as an iterative
optimization strategy in clustering contexts. Once a clustering has been
constructed it is judged by analysts -- often according to task-specific
criteria. Several authors have abstracted these criteria and posited a generic
performance task akin to pattern completion, where the error rate over
completed patterns is used to `externally' judge clustering utility. Given this
performance task, we adapt resampling-based pruning strategies used by
supervised learning systems to the task of simplifying hierarchical
clusterings, thus promising to ease post-clustering analysis. Finally, we
propose a number of objective functions, based on attribute-selection measures
for decision-tree induction, that might perform well on the error rate and
simplicity dimensions.",See http://www.jair.org/ for any accompanying files
Further Experimental Evidence against the Utility of Occam's Razor,"This paper presents new experimental evidence against the utility of Occam's
razor. A~systematic procedure is presented for post-processing decision trees
produced by C4.5. This procedure was derived by rejecting Occam's razor and
instead attending to the assumption that similar objects are likely to belong
to the same class. It increases a decision tree's complexity without altering
the performance of that tree on the training data from which it is inferred.
The resulting more complex decision trees are demonstrated to have, on average,
for a variety of common learning tasks, higher predictive accuracy than the
less complex original decision trees. This result raises considerable doubt
about the utility of Occam's razor as it is commonly applied in modern machine
learning.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Least Generalizations and Greatest Specializations of Sets of Clauses,"The main operations in Inductive Logic Programming (ILP) are generalization
and specialization, which only make sense in a generality order. In ILP, the
three most important generality orders are subsumption, implication and
implication relative to background knowledge. The two languages used most often
are languages of clauses and languages of only Horn clauses. This gives a total
of six different ordered languages. In this paper, we give a systematic
treatment of the existence or non-existence of least generalizations and
greatest specializations of finite sets of clauses in each of these six ordered
sets. We survey results already obtained by others and also contribute some
answers of our own. Our main new results are, firstly, the existence of a
computable least generalization under implication of every finite set of
clauses containing at least one non-tautologous function-free clause (among
other, not necessarily function-free clauses). Secondly, we show that such a
least generalization need not exist under relative implication, not even if
both the set that is to be generalized and the background knowledge are
function-free. Thirdly, we give a complete discussion of existence and
non-existence of greatest specializations in each of the six ordered languages.",See http://www.jair.org/ for any accompanying files
Reinforcement Learning: A Survey,"This paper surveys the field of reinforcement learning from a
computer-science perspective. It is written to be accessible to researchers
familiar with machine learning. Both the historical basis of the field and a
broad selection of current work are summarized. Reinforcement learning is the
problem faced by an agent that learns behavior through trial-and-error
interactions with a dynamic environment. The work described here has a
resemblance to work in psychology, but differs considerably in the details and
in the use of the word ``reinforcement.'' The paper discusses central issues of
reinforcement learning, including trading off exploration and exploitation,
establishing the foundations of the field via Markov decision theory, learning
from delayed reinforcement, constructing empirical models to accelerate
learning, making use of generalization and hierarchy, and coping with hidden
state. It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning.",See http://www.jair.org/ for any accompanying files
"Adaptive Problem-solving for Large-scale Scheduling Problems: A Case
  Study","Although most scheduling problems are NP-hard, domain specific techniques
perform well in practice but are quite expensive to construct. In adaptive
problem-solving solving, domain specific knowledge is acquired automatically
for a general problem solver with a flexible control architecture. In this
approach, a learning system explores a space of possible heuristic methods for
one well-suited to the eccentricities of the given domain and problem
distribution. In this article, we discuss an application of the approach to
scheduling satellite communications. Using problem distributions based on
actual mission requirements, our approach identifies strategies that not only
decrease the amount of CPU time required to produce schedules, but also
increase the percentage of problems that are solvable within computational
resource limitations.",See http://www.jair.org/ for any accompanying files
A Formal Framework for Speedup Learning from Problems and Solutions,"Speedup learning seeks to improve the computational efficiency of problem
solving with experience. In this paper, we develop a formal framework for
learning efficient problem solving from random problems and their solutions. We
apply this framework to two different representations of learned knowledge,
namely control rules and macro-operators, and prove theorems that identify
sufficient conditions for learning in each representation. Our proofs are
constructive in that they are accompanied with learning algorithms. Our
framework captures both empirical and explanation-based speedup learning in a
unified fashion. We illustrate our framework with implementations in two
domains: symbolic integration and Eight Puzzle. This work integrates many
strands of experimental and theoretical work in machine learning, including
empirical learning of control rules, macro-operator learning, Explanation-Based
Learning (EBL), and Probably Approximately Correct (PAC) Learning.",See http://www.jair.org/ for any accompanying files
2Planning for Contingencies: A Decision-based Approach,"A fundamental assumption made by classical AI planners is that there is no
uncertainty in the world: the planner has full knowledge of the conditions
under which the plan will be executed and the outcome of every action is fully
predictable. These planners cannot therefore construct contingency plans, i.e.,
plans in which different actions are performed in different circumstances. In
this paper we discuss some issues that arise in the representation and
construction of contingency plans and describe Cassandra, a partial-order
contingency planner. Cassandra uses explicit decision-steps that enable the
agent executing the plan to decide which plan branch to follow. The
decision-steps in a plan result in subgoals to acquire knowledge, which are
planned for in the same way as any other subgoals. Cassandra thus distinguishes
the process of gathering information from the process of making decisions. The
explicit representation of decisions in Cassandra allows a coherent approach to
the problems of contingent planning, and provides a solid base for extensions
such as the use of different decision-making procedures.",See http://www.jair.org/ for any accompanying files
A Principled Approach Towards Symbolic Geometric Constraint Satisfaction,"An important problem in geometric reasoning is to find the configuration of a
collection of geometric bodies so as to satisfy a set of given constraints.
Recently, it has been suggested that this problem can be solved efficiently by
symbolically reasoning about geometry. This approach, called degrees of freedom
analysis, employs a set of specialized routines called plan fragments that
specify how to change the configuration of a set of bodies to satisfy a new
constraint while preserving existing constraints. A potential drawback, which
limits the scalability of this approach, is concerned with the difficulty of
writing plan fragments. In this paper we address this limitation by showing how
these plan fragments can be automatically synthesized using first principles
about geometric bodies, actions, and topology.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
On Partially Controlled Multi-Agent Systems,"Motivated by the control theoretic distinction between controllable and
uncontrollable events, we distinguish between two types of agents within a
multi-agent system: controllable agents, which are directly controlled by the
system's designer, and uncontrollable agents, which are not under the
designer's direct control. We refer to such systems as partially controlled
multi-agent systems, and we investigate how one might influence the behavior of
the uncontrolled agents through appropriate design of the controlled agents. In
particular, we wish to understand which problems are naturally described in
these terms, what methods can be applied to influence the uncontrollable
agents, the effectiveness of such methods, and whether similar methods work
across different domains. Using a game-theoretic framework, this paper studies
the design of partially controlled multi-agent systems in two contexts: in one
context, the uncontrollable agents are expected utility maximizers, while in
the other they are reinforcement learners. We suggest different techniques for
controlling agents' behavior in each domain, assess their success, and examine
their relationship.",See http://www.jair.org/ for any accompanying files
Spatial Aggregation: Theory and Applications,"Visual thinking plays an important role in scientific reasoning. Based on the
research in automating diverse reasoning tasks about dynamical systems,
nonlinear controllers, kinematic mechanisms, and fluid motion, we have
identified a style of visual thinking, imagistic reasoning. Imagistic reasoning
organizes computations around image-like, analogue representations so that
perceptual and symbolic operations can be brought to bear to infer structure
and behavior. Programs incorporating imagistic reasoning have been shown to
perform at an expert level in domains that defy current analytic or numerical
methods. We have developed a computational paradigm, spatial aggregation, to
unify the description of a class of imagistic problem solvers. A program
written in this paradigm has the following properties. It takes a continuous
field and optional objective functions as input, and produces high-level
descriptions of structure, behavior, or control actions. It computes a
multi-layer of intermediate representations, called spatial aggregates, by
forming equivalence classes and adjacency relations. It employs a small set of
generic operators such as aggregation, classification, and localization to
perform bidirectional mapping between the information-rich field and
successively more abstract spatial aggregates. It uses a data structure, the
neighborhood graph, as a common interface to modularize computations. To
illustrate our theory, we describe the computational structure of three
implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the
spatial aggregation generic operators by mixing and matching a library of
commonly used routines.",See http://www.jair.org/ for any accompanying files
A Hierarchy of Tractable Subsets for Computing Stable Models,"Finding the stable models of a knowledge base is a significant computational
problem in artificial intelligence. This task is at the computational heart of
truth maintenance systems, autoepistemic logic, and default logic.
Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes
of knowledge bases, Omega_1,Omega_2,..., with the following properties: first,
Omega_1 is the class of all stratified knowledge bases; second, if a knowledge
base Pi is in Omega_k, then Pi has at most k stable models, and all of them may
be found in time O(lnk), where l is the length of the knowledge base and n the
number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find
the minimum k such that Pi belongs to Omega_k in time polynomial in the size of
Pi; and, last, where K is the class of all knowledge bases, it is the case that
union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some
class in the hierarchy.",See http://www.jair.org/ for any accompanying files
"Accelerating Partial-Order Planners: Some Techniques for Effective
  Search Control and Pruning","We propose some domain-independent techniques for bringing well-founded
partial-order planners closer to practicality. The first two techniques are
aimed at improving search control while keeping overhead costs low. One is
based on a simple adjustment to the default A* heuristic used by UCPOP to
select plans for refinement. The other is based on preferring ``zero
commitment'' (forced) plan refinements whenever possible, and using LIFO
prioritization otherwise. A more radical technique is the use of operator
parameter domains to prune search. These domains are initially computed from
the definitions of the operators and the initial and goal conditions, using a
polynomial-time algorithm that propagates sets of constants through the
operator graph, starting in the initial conditions. During planning, parameter
domains can be used to prune nonviable operator instances and to remove
spurious clobbering threats. In experiments based on modifications of UCPOP,
our improved plan and goal selection strategies gave speedups by factors
ranging from 5 to more than 1000 for a variety of problems that are nontrivial
for the unmodified version. Crucially, the hardest problems gave the greatest
improvements. The pruning technique based on parameter domains often gave
speedups by an order of magnitude or more for difficult problems, both with the
default UCPOP search strategy and with our improved strategy. The Lisp code for
our techniques and for the test problems is provided in on-line appendices.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Cue Phrase Classification Using Machine Learning,"Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure, e.g., for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models, but also for improving upon previous
results. When compared to manually derived classification models already in the
literature, the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition, the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally, the ease of retraining makes the learning approach more scalable and
flexible than manual methods.",See http://www.jair.org/ for any accompanying files
Mechanisms for Automated Negotiation in State Oriented Domains,"This paper lays part of the groundwork for a domain theory of negotiation,
that is, a way of classifying interactions so that it is clear, given a domain,
which negotiation mechanisms and strategies are appropriate. We define State
Oriented Domains, a general category of interaction. Necessary and sufficient
conditions for cooperation are outlined. We use the notion of worth in an
altered definition of utility, thus enabling agreements in a wider class of
joint-goal reachable situations. An approach is offered for conflict
resolution, and it is shown that even in a conflict situation, partial
cooperative steps can be taken by interacting agents (that is, agents in
fundamental conflict might still agree to cooperate up to a certain point). A
Unified Negotiation Protocol (UNP) is developed that can be used in all types
of encounters. It is shown that in certain borderline cooperative situations, a
partial cooperative agreement (i.e., one that does not achieve all agents'
goals) might be preferred by all agents, even though there exists a rational
agreement that would achieve all their goals. Finally, we analyze cases where
agents have incomplete information on the goals and worth of other agents.
First we consider the case where agents' goals are private information, and we
analyze what goal declaration strategies the agents might adopt to increase
their utility. Then, we consider the situation where the agents' goals (and
therefore stand-alone costs) are common knowledge, but the worth they attach to
their goals is private information. We introduce two mechanisms, one 'strict',
the other 'tolerant', and analyze their affects on the stability and efficiency
of negotiation outcomes.",See http://www.jair.org/ for any accompanying files
Learning First-Order Definitions of Functions,"First-order learning involves finding a clause-form definition of a relation
from examples of the relation and relevant background information. In this
paper, a particular first-order learning system is modified to customize it for
finding definitions of functional relations. This restriction leads to faster
learning times and, in some cases, to definitions that have higher predictive
accuracy. Other first-order learning systems might benefit from similar
specialization.",See http://www.jair.org/ for any accompanying files
MUSE CSP: An Extension to the Constraint Satisfaction Problem,"This paper describes an extension to the constraint satisfaction problem
(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).
This extension is especially useful for those problems which segment into
multiple sets of partially shared variables. Such problems arise naturally in
signal processing applications including computer vision, speech processing,
and handwriting recognition. For these applications, it is often difficult to
segment the data in only one way given the low-level information utilized by
the segmentation algorithms. MUSE CSP can be used to compactly represent
several similar instances of the constraint satisfaction problem. If multiple
instances of a CSP have some common variables which have the same domains and
constraints, then they can be combined into a single instance of a MUSE CSP,
reducing the work required to apply the constraints. We introduce the concepts
of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We
then demonstrate how MUSE CSP can be used to compactly represent lexically
ambiguous sentences and the multiple sentence hypotheses that are often
generated by speech recognition algorithms so that grammar constraints can be
used to provide parses for all syntactically correct sentences. Algorithms for
MUSE arc and path consistency are provided. Finally, we discuss how to create a
MUSE CSP from a set of CSPs which are labeled to indicate when the same
variable is shared by more than a single CSP.",See http://www.jair.org/ for any accompanying files
Exploiting Causal Independence in Bayesian Network Inference,"A new method is proposed for exploiting causal independencies in exact
Bayesian network inference. A Bayesian network can be viewed as representing a
factorization of a joint probability into the multiplication of a set of
conditional probabilities. We present a notion of causal independence that
enables one to further factorize the conditional probabilities into a
combination of even smaller factors and consequently obtain a finer-grain
factorization of the joint probability. The new formulation of causal
independence lets us specify the conditional probability of a variable given
its parents in terms of an associative and commutative operator, such as
``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a
simple algorithm VE for Bayesian network inference that, given evidence and a
query variable, uses the factorization to find the posterior distribution of
the query. We show how this algorithm can be extended to exploit causal
independence. Empirical studies, based on the CPCS networks for medical
diagnosis, show that this method is more efficient than previous methods and
allows for inference in larger networks than previous algorithms.",See http://www.jair.org/ for any accompanying files
"Quantitative Results Comparing Three Intelligent Interfaces for
  Information Capture: A Case Study Adding Name Information into an Electronic
  Personal Organizer","Efficiently entering information into a computer is key to enjoying the
benefits of computing. This paper describes three intelligent user interfaces:
handwriting recognition, adaptive menus, and predictive fillin. In the context
of adding a personUs name and address to an electronic organizer, tests show
handwriting recognition is slower than typing on an on-screen, soft keyboard,
while adaptive menus and predictive fillin can be twice as fast. This paper
also presents strategies for applying these three interfaces to other
information collection domains.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Characterizations of Decomposable Dependency Models,"Decomposable dependency models possess a number of interesting and useful
properties. This paper presents new characterizations of decomposable models in
terms of independence relationships, which are obtained by adding a single
axiom to the well-known set characterizing dependency models that are
isomorphic to undirected graphs. We also briefly discuss a potential
application of our results to the problem of learning graphical models from
data.",See http://www.jair.org/ for any accompanying files
Improved Heterogeneous Distance Functions,"Instance-based learning techniques typically handle continuous and linear
input values well, but often do not handle nominal input attributes
appropriately. The Value Difference Metric (VDM) was designed to find
reasonable distance values between nominal attribute values, but it largely
ignores continuous attributes, requiring discretization to map continuous
values into nominal values. This paper proposes three new heterogeneous
distance functions, called the Heterogeneous Value Difference Metric (HVDM),
the Interpolated Value Difference Metric (IVDM), and the Windowed Value
Difference Metric (WVDM). These new distance functions are designed to handle
applications with nominal attributes, continuous attributes, or both. In
experiments on 48 applications the new distance metrics achieve higher
classification accuracy on average than three previous distance functions on
those datasets that have both nominal and continuous attributes.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis
  Using Artificial Neural Networks","Previous approaches of analyzing spontaneously spoken language often have
been based on encoding syntactic and semantic knowledge manually and
symbolically. While there has been some progress using statistical or
connectionist language models, many current spoken- language systems still use
a relatively brittle, hand-coded symbolic grammar or symbolic semantic
component. In contrast, we describe a so-called screening approach for learning
robust processing of spontaneously spoken language. A screening approach is a
flat analysis which uses shallow sequences of category representations for
analyzing an utterance at various syntactic, semantic and dialog levels. Rather
than using a deeply structured symbolic analysis, we use a flat connectionist
analysis. This screening approach aims at supporting speech and language
processing by using (1) data-driven learning and (2) robustness of
connectionist networks. In order to test this approach, we have developed the
SCREEN system which is based on this new robust, learned and flat analysis. In
this paper, we focus on a detailed description of SCREEN's architecture, the
flat syntactic and semantic analysis, the interaction with a speech recognizer,
and a detailed evaluation analysis of the robustness under the influence of
noisy or incomplete input. The main result of this paper is that flat
representations allow more robust processing of spontaneous spoken language
than deeply structured representations. In particular, we show how the
fault-tolerance and learning capability of connectionist networks can support a
flat analysis for providing more robust spoken-language processing within an
overall hybrid symbolic/connectionist framework.",See http://www.jair.org/ for any accompanying files
A Uniform Framework for Concept Definitions in Description Logics,"Most modern formalisms used in Databases and Artificial Intelligence for
describing an application domain are based on the notions of class (or concept)
and relationship among classes. One interesting feature of such formalisms is
the possibility of defining a class, i.e., providing a set of properties that
precisely characterize the instances of the class. Many recent articles point
out that there are several ways of assigning a meaning to a class definition
containing some sort of recursion. In this paper, we argue that, instead of
choosing a single style of semantics, we achieve better results by adopting a
formalism that allows for different semantics to coexist. We demonstrate the
feasibility of our argument, by presenting a knowledge representation
formalism, the description logic muALCQ, with the above characteristics. In
addition to the constructs for conjunction, disjunction, negation, quantifiers,
and qualified number restrictions, muALCQ includes special fixpoint constructs
to express (suitably interpreted) recursive definitions. These constructs
enable the usual frame-based descriptions to be combined with definitions of
recursive data structures such as directed acyclic graphs, lists, streams, etc.
We establish several properties of muALCQ, including the decidability and the
computational complexity of reasoning, by formulating a correspondence with a
particular modal logic of programs called the modal mu-calculus.",See http://www.jair.org/ for any accompanying files
Lifeworld Analysis,"We argue that the analysis of agent/environment interactions should be
extended to include the conventions and invariants maintained by agents
throughout their activity. We refer to this thicker notion of environment as a
lifeworld and present a partial set of formal tools for describing structures
of lifeworlds and the ways in which they computationally simplify activity. As
one specific example, we apply the tools to the analysis of the Toast system
and show how versions of the system with very different control structures in
fact implement a common control structure together with different conventions
for encoding task state in the positions or states of objects in the
environment.",See http://www.jair.org/ for any accompanying files
"Query DAGs: A Practical Paradigm for Implementing Belief-Network
  Inference","We describe a new paradigm for implementing inference in belief networks,
which consists of two steps: (1) compiling a belief network into an arithmetic
expression called a Query DAG (Q-DAG); and (2) answering queries using a simple
evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a
number, or a symbol for evidence. Each leaf node of a Q-DAG represents the
answer to a network query, that is, the probability of some event of interest.
It appears that Q-DAGs can be generated using any of the standard algorithms
for exact inference in belief networks (we show how they can be generated using
clustering and conditioning algorithms). The time and space complexity of a
Q-DAG generation algorithm is no worse than the time complexity of the
inference algorithm on which it is based. The complexity of a Q-DAG evaluation
algorithm is linear in the size of the Q-DAG, and such inference amounts to a
standard evaluation of the arithmetic expression it represents. The intended
value of Q-DAGs is in reducing the software and hardware resources required to
utilize belief networks in on-line, real-world applications. The proposed
framework also facilitates the development of on-line inference on different
software and hardware platforms due to the simplicity of the Q-DAG evaluation
algorithm. Interestingly enough, Q-DAGs were found to serve other purposes:
simple techniques for reducing Q-DAGs tend to subsume relatively complex
optimization techniques for belief-network inference, such as network-pruning
and computation-caching.",See http://www.jair.org/ for any accompanying files
"Connectionist Theory Refinement: Genetically Searching the Space of
  Network Topologies","An algorithm that learns from a set of examples should ideally be able to
exploit the available resources of (a) abundant computing power and (b)
domain-specific knowledge to improve its ability to generalize. Connectionist
theory-refinement systems, which use background knowledge to select a neural
network's topology and initial weights, have proven to be effective at
exploiting domain-specific knowledge; however, most do not exploit available
computing power. This weakness occurs because they lack the ability to refine
the topology of the neural networks they produce, thereby limiting
generalization, especially when given impoverished domain theories. We present
the REGENT algorithm which uses (a) domain-specific knowledge to help create an
initial population of knowledge-based neural networks and (b) genetic operators
of crossover and mutation (specifically designed for knowledge-based networks)
to continually search for better network topologies. Experiments on three
real-world domains indicate that our new algorithm is able to significantly
increase generalization compared to a standard connectionist theory-refinement
system, as well as our previous algorithm for growing knowledge-based networks.",See http://www.jair.org/ for any accompanying files
Flaw Selection Strategies for Partial-Order Planning,"Several recent studies have compared the relative efficiency of alternative
flaw selection strategies for partial-order causal link (POCL) planning. We
review this literature, and present new experimental results that generalize
the earlier work and explain some of the discrepancies in it. In particular, we
describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by
Joslin and Pollack (1994), and compare it with other strategies, including
Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very
different, and apparently conflicting claims about the most effective way to
reduce search-space size in POCL planning. We resolve this conflict, arguing
that much of the benefit that Gerevini and Schubert ascribe to the LIFO
component of their ZLIFO strategy is better attributed to other causes. We show
that for many problems, a strategy that combines least-cost flaw selection with
the delay of separable threats will be effective in reducing search-space size,
and will do so without excessive computational overhead. Although such a
strategy thus provides a good default, we also show that certain domain
characteristics may reduce its effectiveness.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
A Complete Classification of Tractability in RCC-5,"We investigate the computational properties of the spatial algebra RCC-5
which is a restricted version of the RCC framework for spatial reasoning. The
satisfiability problem for RCC-5 is known to be NP-complete but not much is
known about its approximately four billion subclasses. We provide a complete
classification of satisfiability for all these subclasses into polynomial and
NP-complete respectively. In the process, we identify all maximal tractable
subalgebras which are four in total.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search
  Difficulty","The easy-hard-easy pattern in the difficulty of combinatorial search problems
as constraints are added has been explained as due to a competition between the
decrease in number of solutions and increased pruning. We test the generality
of this explanation by examining one of its predictions: if the number of
solutions is held fixed by the choice of problems, then increased pruning
should lead to a monotonic decrease in search cost. Instead, we find the
easy-hard-easy pattern in median search cost even when the number of solutions
is held constant, for some search methods. This generalizes previous
observations of this pattern and shows that the existing theory does not
explain the full range of the peak in search cost. In these cases the pattern
appears to be due to changes in the size of the minimal unsolvable subproblems,
rather than changing numbers of solutions.",See http://www.jair.org/ for any accompanying files
Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time,"This paper combines two important directions of research in temporal
resoning: that of finding maximal tractable subclasses of Allen's interval
algebra, and that of reasoning with metric temporal information. Eight new
maximal tractable subclasses of Allen's interval algebra are presented, some of
them subsuming previously reported tractable algebras. The algebras allow for
metric temporal constraints on interval starting or ending points, using the
recent framework of Horn DLRs. Two of the algebras can express the notion of
sequentiality between intervals, being the first such algebras admitting both
qualitative and metric time.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"Defining Relative Likelihood in Partially-Ordered Preferential
  Structures","Starting with a likelihood or preference order on worlds, we extend it to a
likelihood ordering on sets of worlds in a natural way, and examine the
resulting logic. Lewis earlier considered such a notion of relative likelihood
in the context of studying counterfactuals, but he assumed a total preference
order on worlds. Complications arise when examining partial orders that are not
present for total orders. There are subtleties involving the exact approach to
lifting the order on worlds to an order on sets of worlds. In addition, the
axiomatization of the logic of relative likelihood in the case of partial
orders gives insight into the connection between relative likelihood and
default reasoning.",See http://www.jair.org/ for any accompanying files
Towards Flexible Teamwork,"Many AI researchers are today striving to build agent teams for complex,
dynamic multi-agent domains, with intended applications in arenas such as
education, training, entertainment, information integration, and collective
robotics. Unfortunately, uncertainties in these complex, dynamic domains
obstruct coherent teamwork. In particular, team members often encounter
differing, incomplete, and possibly inconsistent views of their environment.
Furthermore, team members can unexpectedly fail in fulfilling responsibilities
or discover unexpected opportunities. Highly flexible coordination and
communication is key in addressing such uncertainties. Simply fitting
individual agents with precomputed coordination plans will not do, for their
inflexibility can cause severe failures in teamwork, and their
domain-specificity hinders reusability. Our central hypothesis is that the key
to such flexibility and reusability is providing agents with general models of
teamwork. Agents exploit such models to autonomously reason about coordination
and communication, providing requisite flexibility. Furthermore, the models
enable reuse across domains, both saving implementation effort and enforcing
consistency. This article presents one general, implemented model of teamwork,
called STEAM. The basic building block of teamwork in STEAM is joint intentions
(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a
(partial) hierarchy of joint intentions (this hierarchy is seen to parallel
Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members
monitor the team's and individual members' performance, reorganizing the team
as necessary. Finally, decision-theoretic communication selectivity in STEAM
ensures reduction in communication overheads of teamwork, with appropriate
sensitivity to the environmental conditions. This article describes STEAM's
application in three different complex domains, and presents detailed empirical
results.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
Identifying Hierarchical Structure in Sequences: A linear-time algorithm,"SEQUITUR is an algorithm that infers a hierarchical structure from a sequence
of discrete symbols by replacing repeated phrases with a grammatical rule that
generates the phrase, and continuing this process recursively. The result is a
hierarchical representation of the original sequence, which offers insights
into its lexical structure. The algorithm is driven by two constraints that
reduce the size of the grammar, and produce structure as a by-product. SEQUITUR
breaks new ground by operating incrementally. Moreover, the method's simple
structure permits a proof that it operates in space and time that is linear in
the size of the input. Our implementation can process 50,000 symbols per second
and has been applied to an extensive range of real world sequences.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
"Storing and Indexing Plan Derivations through Explanation-based Analysis
  of Retrieval Failures","Case-Based Planning (CBP) provides a way of scaling up domain-independent
planning to solve large problems in complex domains. It replaces the detailed
and lengthy search for a solution with the retrieval and adaptation of previous
planning experiences. In general, CBP has been demonstrated to improve
performance over generative (from-scratch) planning. However, the performance
improvements it provides are dependent on adequate judgements as to problem
similarity. In particular, although CBP may substantially reduce planning
effort overall, it is subject to a mis-retrieval problem. The success of CBP
depends on these retrieval errors being relatively rare. This paper describes
the design and implementation of a replay framework for the case-based planner
DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating
explanation-based learning techniques that allow it to explain and learn from
the retrieval failures it encounters. These techniques are used to refine
judgements about case similarity in response to feedback when a wrong decision
has been made. The same failure analysis is used in building the case library,
through the addition of repairing cases. Large problems are split and stored as
single goal subproblems. Multi-goal problems are stored only when these smaller
cases fail to be merged into a full solution. An empirical evaluation of this
approach demonstrates the advantage of learning from experienced retrieval
failure.",See http://www.jair.org/ for any accompanying files
"A Model Approximation Scheme for Planning in Partially Observable
  Stochastic Domains","Partially observable Markov decision processes (POMDPs) are a natural model
for planning problems where effects of actions are nondeterministic and the
state of the world is not completely observable. It is difficult to solve
POMDPs exactly. This paper proposes a new approximation scheme. The basic idea
is to transform a POMDP into another one where additional information is
provided by an oracle. The oracle informs the planning agent that the current
state of the world is in a certain region. The transformed POMDP is
consequently said to be region observable. It is easier to solve than the
original POMDP. We propose to solve the transformed POMDP and use its optimal
policy to construct an approximate policy for the original POMDP. By
controlling the amount of additional information that the oracle provides, it
is possible to find a proper tradeoff between computational time and
approximation quality. In terms of algorithmic contributions, we study in
details how to exploit region observability in solving the transformed POMDP.
To facilitate the study, we also propose a new exact algorithm for general
POMDPs. The algorithm is conceptually simple and yet is significantly more
efficient than all previous exact algorithms.",See http://www.jair.org/ for any accompanying files
Dynamic Non-Bayesian Decision Making,"The model of a non-Bayesian agent who faces a repeated game with incomplete
information against Nature is an appropriate tool for modeling general
agent-environment interactions. In such a model the environment state
(controlled by Nature) may change arbitrarily, and the feedback/reward function
is initially unknown. The agent is not Bayesian, that is he does not form a
prior probability neither on the state selection strategy of Nature, nor on his
reward function. A policy for the agent is a function which assigns an action
to every history of observations and actions. Two basic feedback structures are
considered. In one of them -- the perfect monitoring case -- the agent is able
to observe the previous environment state as part of his feedback, while in the
other -- the imperfect monitoring case -- all that is available to the agent is
the reward obtained. Both of these settings refer to partially observable
processes, where the current environment state is unknown. Our main result
refers to the competitive ratio criterion in the perfect monitoring case. We
prove the existence of an efficient stochastic policy that ensures that the
competitive ratio is obtained at almost all stages with an arbitrarily high
probability, where efficiency is measured in terms of rate of convergence. It
is further shown that such an optimal policy does not exist in the imperfect
monitoring case. Moreover, it is proved that in the perfect monitoring case
there does not exist a deterministic policy that satisfies our long run
optimality criterion. In addition, we discuss the maxmin criterion and prove
that a deterministic efficient optimal strategy does exist in the imperfect
monitoring case under this criterion. Finally we show that our approach to
long-run optimality can be viewed as qualitative, which distinguishes it from
previous work in this area.",See http://www.jair.org/ for any accompanying files
When Gravity Fails: Local Search Topology,"Local search algorithms for combinatorial search problems frequently
encounter a sequence of states in which it is impossible to improve the value
of the objective function; moves through these regions, called plateau moves,
dominate the time spent in local search. We analyze and characterize plateaus
for three different classes of randomly generated Boolean Satisfiability
problems. We identify several interesting features of plateaus that impact the
performance of local search algorithms. We show that local minima tend to be
small but occasionally may be very large. We also show that local minima can be
escaped without unsatisfying a large number of clauses, but that systematically
searching for an escape route may be computationally expensive if the local
minimum is large. We show that plateaus with exits, called benches, tend to be
much larger than minima, and that some benches have very few exit states which
local search can use to escape. We show that the solutions (i.e., global
minima) of randomly generated problem instances form clusters, which behave
similarly to local minima. We revisit several enhancements of local search
algorithms and explain their performance in light of our results. Finally we
discuss strategies for creating the next generation of local search algorithms.",See http://www.jair.org/ for any accompanying files
Bidirectional Heuristic Search Reconsidered,"The assessment of bidirectional heuristic search has been incorrect since it
was first published more than a quarter of a century ago. For quite a long
time, this search strategy did not achieve the expected results, and there was
a major misunderstanding about the reasons behind it. Although there is still
wide-spread belief that bidirectional heuristic search is afflicted by the
problem of search frontiers passing each other, we demonstrate that this
conjecture is wrong. Based on this finding, we present both a new generic
approach to bidirectional heuristic search and a new approach to dynamically
improving heuristic values that is feasible in bidirectional search only. These
approaches are put into perspective with both the traditional and more recently
proposed approaches in order to facilitate a better overall understanding.
Empirical results of experiments with our new approaches show that
bidirectional heuristic search can be performed very efficiently and also with
limited memory. These results suggest that bidirectional heuristic search
appears to be better for solving certain difficult problems than corresponding
unidirectional search. This provides some evidence for the usefulness of a
search strategy that was long neglected. In summary, we show that bidirectional
heuristic search is viable and consequently propose that it be reconsidered.",See http://www.jair.org/ for any accompanying files
Incremental Recompilation of Knowledge,"Approximating a general formula from above and below by Horn formulas (its
Horn envelope and Horn core, respectively) was proposed by Selman and Kautz
(1991, 1996) as a form of ``knowledge compilation,'' supporting rapid
approximate reasoning; on the negative side, this scheme is static in that it
supports no updates, and has certain complexity drawbacks pointed out by
Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many
frameworks and schemes proposed in the literature for theory update and
revision are plagued by serious complexity-theoretic impediments, even in the
Horn case, as was pointed out by Eiter and Gottlob (1992), and is further
demonstrated in the present paper. More fundamentally, these schemes are not
inductive, in that they may lose in a single update any positive properties of
the represented sets of formulas (small size, Horn structure, etc.). In this
paper we propose a new scheme, incremental recompilation, which combines Horn
approximation and model-based updates; this scheme is inductive and very
efficient, free of the problems facing its constituents. A set of formulas is
represented by an upper and lower Horn approximation. To update, we replace the
upper Horn formula by the Horn envelope of its minimum-change update, and
similarly the lower one by the Horn core of its update; the key fact which
enables this scheme is that Horn envelopes and cores are easy to compute when
the underlying formula is the result of a minimum-change update of a Horn
formula by a clause. We conjecture that efficient algorithms are possible for
more complex updates.",See http://www.jair.org/ for any accompanying files
Monotonicity and Persistence in Preferential Logics,"An important characteristic of many logics for Artificial Intelligence is
their nonmonotonicity. This means that adding a formula to the premises can
invalidate some of the consequences. There may, however, exist formulae that
can always be safely added to the premises without destroying any of the
consequences: we say they respect monotonicity. Also, there may be formulae
that, when they are a consequence, can not be invalidated when adding any
formula to the premises: we call them conservative. We study these two classes
of formulae for preferential logics, and show that they are closely linked to
the formulae whose truth-value is preserved along the (preferential) ordering.
We will consider some preferential logics for illustration, and prove syntactic
characterization results for them. The results in this paper may improve the
efficiency of theorem provers for preferential logics.",See http://www.jair.org/ for any accompanying files
Synthesizing Customized Planners from Specifications,"Existing plan synthesis approaches in artificial intelligence fall into two
categories -- domain independent and domain dependent. The domain independent
approaches are applicable across a variety of domains, but may not be very
efficient in any one given domain. The domain dependent approaches need to be
(re)designed for each domain separately, but can be very efficient in the
domain for which they are designed. One enticing alternative to these
approaches is to automatically synthesize domain independent planners given the
knowledge about the domain and the theory of planning. In this paper, we
investigate the feasibility of using existing automated software synthesis
tools to support such synthesis. Specifically, we describe an architecture
called CLAY in which the Kestrel Interactive Development System (KIDS) is used
to derive a domain-customized planner through a semi-automatic combination of a
declarative theory of planning, and the declarative control knowledge specific
to a given domain, to semi-automatically combine them to derive
domain-customized planners. We discuss what it means to write a declarative
theory of planning and control knowledge for KIDS, and illustrate our approach
by generating a class of domain-specific planners using state space
refinements. Our experiments show that the synthesized planners can outperform
classical refinement planners (implemented as instantiations of UCP,
Kambhampati & Srivastava, 1995), using the same control knowledge. We will
contrast the costs and benefits of the synthesis approach with conventional
methods for customizing domain independent planners.",See http://www.jair.org/ for any accompanying files
"Cached Sufficient Statistics for Efficient Machine Learning with Large
  Datasets","This paper introduces new algorithms and data structures for quick counting
for machine learning datasets. We focus on the counting task of constructing
contingency tables, but our approach is also applicable to counting the number
of records in a dataset that match conjunctive queries. Subject to certain
assumptions, the costs of these operations can be shown to be independent of
the number of records in the dataset and loglinear in the number of non-zero
entries in the contingency table. We provide a very sparse data structure, the
ADtree, to minimize memory use. We provide analytical worst-case bounds for
this structure for several models of data distribution. We empirically
demonstrate that tractably-sized data structures can be produced for large
real-world datasets by (a) using a sparse tree structure that never allocates
memory for counts of zero, (b) never allocating memory for counts that can be
deduced from other counts, and (c) not bothering to expand the tree fully near
its leaves. We show how the ADtree can be used to accelerate Bayes net
structure finding algorithms, rule learning algorithms, and feature selection
algorithms, and we provide a number of empirical results comparing ADtree
methods against traditional direct counting approaches. We also discuss the
possible uses of ADtrees in other machine learning methods, and discuss the
merits of ADtrees in comparison with alternative representations such as
kd-trees, R-trees and Frequent Sets.",See http://www.jair.org/ for any accompanying files
Tractability of Theory Patching,"In this paper we consider the problem of `theory patching', in which we are
given a domain theory, some of whose components are indicated to be possibly
flawed, and a set of labeled training examples for the domain concept. The
theory patching problem is to revise only the indicated components of the
theory, such that the resulting theory correctly classifies all the training
examples. Theory patching is thus a type of theory revision in which revisions
are made to individual components of the theory. Our concern in this paper is
to determine for which classes of logical domain theories the theory patching
problem is tractable. We consider both propositional and first-order domain
theories, and show that the theory patching problem is equivalent to that of
determining what information contained in a theory is `stable' regardless of
what revisions might be performed to the theory. We show that determining
stability is tractable if the input theory satisfies two conditions: that
revisions to each theory component have monotonic effects on the classification
of examples, and that theory components act independently in the classification
of examples in the theory. We also show how the concepts introduced can be used
to determine the soundness and completeness of particular theory patching
algorithms.",See http://www.jair.org/ for any accompanying files
Integrative Windowing,"In this paper we re-investigate windowing for rule learning algorithms. We
show that, contrary to previous results for decision tree learning, windowing
can in fact achieve significant run-time gains in noise-free domains and
explain the different behavior of rule learning algorithms by the fact that
they learn each rule independently. The main contribution of this paper is
integrative windowing, a new type of algorithm that further exploits this
property by integrating good rules into the final theory right after they have
been discovered. Thus it avoids re-learning these rules in subsequent
iterations of the windowing process. Experimental evidence in a variety of
noise-free domains shows that integrative windowing can in fact achieve
substantial run-time gains. Furthermore, we discuss the problem of noise in
windowing and present an algorithm that is able to achieve run-time gains in a
set of experiments in a simple domain with artificial noise.",See http://www.jair.org/ for any accompanying files
Model-Based Diagnosis using Structured System Descriptions,"This paper presents a comprehensive approach for model-based diagnosis which
includes proposals for characterizing and computing preferred diagnoses,
assuming that the system description is augmented with a system structure (a
directed graph explicating the interconnections between system components).
Specifically, we first introduce the notion of a consequence, which is a
syntactically unconstrained propositional sentence that characterizes all
consistency-based diagnoses and show that standard characterizations of
diagnoses, such as minimal conflicts, correspond to syntactic variations on a
consequence. Second, we propose a new syntactic variation on the consequence
known as negation normal form (NNF) and discuss its merits compared to standard
variations. Third, we introduce a basic algorithm for computing consequences in
NNF given a structured system description. We show that if the system structure
does not contain cycles, then there is always a linear-size consequence in NNF
which can be computed in linear time. For arbitrary system structures, we show
a precise connection between the complexity of computing consequences and the
topology of the underlying system structure. Finally, we present an algorithm
that enumerates the preferred diagnoses characterized by a consequence. The
algorithm is shown to take linear time in the size of the consequence if the
preference criterion satisfies some general conditions.",See http://www.jair.org/ for any accompanying files
"A Selective Macro-learning Algorithm and its Application to the NxN
  Sliding-Tile Puzzle","One of the most common mechanisms used for speeding up problem solvers is
macro-learning. Macros are sequences of basic operators acquired during problem
solving. Macros are used by the problem solver as if they were basic operators.
The major problem that macro-learning presents is the vast number of macros
that are available for acquisition. Macros increase the branching factor of the
search space and can severely degrade problem-solving efficiency. To make macro
learning useful, a program must be selective in acquiring and utilizing macros.
This paper describes a general method for selective acquisition of macros.
Solvable training problems are generated in increasing order of difficulty. The
only macros acquired are those that take the problem solver out of a local
minimum to a better state. The utility of the method is demonstrated in several
domains, including the domain of NxN sliding-tile puzzles. After learning on
small puzzles, the system is able to efficiently solve puzzles of any size.","See http://www.jair.org/ for an online appendix and other files
  accompanying this article"
The Computational Complexity of Probabilistic Planning,"We examine the computational complexity of testing and finding small plans in
probabilistic planning domains with both flat and propositional
representations. The complexity of plan evaluation and existence varies with
the plan type sought; we examine totally ordered plans, acyclic plans, and
looping plans, and partially ordered plans under three natural definitions of
plan value. We show that problems of interest are complete for a variety of
complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the
process of proving that certain planning problems are complete for NP^PP, we
introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the
standard Boolean satisfiability problem to computations involving probabilistic
quantities; our results suggest that the development of good heuristics for
E-MAJSAT could be important for the creation of efficient algorithms for a wide
variety of problems.",See http://www.jair.org/ for any accompanying files
SYNERGY: A Linear Planner Based on Genetic Programming,"In this paper we describe SYNERGY, which is a highly parallelizable, linear
planning system that is based on the genetic programming paradigm. Rather than
reasoning about the world it is planning for, SYNERGY uses artificial
selection, recombination and fitness measure to generate linear plans that
solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase
problem and a few variants of the robot navigation problem), and the
experimental results show that our planner is capable of handling problem
instances that are one to two orders of magnitude larger than the ones solved
by UCPOP. In order to facilitate the search reduction and to enhance the
expressive power of SYNERGY, we also propose two major extensions to our
planning system: a formalism for using hierarchical planning operators, and a
framework for planning in dynamic environments.","13 pages, European Conference on Planning 1997"
The Essence of Constraint Propagation,"We show that several constraint propagation algorithms (also called (local)
consistency, consistency enforcing, Waltz, filtering or narrowing algorithms)
are instances of algorithms that deal with chaotic iteration. To this end we
propose a simple abstract framework that allows us to classify and compare
these algorithms and to establish in a uniform way their basic properties.","To appear in Theoretical Computer Science in the special issue
  devoted to the 24th ICALP conference (Bologna 1997)"
Towards a computational theory of human daydreaming,"This paper examines the phenomenon of daydreaming: spontaneously recalling or
imagining personal or vicarious experiences in the past or future. The
following important roles of daydreaming in human cognition are postulated:
plan preparation and rehearsal, learning from failures and successes, support
for processes of creativity, emotion regulation, and motivation.
  A computational theory of daydreaming and its implementation as the program
DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based
on relaxed planning, 2) a dynamic episodic memory of experiences used by the
scenario generator, 3) a collection of personal goals and control goals which
guide the scenario generator, 4) an emotion component in which daydreams
initiate, and are initiated by, emotional states arising from goal outcomes,
and 5) domain knowledge of interpersonal relations and common everyday
occurrences.
  The role of emotions and control goals in daydreaming is discussed. Four
control goals commonly used in guiding daydreaming are presented:
rationalization, failure/success reversal, revenge, and preparation. The role
of episodic memory in daydreaming is considered, including how daydreamed
information is incorporated into memory and later used. An initial version of
DAYDREAMER which produces several daydreams (in English) is currently running.","10 pages. Appears in: Proceedings of the Seventh Annual Conference of
  the Cognitive Science Society (pp. 120-129). Irvine, CA. 1985"
"A reusable iterative optimization software library to solve
  combinatorial problems with approximate reasoning","Real world combinatorial optimization problems such as scheduling are
typically too complex to solve with exact methods. Additionally, the problems
often have to observe vaguely specified constraints of different importance,
the available data may be uncertain, and compromises between antagonistic
criteria may be necessary. We present a combination of approximate reasoning
based constraints and iterative optimization based heuristics that help to
model and solve such problems in a framework of C++ software libraries called
StarFLIP++. While initially developed to schedule continuous caster units in
steel plants, we present in this paper results from reusing the library
components in a shift scheduling system for the workforce of an industrial
production plant.","33 pages, 9 figures; for a project overview see
  http://www.dbai.tuwien.ac.at/proj/StarFLIP/"
"Modeling Belief in Dynamic Systems, Part II: Revision and Update","The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change, belief revision and belief
update, have been studied in detail. In a companion paper (Friedman & Halpern,
1997), we introduce a new framework to model belief change. This framework
combines temporal and epistemic modalities with a notion of plausibility,
allowing us to examine the change of beliefs over time. In this paper, we show
how belief revision and belief update can be captured in our framework. This
allows us to compare the assumptions made by each method, and to better
understand the principles underlying them. In particular, it shows that Katsuno
and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on
several strong assumptions that may limit its applicability in artificial
intelligence. Finally, our analysis allow us to identify a notion of minimal
change that underlies a broad range of belief change operations including
revision and update.",See http://www.jair.org/ for other files accompanying this article
The Symbol Grounding Problem,"How can the semantic interpretation of a formal symbol system be made
intrinsic to the system, rather than just parasitic on the meanings in our
heads? How can the meanings of the meaningless symbol tokens, manipulated
solely on the basis of their (arbitrary) shapes, be grounded in anything but
other meaningless symbols? The problem is analogous to trying to learn Chinese
from a Chinese/Chinese dictionary alone. A candidate solution is sketched:
Symbolic representations must be grounded bottom-up in nonsymbolic
representations of two kinds: (1) ""iconic representations,"" which are analogs
of the proximal sensory projections of distal objects and events, and (2)
""categorical representations,"" which are learned and innate feature-detectors
that pick out the invariant features of object and event categories from their
sensory projections. Elementary symbols are the names of these object and event
categories, assigned on the basis of their (nonsymbolic) categorical
representations. Higher-order (3) ""symbolic representations,"" grounded in these
elementary symbols, consist of symbol strings describing category membership
relations (e.g., ""An X is a Y that is Z"").",N/A
Iterative Deepening Branch and Bound,"In tree search problem the best-first search algorithm needs too much of
space . To remove such drawbacks of these algorithms the IDA* was developed
which is both space and time cost efficient. But again IDA* can give an optimal
solution for real valued problems like Flow shop scheduling, Travelling
Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further
modifications are done on it and the Iterative Deepening Branch and Bound
Search Algorithms is developed which meets the requirements. We have tried
using this algorithm for the Flow Shop Scheduling Problem and have found that
it is quite effective.","39 html pages + 4 gif files (fig1,fig1(a),fig2,fig3)"
Probabilistic Agent Programs,"Agents are small programs that autonomously take actions based on changes in
their environment or ``state.'' Over the last few years, there have been an
increasing number of efforts to build agents that can interact and/or
collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd
Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top
of legacy code. However, their framework assumes that agent states are
completely determined, and there is no uncertainty in an agent's state. Thus,
their framework allows an agent developer to specify how his agents will react
when the agent is 100% sure about what is true/false in the world state. In
this paper, we propose the concept of a \emph{probabilistic agent program} and
show how, given an arbitrary program written in any imperative language, we may
build a declarative ``probabilistic'' agent program on top of it which supports
decision making in the presence of uncertainty. We provide two alternative
semantics for probabilistic agent programs. We show that the second semantics,
though more epistemically appealing, is more complex to compute. We provide
sound and complete algorithms to compute the semantics of \emph{positive} agent
programs.","44 pages, 1 figure, Appendix"
Cox's Theorem Revisited,"The assumptions needed to prove Cox's Theorem are discussed and examined.
Various sets of assumptions under which a Cox-style theorem can be proved are
provided, although all are rather strong and, arguably, not natural.",Changed running head from original submission
Uniform semantic treatment of default and autoepistemic logics,"We revisit the issue of connections between two leading formalisms in
nonmonotonic reasoning: autoepistemic logic and default logic. For each logic
we develop a comprehensive semantic framework based on the notion of a belief
pair. The set of all belief pairs together with the so called knowledge
ordering forms a complete lattice. For each logic, we introduce several
semantics by means of fixpoints of operators on the lattice of belief pairs.
Our results elucidate an underlying isomorphism of the respective semantic
constructions. In particular, we show that the interpretation of defaults as
modal formulas proposed by Konolige allows us to represent all semantics for
default logic in terms of the corresponding semantics for autoepistemic logic.
Thus, our results conclusively establish that default logic can indeed be
viewed as a fragment of autoepistemic logic. However, as we also demonstrate,
the semantics of Moore and Reiter are given by different operators and occupy
different locations in their corresponding families of semantics. This result
explains the source of the longstanding difficulty to formally relate these two
semantics. In the paper, we also discuss approximating skeptical reasoning with
autoepistemic and default logics and establish constructive principles behind
such approximations.","Proceedings of the Seventh International Conference on Principles of
  Knowledge Representation and Reasoning (KR2000); 11 pages"
On the accuracy and running time of GSAT,"Randomized algorithms for deciding satisfiability were shown to be effective
in solving problems with thousands of variables. However, these algorithms are
not complete. That is, they provide no guarantee that a satisfying assignment,
if one exists, will be found. Thus, when studying randomized algorithms, there
are two important characteristics that need to be considered: the running time
and, even more importantly, the accuracy --- a measure of likelihood that a
satisfying assignment will be found, provided one exists. In fact, we argue
that without a reference to the accuracy, the notion of the running time for
randomized algorithms is not well-defined. In this paper, we introduce a formal
notion of accuracy. We use it to define a concept of the running time. We use
both notions to study the random walk strategy GSAT algorithm. We investigate
the dependence of accuracy on properties of input formulas such as
clause-to-variable ratio and the number of satisfying assignments. We
demonstrate that the running time of GSAT grows exponentially in the number of
variables of the input formula for randomly generated 3-CNF formulas and for
the formulas encoding 3- and 4-colorability of graphs.","Proceedings of the 9th Portuguese Conference on Artificial
  Intelligence (EPIA'99), Lecture Notes in Artificial Intelligence, vol. 1695,
  Springer-Verlag, 1999"
"Syntactic Autonomy: Why There is no Autonomy without Symbols and How
  Self-Organization Might Evolve Them","Two different types of agency are discussed based on dynamically coherent and
incoherent couplings with an environment respectively. I propose that until a
private syntax (syntactic autonomy) is discovered by dynamically coherent
agents, there are no significant or interesting types of closure or autonomy.
When syntactic autonomy is established, then, because of a process of
description-based selected self-organization, open-ended evolution is enabled.
At this stage, agents depend, in addition to dynamics, on localized, symbolic
memory, thus adding a level of dynamical incoherence to their interaction with
the environment. Furthermore, it is the appearance of syntactic autonomy which
enables much more interesting types of closures amongst agents which share the
same syntax. To investigate how we can study the emergence of syntax from
dynamical systems, experiments with cellular automata leading to emergent
computation to solve non-trivial tasks are discussed. RNA editing is also
mentioned as a process that may have been used to obtain a primordial
biological code necessary open-ended evolution.",N/A
"Consistency Management of Normal Logic Program by Top-down Abductive
  Proof Procedure","This paper presents a method of computing a revision of a function-free
normal logic program. If an added rule is inconsistent with a program, that is,
if it leads to a situation such that no stable model exists for a new program,
then deletion and addition of rules are performed to avoid inconsistency. We
specify a revision by translating a normal logic program into an abductive
logic program with abducibles to represent deletion and addition of rules. To
compute such deletion and addition, we propose an adaptation of our top-down
abductive proof procedure to compute a relevant abducibles to an added rule. We
compute a minimally revised program, by choosing a minimal set of abducibles
among all the sets of abducibles computed by a top-down proof procedure.",N/A
Defeasible Reasoning in OSCAR,This is a system description for the OSCAR defeasible reasoner.,"Nonmonotonic Reasoning Workshop, 2000"
"Abductive and Consistency-Based Diagnosis Revisited: a Modeling
  Perspective","Diagnostic reasoning has been characterized logically as consistency-based
reasoning or abductive reasoning. Previous analyses in the literature have
shown, on the one hand, that choosing the (in general more restrictive)
abductive definition may be appropriate or not, depending on the content of the
knowledge base [Console&Torasso91], and, on the other hand, that, depending on
the choice of the definition the same knowledge should be expressed in
different form [Poole94].
  Since in Model-Based Diagnosis a major problem is finding the right way of
abstracting the behavior of the system to be modeled, this paper discusses the
relation between modeling, and in particular abstraction in the model, and the
notion of diagnosis.","5 pages, 8th Int. Workshop on Nonmonotonic Reasoning, 2000"
ACLP: Integrating Abduction and Constraint Solving,"ACLP is a system which combines abductive reasoning and constraint solving by
integrating the frameworks of Abductive Logic Programming (ALP) and Constraint
Logic Programming (CLP). It forms a general high-level knowledge representation
environment for abductive problems in Artificial Intelligence and other areas.
In ACLP, the task of abduction is supported and enhanced by its non-trivial
integration with constraint solving facilitating its application to complex
problems. The ACLP system is currently implemented on top of the CLP language
of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver
for finite domains. It has been applied to the problems of planning and
scheduling in order to test its computational effectiveness compared with the
direct use of the (lower level) constraint solving framework of CLP on which it
is built. These experiments provide evidence that the abductive framework of
ACLP does not compromise significantly the computational efficiency of the
solutions. Other experiments show the natural ability of ACLP to accommodate
easily and in a robust way new or changing requirements of the original
problem.",6 pages
Relevance Sensitive Non-Monotonic Inference on Belief Sequences,"We present a method for relevance sensitive non-monotonic inference from
belief sequences which incorporates insights pertaining to prioritized
inference and relevance sensitive, inconsistency tolerant belief revision.
  Our model uses a finite, logically open sequence of propositional formulas as
a representation for beliefs and defines a notion of inference from
maxiconsistent subsets of formulas guided by two orderings: a temporal
sequencing and an ordering based on relevance relations between the conclusion
and formulas in the sequence. The relevance relations are ternary (using
context as a parameter) as opposed to standard binary axiomatizations. The
inference operation thus defined easily handles iterated revision by
maintaining a revision history, blocks the derivation of inconsistent answers
from a possibly inconsistent sequence and maintains the distinction between
explicit and implicit beliefs. In doing so, it provides a finitely presented
formalism and a plausible model of reasoning for automated agents.",N/A
Probabilistic Default Reasoning with Conditional Constraints,"We propose a combination of probabilistic reasoning from conditional
constraints with approaches to default reasoning from conditional knowledge
bases. In detail, we generalize the notions of Pearl's entailment in system Z,
Lehmann's lexicographic entailment, and Geffner's conditional entailment to
conditional constraints. We give some examples that show that the new notions
of z-, lexicographic, and conditional entailment have similar properties like
their classical counterparts. Moreover, we show that the new notions of z-,
lexicographic, and conditional entailment are proper generalizations of both
their classical counterparts and the classical notion of logical entailment for
conditional constraints.","8 pages; to appear in Proceedings of the Eighth International
  Workshop on Nonmonotonic Reasoning, Special Session on Uncertainty Frameworks
  in Nonmonotonic Reasoning, Breckenridge, Colorado, USA, 9-11 April 2000"
A Compiler for Ordered Logic Programs,"This paper describes a system, called PLP, for compiling ordered logic
programs into standard logic programs under the answer set semantics. In an
ordered logic program, rules are named by unique terms, and preferences among
rules are given by a set of dedicated atoms. An ordered logic program is
transformed into a second, regular, extended logic program wherein the
preferences are respected, in that the answer sets obtained in the transformed
theory correspond with the preferred answer sets of the original theory. Since
the result of the translation is an extended logic program, existing logic
programming systems can be used as underlying reasoning engine. In particular,
PLP is conceived as a front-end to the logic programming systems dlv and
smodels.",N/A
SLDNFA-system,"The SLDNFA-system results from the LP+ project at the K.U.Leuven, which
investigates logics and proof procedures for these logics for declarative
knowledge representation. Within this project inductive definition logic
(ID-logic) is used as representation logic. Different solvers are being
developed for this logic and one of these is SLDNFA. A prototype of the system
is available and used for investigating how to solve efficiently problems
represented in ID-logic.","6 pages conference:NMR2000, special track on System descriptions and
  demonstration"
Logic Programs with Compiled Preferences,"We describe an approach for compiling preferences into logic programs under
the answer set semantics. An ordered logic program is an extended logic program
in which rules are named by unique terms, and in which preferences among rules
are given by a set of dedicated atoms. An ordered logic program is transformed
into a second, regular, extended logic program wherein the preferences are
respected, in that the answer sets obtained in the transformed theory
correspond with the preferred answer sets of the original theory. Our approach
allows both the specification of static orderings (as found in most previous
work), in which preferences are external to a logic program, as well as
orderings on sets of rules. In large part then, we are interested in describing
a general methodology for uniformly incorporating preference information in a
logic program. Since the result of our translation is an extended logic
program, we can make use of existing implementations, such as dlv and smodels.
To this end, we have developed a compiler, available on the web, as a front-end
for these programming systems.",N/A
Fuzzy Approaches to Abductive Inference,"This paper proposes two kinds of fuzzy abductive inference in the framework
of fuzzy rule base. The abductive inference processes described here depend on
the semantic of the rule. We distinguish two classes of interpretation of a
fuzzy rule, certainty generation rules and possible generation rules. In this
paper we present the architecture of abductive inference in the first class of
interpretation. We give two kinds of problem that we can resolve by using the
proposed models of inference.",7 pages and 8 files
Problem solving in ID-logic with aggregates: some experiments,"The goal of the LP+ project at the K.U.Leuven is to design an expressive
logic, suitable for declarative knowledge representation, and to develop
intelligent systems based on Logic Programming technology for solving
computational problems using the declarative specifications. The ID-logic is an
integration of typed classical logic and a definition logic. Different
abductive solvers for this language are being developed. This paper is a report
of the integration of high order aggregates into ID-logic and the consequences
on the solver SLDNFA.","9 pages conference: NMR2000, special track on abductive reasoning"
Optimal Belief Revision,"We propose a new approach to belief revision that provides a way to change
knowledge bases with a minimum of effort. We call this way of revising belief
states optimal belief revision. Our revision method gives special attention to
the fact that most belief revision processes are directed to a specific
informational objective. This approach to belief change is founded on notions
such as optimal context and accessibility. For the sentential model of belief
states we provide both a formal description of contexts as sub-theories
determined by three parameters and a method to construct contexts. Next, we
introduce an accessibility ordering for belief sets, which we then use for
selecting the best (optimal) contexts with respect to the processing effort
involved in the revision. Then, for finitely axiomatizable knowledge bases, we
characterize a finite accessibility ranking from which the accessibility
ordering for the entire base is generated and show how to determine the ranking
of an arbitrary sentence in the language. Finally, we define the adjustment of
the accessibility ranking of a revised base of a belief set.",NMR'2000 Workshop 6 pages
cc-Golog: Towards More Realistic Logic-Based Robot Controllers,"High-level robot controllers in realistic domains typically deal with
processes which operate concurrently, change the world continuously, and where
the execution of actions is event-driven as in ``charge the batteries as soon
as the voltage level is low''. While non-logic-based robot control languages
are well suited to express such scenarios, they fare poorly when it comes to
projecting, in a conspicuous way, how the world evolves when actions are
executed. On the other hand, a logic-based control language like \congolog,
based on the situation calculus, is well-suited for the latter. However, it has
problems expressing event-driven behavior. In this paper, we show how these
problems can be overcome by first extending the situation calculus to support
continuous change and event-driven behavior and then presenting \ccgolog, a
variant of \congolog which is based on the extended situation calculus. One
benefit of \ccgolog is that it narrows the gap in expressiveness compared to
non-logic-based control languages while preserving a semantically well-founded
projection mechanism.",N/A
Smodels: A System for Answer Set Programming,"The Smodels system implements the stable model semantics for normal logic
programs. It handles a subclass of programs which contain no function symbols
and are domain-restricted but supports extensions including built-in functions
as well as cardinality and weight constraints. On top of this core engine more
involved systems can be built. As an example, we have implemented total and
partial stable model computation for disjunctive logic programs. An interesting
application method is based on answer set programming, i.e., encoding an
application problem as a set of rules so that its solutions are captured by the
stable models of the rules. Smodels has been applied to a number of areas
including planning, model checking, reachability analysis, product
configuration, dynamic constraint satisfaction, and feature interaction.","Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado 4 pages, uses aaai.sty"
"E-RES: A System for Reasoning about Actions, Events and Observations","E-RES is a system that implements the Language E, a logic for reasoning about
narratives of action occurrences and observations. E's semantics is
model-theoretic, but this implementation is based on a sound and complete
reformulation of E in terms of argumentation, and uses general computational
techniques of argumentation frameworks. The system derives sceptical
non-monotonic consequences of a given reformulated theory which exactly
correspond to consequences entailed by E's model-theory. The computation relies
on a complimentary ability of the system to derive credulous non-monotonic
consequences together with a set of supporting assumptions which is sufficient
for the (credulous) conclusion to hold. E-RES allows theories to contain
general action laws, statements about action occurrences, observations and
statements of ramifications (or universal laws). It is able to derive
consequences both forward and backward in time. This paper gives a short
overview of the theoretical basis of E-RES and illustrates its use on a variety
of examples. Currently, E-RES is being extended so that the system can be used
for planning.","Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado. 6 pages"
QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks,"In this paper, we outline the prototype of an automated inference tool,
called QUIP, which provides a uniform implementation for several nonmonotonic
reasoning formalisms. The theoretical basis of QUIP is derived from well-known
results about the computational complexity of nonmonotonic logics and exploits
a representation of the different reasoning tasks in terms of quantified
boolean formulae.",N/A
A Splitting Set Theorem for Epistemic Specifications,"Over the past decade a considerable amount of research has been done to
expand logic programming languages to handle incomplete information. One such
language is the language of epistemic specifications. As is usual with logic
programming languages, the problem of answering queries is intractable in the
general case. For extended disjunctive logic programs, an idea that has proven
useful in simplifying the investigation of answer sets is the use of splitting
sets. In this paper we will present an extended definition of splitting sets
that will be applicable to epistemic specifications. Furthermore, an extension
of the splitting set theorem will be presented. Also, a characterization of
stratified epistemic specifications will be given in terms of splitting sets.
This characterization leads us to an algorithmic method of computing world
views of a subclass of epistemic logic programs.",To be published in Proceedings of NMR 2000 Workshop. 6 pages
DES: a Challenge Problem for Nonmonotonic Reasoning Systems,"The US Data Encryption Standard, DES for short, is put forward as an
interesting benchmark problem for nonmonotonic reasoning systems because (i) it
provides a set of test cases of industrial relevance which shares features of
randomly generated problems and real-world problems, (ii) the representation of
DES using normal logic programs with the stable model semantics is simple and
easy to understand, and (iii) this subclass of logic programs can be seen as an
interesting special case for many other formalizations of nonmonotonic
reasoning. In this paper we present two encodings of DES as logic programs: a
direct one out of the standard specifications and an optimized one extending
the work of Massacci and Marraro. The computational properties of the encodings
are studied by using them for DES key search with the Smodels system as the
implementation of the stable model semantics. Results indicate that the
encodings and Smodels are quite competitive: they outperform state-of-the-art
SAT-checkers working with an optimized encoding of DES into SAT and are
comparable with a SAT-checker that is customized and tuned for the optimized
SAT encoding.","10 pages, 1 Postscript figure, uses aaai.sty and graphicx.sty"
Fages' Theorem and Answer Set Programming,"We generalize a theorem by Francois Fages that describes the relationship
between the completion semantics and the answer set semantics for logic
programs with negation as failure. The study of this relationship is important
in connection with the emergence of answer set programming. Whenever the two
semantics are equivalent, answer sets can be computed by a satisfiability
solver, and the use of answer set solvers such as smodels and dlv is
unnecessary. A logic programming representation of the blocks world due to
Ilkka Niemelae is discussed as an example.",N/A
"On the tractable counting of theory models and its application to belief
  revision and truth maintenance","We introduced decomposable negation normal form (DNNF) recently as a
tractable form of propositional theories, and provided a number of powerful
logical operations that can be performed on it in polynomial time. We also
presented an algorithm for compiling any conjunctive normal form (CNF) into
DNNF and provided a structure-based guarantee on its space and time complexity.
We present in this paper a linear-time algorithm for converting an ordered
binary decision diagram (OBDD) representation of a propositional theory into an
equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a
subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the
previous complexity guarantees on compiling DNNF continue to hold for this
stricter subclass, which has stronger properties. In particular, we present a
new operation on d-DNNF which allows us to count its models under the
assertion, retraction and flipping of every literal by traversing the d-DNNF
twice. That is, after such traversal, we can test in constant-time: the
entailment of any literal by the d-DNNF, and the consistency of the d-DNNF
under the retraction or flipping of any literal. We demonstrate the
significance of these new operations by showing how they allow us to implement
linear-time, complete truth maintenance systems and linear-time, complete
belief revision systems for two important classes of propositional theories.",N/A
BDD-based reasoning in the fluent calculus - first results,"The paper reports on first preliminary results and insights gained in a
project aiming at implementing the fluent calculus using methods and techniques
based on binary decision diagrams. After reporting on an initial experiment
showing promising results we discuss our findings concerning various techniques
and heuristics used to speed up the reasoning process.",9 pages; Workshop on Nonmonotonic Reasoning 2000 (NMR 2000)
Planning with Incomplete Information,"Planning is a natural domain of application for frameworks of reasoning about
actions and change. In this paper we study how one such framework, the Language
E, can form the basis for planning under (possibly) incomplete information. We
define two types of plans: weak and safe plans, and propose a planner, called
the E-Planner, which is often able to extend an initial weak plan into a safe
plan even though the (explicit) information available is incomplete, e.g. for
cases where the initial state is not completely known. The E-Planner is based
upon a reformulation of the Language E in argumentation terms and a natural
proof theory resulting from the reformulation. It uses an extension of this
proof theory by means of abduction for the generation of plans and adopts
argumentation-based techniques for extending weak plans into safe plans. We
provide representative examples illustrating the behaviour of the E-Planner, in
particular for cases where the status of fluents is incompletely known.","Proceedings of the 8th International Workshop on Non-Monotonic
  Reasoning, April 9-11, 2000, Breckenridge, Colorado"
Local Diagnosis,"In an earlier work, we have presented operations of belief change which only
affect the relevant part of a belief base. In this paper, we propose the
application of the same strategy to the problem of model-based diangosis. We
first isolate the subset of the system description which is relevant for a
given observation and then solve the diagnosis problem for this subset.",N/A
A Consistency-Based Model for Belief Change: Preliminary Report,"We present a general, consistency-based framework for belief change.
Informally, in revising K by A, we begin with A and incorporate as much of K as
consistently possible. Formally, a knowledge base K and sentence A are
expressed, via renaming propositions in K, in separate languages. Using a
maximization process, we assume the languages are the same insofar as
consistently possible. Lastly, we express the resultant knowledge base in a
single language. There may be more than one way in which A can be so extended
by K: in choice revision, one such ``extension'' represents the revised state;
alternately revision consists of the intersection of all such extensions.
  The most general formulation of our approach is flexible enough to express
other approaches to revision and update, the merging of knowledge bases, and
the incorporation of static and dynamic integrity constraints. Our framework
differs from work based on ordinal conditional functions, notably with respect
to iterated revision. We argue that the approach is well-suited for
implementation: the choice revision operator gives better complexity results
than general revision; the approach can be expressed in terms of a finite
knowledge base; and the scope of a revision can be restricted to just those
propositions mentioned in the sentence for revision A.",N/A
SATEN: An Object-Oriented Web-Based Revision and Extraction Engine,"SATEN is an object-oriented web-based extraction and belief revision engine.
It runs on any computer via a Java 1.1 enabled browser such as Netscape 4.
SATEN performs belief revision based on the AGM approach. The extraction and
belief revision reasoning engines operate on a user specified ranking of
information. One of the features of SATEN is that it can be used to integrate
mutually inconsistent commensuate rankings into a consistent ranking.","The implementation of SATEN can be found at
  http://cafe.newcastle.edu.au/saten"
dcs: An Implementation of DATALOG with Constraints,"Answer-set programming (ASP) has emerged recently as a viable programming
paradigm. We describe here an ASP system, DATALOG with constraints or DC, based
on non-monotonic logic. Informally, DC theories consist of propositional
clauses (constraints) and of Horn rules. The semantics is a simple and natural
extension of the semantics of the propositional logic. However, thanks to the
presence of Horn rules in the system, modeling of transitive closure becomes
straightforward. We describe the syntax, use and implementation of DC and
provide experimental results.","6 pages (AAAI format), 4 ps figures; System descriptions and
  demonstration Session, 8th Intl. Workshop on Non-Monotonic Reasoning"
DATALOG with constraints - an answer-set programming system,"Answer-set programming (ASP) has emerged recently as a viable programming
paradigm well attuned to search problems in AI, constraint satisfaction and
combinatorics. Propositional logic is, arguably, the simplest ASP system with
an intuitive semantics supporting direct modeling of problem constraints.
However, for some applications, especially those requiring that transitive
closure be computed, it requires additional variables and results in large
theories. Consequently, it may not be a practical computational tool for such
problems. On the other hand, ASP systems based on nonmonotonic logics, such as
stable logic programming, can handle transitive closure computation efficiently
and, in general, yield very concise theories as problem representations. Their
semantics is, however, more complex. Searching for the middle ground, in this
paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC.
Informally, DC theories consist of propositional clauses (constraints) and of
Horn rules. The semantics is a simple and natural extension of the semantics of
the propositional logic. However, thanks to the presence of Horn rules in the
system, modeling of transitive closure becomes straightforward. We describe the
syntax and semantics of DC, and study its properties. We discuss an
implementation of DC and present results of experimental study of the
effectiveness of DC, comparing it with CSAT, a satisfiability checker and
SMODELS implementation of stable logic programming. Our results show that DC is
competitive with the other two approaches, in case of many search problems,
often yielding much more efficient solutions.","6 pages, 5 figures, will appear in Proceedings of AAAI-2000"
Some Remarks on Boolean Constraint Propagation,"We study here the well-known propagation rules for Boolean constraints. First
we propose a simple notion of completeness for sets of such rules and establish
a completeness result. Then we show an equivalence in an appropriate sense
between Boolean constraint propagation and unit propagation, a form of
resolution for propositional logic.
  Subsequently we characterize one set of such rules by means of the notion of
hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify
the status of a similar, though different, set of rules introduced in (Simonis
1989a) and more fully in (Codognet and Diaz 1996).","14 pages. To appear in: New Trends in Constraints, Papers from the
  Joint ERCIM/Compulog-Net Workshop Cyprus, October 25-27, 1999.
  Springer-Verlag Lecture Notes in Artificial Intelligence"
Conditional Plausibility Measures and Bayesian Networks,"A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks.",N/A
"Constraint compiling into rules formalism constraint compiling into
  rules formalism for dynamic CSPs computing","In this paper we present a rule based formalism for filtering variables
domains of constraints. This formalism is well adapted for solving dynamic CSP.
We take diagnosis as an instance problem to illustrate the use of these rules.
A diagnosis problem is seen like finding all the minimal sets of constraints to
be relaxed in the constraint network that models the device to be diagnosed",14 pages
Brainstorm/J: a Java Framework for Intelligent Agents,"Despite the effort of many researchers in the area of multi-agent systems
(MAS) for designing and programming agents, a few years ago the research
community began to take into account that common features among different MAS
exists. Based on these common features, several tools have tackled the problem
of agent development on specific application domains or specific types of
agents. As a consequence, their scope is restricted to a subset of the huge
application domain of MAS. In this paper we propose a generic infrastructure
for programming agents whose name is Brainstorm/J. The infrastructure has been
implemented as an object oriented framework. As a consequence, our approach
supports a broader scope of MAS applications than previous efforts, being
flexible and reusable.","15 pages. To be published in Proceedings of the Second Argentinian
  Symposium on Artificial Intelligence (ASAI'2000 - 29th JAIIO). September
  2000. Tandil, Buenos Aires, Argentina. See
  http://www.exa.unicen.edu.ar/~azunino"
On the relationship between fuzzy logic and four-valued relevance logic,"In fuzzy propositional logic, to a proposition a partial truth in [0,1] is
assigned. It is well known that under certain circumstances, fuzzy logic
collapses to classical logic. In this paper, we will show that under dual
conditions, fuzzy logic collapses to four-valued (relevance) logic, where
propositions have truth-value true, false, unknown, or contradiction. As a
consequence, fuzzy entailment may be considered as ``in between'' four-valued
(relevance) entailment and classical entailment.",N/A
"Causes and Explanations: A Structural-Model Approach, Part I: Causes","We propose a new definition of actual cause, using structural equations to
model counterfactuals. We show that the definition yields a plausible and
elegant account of causation that handles well examples which have caused
problems for other definitions and resolves major difficulties in the
traditional account.","Part II of the paper (on Explanation) is also on the arxiv.
  Previously the two parts were submitted as one paper. To appear in the
  British Journal for the Philosophy of Science"
"Logic Programming Approaches for Representing and Solving Constraint
  Satisfaction Problems: A Comparison","Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there is constraint logic
programming which computes a solution as an answer substitution to a query
containing the variables of the constraint satisfaction problem. On the other
hand there are systems based on stable model semantics, abductive systems, and
first order logic model generators which compute solutions as models of some
theory. This paper compares these different approaches from the point of view
of knowledge representation (how declarative are the programs) and from the
point of view of performance (how good are they at solving typical problems).","15 pages, 3 eps-figures"
Multi-Channel Parallel Adaptation Theory for Rule Discovery,"In this paper, we introduce a new machine learning theory based on
multi-channel parallel adaptation for rule discovery. This theory is
distinguished from the familiar parallel-distributed adaptation theory of
neural networks in terms of channel-based convergence to the target rules. We
show how to realize this theory in a learning system named CFRule. CFRule is a
parallel weight-based model, but it departs from traditional neural computing
in that its internal knowledge is comprehensible. Furthermore, when the model
converges upon training, each channel converges to a target rule. The model
adaptation rule is derived by multi-level parallel weight optimization based on
gradient descent. Since, however, gradient descent only guarantees local
optimization, a multi-channel regression-based optimization strategy is
developed to effectively deal with this problem. Formally, we prove that the
CFRule model can explicitly and precisely encode any given rule set. Also, we
prove a property related to asynchronous parallel convergence, which is a
critical element of the multi-channel parallel adaptation theory for rule
learning. Thanks to the quantizability nature of the CFRule model, rules can be
extracted completely and soundly via a threshold-based mechanism. Finally, the
practical application of the theory is demonstrated in DNA promoter recognition
and hepatitis prognosis prediction.","21 pages, 1 figure, 7 tables"
A Constraint-Driven System for Contract Assembly,"We present an approach for modelling the structure and coarse content of
legal documents with a view to providing automated support for the drafting of
contracts and contract database retrieval. The approach is designed to be
applicable where contract drafting is based on model-form contracts or on
existing examples of a similar type. The main features of the approach are: (1)
the representation addresses the structure and the interrelationships between
the constituent parts of contracts, but not the text of the document itself;
(2) the representation of documents is separated from the mechanisms that
manipulate it; and (3) the drafting process is subject to a collection of
explicitly stated constraints that govern the structure of the documents. We
describe the representation of document instances and of 'generic documents',
which are data structures used to drive the creation of new document instances,
and we show extracts from a sample session to illustrate the features of a
prototype system implemented in MacProlog.",N/A
Modelling Contractual Arguments,"One influential approach to assessing the ""goodness"" of arguments is offered
by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can
be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an
approach that originates in discourse analysis. In p-d terms an argument is
good if it avoids committing a fallacy, whereas in RST terms an argument is
good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for
providing only a partially functional account of argument, and similar
criticisms have been raised in the Natural Language Generation (NLG)
community-particularly by Moore & Pollack (1992)- with regards to its account
of intentionality in text in general. Mann and Thompson themselves note that
although RST can be successfully applied to a wide range of texts from diverse
domains, it fails to characterise some types of text, most notably legal
contracts. There is ongoing research in the Artificial Intelligence and Law
community exploring the potential for providing electronic support to contract
negotiators, focusing on long-term, complex engineering agreements (see for
example Daskalopulu & Sergot 1997). This paper provides a brief introduction to
RST and illustrates its shortcomings with respect to contractual text. An
alternative approach for modelling argument structure is presented which not
only caters for contractual text, but also overcomes the aforementioned
limitations of RST.",N/A
Information Integration and Computational Logic,"Information Integration is a young and exciting field with enormous research
and commercial significance in the new world of the Information Society. It
stands at the crossroad of Databases and Artificial Intelligence requiring
novel techniques that bring together different methods from these fields.
Information from disparate heterogeneous sources often with no a-priori common
schema needs to be synthesized in a flexible, transparent and intelligent way
in order to respond to the demands of a query thus enabling a more informed
decision by the user or application program. The field although relatively
young has already found many practical applications particularly for
integrating information over the World Wide Web. This paper gives a brief
introduction of the field highlighting some of the main current and future
research issues and application areas. It attempts to evaluate the current and
potential role of Computational Logic in this and suggests some of the problems
where logic-based techniques could be used.",53 Pages
Enhancing Constraint Propagation with Composition Operators,"Constraint propagation is a general algorithmic approach for pruning the
search space of a CSP. In a uniform way, K. R. Apt has defined a computation as
an iteration of reduction functions over a domain. He has also demonstrated the
need for integrating static properties of reduction functions (commutativity
and semi-commutativity) to design specialized algorithms such as AC3 and DAC.
We introduce here a set of operators for modeling compositions of reduction
functions. Two of the major goals are to tackle parallel computations, and
dynamic behaviours (such as slow convergence).",14 pages
On Properties of Update Sequences Based on Causal Rejection,"We consider an approach to update nonmonotonic knowledge bases represented as
extended logic programs under answer set semantics. New information is
incorporated into the current knowledge base subject to a causal rejection
principle enforcing that, in case of conflicts, more recent rules are preferred
and older rules are overridden. Such a rejection principle is also exploited in
other approaches to update logic programs, e.g., in dynamic logic programming
by Alferes et al. We give a thorough analysis of properties of our approach, to
get a better understanding of the causal rejection principle. We review
postulates for update and revision operators from the area of theory change and
nonmonotonic reasoning, and some new properties are considered as well. We then
consider refinements of our semantics which incorporate a notion of minimality
of change. As well, we investigate the relationship to other approaches,
showing that our approach is semantically equivalent to inheritance programs by
Buccafurri et al. and that it coincides with certain classes of dynamic logic
programs, for which we provide characterizations in terms of graph conditions.
Therefore, most of our results about properties of causal rejection principle
apply to these approaches as well. Finally, we deal with computational
complexity of our approach, and outline how the update semantics and its
refinements can be implemented on top of existing logic programming engines.","59 pages, 2 figures, 3 tables, to be published in ""Theory and
  Practice of Logic Programming"""
Gradient-based Reinforcement Planning in Policy-Search Methods,"We introduce a learning method called ``gradient-based reinforcement
planning'' (GREP). Unlike traditional DP methods that improve their policy
backwards in time, GREP is a gradient-based method that plans ahead and
improves its policy before it actually acts in the environment. We derive
formulas for the exact policy gradient that maximizes the expected future
reward and confirm our ideas with numerical experiments.","This is an extended version of the paper presented at the EWRL 2001
  in Utrecht (The Netherlands)"
Rational Competitive Analysis,"Much work in computer science has adopted competitive analysis as a tool for
decision making under uncertainty. In this work we extend competitive analysis
to the context of multi-agent systems. Unlike classical competitive analysis
where the behavior of an agent's environment is taken to be arbitrary, we
consider the case where an agent's environment consists of other agents. These
agents will usually obey some (minimal) rationality constraints. This leads to
the definition of rational competitive analysis. We introduce the concept of
rational competitive analysis, and initiate the study of competitive analysis
for multi-agent systems. We also discuss the application of rational
competitive analysis to the context of bidding games, as well as to the
classical one-way trading problem.",N/A
A theory of experiment,"This article aims at clarifying the language and practice of scientific
experiment, mainly by hooking observability on calculability.","19 pages LaTeX article (uses some pstricks) thorough revision 2 see
  also http://pierre.albarede.free.fr"
"Nonmonotonic Reasoning, Preferential Models and Cumulative Logics","Many systems that exhibit nonmonotonic behavior have been described and
studied already in the literature. The general notion of nonmonotonic
reasoning, though, has almost always been described only negatively, by the
property it does not enjoy, i.e. monotonicity. We study here general patterns
of nonmonotonic reasoning and try to isolate properties that could help us map
the field of nonmonotonic reasoning by reference to positive properties. We
concentrate on a number of families of nonmonotonic consequence relations,
defined in the style of Gentzen. Both proof-theoretic and semantic points of
view are developed in parallel. The former point of view was pioneered by D.
Gabbay, while the latter has been advocated by Y. Shoham in. Five such families
are defined and characterized by representation theorems, relating the two
points of view. One of the families of interest, that of preferential
relations, turns out to have been studied by E. Adams. The ""preferential""
models proposed here are a much stronger tool than Adams' probabilistic
semantics. The basic language used in this paper is that of propositional
logic. The extension of our results to first order predicate calculi and the
study of the computational complexity of the decision problems described in
this paper will be treated in another paper.","Presented at JELIA, June 1988. Some misprints in the Journal paper
  have been corrected"
What does a conditional knowledge base entail?,"This paper presents a logical approach to nonmonotonic reasoning based on the
notion of a nonmonotonic consequence relation. A conditional knowledge base,
consisting of a set of conditional assertions of the type ""if ... then ..."",
represents the explicit defeasible knowledge an agent has about the way the
world generally behaves. We look for a plausible definition of the set of all
conditional assertions entailed by a conditional knowledge base. In a previous
paper, S. Kraus and the authors defined and studied ""preferential"" consequence
relations. They noticed that not all preferential relations could be considered
as reasonable inference procedures. This paper studies a more restricted class
of consequence relations, ""rational"" relations. It is argued that any
reasonable nonmonotonic inference procedure should define a rational relation.
It is shown that the rational relations are exactly those that may be
represented by a ""ranked"" preferential model, or by a (non-standard)
probabilistic model. The rational closure of a conditional knowledge base is
defined and shown to provide an attractive answer to the question of the title.
Global properties of this closure operation are proved: it is a cumulative
operation. It is also computationally tractable. This paper assumes the
underlying language is propositional.","Preliminary version presented at KR'89. Minor corrections of the
  Journal Version"
A note on Darwiche and Pearl,"It is shown that Darwiche and Pearl's postulates imply an interesting
property, not noticed by the authors.",A small unpublished remark on a paper by Darwiche and Pearl
Distance Semantics for Belief Revision,"A vast and interesting family of natural semantics for belief revision is
defined. Suppose one is given a distance d between any two models. One may then
define the revision of a theory K by a formula a as the theory defined by the
set of all those models of a that are closest, by d, to the set of models of K.
This family is characterized by a set of rationality postulates that extends
the AGM postulates. The new postulates describe properties of iterated
revisions.",Preliminary version presented at TARK '96
Preferred History Semantics for Iterated Updates,"We give a semantics to iterated update by a preference relation on possible
developments. An iterated update is a sequence of formulas, giving (incomplete)
information about successive states of the world. A development is a sequence
of models, describing a possible trajectory through time. We assume a principle
of inertia and prefer those developments, which are compatible with the
information, and avoid unnecessary changes. The logical properties of the
updates defined in this way are considered, and a representation result is
proved.",N/A
Nonmonotonic inference operations,"A. Tarski proposed the study of infinitary consequence operations as the
central topic of mathematical logic. He considered monotonicity to be a
property of all such operations. In this paper, we weaken the monotonicity
requirement and consider more general operations, inference operations. These
operations describe the nonmonotonic logics both humans and machines seem to be
using when infering defeasible information from incomplete knowledge. We single
out a number of interesting families of inference operations. This study of
infinitary inference operations is inspired by the results of Kraus, Lehmann
and Magidor on finitary nonmonotonic operations, but this paper is
self-contained.","54 pages. A short version appeared in Studia Logica, Vol. 53 no. 2
  (1994) pp. 161-201"
The logical meaning of Expansion,"The Expansion property considered by researchers in Social Choice is shown to
correspond to a logical property of nonmonotonic consequence relations that is
the {\em pure}, i.e., not involving connectives, version of a previously known
weak rationality condition. The assumption that the union of two definable sets
of models is definable is needed for the soundness part of the result.",9 pages. Unpublished
Another perspective on Default Reasoning,"The lexicographic closure of any given finite set D of normal defaults is
defined. A conditional assertion ""if a then b"" is in this lexicographic closure
if, given the defaults D and the fact a, one would conclude b. The
lexicographic closure is essentially a rational extension of D, and of its
rational closure, defined in a previous paper. It provides a logic of normal
defaults that is different from the one proposed by R. Reiter and that is rich
enough not to require the consideration of non-normal defaults. A large number
of examples are provided to show that the lexicographic closure corresponds to
the basic intuitions behind Reiter's logic of defaults.","Presented at Workshop on Logical Formalizations of Commense Sense,
  Austin (Texas), January 1993"
Deductive Nonmonotonic Inference Operations: Antitonic Representations,"We provide a characterization of those nonmonotonic inference operations C
for which C(X) may be described as the set of all logical consequences of X
together with some set of additional assumptions S(X) that depends
anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset
of S(X)). The operations represented are exactly characterized in terms of
properties most of which have been studied in Freund-Lehmann(cs.AI/0202031).
Similar characterizations of right-absorbing and cumulative operations are also
provided. For cumulative operations, our results fit in closely with those of
Freund. We then discuss extending finitary operations to infinitary operations
in a canonical way and discuss co-compactness properties. Our results provide a
satisfactory notion of pseudo-compactness, generalizing to deductive
nonmonotonic operations the notion of compactness for monotonic operations.
They also provide an alternative, more elegant and more general, proof of the
existence of an infinitary deductive extension for any finitary deductive
operation (Theorem 7.9 of Freund-Lehmann).",N/A
Stereotypical Reasoning: Logical Properties,"Stereotypical reasoning assumes that the situation at hand is one of a kind
and that it enjoys the properties generally associated with that kind of
situation. It is one of the most basic forms of nonmonotonic reasoning. A
formal model for stereotypical reasoning is proposed and the logical properties
of this form of reasoning are studied. Stereotypical reasoning is shown to be
cumulative under weak assumptions.","Presented at Fourth Workshop on Logic, Language, Information and
  Computation, Fortaleza (Brasil), August 1997"
A Framework for Compiling Preferences in Logic Programs,"We introduce a methodology and framework for expressing general preference
information in logic programming under the answer set semantics. An ordered
logic program is an extended logic program in which rules are named by unique
terms, and in which preferences among rules are given by a set of atoms of form
s < t where s and t are names. An ordered logic program is transformed into a
second, regular, extended logic program wherein the preferences are respected,
in that the answer sets obtained in the transformed program correspond with the
preferred answer sets of the original program. Our approach allows the
specification of dynamic orderings, in which preferences can appear arbitrarily
within a program. Static orderings (in which preferences are external to a
logic program) are a trivial restriction of the general dynamic case. First, we
develop a specific approach to reasoning with preferences, wherein the
preference ordering specifies the order in which rules are to be applied. We
then demonstrate the wide range of applicability of our framework by showing
how other approaches, among them that of Brewka and Eiter, can be captured
within our framework. Since the result of each of these transformations is an
extended logic program, we can make use of existing implementations, such as
dlv and smodels. To this end, we have developed a publicly available compiler
as a front-end for these programming systems.",To appear in Theory and Practice of Logic Programming
Two results for proiritized logic programming,"Prioritized default reasoning has illustrated its rich expressiveness and
flexibility in knowledge representation and reasoning. However, many important
aspects of prioritized default reasoning have yet to be thoroughly explored. In
this paper, we investigate two properties of prioritized logic programs in the
context of answer set semantics. Specifically, we reveal a close relationship
between mutual defeasibility and uniqueness of the answer set for a prioritized
logic program. We then explore how the splitting technique for extended logic
programs can be extended to prioritized logic programs. We prove splitting
theorems that can be used to simplify the evaluation of a prioritized logic
program under certain conditions.","20 pages, to be appeared in journal Theory and Practice of Logic
  Programming"
Belief Revision and Rational Inference,"The (extended) AGM postulates for belief revision seem to deal with the
revision of a given theory K by an arbitrary formula, but not to constrain the
revisions of two different theories by the same formula. A new postulate is
proposed and compared with other similar postulates that have been proposed in
the literature. The AGM revisions that satisfy this new postulate stand in
one-to-one correspondence with the rational, consistency-preserving relations.
This correspondence is described explicitly. Two viewpoints on iterative
revisions are distinguished and discussed.",25 pages
Ultimate approximations in nonmonotonic knowledge representation systems,"We study fixpoints of operators on lattices. To this end we introduce the
notion of an approximation of an operator. We order approximations by means of
a precision ordering. We show that each lattice operator O has a unique most
precise or ultimate approximation. We demonstrate that fixpoints of this
ultimate approximation provide useful insights into fixpoints of the operator
O.
  We apply our theory to logic programming and introduce the ultimate
Kripke-Kleene, well-founded and stable semantics. We show that the ultimate
Kripke-Kleene and well-founded semantics are more precise then their standard
counterparts We argue that ultimate semantics for logic programming have
attractive epistemological properties and that, while in general they are
computationally more complex than the standard semantics, for many classes of
theories, their complexity is no worse.","This paper was published in Principles of Knowledge Representation
  and Reasoning, Proceedings of the Eighth International Conference (KR2002)"
Handling Defeasibilities in Action Domains,"Representing defeasibility is an important issue in common sense reasoning.
In reasoning about action and change, this issue becomes more difficult because
domain and action related defeasible information may conflict with general
inertia rules. Furthermore, different types of defeasible information may also
interfere with each other during the reasoning. In this paper, we develop a
prioritized logic programming approach to handle defeasibilities in reasoning
about action. In particular, we propose three action languages {\cal AT}^{0},
{\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in
action domains named defeasible constraints, defeasible observations and
actions with defeasible and abnormal effects respectively. Each language with a
higher superscript can be viewed as an extension of the language with a lower
superscript. These action languages inherit the simple syntax of {\cal A}
language but their semantics is developed in terms of transition systems where
transition functions are defined based on prioritized logic programs. By
illustrating various examples, we show that our approach eventually provides a
powerful mechanism to handle various defeasibilities in temporal prediction and
postdiction. We also investigate semantic properties of these three action
languages and characterize classes of action domains that present more
desirable solutions in reasoning about action within the underlying action
languages.","49 pages, 1 figure, to be appeared in journal Theory and Practice
  Logic Programming"
Anticipatory Guidance of Plot,"An anticipatory system for guiding plot development in interactive narratives
is described. The executable model is a finite automaton that provides the
implemented system with a look-ahead. The identification of undesirable future
states in the model is used to guide the player, in a transparent manner. In
this way, too radical twists of the plot can be avoided. Since the player
participates in the development of the plot, such guidance can have many forms,
depending on the environment of the player, on the behavior of the other
players, and on the means of player interaction. We present a design method for
interactive narratives which produces designs suitable for the implementation
of anticipatory mechanisms. Use of the method is illustrated by application to
our interactive computer game Kaktus.","19 pages, 5 figures"
"Abduction, ASP and Open Logic Programs","Open logic programs and open entailment have been recently proposed as an
abstract framework for the verification of incomplete specifications based upon
normal logic programs and the stable model semantics. There are obvious
analogies between open predicates and abducible predicates. However, despite
superficial similarities, there are features of open programs that have no
immediate counterpart in the framework of abduction and viceversa. Similarly,
open programs cannot be immediately simulated with answer set programming
(ASP). In this paper we start a thorough investigation of the relationships
between open inference, abduction and ASP. We shall prove that open programs
generalize the other two frameworks. The generalized framework suggests
interesting extensions of abduction under the generalized stable model
semantics. In some cases, we will be able to reduce open inference to abduction
and ASP, thereby estimating its computational complexity. At the same time, the
aforementioned reduction opens the way to new applications of abduction and
ASP.","7 pages, NMR'02 Workshop"
Domain-Dependent Knowledge in Answer Set Planning,"In this paper we consider three different kinds of domain-dependent control
knowledge (temporal, procedural and HTN-based) that are useful in planning. Our
approach is declarative and relies on the language of logic programming with
answer set semantics (AnsProlog*). AnsProlog* is designed to plan without
control knowledge. We show how temporal, procedural and HTN-based control
knowledge can be incorporated into AnsProlog* by the modular addition of a
small number of domain-dependent rules, without the need to modify the planner.
We formally prove the correctness of our planner, both in the absence and
presence of the control knowledge. Finally, we perform some initial
experimentation that demonstrates the potential reduction in planning time that
can be achieved when procedural domain knowledge is used to solve planning
problems with large plan length.","70 pages, accepted for publication, TOCL Version with all proofs"
"""Minimal defence"": a refinement of the preferred semantics for
  argumentation frameworks","Dung's abstract framework for argumentation enables a study of the
interactions between arguments based solely on an ``attack'' binary relation on
the set of arguments. Various ways to solve conflicts between contradictory
pieces of information have been proposed in the context of argumentation,
nonmonotonic reasoning or logic programming, and can be captured by appropriate
semantics within Dung's framework. A common feature of these semantics is that
one can always maximize in some sense the set of acceptable arguments. We
propose in this paper to extend Dung's framework in order to allow for the
representation of what we call ``restricted'' arguments: these arguments should
only be used if absolutely necessary, that is, in order to support other
arguments that would otherwise be defeated. We modify Dung's preferred
semantics accordingly: a set of arguments becomes acceptable only if it
contains a minimum of restricted arguments, for a maximum of unrestricted
arguments.","8 pages, 3 figures"
Two Representations for Iterative Non-prioritized Change,"We address a general representation problem for belief change, and describe
two interrelated representations for iterative non-prioritized change: a
logical representation in terms of persistent epistemic states, and a
constructive representation in terms of flocks of bases.","7 pages,Proceedings NMR'02, references added"
Collective Argumentation,"An extension of an abstract argumentation framework, called collective
argumentation, is introduced in which the attack relation is defined directly
among sets of arguments. The extension turns out to be suitable, in particular,
for representing semantics of disjunctive logic programs. Two special kinds of
collective argumentation are considered in which the opponents can share their
arguments.","8 pages, Proceedings NMR'02, references added"
Logic Programming with Ordered Disjunction,"Logic programs with ordered disjunction (LPODs) combine ideas underlying
Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming.
Logic programming under answer set semantics is extended with a new connective
called ordered disjunction. The new connective allows us to represent
alternative, ranked options for problem solutions in the heads of rules: A
\times B intuitively means: if possible A, but if A is not possible then at
least B. The semantics of logic programs with ordered disjunction is based on a
preference relation on answer sets. LPODs are useful for applications in design
and configuration and can serve as a basis for qualitative decision making.",N/A
Compilation of Propositional Weighted Bases,"In this paper, we investigate the extent to which knowledge compilation can
be used to improve inference from propositional weighted bases. We present a
general notion of compilation of a weighted base that is parametrized by any
equivalence--preserving compilation function. Both negative and positive
results are presented. On the one hand, complexity results are identified,
showing that the inference problem from a compiled weighted base is as
difficult as in the general case, when the prime implicates, Horn cover or
renamable Horn cover classes are targeted. On the other hand, we show that the
inference problem becomes tractable whenever DNNF-compilations are used and
clausal queries are considered. Moreover, we show that the set of all preferred
models of a DNNF-compilation of a weighted base can be computed in time
polynomial in the output size. Finally, we sketch how our results can be used
in model-based diagnosis in order to compute the most probable diagnoses of a
system.","Proceedings of the Ninth International Workshop on Non-Monotonic
  Reasoning (NMR'02), Toulouse, 2002 (6-14)"
Modeling Complex Domains of Actions and Change,"This paper studies the problem of modeling complex domains of actions and
change within high-level action description languages. We investigate two main
issues of concern: (a) can we represent complex domains that capture together
different problems such as ramifications, non-determinism and concurrency of
actions, at a high-level, close to the given natural ontology of the problem
domain and (b) what features of such a representation can affect, and how, its
computational behaviour. The paper describes the main problems faced in this
representation task and presents the results of an empirical study, carried out
through a series of controlled experiments, to analyze the computational
performance of reasoning in these representations. The experiments compare
different representations obtained, for example, by changing the basic ontology
of the domain or by varying the degree of use of indirect effect laws through
domain constraints. This study has helped to expose the main sources of
computational difficulty in the reasoning and suggest some methodological
guidelines for representing complex domains. Although our work has been carried
out within one particular high-level description language, we believe that the
results, especially those that relate to the problems of representation, are
independent of the specific modeling language.","9 pages, 3 figures, to download the E-RES system and a full
  representation of the Zoo Scenario World, visit
  http://www.cs.ucy.ac.cy/~pslogic/"
Value Based Argumentation Frameworks,"This paper introduces the notion of value-based argumentation frameworks, an
extension of the standard argumentation frameworks proposed by Dung, which are
able toshow how rational decision is possible in cases where arguments derive
their force from the social values their acceptance would promote.",N/A
"Preferred well-founded semantics for logic programming by alternating
  fixpoints: Preliminary report","We analyze the problem of defining well-founded semantics for ordered logic
programs within a general framework based on alternating fixpoint theory. We
start by showing that generalizations of existing answer set approaches to
preference are too weak in the setting of well-founded semantics. We then
specify some informal yet intuitive criteria and propose a semantical framework
for preference handling that is more suitable for defining well-founded
semantics for ordered logic programs. The suitability of the new approach is
convinced by the fact that many attractive properties are satisfied by our
semantics. In particular, our semantics is still correct with respect to
various existing answer sets semantics while it successfully overcomes the
weakness of their generalization to well-founded semantics. Finally, we
indicate how an existing preferred well-founded semantics can be captured
within our semantical framework.","Proceedings of the Workshop on Preferences in Artificial Intelligence
  and Constraint In: Proceedings of the Workshop on Non-Monotonic Reasoning
  (NMR'2002)"
Embedding Default Logic in Propositional Argumentation Systems,"In this paper we present a transformation of finite propositional default
theories into so-called propositional argumentation systems. This
transformation allows to characterize all notions of Reiter's default logic in
the framework of argumentation systems. As a consequence, computing extensions,
or determining wether a given formula belongs to one extension or all
extensions can be answered without leaving the field of classical propositional
logic. The transformation proposed is linear in the number of defaults.",9 pages
"On the existence and multiplicity of extensions in dialectical
  argumentation","In the present paper, the existence and multiplicity problems of extensions
are addressed. The focus is on extension of the stable type. The main result of
the paper is an elegant characterization of the existence and multiplicity of
extensions in terms of the notion of dialectical justification, a close cousin
of the notion of admissibility. The characterization is given in the context of
the particular logic for dialectical argumentation DEFLOG. The results are of
direct relevance for several well-established models of defeasible reasoning
(like default logic, logic programming and argumentation frameworks), since
elsewhere dialectical argumentation has been shown to have close formal
connections with these models.","10 pages; 9th International Workshop on Non-Monotonic Reasoning
  (NMR'2002)"
"Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic
  Logic and Probabilistic Logic under Coherence","Recently, it has been shown that probabilistic entailment under coherence is
weaker than model-theoretic probabilistic entailment. Moreover, probabilistic
entailment under coherence is a generalization of default entailment in System
P. In this paper, we continue this line of research by presenting probabilistic
generalizations of more sophisticated notions of classical default entailment
that lie between model-theoretic probabilistic entailment and probabilistic
entailment under coherence. That is, the new formalisms properly generalize
their counterparts in classical default reasoning, they are weaker than
model-theoretic probabilistic entailment, and they are stronger than
probabilistic entailment under coherence. The new formalisms are useful
especially for handling probabilistic inconsistencies related to conditioning
on zero events. They can also be applied for probabilistic belief revision.
More generally, in the same spirit as a similar previous paper, this paper
sheds light on exciting new formalisms for probabilistic reasoning beyond the
well-known standard ones.","10 pages; in Proceedings of the 9th International Workshop on
  Non-Monotonic Reasoning (NMR-2002), Special Session on Uncertainty Frameworks
  in Nonmonotonic Reasoning, pages 265-274, Toulouse, France, April 2002"
Evaluating Defaults,"We seek to find normative criteria of adequacy for nonmonotonic logic similar
to the criterion of validity for deductive logic. Rather than stipulating that
the conclusion of an inference be true in all models in which the premises are
true, we require that the conclusion of a nonmonotonic inference be true in
``almost all'' models of a certain sort in which the premises are true. This
``certain sort'' specification picks out the models that are relevant to the
inference, taking into account factors such as specificity and vagueness, and
previous inferences. The frequencies characterizing the relevant models reflect
known frequencies in our actual world. The criteria of adequacy for a default
inference can be extended by thresholding to criteria of adequacy for an
extension. We show that this avoids the implausibilities that might otherwise
result from the chaining of default inferences. The model proportions, when
construed in terms of frequencies, provide a verifiable grounding of default
rules, and can become the basis for generating default rules from statistics.",8 pages
Linking Makinson and Kraus-Lehmann-Magidor preferential entailments,"About ten years ago, various notions of preferential entailment have been
introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM),
one of the main competitor being a more general version defined by Makinson
(MAK). These two versions have already been compared, but it is time to revisit
these comparisons. Here are our three main results: (1) These two notions are
equivalent, provided that we restrict our attention, as done in KLM, to the
cases where the entailment respects logical equivalence (on the left and on the
right). (2) A serious simplification of the description of the fundamental
cases in which MAK is equivalent to KLM, including a natural passage in both
ways. (3) The two previous results are given for preferential entailments more
general than considered in some of the original texts, but they apply also to
the original definitions and, for this particular case also, the models can be
simplified.","Proceedings of the 9th Int. Workshop on Non-Monotonic Reasoning
  (NMR'2002), Toulouse, France, April 19-21, 2002. Also, paper with the same
  Title at ECAI 2002 (15th European Conf. on A.I.)"
Knowledge Representation,"This work analyses main features that should be present in knowledge
representation. It suggests a model for representation and a way to implement
this model in software. Representation takes care of both low-level sensor
information and high-level concepts.",N/A
"Causes and Explanations: A Structural-Model Approach. Part II:
  Explanations","We propose new definitions of (causal) explanation, using structural
equations to model counterfactuals. The definition is based on the notion of
actual cause, as defined and motivated in a companion paper. Essentially, an
explanation is a fact that is not known for certain but, if found to be true,
would constitute an actual cause of the fact to be explained, regardless of the
agent's initial uncertainty. We show that the definition handles well a number
of problematic examples from the literature.","Part I of the paper (on causes) is also on the arxiv. The two papers
  originally were posted as one submission. The conference version of the paper
  appears in IJCAI '01. This paper will appear in the British Journal for
  Philosophy of Science"
Reasoning about Evolving Nonmonotonic Knowledge Bases,"Recently, several approaches to updating knowledge bases modeled as extended
logic programs have been introduced, ranging from basic methods to incorporate
(sequences of) sets of rules into a logic program, to more elaborate methods
which use an update policy for specifying how updates must be incorporated. In
this paper, we introduce a framework for reasoning about evolving knowledge
bases, which are represented as extended logic programs and maintained by an
update policy. We first describe a formal model which captures various update
approaches, and we define a logical language for expressing properties of
evolving knowledge bases. We then investigate semantical and computational
properties of our framework, where we focus on properties of knowledge states
with respect to the canonical reasoning task of whether a given formula holds
on a given evolving knowledge base. In particular, we present finitary
characterizations of the evolution for certain classes of framework instances,
which can be exploited for obtaining decidability results. In more detail, we
characterize the complexity of reasoning for some meaningful classes of
evolving knowledge bases, ranging from polynomial to double exponential space
complexity.","47 pages.A preliminary version appeared in: Proc. 8th International
  Conference on Logic for Programming, Artificial Intelligence and Reasoning
  (LPAR 2001), R. Nieuwenhuis and A. Voronkov (eds), pp. 407--421, LNCS 2250,
  Springer 2001"
"A Comparison of Different Cognitive Paradigms Using Simple Animats in a
  Virtual Laboratory, with Implications to the Notion of Cognition","In this thesis I present a virtual laboratory which implements five different
models for controlling animats: a rule-based system, a behaviour-based system,
a concept-based system, a neural network, and a Braitenberg architecture.
Through different experiments, I compare the performance of the models and
conclude that there is no ""best"" model, since different models are better for
different things in different contexts.
  The models I chose, although quite simple, represent different approaches for
studying cognition. Using the results as an empirical philosophical aid,
  I note that there is no ""best"" approach for studying cognition, since
different approaches have all advantages and disadvantages, because they study
different aspects of cognition from different contexts. This has implications
for current debates on ""proper"" approaches for cognition: all approaches are a
bit proper, but none will be ""proper enough"". I draw remarks on the notion of
cognition abstracting from all the approaches used to study it, and propose a
simple classification for different types of cognition.","MSc Thesis, University of Sussex. pdf available from
  http://www.cogs.susx.ac.uk/users/carlos"
Revising Partially Ordered Beliefs,"This paper deals with the revision of partially ordered beliefs. It proposes
a semantic representation of epistemic states by partial pre-orders on
interpretations and a syntactic representation by partially ordered belief
bases. Two revision operations, the revision stemming from the history of
observations and the possibilistic revision, defined when the epistemic state
is represented by a total pre-order, are generalized, at a semantic level, to
the case of a partial pre-order on interpretations, and at a syntactic level,
to the case of a partially ordered belief base. The equivalence between the two
representations is shown for the two revision operations.",figures made with the pstricks latex packages
"Can the whole brain be simpler than its ""parts""?","This is the first in a series of connected papers discussing the problem of a
dynamically reconfigurable universal learning neurocomputer that could serve as
a computational model for the whole human brain. The whole series is entitled
""The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal
Learning Neurocomputer."" (For more information visit the website
www.brain0.com.) This introductory paper is concerned with general methodology.
Its main goal is to explain why it is critically important for both neural
modeling and cognitive modeling to pay much attention to the basic requirements
of the whole brain as a complex computing system. The author argues that it can
be easier to develop an adequate computational model for the whole
""unprogrammed"" (untrained) human brain than to find adequate formal
representations of some nontrivial parts of brain's performance. (In the same
way as, for example, it is easier to describe the behavior of a complex
analytical function than the behavior of its real and/or imaginary part.) The
""curse of dimensionality"" that plagues purely phenomenological (""brainless"")
cognitive theories is a natural penalty for an attempt to represent
insufficiently large parts of brain's performance in a state space of
insufficiently high dimensionality. A ""partial"" modeler encounters ""Catch 22.""
An attempt to simplify a cognitive problem by artificially reducing its
dimensionality makes the problem more difficult.",No figures
"Adaptive Development of Koncepts in Virtual Animats: Insights into the
  Development of Knowledge","As a part of our effort for studying the evolution and development of
cognition, we present results derived from synthetic experimentations in a
virtual laboratory where animats develop koncepts adaptively and ground their
meaning through action. We introduce the term ""koncept"" to avoid confusions and
ambiguity derived from the wide use of the word ""concept"". We present the
models which our animats use for abstracting koncepts from perceptions,
plastically adapt koncepts, and associate koncepts with actions. On a more
philosophical vein, we suggest that knowledge is a property of a cognitive
system, not an element, and therefore observer-dependent.","15 pages, COGS Adaptive Systems Essay"
"Dynamic Adjustment of the Motivation Degree in an Action Selection
  Mechanism","This paper presents a model for dynamic adjustment of the motivation degree,
using a reinforcement learning approach, in an action selection mechanism
previously developed by the authors. The learning takes place in the
modification of a parameter of the model of combination of internal and
external stimuli. Experiments that show the claimed properties are presented,
using a VR simulation developed for such purposes. The importance of adaptation
by learning in action selection is also discussed.","7 pages, Proceedings of ISA '2000. Wollongong, Australia"
Action Selection Properties in a Software Simulated Agent,"This article analyses the properties of the Internal Behaviour network, an
action selection mechanism previously proposed by the authors, with the aid of
a simulation developed for such ends. A brief review of the Internal Behaviour
network is followed by the explanation of the implementation of the simulation.
Then, experiments are presented and discussed analysing the properties of the
action selection in the proposed model.","12 pages, in MICAI 2000: Advances in Artificial Intelligence. Lecture
  Notes in Artificial Intelligence 1793, pp. 634-648. Springer-Verlag"
"A Model for Combination of External and Internal Stimuli in the Action
  Selection of an Autonomous Agent","This paper proposes a model for combination of external and internal stimuli
for the action selection in an autonomous agent, based in an action selection
mechanism previously proposed by the authors. This combination model includes
additive and multiplicative elements, which allows to incorporate new
properties, which enhance the action selection. A given parameter a, which is
part of the proposed model, allows to regulate the degree of dependence of the
observed external behaviour from the internal states of the entity.","13 pages, in MICAI 2000: Advances in Artificial Intelligence. Lecture
  Notes in Artificial Intelligence 1793, pp. 621-633. Springer-Verlag"
Searching for Plannable Domains can Speed up Reinforcement Learning,"Reinforcement learning (RL) involves sequential decision making in uncertain
environments. The aim of the decision-making agent is to maximize the benefit
of acting in its environment over an extended period of time. Finding an
optimal policy in RL may be very slow. To speed up learning, one often used
solution is the integration of planning, for example, Sutton's Dyna algorithm,
or various other methods using macro-actions.
  Here we suggest to separate plannable, i.e., close to deterministic parts of
the world, and focus planning efforts in this domain. A novel reinforcement
learning method called plannable RL (pRL) is proposed here. pRL builds a simple
model, which is used to search for macro actions. The simplicity of the model
makes planning computationally inexpensive. It is shown that pRL finds an
optimal policy, and that plannable macro actions found by pRL are near-optimal.
In turn, it is unnecessary to try large numbers of macro actions, which enables
fast learning. The utility of pRL is demonstrated by computer simulations.",N/A
Temporal plannability by variance of the episode length,"Optimization of decision problems in stochastic environments is usually
concerned with maximizing the probability of achieving the goal and minimizing
the expected episode length. For interacting agents in time-critical
applications, learning of the possibility of scheduling of subtasks (events) or
the full task is an additional relevant issue. Besides, there exist highly
stochastic problems where the actual trajectories show great variety from
episode to episode, but completing the task takes almost the same amount of
time. The identification of sub-problems of this nature may promote e.g.,
planning, scheduling and segmenting Markov decision processes. In this work,
formulae for the average duration as well as the standard deviation of the
duration of events are derived. The emerging Bellman-type equation is a simple
extension of Sobel's work (1982). Methods of dynamic programming as well as
methods of reinforcement learning can be applied for our extension. Computer
demonstration on a toy problem serve to highlight the principle.",N/A
"Comparisons and Computation of Well-founded Semantics for Disjunctive
  Logic Programs","Much work has been done on extending the well-founded semantics to general
disjunctive logic programs and various approaches have been proposed. However,
these semantics are different from each other and no consensus is reached about
which semantics is the most intended. In this paper we look at disjunctive
well-founded reasoning from different angles. We show that there is an
intuitive form of the well-founded reasoning in disjunctive logic programming
which can be characterized by slightly modifying some exisitng approaches to
defining disjunctive well-founded semantics, including program transformations,
argumentation, unfounded sets (and resolution-like procedure). We also provide
a bottom-up procedure for this semantics. The significance of our work is not
only in clarifying the relationship among different approaches, but also shed
some light on what is an intended well-founded semantics for disjunctive logic
programs.",31 pages
A semantic framework for preference handling in answer set programming,"We provide a semantic framework for preference handling in answer set
programming. To this end, we introduce preference preserving consequence
operators. The resulting fixpoint characterizations provide us with a uniform
semantic framework for characterizing preference handling in existing
approaches. Although our approach is extensible to other semantics by means of
an alternating fixpoint theory, we focus here on the elaboration of preferences
under answer set semantics. Alternatively, we show how these approaches can be
characterized by the concept of order preservation. These uniform semantic
characterizations provide us with new insights about interrelationships and
moreover about ways of implementation.",39 pages. To appear in Theory and Practice of Logic Programming
Defeasible Logic Programming: An Argumentative Approach,"The work reported here introduces Defeasible Logic Programming (DeLP), a
formalism that combines results of Logic Programming and Defeasible
Argumentation. DeLP provides the possibility of representing information in the
form of weak rules in a declarative manner, and a defeasible argumentation
inference mechanism for warranting the entailed conclusions.
  In DeLP an argumentation formalism will be used for deciding between
contradictory goals. Queries will be supported by arguments that could be
defeated by other arguments. A query q will succeed when there is an argument A
for q that is warranted, ie, the argument A that supports q is found undefeated
by a warrant procedure that implements a dialectical analysis.
  The defeasible argumentation basis of DeLP allows to build applications that
deal with incomplete and contradictory information in dynamic domains. Thus,
the resulting approach is suitable for representing agent's knowledge and for
providing an argumentation based reasoning mechanism to agents.","43 pages, to appear in the journal ""Theory and Practice of Logic
  Programming"""
Constraint-based analysis of composite solvers,"Cooperative constraint solving is an area of constraint programming that
studies the interaction between constraint solvers with the aim of discovering
the interaction patterns that amplify the positive qualities of individual
solvers. Automatisation and formalisation of such studies is an important issue
of cooperative constraint solving.
  In this paper we present a constraint-based analysis of composite solvers
that integrates reasoning about the individual solvers and the processed data.
The idea is to approximate this reasoning by resolution of set constraints on
the finite sets representing the predicates that express all the necessary
properties. We illustrate application of our analysis to two important
cooperation patterns: deterministic choice and loop.",submitted to AI SAC 2004
Kalman-filtering using local interactions,"There is a growing interest in using Kalman-filter models for brain
modelling. In turn, it is of considerable importance to represent Kalman-filter
in connectionist forms with local Hebbian learning rules. To our best
knowledge, Kalman-filter has not been given such local representation. It seems
that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a
connectionist representation is presented, which is derived by means of the
recursive prediction error method. We show that this method gives rise to
attractive local learning rules and can adapt the Kalman-gain.",N/A
On the Notion of Cognition,"We discuss philosophical issues concerning the notion of cognition basing
ourselves in experimental results in cognitive sciences, especially in computer
simulations of cognitive systems. There have been debates on the ""proper""
approach for studying cognition, but we have realized that all approaches can
be in theory equivalent. Different approaches model different properties of
cognitive systems from different perspectives, so we can only learn from all of
them. We also integrate ideas from several perspectives for enhancing the
notion of cognition, such that it can contain other definitions of cognition as
special cases. This allows us to propose a simple classification of different
types of cognition.","6 pages, 2 figures"
Unfolding Partiality and Disjunctions in Stable Model Semantics,"The paper studies an implementation methodology for partial and disjunctive
stable models where partiality and disjunctions are unfolded from a logic
program so that an implementation of stable models for normal
(disjunction-free) programs can be used as the core inference engine. The
unfolding is done in two separate steps. Firstly, it is shown that partial
stable models can be captured by total stable models using a simple linear and
modular program transformation. Hence, reasoning tasks concerning partial
stable models can be solved using an implementation of total stable models.
Disjunctive partial stable models have been lacking implementations which now
become available as the translation handles also the disjunctive case.
Secondly, it is shown how total stable models of disjunctive programs can be
determined by computing stable models for normal programs. Hence, an
implementation of stable models of normal programs can be used as a core engine
for implementing disjunctive programs. The feasibility of the approach is
demonstrated by constructing a system for computing stable models of
disjunctive programs using the smodels system as the core engine. The
performance of the resulting system is compared to that of dlv which is a
state-of-the-art special purpose system for disjunctive programs.","49 pages, 4 figures, 1 table"
Multi-target particle filtering for the probability hypothesis density,"When tracking a large number of targets, it is often computationally
expensive to represent the full joint distribution over target states. In cases
where the targets move independently, each target can instead be tracked with a
separate filter. However, this leads to a model-data association problem.
Another approach to solve the problem with computational complexity is to track
only the first moment of the joint distribution, the probability hypothesis
density (PHD). The integral of this distribution over any area S is the
expected number of targets within S. Since no record of object identity is
kept, the model-data association problem is avoided.
  The contribution of this paper is a particle filter implementation of the PHD
filter mentioned above. This PHD particle filter is applied to tracking of
multiple vehicles in terrain, a non-linear tracking problem. Experiments show
that the filter can track a changing number of vehicles robustly, achieving
near-real-time performance.",Submitted to International Conference on Information Fusion 2003
A Framework for Searching AND/OR Graphs with Cycles,"Search in cyclic AND/OR graphs was traditionally known to be an unsolved
problem. In the recent past several important studies have been reported in
this domain. In this paper, we have taken a fresh look at the problem. First, a
new and comprehensive theoretical framework for cyclic AND/OR graphs has been
presented, which was found missing in the recent literature. Based on this
framework, two best-first search algorithms, S1 and S2, have been developed. S1
does uninformed search and is a simple modification of the Bottom-up algorithm
by Martelli and Montanari. S2 performs a heuristically guided search and
replicates the modification in Bottom-up's successors, namely HS and AO*. Both
S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles.
We then present a detailed analysis for the correctness and complexity results
of S1 and S2, using the proposed framework. We have observed through
experiments that S1 and S2 output correct results in all cases.","40 pages, 20 figures, 5 tables"
On rho in a Decision-Theoretic Apparatus of Dempster-Shafer Theory,"Thomas M. Strat has developed a decision-theoretic apparatus for
Dempster-Shafer theory (Decision analysis using belief functions, Intern. J.
Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility
intervals are constructed for different choices. The choice with the highest
expected utility is preferable to others. However, to find the preferred choice
when the expected utility interval of one choice is included in that of
another, it is necessary to interpolate a discerning point in the intervals.
This is done by the parameter rho, defined as the probability that the
ambiguity about the utility of every nonsingleton focal element will turn out
as favorable as possible. If there are several different decision makers, we
might sometimes be more interested in having the highest expected utility among
the decision makers rather than only trying to maximize our own expected
utility regardless of choices made by other decision makers. The preference of
each choice is then determined by the probability of yielding the highest
expected utility. This probability is equal to the maximal interval length of
rho under which an alternative is preferred. We must here take into account not
only the choices already made by other decision makers but also the rational
choices we can assume to be made by later decision makers. In Strats apparatus,
an assumption, unwarranted by the evidence at hand, has to be made about the
value of rho. We demonstrate that no such assumption is necessary. It is
sufficient to assume a uniform probability distribution for rho to be able to
discern the most preferable choice. We discuss when this approach is
justifiable.","16 pages, 2 figures"
Updating beliefs with incomplete observations,"Currently, there is renewed interest in the problem, raised by Shafer in
1985, of updating probabilities when observations are incomplete. This is a
fundamental problem in general, and of particular interest for Bayesian
networks. Recently, Grunwald and Halpern have shown that commonly used updating
strategies fail in this case, except under very special assumptions. In this
paper we propose a new method for updating probabilities with incomplete
observations. Our approach is deliberately conservative: we make no assumptions
about the so-called incompleteness mechanism that associates complete with
incomplete observations. We model our ignorance about this mechanism by a
vacuous lower prevision, a tool from the theory of imprecise probabilities, and
we use only coherence arguments to turn prior into posterior probabilities. In
general, this new approach to updating produces lower and upper posterior
probabilities and expectations, as well as partially determinate decisions.
This is a logical consequence of the existing ignorance about the
incompleteness mechanism. We apply the new approach to the problem of
classification of new evidence in probabilistic expert systems, where it leads
to a new, so-called conservative updating rule. In the special case of Bayesian
networks constructed using expert knowledge, we provide an exact algorithm for
classification based on our updating rule, which has linear-time complexity for
a class of networks wider than polytrees. This result is then extended to the
more general framework of credal networks, where computations are often much
harder than with Bayesian nets. Using an example, we show that our rule appears
to provide a solid basis for reliable updating with incomplete observations,
when no strong assumptions about the incompleteness mechanism are justified.",Replaced with extended version
Updating Probabilities,"As examples such as the Monty Hall puzzle show, applying conditioning to
update a probability distribution on a ``naive space'', which does not take
into account the protocol used, can often lead to counterintuitive results.
Here we examine why. A criterion known as CAR (``coarsening at random'') in the
statistical literature characterizes when ``naive'' conditioning in a naive
space works. We show that the CAR condition holds rather infrequently, and we
provide a procedural characterization of it, by giving a randomized algorithm
that generates all and only distributions for which CAR holds. This
substantially extends previous characterizations of CAR. We also consider more
generalized notions of update such as Jeffrey conditioning and minimizing
relative entropy (MRE). We give a generalization of the CAR condition that
characterizes when Jeffrey conditioning leads to appropriate answers, and show
that there exist some very simple settings in which MRE essentially never gives
the right results. This generalizes and interconnects previous results obtained
in the literature on CAR and MRE.","This is an expanded version of a paper that appeared in Proceedings
  of the Eighteenth Conference on Uncertainty in AI, 2002, pp. 187--196. to
  appear, Journal of AI Research"
Pruning Isomorphic Structural Sub-problems in Configuration,"Configuring consists in simulating the realization of a complex product from
a catalog of component parts, using known relations between types, and picking
values for object attributes. This highly combinatorial problem in the field of
constraint programming has been addressed with a variety of approaches since
the foundation system R1(McDermott82). An inherent difficulty in solving
configuration problems is the existence of many isomorphisms among
interpretations. We describe a formalism independent approach to improve the
detection of isomorphisms by configurators, which does not require to adapt the
problem model. To achieve this, we exploit the properties of a characteristic
subset of configuration problems, called the structural sub-problem, which
canonical solutions can be produced or tested at a limited cost. In this paper
we present an algorithm for testing the canonicity of configurations, that can
be added as a symmetry breaking constraint to any configurator. The cost and
efficiency of this canonicity test are given.","This research report contains the proofs and full details missing
  from the short paper ""A Canonicity Test for Configuration"" in proceedings of
  conference CP'03"
"Probabilistic Reasoning as Information Compression by Multiple
  Alignment, Unification and Search: An Introduction and Overview","This article introduces the idea that probabilistic reasoning (PR) may be
understood as ""information compression by multiple alignment, unification and
search"" (ICMAUS). In this context, multiple alignment has a meaning which is
similar to but distinct from its meaning in bio-informatics, while unification
means a simple merging of matching patterns, a meaning which is related to but
simpler than the meaning of that term in logic.
  A software model, SP61, has been developed for the discovery and formation of
'good' multiple alignments, evaluated in terms of information compression. The
model is described in outline.
  Using examples from the SP61 model, this article describes in outline how the
ICMAUS framework can model various kinds of PR including: PR in best-match
pattern recognition and information retrieval; one-step 'deductive' and
'abductive' PR; inheritance of attributes in a class hierarchy; chains of
reasoning (probabilistic decision networks and decision trees, and PR with
'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with
default values; modelling the function of a Bayesian network.",N/A
"Information Compression by Multiple Alignment, Unification and Search as
  a Unifying Principle in Computing and Cognition","This article presents an overview of the idea that ""information compression
by multiple alignment, unification and search"" (ICMAUS) may serve as a unifying
principle in computing (including mathematics and logic) and in such aspects of
human cognition as the analysis and production of natural language, fuzzy
pattern recognition and best-match information retrieval, concept hierarchies
with inheritance of attributes, probabilistic reasoning, and unsupervised
inductive learning. The ICMAUS concepts are described together with an outline
of the SP61 software model in which the ICMAUS concepts are currently realised.
A range of examples is presented, illustrated with output from the SP61 model.",N/A
"Integrating cardinal direction relations and other orientation relations
  in Qualitative Spatial Reasoning","We propose a calculus integrating two calculi well-known in Qualitative
Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus,
and a coarser version of Freksa's relative orientation calculus. An original
constraint propagation procedure is presented, which implements the interaction
between the two integrated calculi. The importance of taking into account the
interaction is shown with a real example providing an inconsistent knowledge
base, whose inconsistency (a) cannot be detected by reasoning separately about
each of the two components of the knowledge, just because, taken separately,
each is consistent, but (b) is detected by the proposed algorithm, thanks to
the interaction knowledge propagated from each of the two compnents to the
other.","Includes new material, such as a section on the use of the work in
  the concrete domain of the ALC(D) spatio-temporalisation defined in
  http://arXiv.org/abs/cs.AI/0307040"
A ternary Relation Algebra of directed lines,"We define a ternary Relation Algebra (RA) of relative position relations on
two-dimensional directed lines (d-lines for short). A d-line has two degrees of
freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The
representation of the RDF of a d-line will be handled by an RA of 2D
orientations, CYC_t, known in the literature. A second algebra, TA_t, which
will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and
TA_t, will constitute, respectively, the translational and the rotational
components of the RA, PA_t, of relative position relations on d-lines: the PA_t
atoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that
are compatible. We present in detail the RA PA_t, with its converse table, its
rotation table and its composition tables. We show that a (polynomial)
constraint propagation algorithm, known in the literature, is complete for a
subset of PA_t relations including almost all of the atomic relations. We will
discuss the application scope of the RA, which includes incidence geometry, GIS
(Geographic Information Systems), shape representation, localisation in
(multi-)robot navigation, and the representation of motion prepositions in NLP
(Natural Language Processing). We then compare the RA to existing ones, such as
an algebra for reasoning about rectangles parallel to the axes of an
(orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval
algebra, and an algebra for reasoning about 2D segments.","60 pages. Submitted. Technical report mentioned in ""Report-no"" below
  is an earlier version of the work, and its title differs slightly (Reasoning
  about relative position of directed lines as a ternary Relation Algebra (RA):
  presentation of the RA and of its use in the concrete domain of an
  ALC(D)-like description logic)"
From Statistical Knowledge Bases to Degrees of Belief,"An intelligent agent will often be uncertain about various properties of its
environment, and when acting in that environment it will frequently need to
quantify its uncertainty. For example, if the agent wishes to employ the
expected-utility paradigm of decision theory to guide its actions, it will need
to assign degrees of belief (subjective probabilities) to various assertions.
Of course, these degrees of belief should not be arbitrary, but rather should
be based on the information available to the agent. This paper describes one
approach for inducing degrees of belief from very rich knowledge bases, that
can include information about particular individuals, statistical correlations,
physical laws, and default rules. We call our approach the random-worlds
method. The method is based on the principle of indifference: it treats all of
the worlds the agent considers possible as being equally likely. It is able to
integrate qualitative default reasoning with quantitative probabilistic
reasoning by providing a language in which both types of information can be
easily expressed. Our results show that a number of desiderata that arise in
direct inference (reasoning from statistical information to conclusions about
individuals) and default reasoning follow directly {from} the semantics of
random worlds. For example, random worlds captures important patterns of
reasoning such as specificity, inheritance, indifference to irrelevant
information, and default assumptions of independence. Furthermore, the
expressive power of the language used and the intuitive semantics of random
worlds allow the method to deal with problems that are beyond the scope of many
other non-deductive reasoning systems.",N/A
"An Alternative to RDF-Based Languages for the Representation and
  Processing of Ontologies in the Semantic Web","This paper describes an approach to the representation and processing of
ontologies in the Semantic Web, based on the ICMAUS theory of computation and
AI. This approach has strengths that complement those of languages based on the
Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main
benefits of the ICMAUS approach are simplicity and comprehensibility in the
representation of ontologies, an ability to cope with errors and uncertainties
in knowledge, and a versatile reasoning system with capabilities in the kinds
of probabilistic reasoning that seem to be required in the Semantic Web.",N/A
Quantifying and Visualizing Attribute Interactions,"Interactions are patterns between several attributes in data that cannot be
inferred from any subset of these attributes. While mutual information is a
well-established approach to evaluating the interactions between two
attributes, we surveyed its generalizations as to quantify interactions between
several attributes. We have chosen McGill's interaction information, which has
been independently rediscovered a number of times under various names in
various disciplines, because of its many intuitively appealing properties. We
apply interaction information to visually present the most important
interactions of the data. Visualization of interactions has provided insight
into the structure of data on a number of domains, identifying redundant
attributes and opportunities for constructing new features, discovering
unexpected regularities in data, and have helped during construction of
predictive models; we illustrate the methods on numerous examples. A machine
learning method that disregards interactions may get caught in two traps:
myopia is caused by learning algorithms assuming independence in spite of
interactions, whereas fragmentation arises from assuming an interaction in
spite of independence.","30 pages, 11 figures. Changes from v2: improved bibliography"
Evidential Force Aggregation,"In this paper we develop an evidential force aggregation method intended for
classification of evidential intelligence into recognized force structures. We
assume that the intelligence has already been partitioned into clusters and use
the classification method individually in each cluster. The classification is
based on a measure of fitness between template and fused intelligence that
makes it possible to handle intelligence reports with multiple nonspecific and
uncertain propositions. With this measure we can aggregate on a level-by-level
basis, starting from general intelligence to achieve a complete force structure
with recognized units on all hierarchical levels.","7 pages, 2 figures"
Application of Kullback-Leibler Metric to Speech Recognition,"Article discusses the application of Kullback-Leibler divergence to the
recognition of speech signals and suggests three algorithms implementing this
divergence criterion: correlation algorithm, spectral algorithm and filter
algorithm. Discussion covers an approach to the problem of speech variability
and is illustrated with the results of experimental modeling of speech signals.
The article gives a number of recommendations on the choice of appropriate
model parameters and provides a comparison to some other methods of speech
recognition.","10 pages, 4 figures, Word to PDF auto converted"
The Algebra of Utility Inference,"Richard Cox [1] set the axiomatic foundations of probable inference and the
algebra of propositions. He showed that consistency within these axioms
requires certain rules for updating belief. In this paper we use the analogy
between probability and utility introduced in [2] to propose an axiomatic
foundation for utility inference and the algebra of preferences. We show that
consistency within these axioms requires certain rules for updating preference.
We discuss a class of utility functions that stems from the axioms of utility
inference and show that this class is the basic building block for any general
multiattribute utility function. We use this class of utility functions
together with the algebra of preferences to construct utility functions
represented by logical operations on the attributes.",15 pages
An information theory for preferences,"Recent literature in the last Maximum Entropy workshop introduced an analogy
between cumulative probability distributions and normalized utility functions.
Based on this analogy, a utility density function can de defined as the
derivative of a normalized utility function. A utility density function is
non-negative and integrates to unity. These two properties form the basis of a
correspondence between utility and probability. A natural application of this
analogy is a maximum entropy principle to assign maximum entropy utility
values. Maximum entropy utility interprets many of the common utility functions
based on the preference information needed for their assignment, and helps
assign utility values based on partial preference information. This paper
reviews maximum entropy utility and introduces further results that stem from
the duality between probability and utility.",N/A
"Abductive Logic Programs with Penalization: Semantics, Complexity and
  Implementation","Abduction, first proposed in the setting of classical logics, has been
studied with growing interest in the logic programming area during the last
years.
  In this paper we study abduction with penalization in the logic programming
framework. This form of abductive reasoning, which has not been previously
analyzed in logic programming, turns out to represent several relevant
problems, including optimization problems, very naturally. We define a formal
model for abduction with penalization over logic programs, which extends the
abductive framework proposed by Kakas and Mancarella. We address knowledge
representation issues, encoding a number of problems in our abductive
framework. In particular, we consider some relevant problems, taken from
different domains, ranging from optimization theory to diagnosis and planning;
their encodings turn out to be simple and elegant in our formalism. We
thoroughly analyze the computational complexity of the main problems arising in
the context of abduction with penalization from logic programs. Finally, we
implement a system supporting the proposed abductive framework on top of the
DLV engine. To this end, we design a translation from abduction problems with
penalties into logic programs with weak constraints. We prove that this
approach is sound and complete.","36 pages; will be published in Theory and Practice of Logic
  Programming"
"Local-search techniques for propositional logic extended with
  cardinality constraints","We study local-search satisfiability solvers for propositional logic extended
with cardinality atoms, that is, expressions that provide explicit ways to
model constraints on cardinalities of sets. Adding cardinality atoms to the
language of propositional logic facilitates modeling search problems and often
results in concise encodings. We propose two ``native'' local-search solvers
for theories in the extended language. We also describe techniques to reduce
the problem to standard propositional satisfiability and allow us to use
off-the-shelf SAT solvers. We study these methods experimentally. Our general
finding is that native solvers designed specifically for the extended language
perform better than indirect methods relying on SAT solvers.","Proceedings of the 9th International Conference on Pronciples and
  Practice of Constraint Programming - CP 2003, LNCS 2833, pp. 495-509"
WSAT(cc) - a fast local-search ASP solver,"We describe WSAT(cc), a local-search solver for computing models of theories
in the language of propositional logic extended by cardinality atoms. WSAT(cc)
is a processing back-end for the logic PS+, a recently proposed formalism for
answer-set programming.","Proceedings of LPNMR-03 (7th International Conference), LNCS,
  Springer Verlag"
Utility-Probability Duality,"This paper presents duality between probability distributions and utility
functions.",N/A
Parametric Connectives in Disjunctive Logic Programming,"Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge
Representation and Reasoning (KRR). \DLP is very expressive in a precise
mathematical sense: it allows to express every property of finite structures
that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$).
Importantly, the \DLP encodings are often simple and natural.
  In this paper, we single out some limitations of \DLP for KRR, which cannot
naturally express problems where the size of the disjunction is not known ``a
priori'' (like N-Coloring), but it is part of the input. To overcome these
limitations, we further enhance the knowledge modelling abilities of \DLP, by
extending this language by {\em Parametric Connectives (OR and AND)}. These
connectives allow us to represent compactly the disjunction/conjunction of a
set of atoms having a given property. We formally define the semantics of the
new language, named $DLP^{\bigvee,\bigwedge}$ and we show the usefulness of the
new constructs on relevant knowledge-based problems. We address implementation
issues and discuss related works.",N/A
Logic-Based Specification Languages for Intelligent Software Agents,"The research field of Agent-Oriented Software Engineering (AOSE) aims to find
abstractions, languages, methodologies and toolkits for modeling, verifying,
validating and prototyping complex applications conceptualized as Multiagent
Systems (MASs). A very lively research sub-field studies how formal methods can
be used for AOSE. This paper presents a detailed survey of six logic-based
executable agent specification languages that have been chosen for their
potential to be integrated in our ARPEGGIO project, an open framework for
specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the
IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each
executable language, the logic foundations are described and an example of use
is shown. A comparison of the six languages and a survey of similar approaches
complete the paper, together with considerations of the advantages of using
logic-based languages in MAS modeling and prototyping.","67 pages, 1 table, 1 figure. Accepted for publication by the Journal
  ""Theory and Practice of Logic Programming"", volume 4, Maurice Bruynooghe
  Editor-in-Chief"
"Great Expectations. Part I: On the Customizability of Generalized
  Expected Utility","We propose a generalization of expected utility that we call generalized EU
(GEU), where a decision maker's beliefs are represented by plausibility
measures, and the decision maker's tastes are represented by general (i.e.,not
necessarily real-valued) utility functions. We show that every agent,
``rational'' or not, can be modeled as a GEU maximizer. We then show that we
can customize GEU by selectively imposing just the constraints we want. In
particular, we show how each of Savage's postulates corresponds to constraints
on GEU.","Preliminary version appears in Proc. 18th International Joint
  Conference on AI (IJCAI), 2003, pp. 291-296"
"Great Expectations. Part II: Generalized Expected Utility as a Universal
  Decision Rule","Many different rules for decision making have been introduced in the
literature. We show that a notion of generalized expected utility proposed in
Part I of this paper is a universal decision rule, in the sense that it can
represent essentially all other decision rules.","Preliminary version appears in Proc. 18th International Joint
  Conference on AI (IJCAI), 2003, pp. 297-302"
"Unsupervised Grammar Induction in a Framework of Information Compression
  by Multiple Alignment, Unification and Search","This paper describes a novel approach to grammar induction that has been
developed within a framework designed to integrate learning with other aspects
of computing, AI, mathematics and logic. This framework, called ""information
compression by multiple alignment, unification and search"" (ICMAUS), is founded
on principles of Minimum Length Encoding pioneered by Solomonoff and others.
Most of the paper describes SP70, a computer model of the ICMAUS framework that
incorporates processes for unsupervised learning of grammars. An example is
presented to show how the model can infer a plausible grammar from appropriate
input. Limitations of the current model and how they may be overcome are
briefly discussed.",N/A
"Integrating existing cone-shaped and projection-based cardinal direction
  relations and a TCSP-like decidable generalisation","We consider the integration of existing cone-shaped and projection-based
calculi of cardinal direction relations, well-known in QSR. The more general,
integrating language we consider is based on convex constraints of the
qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal
direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$,
with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning
of the quantitative constraint, in particular, is that point $x$ belongs to the
(convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and
$\beta$. The general form of a constraint is a disjunction of the form
$[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha
_{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha
_i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above:
the meaning of such a general constraint is that, for some $i=1... n_1$,
$r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A
conjunction of such general constraints is a $\tcsp$-like CSP, which we will
refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective
solution search algorithm for an $\scsp$ will be described, which uses (1)
constraint propagation, based on a composition operation to be defined, as the
filtering method during the search, and (2) the Simplex algorithm, guaranteeing
completeness, at the leaves of the search tree. The approach is particularly
suited for large-scale high-level vision, such as, e.g., satellite-like
surveillance of a geographic area.","I should be able to provide a longer version soon. A shorter version
  has been submitted to the conference KR'2004"
Modeling Object Oriented Constraint Programs in Z,"Object oriented constraint programs (OOCPs) emerge as a leading evolution of
constraint programming and artificial intelligence, first applied to a range of
industrial applications called configuration problems. The rich variety of
technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP,
Terminological systems, constraint programs with set variables ...) is a source
of difficulty. No universally accepted formal language exists for communicating
about OOCPs, which makes the comparison of systems difficult. We present here a
Z based specification of OOCPs which avoids the falltrap of hidden object
semantics. The object system is part of the specification, and captures all of
the most advanced notions from the object oriented modeling standard UML. The
paper illustrates these issues and the conciseness and precision of Z by the
specification of a working OOCP that solves an historical AI problem : parsing
a context free grammar. Being written in Z, an OOCP specification also supports
formal proofs. The whole builds the foundation of an adaptative and evolving
framework for communicating about constrained object models and programs.",N/A
Diagnostic reasoning with A-Prolog,"In this paper we suggest an architecture for a software agent which operates
a physical device and is capable of making observations and of testing and
repairing the device's components. We present simplified definitions of the
notions of symptom, candidate diagnosis, and diagnosis which are based on the
theory of action language ${\cal AL}$. The definitions allow one to give a
simple account of the agent's behavior in which many of the agent's tasks are
reduced to computing stable models of logic programs.","46 pages, 1 Postscript figure"
Weight Constraints as Nested Expressions,"We compare two recent extensions of the answer set (stable model) semantics
of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the
bodies and heads of rules to contain nested expressions. The other, due to
Niemela and Simons, uses weight constraints. We show that there is a simple,
modular translation from the language of weight constraints into the language
of nested expressions that preserves the program's answer sets. Nested
expressions can be eliminated from the result of this translation in favor of
additional atoms. The translation makes it possible to compute answer sets for
some programs with weight constraints using satisfiability solvers, and to
prove the strong equivalence of programs with weight constraints using the
logic of here-and there.",To appear in Theory and Practice of Logic Programming
On the Expressibility of Stable Logic Programming,"(We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic
Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend
Schlipf's result to prove that SLP solves all search problems in the class
$\mathit{NP}$. Moreover, we do this in a uniform way as defined in \cite{mt99}.
Specifically, we show that there is a single $\mathrm{DATALOG}^{\neg}$ program
$P_{\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$
with non-negative integer coefficients and any input $\sigma$ of size $n$ over
a fixed alphabet $\Sigma$, there is an extensional database
$\mathit{edb}_{M,p,\sigma}$ such that there is a one-to-one correspondence
between the stable models of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$
and the accepting computations of the machine $M$ that reach the final state in
at most $p(n)$ steps. Moreover, $\mathit{edb}_{M,p,\sigma}$ can be computed in
polynomial time from $p$, $\sigma$ and the description of $M$ and the decoding
of such accepting computations from its corresponding stable model of
$\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ can be computed in linear
time. A similar statement holds for Default Logic with respect to
$\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result
involves additional technical complications and will be a subject of another
publication.}.",17 pages
Unifying Computing and Cognition: The SP Theory and its Applications,"This book develops the conjecture that all kinds of information processing in
computers and in brains may usefully be understood as ""information compression
by multiple alignment, unification and search"". This ""SP theory"", which has
been under development since 1987, provides a unified view of such things as
the workings of a universal Turing machine, the nature of 'knowledge', the
interpretation and production of natural language, pattern recognition and
best-match information retrieval, several kinds of probabilistic reasoning,
planning and problem solving, unsupervised learning, and a range of concepts in
mathematics and logic. The theory also provides a basis for the design of an
'SP' computer with several potential advantages compared with traditional
digital computers.",N/A
Recycling Computed Answers in Rewrite Systems for Abduction,"In rule-based systems, goal-oriented computations correspond naturally to the
possible ways that an observation may be explained. In some applications, we
need to compute explanations for a series of observations with the same domain.
The question whether previously computed answers can be recycled arises. A yes
answer could result in substantial savings of repeated computations. For
systems based on classic logic, the answer is YES. For nonmonotonic systems
however, one tends to believe that the answer should be NO, since recycling is
a form of adding information. In this paper, we show that computed answers can
always be recycled, in a nontrivial way, for the class of rewrite procedures
that we proposed earlier for logic programs with negation. We present some
experimental results on an encoding of the logistics domain.",20 pages. Full version of our IJCAI-03 paper
Memory As A Monadic Control Construct In Problem-Solving,"Recent advances in programming languages study and design have established a
standard way of grounding computational systems representation in category
theory. These formal results led to a better understanding of issues of control
and side-effects in functional and imperative languages. This framework can be
successfully applied to the investigation of the performance of Artificial
Intelligence (AI) inference and cognitive systems. In this paper, we delineate
a categorical formalisation of memory as a control structure driving
performance in inference systems. Abstracting away control mechanisms from
three widely used representations of memory in cognitive systems (scripts,
production rules and clusters) we explain how categorical triples capture the
interaction between learning and problem-solving.",N/A
Integrating Defeasible Argumentation and Machine Learning Techniques,"The field of machine learning (ML) is concerned with the question of how to
construct algorithms that automatically improve with experience. In recent
years many successful ML applications have been developed, such as datamining
programs, information-filtering systems, etc. Although ML algorithms allow the
detection and extraction of interesting patterns of data for several kinds of
problems, most of these algorithms are based on quantitative reasoning, as they
rely on training data in order to infer so-called target functions.
  In the last years defeasible argumentation has proven to be a sound setting
to formalize common-sense qualitative reasoning. This approach can be combined
with other inference techniques, such as those provided by machine learning
theory.
  In this paper we outline different alternatives for combining defeasible
argumentation and machine learning techniques. We suggest how different aspects
of a generic argument-based framework can be integrated with other ML-based
approaches.",5 pages
Epistemic Foundation of Stable Model Semantics,"Stable model semantics has become a very popular approach for the management
of negation in logic programming. This approach relies mainly on the closed
world assumption to complete the available knowledge and its formulation has
its basis in the so-called Gelfond-Lifschitz transformation.
  The primary goal of this work is to present an alternative and
epistemic-based characterization of stable model semantics, to the
Gelfond-Lifschitz transformation. In particular, we show that stable model
semantics can be defined entirely as an extension of the Kripke-Kleene
semantics. Indeed, we show that the closed world assumption can be seen as an
additional source of `falsehood' to be added cumulatively to the Kripke-Kleene
semantics. Our approach is purely algebraic and can abstract from the
particular formalism of choice as it is based on monotone operators (under the
knowledge order) over bilattices only.","41 pages. To appear in Theory and Practice of Logic Programming
  (TPLP)"
The role of behavior modifiers in representation development,"We address the problem of the development of representations and their
relationship to the environment. We study a software agent which develops in a
network a representation of its simple environment which captures and
integrates the relationships between agent and environment through a closure
mechanism. The inclusion of a variable behavior modifier allows better
representation development. This can be confirmed with an internal description
of the closure mechanism, and with an external description of the properties of
the representation network.",8 pages
Parametric external predicates for the DLV System,"This document describes syntax, semantics and implementation guidelines in
order to enrich the DLV system with the possibility to make external C function
calls. This feature is realized by the introduction of parametric external
predicates, whose extension is not specified through a logic program but
implicitly computed through external code.",10 pages
"Toward the Implementation of Functions in the DLV System (Preliminary
  Technical Report)","This document describes the functions as they are treated in the DLV system.
We give first the language, then specify the main implementation issues.",7 pages
Knowledge And The Action Description Language A,"We introduce Ak, an extension of the action description language A (Gelfond
and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing
actions to increase an agent's knowledge of the world and non-deterministic
actions to remove knowledge. We include complex plans involving conditionals
and loops in our query language for hypothetical reasoning. We also present a
translation of Ak domain descriptions into epistemic logic programs.","Appeared in Theory and Practice of Logic Programming, vol. 1, no. 2,
  2001"
"A Comparative Study of Fuzzy Classification Methods on Breast Cancer
  Data","In this paper, we examine the performance of four fuzzy rule generation
methods on Wisconsin breast cancer data. The first method generates fuzzy if
then rules using the mean and the standard deviation of attribute values. The
second approach generates fuzzy if then rules using the histogram of attributes
values. The third procedure generates fuzzy if then rules with certainty of
each attribute into homogeneous fuzzy sets. In the fourth approach, only
overlapping areas are partitioned. The first two approaches generate a single
fuzzy if then rule for each class by specifying the membership function of each
antecedent fuzzy set using the information about attribute values of training
patterns. The other two approaches are based on fuzzy grids with homogeneous
fuzzy partitions of each attribute. The performance of each approach is
evaluated on breast cancer data sets. Simulation results show that the Modified
grid approach has a high classification rate of 99.73 %.",N/A
Intelligent Systems: Architectures and Perspectives,"The integration of different learning and adaptation techniques to overcome
individual limitations and to achieve synergetic effects through the
hybridization or fusion of these techniques has, in recent years, contributed
to a large number of new intelligent system designs. Computational intelligence
is an innovative framework for constructing intelligent hybrid architectures
involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic
Reasoning (PR) and derivative free optimization techniques such as Evolutionary
Computation (EC). Most of these hybridization approaches, however, follow an ad
hoc design methodology, justified by success in certain application domains.
Due to the lack of a common framework it often remains difficult to compare the
various hybrid systems conceptually and to evaluate their performance
comparatively. This chapter introduces the different generic architectures for
integrating intelligent systems. The designing aspects and perspectives of
different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC
systems are presented. Some conclusions are also provided towards the end.",N/A
A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria,"Neuro-fuzzy systems have attracted growing interest of researchers in various
scientific and engineering areas due to the increasing need of intelligent
systems. This paper evaluates the use of two popular soft computing techniques
and conventional statistical approach based on Box--Jenkins autoregressive
integrated moving average (ARIMA) model to predict electricity demand in the
State of Victoria, Australia. The soft computing methods considered are an
evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN)
trained using scaled conjugate gradient algorithm (CGA) and backpropagation
(BP) algorithm. The forecast accuracy is compared with the forecasts used by
Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we
considered load demand patterns for 10 consecutive months taken every 30 min
for training the different prediction models. Test results show that the
neuro-fuzzy system performed better than neural networks, ARIMA model and the
VPX forecasts.",N/A
Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques,"Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)
have attracted the growing interest of researchers in various scientific and
engineering areas due to the growing need of adaptive intelligent systems to
solve the real world problems. ANN learns from scratch by adjusting the
interconnections between layers. FIS is a popular computing framework based on
the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The
advantages of a combination of ANN and FIS are obvious. There are several
approaches to integrate ANN and FIS and very often it depends on the
application. We broadly classify the integration of ANN and FIS into three
categories namely concurrent model, cooperative model and fully fused model.
This paper starts with a discussion of the features of each model and
generalize the advantages and deficiencies of each model. We further focus the
review on the different types of fused neuro-fuzzy systems and citing the
advantages and disadvantages of each model.",N/A
Is Neural Network a Reliable Forecaster on Earth? A MARS Query!,"Long-term rainfall prediction is a challenging task especially in the modern
world where we are facing the major environmental problem of global warming. In
general, climate and rainfall are highly non-linear phenomena in nature
exhibiting what is known as the butterfly effect. While some regions of the
world are noticing a systematic decrease in annual rainfall, others notice
increases in flooding and severe storms. The global nature of this phenomenon
is very complicated and requires sophisticated computer modeling and simulation
to predict accurately. In this paper, we report a performance analysis for
Multivariate Adaptive Regression Splines (MARS)and artificial neural networks
for one month ahead prediction of rainfall. To evaluate the prediction
efficiency, we made use of 87 years of rainfall data in Kerala state, the
southern part of the Indian peninsula situated at latitude -longitude pairs
(8o29'N - 76o57' E). We used an artificial neural network trained using the
scaled conjugate gradient algorithm. The neural network and MARS were trained
with 40 years of rainfall data. For performance evaluation, network predicted
outputs were compared with the actual rainfall data. Simulation results reveal
that MARS is a good forecasting tool and performed better than the considered
neural network.",N/A
DCT Based Texture Classification Using Soft Computing Approach,"Classification of texture pattern is one of the most important problems in
pattern recognition. In this paper, we present a classification method based on
the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works
on gray level image, the color scheme of each image is transformed into gray
levels. For classifying the images using DCT we used two popular soft computing
techniques namely neurocomputing and neuro-fuzzy computing. We used a
feedforward neural network trained using the backpropagation learning and an
evolving fuzzy neural network to classify the textures. The soft computing
models were trained using 80% of the texture data and remaining was used for
testing and validation purposes. A performance comparison was made among the
soft computing models for the texture classification problem. We also analyzed
the effects of prolonged training of neural networks. It is observed that the
proposed neuro-fuzzy model performed better than neural network.",N/A
Estimating Genome Reversal Distance by Genetic Algorithm,"Sorting by reversals is an important problem in inferring the evolutionary
relationship between two genomes. The problem of sorting unsigned permutation
has been proven to be NP-hard. The best guaranteed error bounded is the 3/2-
approximation algorithm. However, the problem of sorting signed permutation can
be solved easily. Fast algorithms have been developed both for finding the
sorting sequence and finding the reversal distance of signed permutation. In
this paper, we present a way to view the problem of sorting unsigned
permutation as signed permutation. And the problem can then be seen as
searching an optimal signed permutation in all n2 corresponding signed
permutations. We use genetic algorithm to conduct the search. Our experimental
result shows that the proposed method outperform the 3/2-approximation
algorithm.",N/A
Intrusion Detection Systems Using Adaptive Regression Splines,"Past few years have witnessed a growing recognition of intelligent techniques
for the construction of efficient and reliable intrusion detection systems. Due
to increasing incidents of cyber attacks, building effective intrusion
detection systems (IDS) are essential for protecting information systems
security, and yet it remains an elusive goal and a great challenge. In this
paper, we report a performance analysis between Multivariate Adaptive
Regression Splines (MARS), neural networks and support vector machines. The
MARS procedure builds flexible regression models by fitting separate splines to
distinct intervals of the predictor variables. A brief comparison of different
neural network learning algorithms is also given.",N/A
Data Mining Approach for Analyzing Call Center Performance,"The aim of our research was to apply well-known data mining techniques (such
as linear neural networks, multi-layered perceptrons, probabilistic neural
networks, classification and regression trees, support vector machines and
finally a hybrid decision tree neural network approach) to the problem of
predicting the quality of service in call centers; based on the performance
data actually collected in a call center of a large insurance company. Our aim
was two-fold. First, to compare the performance of models built using the
above-mentioned techniques and, second, to analyze the characteristics of the
input sensitivity in order to better understand the relationship between the
perform-ance evaluation process and the actual performance and in this way help
improve the performance of call centers. In this paper we summarize our
findings.",N/A
Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms,"The use of intelligent systems for stock market predictions has been widely
established. In this paper, we investigate how the seemingly chaotic behavior
of stock markets could be well represented using several connectionist
paradigms and soft computing techniques. To demonstrate the different
techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P
CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4
year's NIFTY index values. This paper investigates the development of a
reliable and efficient technique to model the seemingly chaotic behavior of
stock markets. We considered an artificial neural network trained using
Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno
neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper
briefly explains how the different connectionist paradigms could be formulated
using different learning methods and then investigates whether they can provide
the required level of performance, which are sufficiently good and robust so as
to provide a reliable forecast model for stock market indices. Experiment
results reveal that all the connectionist paradigms considered could represent
the stock indices behavior very accurately.",N/A
"Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision
  Making Problems","The purpose of this paper is to point to the usefulness of applying a linear
mathematical formulation of fuzzy multiple criteria objective decision methods
in organising business activities. In this respect fuzzy parameters of linear
programming are modelled by preference-based membership functions. This paper
begins with an introduction and some related research followed by some
fundamentals of fuzzy set theory and technical concepts of fuzzy multiple
objective decision models. Further a real case study of a manufacturing plant
and the implementation of the proposed technique is presented. Empirical
results clearly show the superiority of the fuzzy technique in optimising
individual objective functions when compared to non-fuzzy approach.
Furthermore, for the problem considered, the optimal solution helps to infer
that by incorporating fuzziness in a linear programming model either in
constraints, or both in objective functions and constraints, provides a similar
(or even better) level of satisfaction for obtained results compared to
non-fuzzy linear programming.",N/A
Meta-Learning Evolutionary Artificial Neural Networks,"In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance.",N/A
The Largest Compatible Subset Problem for Phylogenetic Data,"The phylogenetic tree construction is to infer the evolutionary relationship
between species from the experimental data. However, the experimental data are
often imperfect and conflicting each others. Therefore, it is important to
extract the motif from the imperfect data. The largest compatible subset
problem is that, given a set of experimental data, we want to discard the
minimum such that the remaining is compatible. The largest compatible subset
problem can be viewed as the vertex cover problem in the graph theory that has
been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary
Computing (EC) method for this problem. The proposed method combines the EC
approach and the algorithmic approach for special structured graphs. As a
result, the complexity of the problem is dramatically reduced. Experiments were
performed on randomly generated graphs with different edge densities. The
vertex covers produced by the proposed method were then compared to the vertex
covers produced by a 2-approximation algorithm. The experimental results showed
that the proposed method consistently outperformed a classical 2- approximation
algorithm. Furthermore, a significant improvement was found when the graph
density was small.",N/A
A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems,"Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing
technologies that underlie the conception, design and utilization of
intelligent systems. Several works have been done where engineers and
scientists have applied intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a concurrent
fuzzy-neural network approach combining unsupervised and supervised learning
techniques to develop the Tactical Air Combat Decision Support System (TACDSS).
Experiment results clearly demonstrate the efficiency of the proposed
technique.",N/A
"Analysis of Hybrid Soft and Hard Computing Techniques for Forex
  Monitoring Systems","In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.",N/A
Business Intelligence from Web Usage Mining,"The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management, creating adaptive Web
sites, business and support services, personalization, network traffic flow
analysis and so on. In this paper, we present the important concepts of Web
usage mining and its various practical applications. We further present a novel
approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture
of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy
inference system to analyze the Web site visitor trends. A hybrid evolutionary
fuzzy clustering algorithm is proposed in this paper to optimally segregate
similar user interests. The clustered data is then used to analyze the trends
using a Takagi-Sugeno fuzzy inference system learned using a combination of
evolutionary algorithm and neural network learning. Proposed approach is
compared with self-organizing maps (to discover patterns) and several function
approximation techniques like neural networks, linear genetic programming and
Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are
graphically illustrated and the practical significance is discussed in detail.
Empirical results clearly show that the proposed Web usage-mining framework is
efficient.",N/A
"Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic
  Approach for Tactical Air Combat Decision Support System","Normally a decision support system is build to solve problem where
multi-criteria decisions are involved. The knowledge base is the vital part of
the decision support containing the information or data that is used in
decision-making process. This is the field where engineers and scientists have
applied several intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a hybrid
neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference
system for the Tactical Air Combat Decision Support System (TACDSS). Some
simulation results demonstrating the difference of the learning techniques and
are also provided.",N/A
"EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using
  Neural Network Learning and Evolutionary Computation","Several adaptation techniques have been investigated to optimize fuzzy
inference systems. Neural network learning algorithms have been used to
determine the parameters of fuzzy inference system. Such models are often
called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model
there is no guarantee that the neural network learning algorithm converges and
the tuning of fuzzy inference system will be successful. Success of
evolutionary search procedures for optimization of fuzzy inference system is
well proven and established in many application areas. In this paper, we will
explore how the optimization of fuzzy inference systems could be further
improved using a meta-heuristic approach combining neural network learning and
evolutionary computation. The proposed technique could be considered as a
methodology to integrate neural networks, fuzzy inference systems and
evolutionary search procedures. We present the theoretical frameworks and some
experimental results to demonstrate the efficiency of the proposed technique.",N/A
"Optimization of Evolutionary Neural Networks Using Hybrid Learning
  Algorithms","Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity.",N/A
Export Behaviour Modeling Using EvoNF Approach,"The academic literature suggests that the extent of exporting by
multinational corporation subsidiaries (MCS) depends on their product
manufactured, resources, tax protection, customers and markets, involvement
strategy, financial independence and suppliers' relationship with a
multinational corporation (MNC). The aim of this paper is to model the complex
export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order
to determine the actual volume of MCS export output (sales exported). The
proposed fuzzy inference system is optimised by using neural network learning
and evolutionary computation. Empirical results clearly show that the proposed
approach could model the export behaviour reasonable well compared to a direct
neural network approach.",N/A
Traffic Accident Analysis Using Decision Trees and Neural Networks,"The costs of fatalities and injuries due to traffic accident have a great
impact on society. This paper presents our research to model the severity of
injury resulting from traffic accidents using artificial neural networks and
decision trees. We have applied them to an actual data set obtained from the
National Automotive Sampling System (NASS) General Estimates System (GES).
Experiment results reveal that in all the cases the decision tree outperforms
the neural network. Our research analysis also shows that the three most
important factors in fatal injury are: driver's seat belt usage, light
condition of the roadway, and driver's alcohol usage.",N/A
"Short Term Load Forecasting Models in Czech Republic Using Soft
  Computing Paradigms","This paper presents a comparative study of six soft computing models namely
multilayer perceptron networks, Elman recurrent neural network, radial basis
function network, Hopfield model, fuzzy inference system and hybrid fuzzy
neural network for the hourly electricity demand forecast of Czech Republic.
The soft computing models were trained and tested using the actual hourly load
data for seven years. A comparison of the proposed techniques is presented for
predicting 2 day ahead demands for electricity. Simulation results indicate
that hybrid fuzzy neural network and radial basis function networks are the
best candidates for the analysis and forecasting of electricity demand.",N/A
Decision Support Systems Using Intelligent Paradigms,"Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing (SC)
technologies that underlie the conception, design and utilization of
intelligent systems. In this paper, we present different SC paradigms involving
an artificial neural network trained using the scaled conjugate gradient
algorithm, two different fuzzy inference methods optimised using neural network
learning/evolutionary algorithms and regression trees for developing
intelligent decision support systems. We demonstrate the efficiency of the
different algorithms by developing a decision support system for a Tactical Air
Combat Environment (TACE). Some empirical comparisons between the different
algorithms are also provided.",N/A
Regression with respect to sensing actions and partial states,"In this paper, we present a state-based regression function for planning
domains where an agent does not have complete information and may have sensing
actions. We consider binary domains and employ the 0-approximation [Son & Baral
2001] to define the regression function. In binary domains, the use of
0-approximation means using 3-valued states. Although planning using this
approach is incomplete with respect to the full semantics, we adopt it to have
a lower complexity. We prove the soundness and completeness of our regression
formulation with respect to the definition of progression. More specifically,
we show that (i) a plan obtained through regression for a planning problem is
indeed a progression solution of that planning problem, and that (ii) for each
plan found through progression, using regression one obtains that plan or an
equivalent one. We then develop a conditional planner that utilizes our
regression function. We prove the soundness and completeness of our planning
algorithm and present experimental results with respect to several well known
planning problems in the literature.",38 pages
Propositional Defeasible Logic has Linear Complexity,"Defeasible logic is a rule-based nonmonotonic logic, with both strict and
defeasible rules, and a priority relation on rules. We show that inference in
the propositional form of the logic can be performed in linear time. This
contrasts markedly with most other propositional nonmonotonic logics, in which
inference is intractable.","Appeared in Theory and Practice of Logic Programming, vol. 1, no. 6,
  2001"
Pruning Search Space in Defeasible Argumentation,"Defeasible argumentation has experienced a considerable growth in AI in the
last decade. Theoretical results have been combined with development of
practical applications in AI & Law, Case-Based Reasoning and various
knowledge-based systems. However, the dialectical process associated with
inference is computationally expensive. This paper focuses on speeding up this
inference process by pruning the involved search space. Our approach is
twofold. On one hand, we identify distinguished literals for computing defeat.
On the other hand, we restrict ourselves to a subset of all possible
conflicting arguments by introducing dialectical constraints.",11 pages
"A proposal to design expert system for the calculations in the domain of
  QFT","Main purposes of the paper are followings: 1) To show examples of the
calculations in domain of QFT via ``derivative rules'' of an expert system; 2)
To consider advantages and disadvantage that technology of the calculations; 3)
To reflect about how one would develop new physical theories, what knowledge
would be useful in their investigations and how this problem can be connected
with designing an expert system.",N/A
"A New Approach to Draw Detection by Move Repetition in Computer Chess
  Programming","We will try to tackle both the theoretical and practical aspects of a very
important problem in chess programming as stated in the title of this article -
the issue of draw detection by move repetition. The standard approach that has
so far been employed in most chess programs is based on utilising positional
matrices in original and compressed format as well as on the implementation of
the so-called bitboard format.
  The new approach that we will be trying to introduce is based on using
variant strings generated by the search algorithm (searcher) during the tree
expansion in decision making. We hope to prove that this approach is more
efficient than the standard treatment of the issue, especially in positions
with few pieces (endgames). To illustrate what we have in mind a machine
language routine that implements our theoretical assumptions is attached. The
routine is part of the Axon chess program, developed by the authors. Axon, in
its current incarnation, plays chess at master strength (ca. 2400-2450 Elo,
based on both Axon vs computer programs and Axon vs human masters in over 3000
games altogether).","15 pages, 4 figures"
"Autogenic Training With Natural Language Processing Modules: A Recent
  Tool For Certain Neuro Cognitive Studies","Learning to respond to voice-text input involves the subject's ability in
understanding the phonetic and text based contents and his/her ability to
communicate based on his/her experience. The neuro-cognitive facility of the
subject has to support two important domains in order to make the learning
process complete. In many cases, though the understanding is complete, the
response is partial. This is one valid reason why we need to support the
information from the subject with scalable techniques such as Natural Language
Processing (NLP) for abstraction of the contents from the output. This paper
explores the feasibility of using NLP modules interlaced with Neural Networks
to perform the required task in autogenic training related to medical
applications.","2 Pages. Proceedings of 11th International Congress on Biological &
  Medical Engineering, Singapore (IEEE-EMBS & IFMBE endorsed)"
Generalized Evolutionary Algorithm based on Tsallis Statistics,"Generalized evolutionary algorithm based on Tsallis canonical distribution is
proposed. The algorithm uses Tsallis generalized canonical distribution to
weigh the configurations for `selection' instead of Gibbs-Boltzmann
distribution. Our simulation results show that for an appropriate choice of
non-extensive index that is offered by Tsallis statistics, evolutionary
algorithms based on this generalization outperform algorithms based on
Gibbs-Boltzmann distribution.","Submitted to Physical Review E, 5 pages, 6 figures"
Decomposition Based Search - A theoretical and experimental evaluation,"In this paper we present and evaluate a search strategy called Decomposition
Based Search (DBS) which is based on two steps: subproblem generation and
subproblem solution. The generation of subproblems is done through value
ranking and domain splitting. Subdomains are explored so as to generate,
according to the heuristic chosen, promising subproblems first.
  We show that two well known search strategies, Limited Discrepancy Search
(LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First
we present a tuning of DBS that visits the same search nodes as IB, but avoids
restarts. Then we compare both theoretically and computationally DBS and LDS
using the same heuristic. We prove that DBS has a higher probability of being
successful than LDS on a comparable number of nodes, under realistic
assumptions. Experiments on a constraint satisfaction problem and an
optimization problem show that DBS is indeed very effective if compared to LDS.","16 pages, 8 figures. LIA Technical Report LIA00203, University of
  Bologna, 2003"
Postponing Branching Decisions,"Solution techniques for Constraint Satisfaction and Optimisation Problems
often make use of backtrack search methods, exploiting variable and value
ordering heuristics. In this paper, we propose and analyse a very simple method
to apply in case the value ordering heuristic produces ties: postponing the
branching decision. To this end, we group together values in a tie, branch on
this sub-domain, and defer the decision among them to lower levels of the
search tree. We show theoretically and experimentally that this simple
modification can dramatically improve the efficiency of the search strategy.
Although in practise similar methods may have been applied already, to our
knowledge, no empirical or theoretical study has been proposed in the
literature to identify when and to what extent this strategy should be used.","11 pages, 3 figures"
Reduced cost-based ranking for generating promising subproblems,"In this paper, we propose an effective search procedure that interleaves two
steps: subproblem generation and subproblem solution. We mainly focus on the
first part. It consists of a variable domain value ranking based on reduced
costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search
tree, the most promising subproblems first. An interesting result is that
reduced costs provide a very precise ranking that allows to almost always find
the optimal solution in the first generated subproblem, even if its dimension
is significantly smaller than that of the original problem. Concerning the
proof of optimality, we exploit a way to increase the lower bound for
subproblems at higher discrepancies. We show experimental results on the TSP
and its time constrained variant to show the effectiveness of the proposed
approach, but the technique could be generalized for other problems.","15 pages, 1 figure. Accepted at CP 2002"
A Simple Proportional Conflict Redistribution Rule,"One proposes a first alternative rule of combination to WAO (Weighted Average
Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called
Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are
particular cases of WO (the Weighted Operator) because the conflicting mass is
redistributed with respect to some weighting factors. In this first PCR rule,
the proportionalization is done for each non-empty set with respect to the
non-zero sum of its corresponding mass matrix - instead of its mass column
average as in WAO, but the results are the same as Ph. Smets has pointed out.
Also, we extend WAO (which herein gives no solution) for the degenerate case
when all column sums of all non-empty sets are zero, and then the conflicting
mass is transferred to the non-empty disjunctive form of all non-empty sets
together; but if this disjunctive form happens to be empty, then one considers
an open world (i.e. the frame of discernment might contain new hypotheses) and
thus all conflicting mass is transferred to the empty set. In addition to WAO,
we propose a general formula for PCR1 (WAO for non-degenerate cases).",21 pages
"An Algorithm for Quasi-Associative and Quasi-Markovian Rules of
  Combination in Information Fusion","In this paper one proposes a simple algorithm of combining the fusion rules,
those rules which first use the conjunctive rule and then the transfer of
conflicting mass to the non-empty sets, in such a way that they gain the
property of associativity and fulfill the Markovian requirement for dynamic
fusion. Also, a new rule, SDL-improved, is presented.",9 pages
FLUX: A Logic Programming Method for Reasoning Agents,"FLUX is a programming method for the design of agents that reason logically
about their actions and sensor information in the presence of incomplete
knowledge. The core of FLUX is a system of Constraint Handling Rules, which
enables agents to maintain an internal model of their environment by which they
control their own behavior. The general action representation formalism of the
fluent calculus provides the formal semantics for the constraint solver. FLUX
exhibits excellent computational behavior due to both a carefully restricted
expressiveness and the inference paradigm of progression.",N/A
"Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection
  Scheme in Evolutionary Algorithms","Boltzmann selection is an important selection mechanism in evolutionary
algorithms as it has theoretical properties which help in theoretical analysis.
However, Boltzmann selection is not used in practice because a good annealing
schedule for the `inverse temperature' parameter is lacking. In this paper we
propose a Cauchy annealing schedule for Boltzmann selection scheme based on a
hypothesis that selection-strength should increase as evolutionary process goes
on and distance between two selection strengths should decrease for the process
to converge. To formalize these aspects, we develop formalism for selection
mechanisms using fitness distributions and give an appropriate measure for
selection-strength. In this paper, we prove an important result, by which we
derive an annealing schedule called Cauchy annealing schedule. We demonstrate
the novelty of proposed annealing schedule using simulations in the framework
of genetic algorithms.",N/A
Proportional Conflict Redistribution Rules for Information Fusion,"In this paper we propose five versions of a Proportional Conflict
Redistribution rule (PCR) for information fusion together with several
examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of
the rules and also the exactitude of the redistribution of conflicting masses.
PCR1 restricted from the hyper-power set to the power set and without
degenerate cases gives the same result as the Weighted Average Operator (WAO)
proposed recently by J{\o}sang, Daniel and Vannoorenberghe but does not satisfy
the neutrality property of vacuous belief assignment. That's why improved PCR
rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's
rules. The PCR rules redistribute the conflicting mass, after the conjunctive
rule has been applied, proportionally with some functions depending on the
masses assigned to their corresponding columns in the mass matrix. There are
infinitely many ways these functions (weighting factors) can be chosen
depending on the complexity one wants to deal with in specific applications and
fusion systems. Any fusion combination rule is at some degree ad-hoc.",41 pages
The Generalized Pignistic Transformation,"This paper presents in detail the generalized pignistic transformation (GPT)
succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a
tool for decision process. The GPT allows to provide a subjective probability
measure from any generalized basic belief assignment given by any corpus of
evidence. We mainly focus our presentation on the 3D case and provide the
complete result obtained by the GPT and its validation drawn from the
probability theory.","8 pages, 3 graphs, many tables. The Seventh International Conference
  on Information Fusion, Stockholm, Sweden, 28 June - 1 July 2004"
Unification of Fusion Theories,"Since no fusion theory neither rule fully satisfy all needed applications,
the author proposes a Unification of Fusion Theories and a combination of
fusion rules in solving problems/applications. For each particular application,
one selects the most appropriate model, rule(s), and algorithm of
implementation. We are working in the unification of the fusion theories and
rules, which looks like a cooking recipe, better we'd say like a logical chart
for a computer programmer, but we don't see another method to comprise/unify
all things. The unification scenario presented herein, which is now in an
incipient form, should periodically be updated incorporating new discoveries
from the fusion and engineering research.",14 pages
Normal forms for Answer Sets Programming,"Normal forms for logic programs under stable/answer set semantics are
introduced. We argue that these forms can simplify the study of program
properties, mainly consistency. The first normal form, called the {\em kernel}
of the program, is useful for studying existence and number of answer sets. A
kernel program is composed of the atoms which are undefined in the Well-founded
semantics, which are those that directly affect the existence of answer sets.
The body of rules is composed of negative literals only. Thus, the kernel form
tends to be significantly more compact than other formulations. Also, it is
possible to check consistency of kernel programs in terms of colorings of the
Extended Dependency Graph program representation which we previously developed.
The second normal form is called {\em 3-kernel.} A 3-kernel program is composed
of the atoms which are undefined in the Well-founded semantics. Rules in
3-kernel programs have at most two conditions, and each rule either belongs to
a cycle, or defines a connection between cycles. 3-kernel programs may have
positive conditions. The 3-kernel normal form is very useful for the static
analysis of program consistency, i.e., the syntactic characterization of
existence of answer sets. This result can be obtained thanks to a novel
graph-like representation of programs, called Cycle Graph which presented in
the companion article \cite{Cos04b}.","15 pages, To appear in Theory and Practice of Logic Programming
  (TPLP)"
"An In-Depth Look at Information Fusion Rules & the Unification of Fusion
  Theories","This paper may look like a glossary of the fusion rules and we also introduce
new ones presenting their formulas and examples: Conjunctive, Disjunctive,
Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule,
Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache
classical and hybrid rules, Murphy's average rule,
Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as
particular cases: Iganaki's parameterized rule, Weighting Average Operator,
minC (M. Daniel), and newly Proportional Conflict Redistribution rules
(Smarandache-Dezert) among which PCR5 is the most exact way of redistribution
of the conflicting mass to non-empty sets following the path of the conjunctive
rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus
Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and
three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to
information fusion (Tchamova-Smarandache). Introducing the degree of union and
degree of inclusion with respect to the cardinal of sets not with the fuzzy set
point of view, besides that of intersection, many fusion rules can be improved.
There are corner cases where each rule might have difficulties working or may
not get an expected result.","27 pages. To be presented at NASA Langley Research Center (Hampton,
  Virginia), on November 5th, 2004"
Intransitivity and Vagueness,"There are many examples in the literature that suggest that
indistinguishability is intransitive, despite the fact that the
indistinguishability relation is typically taken to be an equivalence relation
(and thus transitive). It is shown that if the uncertainty perception and the
question of when an agent reports that two things are indistinguishable are
both carefully modeled, the problems disappear, and indistinguishability can
indeed be taken to be an equivalence relation. Moreover, this model also
suggests a logic of vagueness that seems to solve many of the problems related
to vagueness discussed in the philosophical literature. In particular, it is
shown here how the logic can handle the sorites paradox.","A preliminary version of this paper appears in Principles of
  Knowledge Representation and Reasoning: Proceedings of the Ninth
  International Conference (KR 2004)"
"Sleeping Beauty Reconsidered: Conditioning and Reflection in
  Asynchronous Systems","A careful analysis of conditioning in the Sleeping Beauty problem is done,
using the formal model for reasoning about knowledge and probability developed
by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as
revealing problems with conditioning in the presence of imperfect recall, the
analysis done here reveals that the problems are not so much due to imperfect
recall as to asynchrony. The implications of this analysis for van Fraassen's
Reflection Principle and Savage's Sure-Thing Principle are considered.","A preliminary version of this paper appears in Principles of
  Knowledge Representation and Reasoning: Proceedings of the Ninth
  International Conference (KR 2004). This version will appear in Oxford
  Studies in Epistemology"
Bounded Input Bounded Predefined Control Bounded Output,"The paper is an attempt to generalize a methodology, which is similar to the
bounded-input bounded-output method currently widely used for the system
stability studies. The presented earlier methodology allows decomposition of
input space into bounded subspaces and defining for each subspace its bounding
surface. It also defines a corresponding predefined control, which maps any
point of a bounded input into a desired bounded output subspace. This
methodology was improved by providing a mechanism for the fast defining a
bounded surface. This paper presents enhanced bounded-input
bounded-predefined-control bounded-output approach, which provides adaptability
feature to the control and allows transferring of a controlled system along a
suboptimal trajectory.","8 pages, 6 figures"
"Generating Conditional Probabilities for Bayesian Networks: Easing the
  Knowledge Acquisition Problem","The number of probability distributions required to populate a conditional
probability table (CPT) in a Bayesian network, grows exponentially with the
number of parent-nodes associated with that table. If the table is to be
populated through knowledge elicited from a domain expert then the sheer
magnitude of the task forms a considerable cognitive barrier. In this paper we
devise an algorithm to populate the CPT while easing the extent of knowledge
acquisition. The input to the algorithm consists of a set of weights that
quantify the relative strengths of the influences of the parent-nodes on the
child-node, and a set of probability distributions the number of which grows
only linearly with the number of associated parent-nodes. These are elicited
from the domain expert. The set of probabilities are obtained by taking into
consideration the heuristics that experts use while arriving at probabilistic
estimations. The algorithm is used to populate the CPT by computing appropriate
weighted sums of the elicited distributions. We invoke the methods of
information geometry to demonstrate how these weighted sums capture the
expert's judgemental strategy.","24pages, 2figures"
Comparing Multi-Target Trackers on Different Force Unit Levels,"Consider the problem of tracking a set of moving targets. Apart from the
tracking result, it is often important to know where the tracking fails, either
to steer sensors to that part of the state-space, or to inform a human operator
about the status and quality of the obtained information. An intuitive quality
measure is the correlation between two tracking results based on uncorrelated
observations. In the case of Bayesian trackers such a correlation measure could
be the Kullback-Leibler difference.
  We focus on a scenario with a large number of military units moving in some
terrain. The units are observed by several types of sensors and ""meta-sensors""
with force aggregation capabilities. The sensors register units of different
size. Two separate multi-target probability hypothesis density (PHD) particle
filters are used to track some type of units (e.g., companies) and their
sub-units (e.g., platoons), respectively, based on observations of units of
those sizes. Each observation is used in one filter only.
  Although the state-space may well be the same in both filters, the posterior
PHD distributions are not directly comparable -- one unit might correspond to
three or four spatially distributed sub-units. Therefore, we introduce a
mapping function between distributions for different unit size, based on
doctrine knowledge of unit configuration.
  The mapped distributions can now be compared -- locally or globally -- using
some measure, which gives the correlation between two PHD distributions in a
bounded volume of the state-space. To locate areas where the tracking fails, a
discretized quality map of the state-space can be generated by applying the
measure locally to different parts of the space.",9 pages
Extremal optimization for sensor report pre-processing,"We describe the recently introduced extremal optimization algorithm and apply
it to target detection and association problems arising in pre-processing for
multi-target tracking.
  Here we consider the problem of pre-processing for multiple target tracking
when the number of sensor reports received is very large and arrives in large
bursts. In this case, it is sometimes necessary to pre-process reports before
sending them to tracking modules in the fusion system. The pre-processing step
associates reports to known tracks (or initializes new tracks for reports on
objects that have not been seen before). It could also be used as a pre-process
step before clustering, e.g., in order to test how many clusters to use.
  The pre-processing is done by solving an approximate version of the original
problem. In this approximation, not all pair-wise conflicts are calculated. The
approximation relies on knowing how many such pair-wise conflicts that are
necessary to compute. To determine this, results on phase-transitions occurring
when coloring (or clustering) large random instances of a particular graph
ensemble are used.",10 pages
"The Combination of Paradoxical, Uncertain, and Imprecise Sources of
  Information based on DSmT and Neutro-Fuzzy Inference","The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been, and
still remains today, of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this chapter, we
present a survey of our recent theory of plausible and paradoxical reasoning,
known as Dezert-Smarandache Theory (DSmT) in the literature, developed for
dealing with imprecise, uncertain and paradoxical sources of information. We
focus our presentation here rather on the foundations of DSmT, and on the two
important new rules of combination, than on browsing specific applications of
DSmT available in literature. Several simple examples are given throughout the
presentation to show the efficiency and the generality of this new approach.
The last part of this chapter concerns the presentation of the neutrosophic
logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and
neutrosophic logic are useful tools in decision making after fusioning the
information using the DSm hybrid rule of combination of masses.",20 pages
"Learning to automatically detect features for mobile robots using
  second-order Hidden Markov Models","In this paper, we propose a new method based on Hidden Markov Models to
interpret temporal sequences of sensor data from mobile robots to automatically
detect features. Hidden Markov Models have been used for a long time in pattern
recognition, especially in speech recognition. Their main advantages over other
methods (such as neural networks) are their ability to model noisy temporal
signals of variable length. We show in this paper that this approach is well
suited for interpretation of temporal sequences of mobile-robot sensor data. We
present two distinct experiments and results: the first one in an indoor
environment where a mobile robot learns to detect features like open doors or
T-intersections, the second one in an outdoor environment where a different
mobile robot has to identify situations like climbing a hill or crossing a
rock.",2004
Inferring knowledge from a large semantic network,"In this paper, we present a rich semantic network based on a differential
analysis. We then detail implemented measures that take into account common and
differential features between words. In a last section, we describe some
industrial applications.",N/A
"Towards Automated Integration of Guess and Check Programs in Answer Set
  Programming: A Meta-Interpreter and Applications","Answer set programming (ASP) with disjunction offers a powerful tool for
declaratively representing and solving hard problems. Many NP-complete problems
can be encoded in the answer set semantics of logic programs in a very concise
and intuitive way, where the encoding reflects the typical ""guess and check""
nature of NP problems: The property is encoded in a way such that polynomial
size certificates for it correspond to stable models of a program. However, the
problem-solving capacity of full disjunctive logic programs (DLPs) is beyond
NP, and captures a class of problems at the second level of the polynomial
hierarchy. While these problems also have a clear ""guess and check"" structure,
finding an encoding in a DLP reflecting this structure may sometimes be a
non-obvious task, in particular if the ""check"" itself is a coNP-complete
problem; usually, such problems are solved by interleaving separate guess and
check programs, where the check is expressed by inconsistency of the check
program. In this paper, we present general transformations of head-cycle free
(extended) disjunctive logic programs into stratified and positive (extended)
disjunctive logic programs based on meta-interpretation techniques. The answer
sets of the original and the transformed program are in simple correspondence,
and, moreover, inconsistency of the original program is indicated by a
designated answer set of the transformed program. Our transformations
facilitate the integration of separate ""guess"" and ""check"" programs, which are
often easy to obtain, automatically into a single disjunctive logic program.
Our results complement recent results on meta-interpretation in ASP, and extend
methods and techniques for a declarative ""guess and check"" problem solving
paradigm through ASP.",To appear in Theory and Practice of Logic Programming (TPLP)
Clever Search: A WordNet Based Wrapper for Internet Search Engines,"This paper presents an approach to enhance search engines with information
about word senses available in WordNet. The approach exploits information about
the conceptual relations within the lexical-semantic net. In the wrapper for
search engines presented, WordNet information is used to specify user's request
or to classify the results of a publicly available web search engine, like
google, yahoo, etc.",N/A
Issues in Exploiting GermaNet as a Resource in Real Applications,"This paper reports about experiments with GermaNet as a resource within
domain specific document analysis. The main question to be answered is: How is
the coverage of GermaNet in a specific domain? We report about results of a
field test of GermaNet for analyses of autopsy protocols and present a sketch
about the integration of GermaNet inside XDOC. Our remarks will contribute to a
GermaNet user's wish list.","10 pages, 3 figures"
Transforming Business Rules Into Natural Language Text,"The aim of the project presented in this paper is to design a system for an
NLG architecture, which supports the documentation process of eBusiness models.
A major task is to enrich the formal description of an eBusiness model with
additional information needed in an NLG task.",3 pages
Corpus based Enrichment of GermaNet Verb Frames,"Lexical semantic resources, like WordNet, are often used in real applications
of natural language document processing. For example, we integrated GermaNet in
our document suite XDOC of processing of German forensic autopsy protocols. In
addition to the hypernymy and synonymy relation, we want to adapt GermaNet's
verb frames for our analysis. In this paper we outline an approach for the
domain related enrichment of GermaNet verb frames by corpus based syntactic and
co-occurred data analyses of real documents.",4 pages
Context Related Derivation of Word Senses,"Real applications of natural language document processing are very often
confronted with domain specific lexical gaps during the analysis of documents
of a new domain. This paper describes an approach for the derivation of domain
specific concepts for the extension of an existing ontology. As resources we
need an initial ontology and a partially processed corpus of a domain. We
exploit the specific characteristic of the sublanguage in the corpus. Our
approach is based on syntactical structures (noun phrases) and compound
analyses to extract information required for the extension of GermaNet's
lexical resources.","5 pages, 2 figures"
Transforming and Enriching Documents for the Semantic Web,"We suggest to employ techniques from Natural Language Processing (NLP) and
Knowledge Representation (KR) to transform existing documents into documents
amenable for the Semantic Web. Semantic Web documents have at least part of
their semantics and pragmatics marked up explicitly in both a machine
processable as well as human readable manner. XML and its related standards
(XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and
methodologies developed for different application scenarios.","10 pages, 1 figure"
Perspectives for Strong Artificial Life,"This text introduces the twin deadlocks of strong artificial life.
Conceptualization of life is a deadlock both because of the existence of a
continuum between the inert and the living, and because we only know one
instance of life. Computationalism is a second deadlock since it remains a
matter of faith. Nevertheless, artificial life realizations quickly progress
and recent constructions embed an always growing set of the intuitive
properties of life. This growing gap between theory and realizations should
sooner or later crystallize in some kind of paradigm shift and then give clues
to break the twin deadlocks.","19 pages, 5 figures"
"Neural-Network Techniques for Visual Mining Clinical
  Electroencephalograms","In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs), the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs.",N/A
"Estimating Classification Uncertainty of Bayesian Decision Tree
  Technique on Financial Data","Bayesian averaging over classification models allows the uncertainty of
classification outcomes to be evaluated, which is of crucial importance for
making reliable decisions in applications such as financial in which risks have
to be estimated. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the diversity of a
classifier ensemble and the required performance. The interpretability of
classification models can also give useful information for experts responsible
for making reliable classifications. For this reason Decision Trees (DTs) seem
to be attractive classification models. The required diversity of the DT
ensemble can be achieved by using the Bayesian model averaging all possible
DTs. In practice, the Bayesian approach can be implemented on the base of a
Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior
distribution. For sampling large DTs, the MCMC method is extended by Reversible
Jump technique which allows inducing DTs under given priors. For the case when
the prior information on the DT size is unavailable, the sweeping technique
defining the prior implicitly reveals a better performance. Within this Chapter
we explore the classification uncertainty of the Bayesian MCMC techniques on
some datasets from the StatLog Repository and real financial data. The
classification uncertainty is compared within an Uncertainty Envelope technique
dealing with the class posterior distribution and a given confidence
probability. This technique provides realistic estimates of the classification
uncertainty which can be easily interpreted in statistical terms with the aim
of risk evaluation.",N/A
"Comparison of the Bayesian and Randomised Decision Tree Ensembles within
  an Uncertainty Envelope Technique","Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of
classification outcomes that is of crucial importance for safety critical
applications. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the classifier diversity and
the required performance. The interpretability of MCSs can also give useful
information for experts responsible for making reliable classifications. For
this reason Decision Trees (DTs) seem to be attractive classification models
for experts. The required diversity of MCSs exploiting such classification
models can be achieved by using two techniques, the Bayesian model averaging
and the randomised DT ensemble. Both techniques have revealed promising results
when applied to real-world problems. In this paper we experimentally compare
the classification uncertainty of the Bayesian model averaging with a
restarting strategy and the randomised DT ensemble on a synthetic dataset and
some domain problems commonly used in the machine learning community. To make
the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo
technique. The classification uncertainty is evaluated within an Uncertainty
Envelope technique dealing with the class posterior distribution and a given
confidence probability. Exploring a full posterior distribution, this technique
produces realistic estimates which can be easily interpreted in statistical
terms. In our experiments we found out that the Bayesian DTs are superior to
the randomised DT ensembles within the Uncertainty Envelope technique.",N/A
Proceedings of the Pacific Knowledge Acquisition Workshop 2004,"Artificial intelligence (AI) research has evolved over the last few decades
and knowledge acquisition research is at the core of AI research. PKAW-04 is
one of three international knowledge acquisition workshops held in the
Pacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong
emphasis on incremental knowledge acquisition, machine learning, neural nets
and active mining.
  The proceedings contain 19 papers that were selected by the program committee
among 24 submitted papers. All papers were peer reviewed by at least two
reviewers. The papers in these proceedings cover the methods and tools as well
as the applications related to develop expert systems or knowledge based
systems.",N/A
Temporal and Spatial Data Mining with Second-Order Hidden Models,"In the frame of designing a knowledge discovery system, we have developed
stochastic models based on high-order hidden Markov models. These models are
capable to map sequences of data into a Markov chain in which the transitions
between the states depend on the \texttt{n} previous states according to the
order of the model. We study the process of achieving information extraction
fromspatial and temporal data by means of an unsupervised classification. We
use therefore a French national database related to the land use of a region,
named Teruti, which describes the land use both in the spatial and temporal
domain. Land-use categories (wheat, corn, forest, ...) are logged every year on
each site regularly spaced in the region. They constitute a temporal sequence
of images in which we look for spatial and temporal dependencies. The temporal
segmentation of the data is done by means of a second-order Hidden Markov Model
(\hmmd) that appears to have very good capabilities to locate stationary
segments, as shown in our previous work in speech recognition. Thespatial
classification is performed by defining a fractal scanning ofthe images with
the help of a Hilbert-Peano curve that introduces atotal order on the sites,
preserving the relation ofneighborhood between the sites. We show that the
\hmmd performs aclassification that is meaningful for the agronomists.Spatial
and temporal classification may be achieved simultaneously by means of a 2
levels \hmmd that measures the \aposteriori probability to map a temporal
sequence of images onto a set of hidden classes.",N/A
An ontological approach to the construction of problem-solving models,"Our ongoing work aims at defining an ontology-centered approach for building
expertise models for the CommonKADS methodology. This approach (which we have
named ""OntoKADS"") is founded on a core problem-solving ontology which
distinguishes between two conceptualization levels: at an object level, a set
of concepts enable us to define classes of problem-solving situations, and at a
meta level, a set of meta-concepts represent modeling primitives. In this
article, our presentation of OntoKADS will focus on the core ontology and, in
particular, on roles - the primitive situated at the interface between domain
knowledge and reasoning, and whose ontological status is still much debated. We
first propose a coherent, global, ontological framework which enables us to
account for this primitive. We then show how this novel characterization of the
primitive allows definition of new rules for the construction of expertise
models.",N/A
A Constrained Object Model for Configuration Based Workflow Composition,"Automatic or assisted workflow composition is a field of intense research for
applications to the world wide web or to business process modeling. Workflow
composition is traditionally addressed in various ways, generally via theorem
proving techniques. Recent research observed that building a composite workflow
bears strong relationships with finite model search, and that some workflow
languages can be defined as constrained object metamodels . This lead to
consider the viability of applying configuration techniques to this problem,
which was proven feasible. Constrained based configuration expects a
constrained object model as input. The purpose of this document is to formally
specify the constrained object model involved in ongoing experiments and
research using the Z specification language.","This is an extended version of the article published at BPM'05, Third
  International Conference on Business Process Management, Nancy France"
A Study for the Feature Core of Dynamic Reduct,"To the reduct problems of decision system, the paper proposes the notion of
dynamic core according to the dynamic reduct model. It describes various formal
definitions of dynamic core, and discusses some properties about dynamic core.
All of these show that dynamic core possesses the essential characters of the
feature core.",9 pages
"Two-dimensional cellular automata and the analysis of correlated time
  series","Correlated time series are time series that, by virtue of the underlying
process to which they refer, are expected to influence each other strongly. We
introduce a novel approach to handle such time series, one that models their
interaction as a two-dimensional cellular automaton and therefore allows them
to be treated as a single entity. We apply our approach to the problems of
filling gaps and predicting values in rainfall time series. Computational
results show that the new approach compares favorably to Kalman smoothing and
filtering.",N/A
ATNoSFERES revisited,"ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which
the rules are represented as edges of an Augmented Transition Network.
Genotypes are strings of tokens of a stack-based language, whose execution
builds the labeled graph. The original ATNoSFERES, using a bitstring to
represent the language tokens, has been favorably compared in previous work to
several Michigan style LCSs architectures in the context of Non Markov
problems. Several modifications of ATNoSFERES are proposed here: the most
important one conceptually being a representational change: each token is now
represented by an integer, hence the genotype is a string of integers; several
other modifications of the underlying grammar language are also proposed. The
resulting ATNoSFERES-II is validated on several standard animat Non Markov
problems, on which it outperforms all previously published results in the LCS
literature. The reasons for these improvement are carefully analyzed, and some
assumptions are proposed on the underlying mechanisms in order to explain these
good results.",N/A
Planning with Preferences using Logic Programming,"We present a declarative language, PP, for the high-level specification of
preferences between possible solutions (or trajectories) of a planning problem.
This novel language allows users to elegantly express non-trivial,
multi-dimensional preferences and priorities over such preferences. The
semantics of PP allows the identification of most preferred trajectories for a
given goal. We also provide an answer set programming implementation of
planning problems with PP preferences.","47 pages, to appear in TPLP"
"Clustering Mixed Numeric and Categorical Data: A Cluster Ensemble
  Approach","Clustering is a widely used technique in data mining applications for
discovering patterns in underlying data. Most traditional clustering algorithms
are limited to handling datasets that contain either numeric or categorical
attributes. However, datasets with mixed types of attributes are common in real
life data mining applications. In this paper, we propose a novel
divide-and-conquer technique to solve this problem. First, the original mixed
dataset is divided into two sub-datasets: the pure categorical dataset and the
pure numeric dataset. Next, existing well established clustering algorithms
designed for different types of datasets are employed to produce corresponding
clusters. Last, the clustering results on the categorical and numeric dataset
are combined as a categorical dataset, on which the categorical data clustering
algorithm is used to get the final clusters. Our contribution in this paper is
to provide an algorithm framework for the mixed attributes clustering problem,
in which existing clustering algorithms can be easily integrated, the
capabilities of different kinds of clustering algorithms and characteristics of
different types of datasets could be fully exploited. Comparisons with other
clustering algorithms on real life datasets illustrate the superiority of our
approach.",14 pages
K-Histograms: An Efficient Clustering Algorithm for Categorical Dataset,"Clustering categorical data is an integral part of data mining and has
attracted much attention recently. In this paper, we present k-histogram, a new
efficient algorithm for clustering categorical data. The k-histogram algorithm
extends the k-means algorithm to categorical domain by replacing the means of
clusters with histograms, and dynamically updates histograms in the clustering
process. Experimental results on real datasets show that k-histogram algorithm
can produce better clustering results than k-modes algorithm, the one related
with our work most closely.",11 pages
"Integration of the DOLCE top-level ontology into the OntoSpec
  methodology","This report describes a new version of the OntoSpec methodology for ontology
building. Defined by the LaRIA Knowledge Engineering Team (University of
Picardie Jules Verne, Amiens, France), OntoSpec aims at helping builders to
model ontological knowledge (upstream of formal representation). The
methodology relies on a set of rigorously-defined modelling primitives and
principles. Its application leads to the elaboration of a semi-informal
ontology, which is independent of knowledge representation languages. We
recently enriched the OntoSpec methodology by endowing it with a new resource,
the DOLCE top-level ontology defined at the LOA (IST-CNR, Trento, Italy). The
goal of this integration is to provide modellers with additional help in
structuring application ontologies, while maintaining independence
vis-\`{a}-vis formal representation languages. In this report, we first provide
an overview of the OntoSpec methodology's general principles and then describe
the DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a
specification of DOLCE in the semi-informal OntoSpec language) is presented in
an appendix.",N/A
"Using Interval Particle Filtering for Marker less 3D Human Motion
  Capture","In this paper we present a new approach for marker less human motion capture
from conventional camera feeds. The aim of our study is to recover 3D positions
of key points of the body that can serve for gait analysis. Our approach is
based on foreground segmentation, an articulated body model and particle
filters. In order to be generic and simple no restrictive dynamic modelling was
used. A new modified particle filtering algorithm was introduced. It is used
efficiently to search the model configuration space. This new algorithm which
we call Interval Particle Filtering reorganizes the configurations search space
in an optimal deterministic way and proved to be efficient in tracking natural
human movement. Results for human motion capture from a single camera are
presented and compared to results obtained from a marker based system. The
system proved to be able to track motion successfully even in partial
occlusions.",N/A
Markerless Human Motion Capture for Gait Analysis,"The aim of our study is to detect balance disorders and a tendency towards
the falls in the elderly, knowing gait parameters. In this paper we present a
new tool for gait analysis based on markerless human motion capture, from
camera feeds. The system introduced here, recovers the 3D positions of several
key points of the human body while walking. Foreground segmentation, an
articulated body model and particle filtering are basic elements of our
approach. No dynamic model is used thus this system can be described as generic
and simple to implement. A modified particle filtering algorithm, which we call
Interval Particle Filtering, is used to reorganise and search through the
model's configurations search space in a deterministic optimal way. This
algorithm was able to perform human movement tracking with success. Results
from the treatment of a single cam feeds are shown and compared to results
obtained using a marker based human motion capture system.",N/A
Evidence with Uncertain Likelihoods,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function \mu_h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",21 pages. A preliminary version appeared in the Proceedings of UAI'05
"Neuronal Spectral Analysis of EEG and Expert Knowledge Integration for
  Automatic Classification of Sleep Stages","Being able to analyze and interpret signal coming from electroencephalogram
(EEG) recording can be of high interest for many applications including medical
diagnosis and Brain-Computer Interfaces. Indeed, human experts are today able
to extract from this signal many hints related to physiological as well as
cognitive states of the recorded subject and it would be very interesting to
perform such task automatically but today no completely automatic system
exists. In previous studies, we have compared human expertise and automatic
processing tools, including artificial neural networks (ANN), to better
understand the competences of each and determine which are the difficult
aspects to integrate in a fully automatic system. In this paper, we bring more
elements to that study in reporting the main results of a practical experiment
which was carried out in an hospital for sleep pathology study. An EEG
recording was studied and labeled by a human expert and an ANN. We describe
here the characteristics of the experiment, both human and neuronal procedure
of analysis, compare their performances and point out the main limitations
which arise from this study.",N/A
"An efficient memetic, permutation-based evolutionary algorithm for
  real-world train timetabling","Train timetabling is a difficult and very tightly constrained combinatorial
problem that deals with the construction of train schedules. We focus on the
particular problem of local reconstruction of the schedule following a small
perturbation, seeking minimisation of the total accumulated delay by adapting
times of departure and arrival for each train and allocation of resources
(tracks, routing nodes, etc.). We describe a permutation-based evolutionary
algorithm that relies on a semi-greedy heuristic to gradually reconstruct the
schedule by inserting trains one after the other following the permutation.
This algorithm can be hybridised with ILOG commercial MIP programming tool
CPLEX in a coarse-grained manner: the evolutionary part is used to quickly
obtain a good but suboptimal solution and this intermediate solution is refined
using CPLEX. Experimental results are presented on a large real-world case
involving more than one million variables and 2 million constraints. Results
are surprisingly good as the evolutionary algorithm, alone or hybridised,
produces excellent solutions much faster than CPLEX alone.",N/A
Evolutionary Computing,"Evolutionary computing (EC) is an exciting development in Computer Science.
It amounts to building, applying and studying algorithms based on the Darwinian
principles of natural selection. In this paper we briefly introduce the main
concepts behind evolutionary computing. We present the main components all
evolutionary algorithms (EA), sketch the differences between different types of
EAs and survey application areas ranging from optimization, modeling and
simulation to entertainment.",N/A
"Towards a Hierarchical Model of Consciousness, Intelligence, Mind and
  Body",This article is taken out.,"12 pages, 2 figures"
Evolution of Voronoi based Fuzzy Recurrent Controllers,"A fuzzy controller is usually designed by formulating the knowledge of a
human expert into a set of linguistic variables and fuzzy rules. Among the most
successful methods to automate the fuzzy controllers development process are
evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi
(RFV) model, a representation for recurrent fuzzy systems. It is an extension
of the FV model proposed by Kavka and Schoenauer that extends the application
domain to include temporal problems. The FV model is a representation for fuzzy
controllers based on Voronoi diagrams that can represent fuzzy systems with
synergistic rules, fulfilling the $\epsilon$-completeness property and
providing a simple way to introduce a priory knowledge. In the proposed
representation, the temporal relations are embedded by including internal units
that provide feedback by connecting outputs to inputs. These internal units act
as memory elements. In the RFV model, the semantic of the internal units can be
specified together with the a priori rules. The geometric interpretation of the
rules allows the use of geometric variational operators during the evolution.
The representation and the algorithms are validated in two problems in the area
of system identification and evolutionary robotics.",N/A
Branch-and-Prune Search Strategies for Numerical Constraint Solving,"When solving numerical constraints such as nonlinear equations and
inequalities, solvers often exploit pruning techniques, which remove redundant
value combinations from the domains of variables, at pruning steps. To find the
complete solution set, most of these solvers alternate the pruning steps with
branching steps, which split each problem into subproblems. This forms the
so-called branch-and-prune framework, well known among the approaches for
solving numerical constraints. The basic branch-and-prune search strategy that
uses domain bisections in place of the branching steps is called the bisection
search. In general, the bisection search works well in case (i) the solutions
are isolated, but it can be improved further in case (ii) there are continuums
of solutions (this often occurs when inequalities are involved). In this paper,
we propose a new branch-and-prune search strategy along with several variants,
which not only allow yielding better branching decisions in the latter case,
but also work as well as the bisection search does in the former case. These
new search algorithms enable us to employ various pruning techniques in the
construction of inner and outer approximations of the solution set. Our
experiments show that these algorithms speed up the solving process often by
one order of magnitude or more when solving problems with continuums of
solutions, while keeping the same performance as the bisection search when the
solutions are isolated.","43 pages, 11 figures"
"Processing Uncertainty and Indeterminacy in Information Systems success
  mapping","IS success is a complex concept, and its evaluation is complicated,
unstructured and not readily quantifiable. Numerous scientific publications
address the issue of success in the IS field as well as in other fields. But,
little efforts have been done for processing indeterminacy and uncertainty in
success research. This paper shows a formal method for mapping success using
Neutrosophic Success Map. This is an emerging tool for processing indeterminacy
and uncertainty in success research. EIS success have been analyzed using this
tool.","13 pages, 2 figures"
Mathematical Models in Schema Theory,"In this paper, a mathematical schema theory is developed. This theory has
three roots: brain theory schemas, grid automata, and block-shemas. In Section
2 of this paper, elements of the theory of grid automata necessary for the
mathematical schema theory are presented. In Section 3, elements of brain
theory necessary for the mathematical schema theory are presented. In Section
4, other types of schemas are considered. In Section 5, the mathematical schema
theory is developed. The achieved level of schema representation allows one to
model by mathematical tools virtually any type of schemas considered before,
including schemas in neurophisiology, psychology, computer science, Internet
technology, databases, logic, and mathematics.",N/A
Truecluster: robust scalable clustering with model selection,"Data-based classification is fundamental to most branches of science. While
recent years have brought enormous progress in various areas of statistical
computing and clustering, some general challenges in clustering remain: model
selection, robustness, and scalability to large datasets. We consider the
important problem of deciding on the optimal number of clusters, given an
arbitrary definition of space and clusteriness. We show how to construct a
cluster information criterion that allows objective model selection. Differing
from other approaches, our truecluster method does not require specific
assumptions about underlying distributions, dissimilarity definitions or
cluster models. Truecluster puts arbitrary clustering algorithms into a generic
unified (sampling-based) statistical framework. It is scalable to big datasets
and provides robust cluster assignments and case-wise diagnostics. Truecluster
will make clustering more objective, allows for automation, and will save time
and costs. Free R software is available.","Article (10 figures). Changes in 2nd version: dropped supplements in
  favor of better integrated presentation, better literature coverage, put into
  proper English. Author's website available via http://www.truecluster.com"
"Divide-and-Evolve: a New Memetic Scheme for Domain-Independent Temporal
  Planning","An original approach, termed Divide-and-Evolve is proposed to hybridize
Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the
domain of Temporal Planning Problems (TPPs). Whereas standard Memetic
Algorithms use local search methods to improve the evolutionary solutions, and
thus fail when the local method stops working on the complete problem, the
Divide-and-Evolve approach splits the problem at hand into several, hopefully
easier, sub-problems, and can thus solve globally problems that are intractable
when directly fed into deterministic OR algorithms. But the most prominent
advantage of the Divide-and-Evolve approach is that it immediately opens up an
avenue for multi-objective optimization, even though the OR method that is used
is single-objective. Proof of concept approach on the standard
(single-objective) Zeno transportation benchmark is given, and a small original
multi-objective benchmark is proposed in the same Zeno framework to assess the
multi-objective capabilities of the proposed methodology, a breakthrough in
Temporal Planning.",N/A
Artificial and Biological Intelligence,"This article considers evidence from physical and biological sciences to show
machines are deficient compared to biological systems at incorporating
intelligence. Machines fall short on two counts: firstly, unlike brains,
machines do not self-organize in a recursive manner; secondly, machines are
based on classical logic, whereas Nature's intelligence may depend on quantum
mechanics.",16 pages
"Certainty Closure: Reliable Constraint Reasoning with Incomplete or
  Erroneous Data","Constraint Programming (CP) has proved an effective paradigm to model and
solve difficult combinatorial satisfaction and optimisation problems from
disparate domains. Many such problems arising from the commercial world are
permeated by data uncertainty. Existing CP approaches that accommodate
uncertainty are less suited to uncertainty arising due to incomplete and
erroneous data, because they do not build reliable models and solutions
guaranteed to address the user's genuine problem as she perceives it. Other
fields such as reliable computation offer combinations of models and associated
methods to handle these types of uncertain data, but lack an expressive
framework characterising the resolution methodology independently of the model.
  We present a unifying framework that extends the CP formalism in both model
and solutions, to tackle ill-defined combinatorial problems with incomplete or
erroneous data. The certainty closure framework brings together modelling and
solving methodologies from different fields into the CP paradigm to provide
reliable and efficient approches for uncertain constraint problems. We
demonstrate the applicability of the framework on a case study in network
diagnosis. We define resolution forms that give generic templates, and their
associated operational semantics, to derive practical solution methods for
reliable solutions.",Revised version
Avoiding the Bloat with Stochastic Grammar-based Genetic Programming,"The application of Genetic Programming to the discovery of empirical laws is
often impaired by the huge size of the search space, and consequently by the
computer resources needed. In many cases, the extreme demand for memory and CPU
is due to the massive growth of non-coding segments, the introns. The paper
presents a new program evolution framework which combines distribution-based
evolution in the PBIL spirit, with grammar-based genetic programming; the
information is stored as a probability distribution on the gra mmar rules,
rather than in a population. Experiments on a real-world like problem show that
this approach gives a practical solution to the problem of intron growth.",N/A
Classifying Signals with Local Classifiers,"This paper deals with the problem of classifying signals. The new method for
building so called local classifiers and local features is presented. The
method is a combination of the lifting scheme and the support vector machines.
Its main aim is to produce effective and yet comprehensible classifiers that
would help in understanding processes hidden behind classified signals. To
illustrate the method we present the results obtained on an artificial and a
real dataset.",N/A
Open Answer Set Programming with Guarded Programs,"Open answer set programming (OASP) is an extension of answer set programming
where one may ground a program with an arbitrary superset of the program's
constants. We define a fixed point logic (FPL) extension of Clark's completion
such that open answer sets correspond to models of FPL formulas and identify a
syntactic subclass of programs, called (loosely) guarded programs. Whereas
reasoning with general programs in OASP is undecidable, the FPL translation of
(loosely) guarded programs falls in the decidable (loosely) guarded fixed point
logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP,
enabling for the first time, a characterization of an answer set semantics by
muLGF formulas. We further extend the open answer set semantics for programs
with generalized literals. Such generalized programs (gPs) have interesting
properties, e.g., the ability to express infinity axioms. We restrict the
syntax of gPs such that both rules and generalized literals are guarded. Via a
translation to guarded fixed point logic, we deduce 2-exptime-completeness of
satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted
GgPs with exptime-complete satisfiability checking, but still sufficiently
expressive to optimally simulate computation tree logic (CTL). We translate
Datalog lite programs to GgPs, establishing equivalence of GgPs under an open
answer set semantics, alternation-free muGF, and Datalog lite.","51 pages, 1 figure, accepted for publication in ACM's TOCL"
Metatheory of actions: beyond consistency,"Consistency check has been the only criterion for theory evaluation in
logic-based approaches to reasoning about actions. This work goes beyond that
and contributes to the metatheory of actions by investigating what other
properties a good domain description in reasoning about actions should have. We
state some metatheoretical postulates concerning this sore spot. When all
postulates are satisfied together we have a modular action theory. Besides
being easier to understand and more elaboration tolerant in McCarthy's sense,
modular theories have interesting properties. We point out the problems that
arise when the postulates about modularity are violated and propose algorithmic
checks that can help the designer of an action theory to overcome them.",N/A
"Estimation of linear, non-gaussian causal models in the presence of
  confounding latent variables","The estimation of linear causal models (also known as structural equation
models) from data is a well-known problem which has received much attention in
the past. Most previous work has, however, made an explicit or implicit
assumption of gaussianity, limiting the identifiability of the models. We have
recently shown (Shimizu et al, 2005; Hoyer et al, 2006) that for non-gaussian
distributions the full causal model can be estimated in the no hidden variables
case. In this contribution, we discuss the estimation of the model when
confounding latent variables are present. Although in this case uniqueness is
no longer guaranteed, there is at most a finite set of models which can fit the
data. We develop an algorithm for estimating this set, and describe numerical
simulations which confirm the theoretical arguments and demonstrate the
practical viability of the approach. Full Matlab code is provided for all
simulations.","8 pages, 4 figures, pdflatex"
"Application of Support Vector Regression to Interpolation of Sparse
  Shock Physics Data Sets","Shock physics experiments are often complicated and expensive. As a result,
researchers are unable to conduct as many experiments as they would like -
leading to sparse data sets. In this paper, Support Vector Machines for
regression are applied to velocimetry data sets for shock damaged and melted
tin metal. Some success at interpolating between data sets is achieved.
Implications for future work are discussed.","13 pages, 7 figures"
Approximation Algorithms for K-Modes Clustering,"In this paper, we study clustering with respect to the k-modes objective
function, a natural formulation of clustering for categorical data. One of the
main contributions of this paper is to establish the connection between k-modes
and k-median, i.e., the optimum of k-median is at most twice the optimum of
k-modes for the same categorical data clustering problem. Based on this
observation, we derive a deterministic algorithm that achieves an approximation
factor of 2. Furthermore, we prove that the distance measure in k-modes defines
a metric. Hence, we are able to extend existing approximation algorithms for
metric k-median to k-modes. Empirical results verify the superiority of our
method.",7 pages
Can an Organism Adapt Itself to Unforeseen Circumstances?,"A model of an organism as an autonomous intelligent system has been proposed.
This model was used to analyze learning of an organism in various environmental
conditions. Processes of learning were divided into two types: strong and weak
processes taking place in the absence and the presence of aprioristic
information about an object respectively. Weak learning is synonymous to
adaptation when aprioristic programs already available in a system (an
organism) are started. It was shown that strong learning is impossible for both
an organism and any autonomous intelligent system. It was shown also that the
knowledge base of an organism cannot be updated. Therefore, all behavior
programs of an organism are congenital. A model of a conditioned reflex as a
series of consecutive measurements of environmental parameters has been
advanced. Repeated measurements are necessary in this case to reduce the error
during decision making.",N/A
"Adaptative combination rule and proportional conflict redistribution
  rule for information fusion","This paper presents two new promising rules of combination for the fusion of
uncertain and potentially highly conflicting sources of evidences in the
framework of the theory of belief functions in order to palliate the well-know
limitations of Dempster's rule and to work beyond the limits of applicability
of the Dempster-Shafer theory. We present both a new class of adaptive
combination rules (ACR) and a new efficient Proportional Conflict
Redistribution (PCR) rule allowing to deal with highly conflicting sources for
static and dynamic fusion applications.","Presented at Cogis '06 Conference, Paris, March 2006"
Retraction and Generalized Extension of Computing with Words,"Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a
formal model of computing with values. Motivated by Zadeh's paradigm of
computing with words rather than numbers, Ying proposed a kind of fuzzy
automata, whose input alphabet consists of all fuzzy subsets of a set of
symbols, as a formal model of computing with all words. In this paper, we
introduce a somewhat general formal model of computing with (some special)
words. The new features of the model are that the input alphabet only comprises
some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy
transition function can be specified arbitrarily. By employing the methodology
of fuzzy control, we establish a retraction principle from computing with words
to computing with values for handling crisp inputs and a generalized extension
principle from computing with words to computing with all words for handling
fuzzy inputs. These principles show that computing with values and computing
with all words can be respectively implemented by computing with words. Some
algebraic properties of retractions and generalized extensions are addressed as
well.","13 double column pages; 3 figures; to be published in the IEEE
  Transactions on Fuzzy Systems"
A Knowledge-Based Approach for Selecting Information Sources,"Through the Internet and the World-Wide Web, a vast number of information
sources has become available, which offer information on various subjects by
different providers, often in heterogeneous formats. This calls for tools and
methods for building an advanced information-processing infrastructure. One
issue in this area is the selection of suitable information sources in query
answering. In this paper, we present a knowledge-based approach to this
problem, in the setting where one among a set of information sources
(prototypically, data repositories) should be selected for evaluating a user
query. We use extended logic programs (ELPs) to represent rich descriptions of
the information sources, an underlying domain theory, and user queries in a
formal query language (here, XML-QL, but other languages can be handled as
well). Moreover, we use ELPs for declarative query analysis and generation of a
query description. Central to our approach are declarative source-selection
programs, for which we define syntax and semantics. Due to the structured
nature of the considered data items, the semantics of such programs must
carefully respect implicit context information in source-selection rules, and
furthermore combine it with possible user preferences. A prototype
implementation of our approach has been realized exploiting the DLV KR system
and its plp front-end for prioritized ELPs. We describe a representative
example involving specific movie databases, and report about experimental
results.","53 pages, 2 Figures; to appear in Theory and Practice of Logic
  Programming (TPLP)"
Perspective alignment in spatial language,"It is well known that perspective alignment plays a major role in the
planning and interpretation of spatial language. In order to understand the
role of perspective alignment and the cognitive processes involved, we have
made precise complete cognitive models of situated embodied agents that
self-organise a communication system for dialoging about the position and
movement of real world objects in their immediate surroundings. We show in a
series of robotic experiments which cognitive mechanisms are necessary and
sufficient to achieve successful spatial language and why and how perspective
alignment can take place, either implicitly or based on explicit marking.","to appear in: K. Coventry, J. Bateman, and T. Tenbrink (eds.) Spatial
  Language in Dialogue. Oxford University Press, 2008"
"Reasoning and Planning with Sensing Actions, Incomplete Information, and
  Static Causal Laws using Answer Set Programming","We extend the 0-approximation of sensing actions and incomplete information
in [Son and Baral 2000] to action theories with static causal laws and prove
its soundness with respect to the possible world semantics. We also show that
the conditional planning problem with respect to this approximation is
NP-complete. We then present an answer set programming based conditional
planner, called ASCP, that is capable of generating both conformant plans and
conditional plans in the presence of sensing actions, incomplete information
about the initial state, and static causal laws. We prove the correctness of
our implementation and argue that our planner is sound and complete with
respect to the proposed approximation. Finally, we present experimental results
comparing ASCP to other planners.","72 pages, 3 figures, a preliminary version of this paper appeared in
  the proceedings of the 7th International Conference on Logic Programming and
  Non-Monotonic Reasoning, 2004. To appear in Theory and Practice of Logic
  Programming"
"Approximate Discrete Probability Distribution Representation using a
  Multi-Resolution Binary Tree","Computing and storing probabilities is a hard problem as soon as one has to
deal with complex distributions over multiple random variables. The problem of
efficient representation of probability distributions is central in term of
computational efficiency in the field of probabilistic reasoning. The main
problem arises when dealing with joint probability distributions over a set of
random variables: they are always represented using huge probability arrays. In
this paper, a new method based on binary-tree representation is introduced in
order to store efficiently very large joint distributions. Our approach
approximates any multidimensional joint distributions using an adaptive
discretization of the space. We make the assumption that the lower is the
probability mass of a particular region of feature space, the larger is the
discretization step. This assumption leads to a very optimized representation
in term of time and memory. The other advantages of our approach are the
ability to refine dynamically the distribution every time it is needed leading
to a more accurate representation of the probability distribution and to an
anytime representation of the distribution.",N/A
Diagnosability of Fuzzy Discrete Event Systems,"In order to more effectively cope with the real-world problems of vagueness,
{\it fuzzy discrete event systems} (FDESs) were proposed recently, and the
supervisory control theory of FDESs was developed. In view of the importance of
failure diagnosis, in this paper, we present an approach of the failure
diagnosis in the framework of FDESs. More specifically: (1) We formalize the
definition of diagnosability for FDESs, in which the observable set and failure
set of events are {\it fuzzy}, that is, each event has certain degree to be
observable and unobservable, and, also, each event may possess different
possibility of failure occurring. (2) Through the construction of
observability-based diagnosers of FDESs, we investigate its some basic
properties. In particular, we present a necessary and sufficient condition for
diagnosability of FDESs. (3) Some examples serving to illuminate the
applications of the diagnosability of FDESs are described. To conclude, some
related issues are raised for further consideration.",14 pages; revisions have been made
Classification of Ordinal Data,"Classification of ordinal data is one of the most important tasks of relation
learning. In this thesis a novel framework for ordered classes is proposed. The
technique reduces the problem of classifying ordered classes to the standard
two-class problem. The introduced method is then mapped into support vector
machines and neural networks. Compared with a well-known approach using
pairwise objects as training samples, the new algorithm has a reduced
complexity and training time. A second novel model, the unimodal model, is also
introduced and a parametric version is mapped into neural networks. Several
case studies are presented to assert the validity of the proposed models.","62 pages, MSc thesis"
Imagination as Holographic Processor for Text Animation,"Imagination is the critical point in developing of realistic artificial
intelligence (AI) systems. One way to approach imagination would be simulation
of its properties and operations. We developed two models: AI-Brain Network
Hierarchy of Languages and Semantical Holographic Calculus as well as
simulation system ScriptWriter that emulate the process of imagination through
an automatic animation of English texts. The purpose of this paper is to
demonstrate the model and to present ScriptWriter system
http://nvo.sdsc.edu/NVO/JCSG/get_SRB_mime_file2.cgi//home/tamara.sdsc/test/demo.zip?F=/home/tamara.sdsc/test/demo.zip&M=application/x-gtar
for simulation of the imagination.","10 pages, 10 figures, prototype presented at 4th International
  Conference on Computer Science and its Applications (ICCSA-2006), paper
  submited to SIGCHI 2007"
Belief Calculus,"In Dempster-Shafer belief theory, general beliefs are expressed as belief
mass distribution functions over frames of discernment. In Subjective Logic
beliefs are expressed as belief mass distribution functions over binary frames
of discernment. Belief representations in Subjective Logic, which are called
opinions, also contain a base rate parameter which express the a priori belief
in the absence of evidence. Philosophically, beliefs are quantitative
representations of evidence as perceived by humans or by other intelligent
agents. The basic operators of classical probability calculus, such as addition
and multiplication, can be applied to opinions, thereby making belief calculus
practical. Through the equivalence between opinions and Beta probability
density functions, this also provides a calculus for Beta probability density
functions. This article explains the basic elements of belief calculus.","22 pages, 10 figures"
The Cumulative Rule for Belief Fusion,"The problem of combining beliefs in the Dempster-Shafer belief theory has
attracted considerable attention over the last two decades. The classical
Dempster's Rule has often been criticised, and many alternative rules for
belief combination have been proposed in the literature. The consensus operator
for combining beliefs has nice properties and produces more intuitive results
than Dempster's rule, but has the limitation that it can only be applied to
belief distribution functions on binary state spaces. In this paper we present
a generalisation of the consensus operator that can be applied to Dirichlet
belief functions on state spaces of arbitrary size. This rule, called the
cumulative rule of belief combination, can be derived from classical
statistical theory, and corresponds well with human intuition.",N/A
New Millennium AI and the Convergence of History,"Artificial Intelligence (AI) has recently become a real formal science: the
new millennium brought the first mathematically sound, asymptotically optimal,
universal problem solvers, providing a new, rigorous foundation for the
previously largely heuristic field of General AI and embedded agents. At the
same time there has been rapid progress in practical methods for learning true
sequence-processing programs, as opposed to traditional methods limited to
stationary pattern association. Here we will briefly review some of the new
results, and speculate about future developments, pointing out that the time
intervals between the most notable events in over 40,000 years or 2^9 lifetimes
of human history have sped up exponentially, apparently converging to zero
within the next few decades. Or is this impression just a by-product of the way
humans allocate memory space to past events?","Speed Prior: clarification / 15 pages, to appear in ""Challenges to
  Computational Intelligence"""
Belief Conditioning Rules (BCRs),"In this paper we propose a new family of Belief Conditioning Rules (BCRs) for
belief revision. These rules are not directly related with the fusion of
several sources of evidence but with the revision of a belief assignment
available at a given time according to the new truth (i.e. conditioning
constraint) one has about the space of solutions of the problem.",26 pages
Islands for SAT,"In this note we introduce the notion of islands for restricting local search.
We show how we can construct islands for CNF SAT problems, and how much search
space can be eliminated by restricting search to the island.",7 pages
About Norms and Causes,"Knowing the norms of a domain is crucial, but there exist no repository of
norms. We propose a method to extract them from texts: texts generally do not
describe a norm, but rather how a state-of-affairs differs from it. Answers
concerning the cause of the state-of-affairs described often reveal the
implicit norm. We apply this idea to the domain of driving, and validate it by
designing algorithms that identify, in a text, the ""basic"" norms to which it
refers implicitly.",N/A
Representing Knowledge about Norms,"Norms are essential to extend inference: inferences based on norms are far
richer than those based on logical implications. In the recent decades, much
effort has been devoted to reason on a domain, once its norms are represented.
How to extract and express those norms has received far less attention.
Extraction is difficult: as the readers are supposed to know them, the norms of
a domain are seldom made explicit. For one thing, extracting norms requires a
language to represent them, and this is the topic of this paper. We apply this
language to represent norms in the domain of driving, and show that it is
adequate to reason on the causes of accidents, as described by car-crash
reports.",N/A
"Target Type Tracking with PCR5 and Dempster's rules: A Comparative
  Analysis","In this paper we consider and analyze the behavior of two combinational rules
for temporal (sequential) attribute data fusion for target type estimation. Our
comparative analysis is based on Dempster's fusion rule proposed in
Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution
rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We
show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a
very efficient Target Type Tracking and reduces drastically the latency delay
for correct Target Type decision with respect to Demspter's rule. For cases
presenting some short Target Type switches, Demspter's rule is proved to be
unable to detect the switches and thus to track correctly the Target Type
changes. The approach proposed here is totally new, efficient and promising to
be incorporated in real-time Generalized Data Association - Multi Target
Tracking systems (GDA-MTT) and provides an important result on the behavior of
PCR5 with respect to Dempster's rule. The MatLab source code is provided in","10 pages, 5 diagrams. Presented to Fusion 2006 International
  Conference, Florence, Italy, July 2006"
Fusion of qualitative beliefs using DSmT,"This paper introduces the notion of qualitative belief assignment to model
beliefs of human experts expressed in natural language (with linguistic
labels). We show how qualitative beliefs can be efficiently combined using an
extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical
quantitative reasoning to qualitative reasoning. We propose a new arithmetic on
linguistic labels which allows a direct extension of classical DSm fusion rule
or DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed
jointly with a Qualitative Average Operator. We also show how crisp or interval
mappings can be used to deal indirectly with linguistic labels. A very simple
example is provided to illustrate our qualitative fusion rules.","13 pages. To appear in ""Advances and Applications of DSmT for
  Information Fusion"", collected works, second volume, 2006"
"An Introduction to the DSm Theory for the Combination of Paradoxical,
  Uncertain, and Imprecise Sources of Information","The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been, and
still remains today, of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this
introduction, we present a survey of our recent theory of plausible and
paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the
literature, developed for dealing with imprecise, uncertain and paradoxical
sources of information. We focus our presentation here rather on the
foundations of DSmT, and on the two important new rules of combination, than on
browsing specific applications of DSmT available in literature. Several simple
examples are given throughout the presentation to show the efficiency and the
generality of this new approach.","21 pages, many tables, figures. To appear in Information&Security
  International Journal, 2006"
Relation Variables in Qualitative Spatial Reasoning,"We study an alternative to the prevailing approach to modelling qualitative
spatial reasoning (QSR) problems as constraint satisfaction problems. In the
standard approach, a relation between objects is a constraint whereas in the
alternative approach it is a variable. The relation-variable approach greatly
simplifies integration and implementation of QSR. To substantiate this point,
we discuss several QSR algorithms from the literature which in the
relation-variable approach reduce to the customary constraint propagation
algorithm enforcing generalised arc-consistency.",14 pages; 27th German Conference on Artificial Intelligence (KI'04)
Using Sets of Probability Measures to Represent Uncertainty,"I explore the use of sets of probability measures as a representation of
uncertainty.",N/A
"A State-Based Regression Formulation for Domains with Sensing
  Actions<br> and Incomplete Information","We present a state-based regression function for planning domains where an
agent does not have complete information and may have sensing actions. We
consider binary domains and employ a three-valued characterization of domains
with sensing actions to define the regression function. We prove the soundness
and completeness of our regression formulation with respect to the definition
of progression. More specifically, we show that (i) a plan obtained through
regression for a planning problem is indeed a progression solution of that
planning problem, and that (ii) for each plan found through progression, using
regression one obtains that plan or an equivalent one.","34 pages, 7 Figures"
Semantic Description of Parameters in Web Service Annotations,"A modification of OWL-S regarding parameter description is proposed. It is
strictly based on Description Logic. In addition to class description of
parameters it also allows the modelling of relations between parameters and the
precise description of the size of data to be supplied to a service. In
particular, it solves two major issues identified within current proposals for
a Semantic Web Service annotation standard.","7 pages, 3 figures"
The ALVIS Format for Linguistically Annotated Documents,"The paper describes the ALVIS annotation format designed for the indexing of
large collections of documents in topic-specific search engines. This paper is
exemplified on the biological domain and on MedLine abstracts, as developing a
specialized search engine for biologists is one of the ALVIS case studies. The
ALVIS principle for linguistic annotations is based on existing works and
standard propositions. We made the choice of stand-off annotations rather than
inserted mark-up. Annotations are encoded as XML elements which form the
linguistic subsection of the document record.",N/A
Modular self-organization,"The aim of this paper is to provide a sound framework for addressing a
difficult problem: the automatic construction of an autonomous agent's modular
architecture. We combine results from two apparently uncorrelated domains:
Autonomous planning through Markov Decision Processes and a General Data
Clustering Approach using a kernel-like method. Our fundamental idea is that
the former is a good framework for addressing autonomy whereas the latter
allows to tackle self-organizing problems.",N/A
"A Typed Hybrid Description Logic Programming Language with Polymorphic
  Order-Sorted DL-Typed Unification for Semantic Web Type Systems","In this paper we elaborate on a specific application in the context of hybrid
description logic programs (hybrid DLPs), namely description logic Semantic Web
type systems (DL-types) which are used for term typing of LP rules based on a
polymorphic, order-sorted, hybrid DL-typed unification as procedural semantics
of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates
interchange of domain-independent rules over domain boundaries via dynamically
typing and mapping of explicitly defined type ontologies.","Full technical report 12/05. Published inn: Proc. of 2nd Int.
  Workshop on OWL: Experiences and Directions 2006 (OWLED'06) at ISWC'06,
  Athens, Georgia, USA, 2006"
Why did the accident happen? A norm-based reasoning approach,"In this paper we describe an architecture of a system that answer the
question : Why did the accident happen? from the textual description of an
accident. We present briefly the different parts of the architecture and then
we describe with more detail the semantic part of the system i.e. the part in
which the norm-based reasoning is performed on the explicit knowlege extracted
from the text.",N/A
Une expérience de sémantique inférentielle,"We develop a system which must be able to perform the same inferences that a
human reader of an accident report can do and more particularly to determine
the apparent causes of the accident. We describe the general framework in which
we are situated, linguistic and semantic levels of the analysis and the
inference rules used by the system.",N/A
"Farthest-Point Heuristic based Initialization Methods for K-Modes
  Clustering","The k-modes algorithm has become a popular technique in solving categorical
data clustering problems in different application domains. However, the
algorithm requires random selection of initial points for the clusters.
Different initial points often lead to considerable distinct clustering
results. In this paper we present an experimental study on applying a
farthest-point heuristic based initialization method to k-modes clustering to
improve its performance. Experiments show that new initialization method leads
to better clustering accuracy than random selection initialization method for
k-modes clustering.",7 pages
Comparing Typical Opening Move Choices Made by Humans and Chess Engines,"The opening book is an important component of a chess engine, and thus
computer chess programmers have been developing automated methods to improve
the quality of their books. For chess, which has a very rich opening theory,
large databases of high-quality games can be used as the basis of an opening
book, from which statistics relating to move choices from given positions can
be collected. In order to find out whether the opening books used by modern
chess engines in machine versus machine competitions are ``comparable'' to
those used by chess players in human versus human competitions, we carried out
analysis on 26 test positions using statistics from two opening books one
compiled from humans' games and the other from machines' games. Our analysis
using several nonparametric measures, shows that, overall, there is a strong
association between humans' and machines' choices of opening moves when using a
book to guide their choices.","12 pages, 1 figure, 6 tables"
Local approximate inference algorithms,"We present a new local approximation algorithm for computing Maximum a
Posteriori (MAP) and log-partition function for arbitrary exponential family
distribution represented by a finite-valued pair-wise Markov random field
(MRF), say $G$. Our algorithm is based on decomposition of $G$ into {\em
appropriately} chosen small components; then computing estimates locally in
each of these components and then producing a {\em good} global solution. We
show that if the underlying graph $G$ either excludes some finite-sized graph
as its minor (e.g. Planar graph) or has low doubling dimension (e.g. any graph
with {\em geometry}), then our algorithm will produce solution for both
questions within {\em arbitrary accuracy}. We present a message-passing
implementation of our algorithm for MAP computation using self-avoiding walk of
graph. In order to evaluate the computational cost of this implementation, we
derive novel tight bounds on the size of self-avoiding walk tree for arbitrary
graph.
  As a consequence of our algorithmic result, we show that the normalized
log-partition function (also known as free-energy) for a class of {\em regular}
MRFs will converge to a limit, that is computable to an arbitrary accuracy.","21 pages, 10 figures"
Constant for associative patterns ensemble,"Creation procedure of associative patterns ensemble in terms of formal logic
with using neural net-work (NN) model is formulated. It is shown that the
associative patterns set is created by means of unique procedure of NN work
which having individual parameters of entrance stimulus transformation. It is
ascer-tained that the quantity of the selected associative patterns possesses
is a constant.",6 pages
Adaptation Knowledge Discovery from a Case Base,"In case-based reasoning, the adaptation step depends in general on
domain-dependent knowledge, which motivates studies on adaptation knowledge
acquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge
discovery from databases. This system explores the variations within the case
base to elicit adaptation knowledge. It has been successfully tested in an
application of case-based decision support to breast cancer treatment.",N/A
Decentralized Failure Diagnosis of Stochastic Discrete Event Systems,"Recently, the diagnosability of {\it stochastic discrete event systems}
(SDESs) was investigated in the literature, and, the failure diagnosis
considered was {\it centralized}. In this paper, we propose an approach to {\it
decentralized} failure diagnosis of SDESs, where the stochastic system uses
multiple local diagnosers to detect failures and each local diagnoser possesses
its own information. In a way, the centralized failure diagnosis of SDESs can
be viewed as a special case of the decentralized failure diagnosis presented in
this paper with only one projection. The main contributions are as follows: (1)
We formalize the notion of codiagnosability for stochastic automata, which
means that a failure can be detected by at least one local stochastic diagnoser
within a finite delay. (2) We construct a codiagnoser from a given stochastic
automaton with multiple projections, and the codiagnoser associated with the
local diagnosers is used to test codiagnosability condition of SDESs. (3) We
deal with a number of basic properties of the codiagnoser. In particular, a
necessary and sufficient condition for the codiagnosability of SDESs is
presented. (4) We give a computing method in detail to check whether
codiagnosability is violated. And (5) some examples are described to illustrate
the applications of the codiagnosability and its computing method.",25 pages. Comments and criticisms are welcome
DSmT: A new paradigm shift for information fusion,"The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been and
still remains of primal importance for the development of reliable information
fusion systems. In this short survey paper, we present the theory of plausible
and paradoxical reasoning, known as DSmT (Dezert-Smarandache Theory) in
literature, developed for dealing with imprecise, uncertain and potentially
highly conflicting sources of information. DSmT is a new paradigm shift for
information fusion and recent publications have shown the interest and the
potential ability of DSmT to solve fusion problems where Dempster's rule used
in Dempster-Shafer Theory (DST) provides counter-intuitive results or fails to
provide useful result at all. This paper is focused on the foundations of DSmT
and on its main rules of combination (classic, hybrid and Proportional Conflict
Redistribution rules). Shafer's model on which is based DST appears as a
particular and specific case of DSm hybrid model which can be easily handled by
DSmT as well. Several simple but illustrative examples are given throughout
this paper to show the interest and the generality of this new theory.","11 pages. Presented to Cogis06 International Conference, Paris,
  France, 2006"
"The Reaction RuleML Classification of the Event / Action / State
  Processing and Reasoning Space","Reaction RuleML is a general, practical, compact and user-friendly
XML-serialized language for the family of reaction rules. In this white paper
we give a review of the history of event / action /state processing and
reaction rule approaches and systems in different domains, define basic
concepts and give a classification of the event, action, state processing and
reasoning space as well as a discussion of relevant / related work","The Reaction RuleML Classification of the Event / Action / State
  Processing and Reasoning Space extracted from Paschke, A.: ECA-RuleML: An
  Approach combining ECA Rules with temporal interval-based KR Event/Action
  Logics and Transactional Update Logics, Internet-based Information Systems,
  Technical University Munich, Technical Report 11 / 2005"
"Fuzzy Logic Classification of Imaging Laser Desorption Fourier Transform
  Mass Spectrometry Data","A fuzzy logic based classification engine has been developed for classifying
mass spectra obtained with an imaging internal source Fourier transform mass
spectrometer (I^2LD-FTMS). Traditionally, an operator uses the relative
abundance of ions with specific mass-to-charge (m/z) ratios to categorize
spectra. An operator does this by comparing the spectrum of m/z versus
abundance of an unknown sample against a library of spectra from known samples.
Automated positioning and acquisition allow I^2LD-FTMS to acquire data from
very large grids, this would require classification of up to 3600 spectrum per
hour to keep pace with the acquisition. The tedious job of classifying numerous
spectra generated in an I^2LD-FTMS imaging application can be replaced by a
fuzzy rule base if the cues an operator uses can be encapsulated. We present
the translation of linguistic rules to a fuzzy classifier for mineral phases in
basalt. This paper also describes a method for gathering statistics on ions,
which are not currently used in the rule base, but which may be candidates for
making the rule base more accurate and complete or to form new rule bases based
on data obtained from known samples. A spatial method for classifying spectra
with low membership values, based on neighboring sample classifications, is
also presented.",N/A
A Neutrosophic Description Logic,"Description Logics (DLs) are appropriate, widely used, logics for managing
structured knowledge. They allow reasoning about individuals and concepts, i.e.
set of individuals with common properties. Typically, DLs are limited to
dealing with crisp, well defined concepts. That is, concepts for which the
problem whether an individual is an instance of it is yes/no question. More
often than not, the concepts encountered in the real world do not have a
precisely defined criteria of membership: we may say that an individual is an
instance of a concept only to a certain degree, depending on the individual's
properties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In
order to deal with fuzzy, incomplete, indeterminate and inconsistent concepts,
we need to extend the fuzzy DLs, combining the neutrosophic logic with a
classical DL. In particular, concepts become neutrosophic (here neutrosophic
means fuzzy, incomplete, indeterminate, and inconsistent), thus reasoning about
neutrosophic concepts is supported. We'll define its syntax, its semantics, and
describe its properties.","18 pages. Presented at the IEEE International Conference on Granular
  Computing, Georgia State University, Atlanta, USA, May 2006"
"Genetic Programming for Kernel-based Learning with Co-evolving Subsets
  Selection","Support Vector Machines (SVMs) are well-established Machine Learning (ML)
algorithms. They rely on the fact that i) linear learning can be formalized as
a well-posed optimization problem; ii) non-linear learning can be brought into
linear learning thanks to the kernel trick and the mapping of the initial
search space onto a high dimensional feature space. The kernel is designed by
the ML expert and it governs the efficiency of the SVM approach. In this paper,
a new approach for the automatic design of kernels by Genetic Programming,
called the Evolutionary Kernel Machine (EKM), is presented. EKM combines a
well-founded fitness function inspired from the margin criterion, and a
co-evolution framework ensuring the computational scalability of the approach.
Empirical validation on standard ML benchmark demonstrates that EKM is
competitive using state-of-the-art SVMs with tuned hyper-parameters.",N/A
"Functional Brain Imaging with Multi-Objective Multi-Modal Evolutionary
  Optimization","Functional brain imaging is a source of spatio-temporal data mining problems.
A new framework hybridizing multi-objective and multi-modal optimization is
proposed to formalize these data mining problems, and addressed through
Evolutionary Computation (EC). The merits of EC for spatio-temporal data mining
are demonstrated as the approach facilitates the modelling of the experts'
requirements, and flexibly accommodates their changing goals.",N/A
A Generic Global Constraint based on MDDs,"The paper suggests the use of Multi-Valued Decision Diagrams (MDDs) as the
supporting data structure for a generic global constraint. We give an algorithm
for maintaining generalized arc consistency (GAC) on this constraint that
amortizes the cost of the GAC computation over a root-to-terminal path in the
search tree. The technique used is an extension of the GAC algorithm for the
regular language constraint on finite length input. Our approach adds support
for skipped variables, maintains the reduced property of the MDD dynamically
and provides domain entailment detection. Finally we also show how to adapt the
approach to constraint types that are closely related to MDDs, such as AOMDDs
and Case DAGs.","Tech report, 31 pages, 3 figures"
Conscious Intelligent Systems - Part 1 : I X I,"Did natural consciousness and intelligent systems arise out of a path that
was co-evolutionary to evolution? Can we explain human self-consciousness as
having risen out of such an evolutionary path? If so how could it have been?
  In this first part of a two-part paper (titled IXI), we take a learning
system perspective to the problem of consciousness and intelligent systems, an
approach that may look unseasonable in this age of fMRI's and high tech
neuroscience.
  We posit conscious intelligent systems in natural environments and wonder how
natural factors influence their design paths. Such a perspective allows us to
explain seamlessly a variety of natural factors, factors ranging from the rise
and presence of the human mind, man's sense of I, his self-consciousness and
his looping thought processes to factors like reproduction, incubation,
extinction, sleep, the richness of natural behavior, etc. It even allows us to
speculate on a possible human evolution scenario and other natural phenomena.",N/A
"Conscious Intelligent Systems - Part II - Mind, Thought, Language and
  Understanding","This is the second part of a paper on Conscious Intelligent Systems. We use
the understanding gained in the first part (Conscious Intelligent Systems Part
1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the
presence of mind affects understanding and intelligent systems; we see that the
presence of mind necessitates language. The rise of language in turn has
important effects on understanding. We discuss the humanoid question and how
the question of self-consciousness (and by association mind/thought/language)
would affect humanoids too.",N/A
Interactive Configuration by Regular String Constraints,"A product configurator which is complete, backtrack free and able to compute
the valid domains at any state of the configuration can be constructed by
building a Binary Decision Diagram (BDD). Despite the fact that the size of the
BDD is exponential in the number of variables in the worst case, BDDs have
proved to work very well in practice. Current BDD-based techniques can only
handle interactive configuration with small finite domains. In this paper we
extend the approach to handle string variables constrained by regular
expressions. The user is allowed to change the strings by adding letters at the
end of the string. We show how to make a data structure that can perform fast
valid domain computations given some assignment on the set of string variables.
  We first show how to do this by using one large DFA. Since this approach is
too space consuming to be of practical use, we construct a data structure that
simulates the large DFA and in most practical cases are much more space
efficient. As an example a configuration problem on $n$ string variables with
only one solution in which each string variable is assigned to a value of
length of $k$ the former structure will use $\Omega(k^n)$ space whereas the
latter only need $O(kn)$. We also show how this framework easily can be
combined with the recent BDD techniques to allow both boolean, integer and
string variables in the configuration problem.",Tech Report
Truncating the loop series expansion for Belief Propagation,"Recently, M. Chertkov and V.Y. Chernyak derived an exact expression for the
partition sum (normalization constant) corresponding to a graphical model,
which is an expansion around the Belief Propagation solution. By adding
correction terms to the BP free energy, one for each ""generalized loop"" in the
factor graph, the exact partition sum is obtained. However, the usually
enormous number of generalized loops generally prohibits summation over all
correction terms. In this article we introduce Truncated Loop Series BP
(TLSBP), a particular way of truncating the loop series of M. Chertkov and V.Y.
Chernyak by considering generalized loops as compositions of simple loops. We
analyze the performance of TLSBP in different scenarios, including the Ising
model, regular random graphs and on Promedas, a large probabilistic medical
diagnostic system. We show that TLSBP often improves upon the accuracy of the
BP solution, at the expense of increased computation time. We also show that
the performance of TLSBP strongly depends on the degree of interaction between
the variables. For weak interactions, truncating the series leads to
significant improvements, whereas for strong interactions it can be
ineffective, even if a high number of terms is considered.","31 pages, 12 figures, submitted to Journal of Machine Learning
  Research"
Attribute Value Weighting in K-Modes Clustering,"In this paper, the traditional k-modes clustering algorithm is extended by
weighting attribute value matches in dissimilarity computation. The use of
attribute value weighting technique makes it possible to generate clusters with
stronger intra-similarities, and therefore achieve better clustering
performance. Experimental results on real life datasets show that these value
weighting based k-modes algorithms are superior to the standard k-modes
algorithm with respect to clustering accuracy.",15 pages
"Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br>
  SAT-Based Planning","In Verification and in (optimal) AI Planning, a successful method is to
formulate the application as boolean satisfiability (SAT), and solve it with
state-of-the-art DPLL-based procedures. There is a lack of understanding of why
this works so well. Focussing on the Planning context, we identify a form of
problem structure concerned with the symmetrical or asymmetrical nature of the
cost of achieving the individual planning goals. We quantify this sort of
structure with a simple numeric parameter called AsymRatio, ranging between 0
and 1. We run experiments in 10 benchmark domains from the International
Planning Competitions since 2000; we show that AsymRatio is a good indicator of
SAT solver performance in 8 of these domains. We then examine carefully crafted
synthetic planning domains that allow control of the amount of structure, and
that are clean enough for a rigorous analysis of the combinatorial search
space. The domains are parameterized by size, and by the amount of structure.
The CNFs we examine are unsatisfiable, encoding one planning step less than the
length of the optimal plan. We prove upper and lower bounds on the size of the
best possible DPLL refutations, under different settings of the amount of
structure, as a function of size. We also identify the best possible sets of
branching variables (backdoors). With minimum AsymRatio, we prove exponential
lower bounds, and identify minimal backdoors of size linear in the number of
variables. With maximum AsymRatio, we identify logarithmic DPLL refutations
(and backdoors), showing a doubly exponential gap between the two structural
extreme cases. The reasons for this behavior -- the proof arguments --
illuminate the prototypical patterns of structure causing the empirical
behavior observed in the competition benchmarks.",N/A
Uniform and Partially Uniform Redistribution Rules,"This short paper introduces two new fusion rules for combining quantitative
basic belief assignments. These rules although very simple have not been
proposed in literature so far and could serve as useful alternatives because of
their low computation cost with respect to the recent advanced Proportional
Conflict Redistribution rules developed in the DSmT framework.","4 pages; ""Advances and Applications of DSmT for Plausible and
  Paradoxical reasoning for Information Fusion"", International Workshop
  organized by the Bulgarian IST Centre of Competence in 21st Century, December
  14, 2006, Bulg. Acad. of Sciences, Sofia, Bulgaria"
Generic Global Constraints based on MDDs,"Constraint Programming (CP) has been successfully applied to both constraint
satisfaction and constraint optimization problems. A wide variety of
specialized global constraints provide critical assistance in achieving a good
model that can take advantage of the structure of the problem in the search for
a solution. However, a key outstanding issue is the representation of 'ad-hoc'
constraints that do not have an inherent combinatorial nature, and hence are
not modeled well using narrowly specialized global constraints. We attempt to
address this issue by considering a hybrid of search and compilation.
Specifically we suggest the use of Reduced Ordered Multi-Valued Decision
Diagrams (ROMDDs) as the supporting data structure for a generic global
constraint. We give an algorithm for maintaining generalized arc consistency
(GAC) on this constraint that amortizes the cost of the GAC computation over a
root-to-leaf path in the search tree without requiring asymptotically more
space than used for the MDD. Furthermore we present an approach for
incrementally maintaining the reduced property of the MDD during the search,
and show how this can be used for providing domain entailment detection.
Finally we discuss how to apply our approach to other similar data structures
such as AOMDDs and Case DAGs. The technique used can be seen as an extension of
the GAC algorithm for the regular language constraint on finite length input.",Preliminary 15 pages version of the tech-report cs.AI/0611141
"Redesigning Decision Matrix Method with an indeterminacy-based inference
  process","For academics and practitioners concerned with computers, business and
mathematics, one central issue is supporting decision makers. In this paper, we
propose a generalization of Decision Matrix Method (DMM), using Neutrosophic
logic. It emerges as an alternative to the existing logics and it represents a
mathematical model of uncertainty and indeterminacy. This paper proposes the
Neutrosophic Decision Matrix Method as a more realistic tool for decision
making. In addition, a de-neutrosophication process is included.","12 pages, 4 figures, one table"
Modelling Complexity in Musical Rhythm,"This paper constructs a tree structure for the music rhythm using the
L-system. It models the structure as an automata and derives its complexity. It
also solves the complexity for the L-system. This complexity can resolve the
similarity between trees. This complexity serves as a measure of psychological
complexity for rhythms. It resolves the music complexity of various
compositions including the Mozart effect K488.
  Keyword: music perception, psychological complexity, rhythm, L-system,
automata, temporal associative memory, inverse problem, rewriting rule,
bracketed string, tree similarity","21 pages, 13 figures, 2 tables"
"Space-contained conflict revision, for geographic information","Using qualitative reasoning with geographic information, contrarily, for
instance, with robotics, looks not only fastidious (i.e.: encoding knowledge
Propositional Logics PL), but appears to be computational complex, and not
tractable at all, most of the time. However, knowledge fusion or revision, is a
common operation performed when users merge several different data sets in a
unique decision making process, without much support. Introducing logics would
be a great improvement, and we propose in this paper, means for deciding -a
priori- if one application can benefit from a complete revision, under only the
assumption of a conjecture that we name the ""containment conjecture"", which
limits the size of the minimal conflicts to revise. We demonstrate that this
conjecture brings us the interesting computational property of performing a
not-provable but global, revision, made of many local revisions, at a tractable
size. We illustrate this approach on an application.",14 pages
Case Base Mining for Adaptation Knowledge Acquisition,"In case-based reasoning, the adaptation of a source case in order to solve
the target problem is at the same time crucial and difficult to implement. The
reason for this difficulty is that, in general, adaptation strongly depends on
domain-dependent knowledge. This fact motivates research on adaptation
knowledge acquisition (AKA). This paper presents an approach to AKA based on
the principles and techniques of knowledge discovery from databases and
data-mining. It is implemented in CABAMAKA, a system that explores the
variations within the case base to elicit adaptation knowledge. This system has
been successfully tested in an application of case-based reasoning to decision
support in the domain of breast cancer treatment.",N/A
Calculating Valid Domains for BDD-Based Interactive Configuration,"In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.",N/A
A study of structural properties on profiles HMMs,"Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.","6 pages, 7 figures"
Bayesian approach to rough set,"This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.","20 pages, 3 figures"
"Comparing Robustness of Pairwise and Multiclass Neural-Network Systems
  for Face Recognition","Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.",N/A
Ensemble Learning for Free with Evolutionary Algorithms ?,"Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.",N/A
"Fault Classification in Cylinders Using Multilayer Perceptrons, Support
  Vector Machines and Guassian Mixture Models","Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.","10 pages, 2 figures, 4 tables"
Learning to Bluff,"The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.",6 pages
Soft constraint abstraction based on semiring homomorphism,"The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.","18 pages, 1 figure"
Bayesian Approach to Neuro-Rough Models,"This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.","24 pages, 5 figures, 1 table"
"Artificial Neural Networks and Support Vector Machines for Water Demand
  Time Series Forecasting","Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.",6 pages
"Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs
  with Missing Values","An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.",7 pages
Artificial Intelligence for Conflict Management,"Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.",20 pages
Evolving Symbolic Controllers,"The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.",N/A
Robust Multi-Cellular Developmental Design,"This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange ""chemicals"" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.",N/A
"Response Prediction of Structural System Subject to Earthquake Motions
  using Artificial Neural Network","This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.",18 pages
"Fault Classification using Pseudomodal Energies and Neuro-fuzzy
  modelling","This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.","8 pages, In Proceedings of the Asia-Pacific Workshop on Structural
  Health Monitoring, Yokohama, Japan, 2006"
On-Line Condition Monitoring using Computational Intelligence,"This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.",8 pages
The Road to Quantum Artificial Intelligence,"This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.","9 pages. Presented at PCI-2007: 11th Panhellenic Conference in
  Informatics, 18-20 May 2007, Patras, Greece"
Truecluster matching,"Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.","15 pages, 2 figures. Details the matching needed for ""Truecluster:
  robust scalable clustering with model selection"" but can also be used in
  different contexts"
Modeling Computations in a Semantic Network,"Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.",project website: http://neno.lanl.gov
Automatically Restructuring Practice Guidelines using the GEM DTD,"This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.",N/A
Temporal Reasoning without Transitive Tables,"Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.",rapport interne
A Collection of Definitions of Intelligence,"This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.",12 LaTeX pages
"A Robust Linguistic Platform for Efficient and Domain specific Web
  Content Analysis","Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.",N/A
"Mixed Integer Linear Programming For Exact Finite-Horizon Planning In
  Decentralized Pomdps","We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.",N/A
"A Leaf Recognition Algorithm for Plant Classification Using
  Probabilistic Neural Network","In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.","6 pages, 3 figures, 2 tables"
"2006: Celebrating 75 years of AI - History and Outlook: the Next 25
  Years","When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.","14 pages; preprint of invited contribution to the Proceedings of the
  ``50th Anniversary Summit of Artificial Intelligence'' at Monte Verita,
  Ascona, Switzerland, 9-14 July 2006"
Lagrangian Relaxation for MAP Estimation in Graphical Models,"We develop a general framework for MAP estimation in discrete and Gaussian
graphical models using Lagrangian relaxation techniques. The key idea is to
reformulate an intractable estimation problem as one defined on a more
tractable graph, but subject to additional constraints. Relaxing these
constraints gives a tractable dual problem, one defined by a thin graph, which
is then optimized by an iterative procedure. When this iterative optimization
leads to a consistent estimate, one which also satisfies the constraints, then
it corresponds to an optimal MAP estimate of the original model. Otherwise
there is a ``duality gap'', and we obtain a bound on the optimal solution.
Thus, our approach combines convex optimization with dynamic programming
techniques applicable for thin graphs. The popular tree-reweighted max-product
(TRMP) method may be seen as solving a particular class of such relaxations,
where the intractable graph is relaxed to a set of spanning trees. We also
consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g.
loops), and a connected tree obtained by ``unwinding'' cycles. In addition, we
propose a new class of multiscale relaxations that introduce ``summary''
variables. The potential benefits of such generalizations include: reducing or
eliminating the ``duality gap'' in hard problems, reducing the number or
Lagrange multipliers in the dual problem, and accelerating convergence of the
iterative optimization procedure.","10 pages, presented at 45th Allerton conference on communication,
  control and computing, to appear in proceedings"
Analyzing covert social network foundation behind terrorism disaster,"This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.","17pages, 10 figures, submitted to Int. J. Services Sciences"
Node discovery problem for a social network,"Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network, but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization.",N/A
Predicting relevant empty spots in social interaction,"An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction, and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach.","11 pages, 5 figures, submitted to J. Systems Science and Complexity"
"Translating OWL and Semantic Web Rules into Prolog: Moving Toward
  Description Logic Programs","To appear in Theory and Practice of Logic Programming (TPLP), 2008.
  We are researching the interaction between the rule and the ontology layers
of the Semantic Web, by comparing two options: 1) using OWL and its rule
extension SWRL to develop an integrated ontology/rule language, and 2) layering
rules on top of an ontology with RuleML and OWL. Toward this end, we are
developing the SWORIER system, which enables efficient automated reasoning on
ontologies and rules, by translating all of them into Prolog and adding a set
of general rules that properly capture the semantics of OWL. We have also
enabled the user to make dynamic changes on the fly, at run time. This work
addresses several of the concerns expressed in previous work, such as negation,
complementary classes, disjunctive heads, and cardinality, and it discusses
alternative approaches for dealing with inconsistencies in the knowledge base.
In addition, for efficiency, we implemented techniques called
extensionalization, avoiding reanalysis, and code minimization.","21 pages, 5 figures, 19 tables. To appear in Theory and Practice of
  Logic Programming (TPLP), 2008"
Evolving localizations in reaction-diffusion cellular automata,"We consider hexagonal cellular automata with immediate cell neighbourhood and
three cell-states. Every cell calculates its next state depending on the
integral representation of states in its neighbourhood, i.e. how many
neighbours are in each one state. We employ evolutionary algorithms to breed
local transition functions that support mobile localizations (gliders), and
characterize sets of the functions selected in terms of quasi-chemical systems.
Analysis of the set of functions evolved allows to speculate that mobile
localizations are likely to emerge in the quasi-chemical systems with limited
diffusion of one reagent, a small number of molecules is required for
amplification of travelling localizations, and reactions leading to stationary
localizations involve relatively equal amount of quasi-chemical species.
Techniques developed can be applied in cascading signals in nature-inspired
spatially extended computing devices, and phenomenological studies and
classification of non-linear discrete systems.",Accepted for publication in Int. J. Modern Physics C
Decomposition During Search for Propagation-Based Constraint Solvers,"We describe decomposition during search (DDS), an integration of And/Or tree
search into propagation-based constraint solvers. The presented search
algorithm dynamically decomposes sub-problems of a constraint satisfaction
problem into independent partial problems, avoiding redundant work.
  The paper discusses how DDS interacts with key features that make
propagation-based solvers successful: constraint propagation, especially for
global constraints, and dynamic search heuristics.
  We have implemented DDS for the Gecode constraint programming library. Two
applications, solution counting in graph coloring and protein structure
prediction, exemplify the benefits of DDS in practice.","20 pages, 9 figures, 2 tables; longer, more detailed version"
Universal Intelligence: A Definition of Machine Intelligence,"A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines.",50 gentle pages
Tests of Machine Intelligence,"Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed.","12 pages; 1 table. Turing test and derivatives; Compression tests;
  Linguistic complexity; Multiple cognitive abilities; Competitive games;
  Psychometric tests; Smith's test; C-test; Universal intelligence"
"Convergence of Expected Utilities with Algorithmic Probability
  Distributions","We consider an agent interacting with an unknown environment. The environment
is a function which maps natural numbers to natural numbers; the agent's set of
hypotheses about the environment contains all such functions which are
computable and compatible with a finite set of known input-output pairs, and
the agent assigns a positive probability to each such hypothesis. We do not
require that this probability distribution be computable, but it must be
bounded below by a positive computable function. The agent has a utility
function on outputs from the environment. We show that if this utility function
is bounded below in absolute value by an unbounded computable function, then
the expected utility of any input is undefined. This implies that a computable
utility function will have convergent expected utilities iff that function is
bounded.","2 pages + title page, references"
Le terme et le concept : fondements d'une ontoterminologie,"Most definitions of ontology, viewed as a ""specification of a
conceptualization"", agree on the fact that if an ontology can take different
forms, it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts, we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure,
i.e. the network of words linked by linguistic relationships, does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics, of which one of the
principles is the incompleteness of text, whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless, the famous Sapir and Whorf
hypothesis, concerning the interdependence of thought and language, is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building.",22 pages
Stream Computing,"Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
""intelligent"" processing. The biological basis of this computing is sketched
and the matter of learning is examined.","7 pages, 4 figures"
Anisotropic selection in cellular genetic algorithms,"In this paper we introduce a new selection scheme in cellular genetic
algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows
accurate control of the selective pressure. First we compare this new scheme
with the classical rectangular grid shapes solution according to the selective
pressure: we can obtain the same takeover time with the two techniques although
the spreading of the best individual is different. We then give experimental
results that show to what extent AS promotes the emergence of niches that
support low coupling and high cohesion. Finally, using a cGA with anisotropic
selection on a Quadratic Assignment Problem we show the existence of an
anisotropic optimal value for which the best average performance is observed.
Further work will focus on the selective pressure self-adjustment ability
provided by this new selection scheme.",N/A
"The Future of Scientific Simulations: from Artificial Life to Artificial
  Cosmogenesis","This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would ""replay the tape of the universe"" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life.","The text was improved in many respects, and a new figure was added.
  Cite as: Vidal, C. 2008. The Future of Scientific Simulations: from
  Artificial Life to Artificial Cosmogenesis. In Death And Anti-Death, Volume
  6: Thirty Years After Kurt Godel (1906-1978), Ed. Charles Tandy, in press"
Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*,This paper has been withdrawn.,This paper has been withdrawn
"Eye-Tracking Evolutionary Algorithm to minimize user's fatigue in IEC
  applied to Interactive One-Max problem","In this paper, we describe a new algorithm that consists in combining an
eye-tracker for minimizing the fatigue of a user during the evaluation process
of Interactive Evolutionary Computation. The approach is then applied to the
Interactive One-Max optimization problem.",N/A
Node discovery in a networked organization,"In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.",N/A
"Multiagent Approach for the Representation of Information in a Decision
  Support System","In an emergency situation, the actors need an assistance allowing them to
react swiftly and efficiently. In this prospect, we present in this paper a
decision support system that aims to prepare actors in a crisis situation
thanks to a decision-making support. The global architecture of this system is
presented in the first part. Then we focus on a part of this system which is
designed to represent the information of the current situation. This part is
composed of a multiagent system that is made of factual agents. Each agent
carries a semantic feature and aims to represent a partial part of a situation.
The agents develop thanks to their interactions by comparing their semantic
features using proximity measures and according to specific ontologies.",N/A
Reflective visualization and verbalization of unconscious preference,"A new method is presented, that can help a person become aware of his or her
unconscious preferences, and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection, visualization, and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment, two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding.",This will be submitted to KES Journal
Application of Rough Set Theory to Analysis of Hydrocyclone Operation,"This paper describes application of rough set theory, on the analysis of
hydrocyclone operation. In this manner, using Self Organizing Map (SOM) as
preprocessing step, best crisp granules of data are obtained. Then, using a
combining of SOM and rough set theory (RST)-called SORST-, the dominant rules
on the information table, obtained from laboratory tests, are extracted. Based
on these rules, an approximate estimation on decision attribute is fulfilled.
Finally, a brief comparison of this method with the SOM-NFIS system (briefly
SONFIS) is highlighted.","InternationalConference on Smart Materials and Adaptive Structures:
  Mathematical Modeling and Computation"
Agent-Based Perception of an Environment in an Emergency Situation,"We are interested in the problem of multiagent systems development for risk
detecting and emergency response in an uncertain and partially perceived
environment. The evaluation of the current situation passes by three stages
inside the multiagent system. In a first time, the situation is represented in
a dynamic way. The second step, consists to characterise the situation and
finally, it is compared with other similar known situations. In this paper, we
present an information modelling of an observed environment, that we have
applied on the RoboCupRescue Simulation System. Information coming from the
environment are formatted according to a taxonomy and using semantic features.
The latter are defined thanks to a fine ontology of the domain and are managed
by factual agents that aim to represent dynamically the current situation.",N/A
"On the Influence of Selection Operators on Performances in Cellular
  Genetic Algorithms","In this paper, we study the influence of the selective pressure on the
performance of cellular genetic algorithms. Cellular genetic algorithms are
genetic algorithms where the population is embedded on a toroidal grid. This
structure makes the propagation of the best so far individual slow down, and
allows to keep in the population potentially good solutions. We present two
selective pressure reducing strategies in order to slow down even more the best
solution propagation. We experiment these strategies on a hard optimization
problem, the quadratic assignment problem, and we show that there is a value
for of the control parameter for both which gives the best performance. This
optimal value does not find explanation on only the selective pressure,
measured either by take over time and diversity evolution. This study makes us
conclude that we need other tools than the sole selective pressure measures to
explain the performances of cellular genetic algorithms.",N/A
"Geometric Data Analysis, From Correspondence Analysis to Structured Data
  Analysis (book review)","Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.","5 pages, 8 citations. Accepted in Journal of Classification"
Phase transition in SONFIS&SORST,"In this study, we introduce general frame of MAny Connected Intelligent
Particles Systems (MACIPS). Connections and interconnections between particles
get a complex behavior of such merely simple system (system in
system).Contribution of natural computing, under information granulation
theory, are the main topics of this spacious skeleton. Upon this clue, we
organize two algorithms involved a few prominent intelligent computing and
approximate reasoning methods: self organizing feature map (SOM), Neuro- Fuzzy
Inference System and Rough Set Theory (RST). Over this, we show how our
algorithms can be taken as a linkage of government-society interaction, where
government catches various fashions of behavior: solid (absolute) or flexible.
So, transition of such society, by changing of connectivity parameters (noise)
from order to disorder is inferred. Add to this, one may find an indirect
mapping among financial systems and eventual market fluctuations with MACIPS.
Keywords: phase transition, SONFIS, SORST, many connected intelligent particles
system, society-government interaction","submitted to :The Sixth International Conference on Rough Sets and
  Current Trends in Computing; Akron, Ohio, USA,2008"
Adaptive Affinity Propagation Clustering,"Affinity propagation clustering (AP) has two limitations: it is hard to know
what value of parameter 'preference' can yield an optimal clustering solution,
and oscillations cannot be eliminated automatically if occur. The adaptive AP
method is proposed to overcome these limitations, including adaptive scanning
of preferences to search space of the number of clusters for finding the
optimal clustering solution, adaptive adjustment of damping factors to
eliminate oscillations, and adaptive escaping from oscillations when the
damping adjustment technique fails. Experimental results on simulated and real
data sets show that the adaptive AP is effective and can outperform AP in
quality of clustering results.",an English version of original paper
"Assessment of effective parameters on dilution using approximate
  reasoning methods in longwall mining method, Iran coal mines","Approximately more than 90% of all coal production in Iranian underground
mines is derived directly longwall mining method. Out of seam dilution is one
of the essential problems in these mines. Therefore the dilution can impose the
additional cost of mining and milling. As a result, recognition of the
effective parameters on the dilution has a remarkable role in industry. In this
way, this paper has analyzed the influence of 13 parameters (attributed
variables) versus the decision attribute (dilution value), so that using two
approximate reasoning methods, namely Rough Set Theory (RST) and Self
Organizing Neuro- Fuzzy Inference System (SONFIS) the best rules on our
collected data sets has been extracted. The other benefit of later methods is
to predict new unknown cases. So, the reduced sets (reducts) by RST have been
obtained. Therefore the emerged results by utilizing mentioned methods shows
that the high sensitive variables are thickness of layer, length of stope, rate
of advance, number of miners, type of advancing.","9 Pages,9 Figures,submitted to :The 21st World Mining Congress &EXPO
  2008; 7-11 ;September 2008,;Krakow, Poland"
Toward Fuzzy block theory,"This study, fundamentals of fuzzy block theory, and its application in
assessment of stability in underground openings, has surveyed. Using fuzzy
topics and inserting them in to key block theory, in two ways, fundamentals of
fuzzy block theory has been presented. In indirect combining, by coupling of
adaptive Neuro Fuzzy Inference System (NFIS) and classic block theory, we could
extract possible damage parts around a tunnel. In direct solution, some
principles of block theory, by means of different fuzzy facets theory, were
rewritten.","8 PAGES,7 FIGURES"
"Analysis of hydrocyclone performance based on information granulation
  theory","This paper describes application of information granulation theory, on the
analysis of hydrocyclone perforamance. In this manner, using a combining of
Self Organizing Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and
fuzzy granules are obtained(briefly called SONFIS). Balancing of crisp granules
and sub fuzzy granules, within non fuzzy information (initial granulation), is
rendered in an open-close iteration. Using two criteria, ""simplicity of rules
""and ""adaptive threoshold error level"", stability of algorithm is guaranteed.
Validation of the proposed method, on the data set of the hydrocyclone is
rendered.","8th. World Congress on Computational Mechanics (WCCM8) 5th. European
  Congress on Computational Methods in Applied Sciences and Engineering
  (ECCOMAS 2008) 2008 Venice, Italy"
Logic programming with social features,"In everyday life it happens that a person has to reason about what other
people think and how they behave, in order to achieve his goals. In other
words, an individual may be required to adapt his behaviour by reasoning about
the others' mental state. In this paper we focus on a knowledge representation
language derived from logic programming which both supports the representation
of mental states of individual communities and provides each with the
capability of reasoning about others' mental states and acting accordingly. The
proposed semantics is shown to be translatable into stable model semantics of
logic programs with aggregates.","49 pages, 0 figures, to appear in Theory and Practice of Logic
  Programming (TPLP)"
Constructing Folksonomies from User-specified Relations on Flickr,"Many social Web sites allow users to publish content and annotate with
descriptive metadata. In addition to flat tags, some social Web sites have
recently began to allow users to organize their content and metadata
hierarchically. The social photosharing site Flickr, for example, allows users
to group related photos in sets, and related sets in collections. The social
bookmarking site Del.icio.us similarly lets users group related tags into
bundles. Although the sites themselves don't impose any constraints on how
these hierarchies are used, individuals generally use them to capture
relationships between concepts, most commonly the broader/narrower relations.
Collective annotation of content with hierarchical relations may lead to an
emergent classification system, called a folksonomy. While some researchers
have explored using tags as evidence for learning folksonomies, we believe that
hierarchical relations described above offer a high-quality source of evidence
for this task.
  We propose a simple approach to aggregate shallow hierarchies created by many
distinct Flickr users into a common folksonomy. Our approach uses statistics to
determine if a particular relation should be retained or discarded. The
relations are then woven together into larger hierarchies. Although we have not
carried out a detailed quantitative evaluation of the approach, it looks very
promising since it generates very reasonable, non-trivial hierarchies.","14 Pages, Submitted to the Workshop on Web Mining and Web Usage
  Analysis (WebKDD 2008)"
The Structure of Narrative: the Case of Film Scripts,"We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, ""Story: Substance, Structure, Style, and the Principles of
Screenwriting"". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.","28 pages, 7 figures, 21 references"
Feature Selection for Bayesian Evaluation of Trauma Death Risk,"In the last year more than 70,000 people have been brought to the UK
hospitals with serious injuries. Each time a clinician has to urgently take a
patient through a screening procedure to make a reliable decision on the trauma
treatment. Typically, such procedure comprises around 20 tests; however the
condition of a trauma patient remains very difficult to be tested properly.
What happens if these tests are ambiguously interpreted, and information about
the severity of the injury will come misleading? The mistake in a decision can
be fatal: using a mild treatment can put a patient at risk of dying from
posttraumatic shock, while using an overtreatment can also cause death. How can
we reduce the risk of the death caused by unreliable decisions? It has been
shown that probabilistic reasoning, based on the Bayesian methodology of
averaging over decision models, allows clinicians to evaluate the uncertainty
in decision making. Based on this methodology, in this paper we aim at
selecting the most important screening tests, keeping a high performance. We
assume that the probabilistic reasoning within the Bayesian methodology allows
us to discover new relationships between the screening tests and uncertainty in
decisions. In practice, selection of the most informative tests can also reduce
the cost of a screening procedure in trauma care centers. In our experiments we
use the UK Trauma data to compare the efficiency of the proposed technique in
terms of the performance. We also compare the uncertainty in decisions in terms
of entropy.","4 pages, 14th Nordic Baltic Conference on Biomedical Engineering and
  Medical Physics"
Fusion for Evaluation of Image Classification in Uncertain Environments,"We present in this article a new evaluation method for classification and
segmentation of textured images in uncertain environments. In uncertain
environments, real classes and boundaries are known with only a partial
certainty given by the experts. Most of the time, in many presented papers,
only classification or only segmentation are considered and evaluated. Here, we
propose to take into account both the classification and segmentation results
according to the certainty given by the experts. We present the results of this
method on a fusion of classifiers of sonar images for a seabed
characterization.",N/A
"Intuitive visualization of the intelligence for the run-down of
  terrorist wire-pullers","The investigation of the terrorist attack is a time-critical task. The
investigators have a limited time window to diagnose the organizational
background of the terrorists, to run down and arrest the wire-pullers, and to
take an action to prevent or eradicate the terrorist attack. The intuitive
interface to visualize the intelligence data set stimulates the investigators'
experience and knowledge, and aids them in decision-making for an immediately
effective action. This paper presents a computational method to analyze the
intelligence data set on the collective actions of the perpetrators of the
attack, and to visualize it into the form of a social network diagram which
predicts the positions where the wire-pullers conceals themselves.","10 pages, and 2 figures"
Rock mechanics modeling based on soft granulation theory,"This paper describes application of information granulation theory, on the
design of rock engineering flowcharts. Firstly, an overall flowchart, based on
information granulation theory has been highlighted. Information granulation
theory, in crisp (non-fuzzy) or fuzzy format, can take into account engineering
experiences (especially in fuzzy shape-incomplete information or superfluous),
or engineering judgments, in each step of designing procedure, while the
suitable instruments modeling are employed. In this manner and to extension of
soft modeling instruments, using three combinations of Self Organizing Map
(SOM), Neuro-Fuzzy Inference System (NFIS), and Rough Set Theory (RST) crisp
and fuzzy granules, from monitored data sets are obtained. The main underlined
core of our algorithms are balancing of crisp(rough or non-fuzzy) granules and
sub fuzzy granules, within non fuzzy information (initial granulation) upon the
open-close iterations. Using different criteria on balancing best granules
(information pockets), are obtained. Validations of our proposed methods, on
the data set of in-situ permeability in rock masses in Shivashan dam, Iran have
been highlighted.",N/A
An Ontology-based Knowledge Management System for Industry Clusters,"Knowledge-based economy forces companies in the nation to group together as a
cluster in order to maintain their competitiveness in the world market. The
cluster development relies on two key success factors which are knowledge
sharing and collaboration between the actors in the cluster. Thus, our study
tries to propose knowledge management system to support knowledge management
activities within the cluster. To achieve the objectives of this study,
ontology takes a very important role in knowledge management process in various
ways; such as building reusable and faster knowledge-bases, better way for
representing the knowledge explicitly. However, creating and representing
ontology create difficulties to organization due to the ambiguity and
unstructured of source of knowledge. Therefore, the objectives of this paper
are to propose the methodology to create and represent ontology for the
organization development by using knowledge engineering approach. The
handicraft cluster in Thailand is used as a case study to illustrate our
proposed methodology.",N/A
The Role of Artificial Intelligence Technologies in Crisis Response,"Crisis response poses many of the most difficult information technology in
crisis management. It requires information and communication-intensive efforts,
utilized for reducing uncertainty, calculating and comparing costs and
benefits, and managing resources in a fashion beyond those regularly available
to handle routine problems. In this paper, we explore the benefits of
artificial intelligence technologies in crisis response. This paper discusses
the role of artificial intelligence technologies; namely, robotics, ontology
and semantic web, and multi-agent systems in crisis response.","6 pages, 5 figures, 1 table, accepted for MENDEL 2008 14th
  International Conference on Soft Computing, June 18-20, Brno, Czech Republic"
"Toward a combination rule to deal with partial conflict and specificity
  in belief functions theory","We present and discuss a mixed conjunctive and disjunctive rule, a
generalization of conflict repartition rules, and a combination of these two
rules. In the belief functions theory one of the major problem is the conflict
repartition enlightened by the famous Zadeh's example. To date, many
combination rules have been proposed in order to solve a solution to this
problem. Moreover, it can be important to consider the specificity of the
responses of the experts. Since few year some unification rules are proposed.
We have shown in our previous works the interest of the proportional conflict
redistribution rule. We propose here a mixed combination rule following the
proportional conflict redistribution rule modified by a discounting procedure.
This rule generalizes many combination rules.","International Conference on Information Fusion, Qu\'ebec : Canada
  (2007)"
"A new generalization of the proportional conflict redistribution rule
  stable in terms of decision","In this chapter, we present and discuss a new generalized proportional
conflict redistribution rule. The Dezert-Smarandache extension of the
Demster-Shafer theory has relaunched the studies on the combination rules
especially for the management of the conflict. Many combination rules have been
proposed in the last few years. We study here different combination rules and
compare them in terms of decision on didactic example and on generated data.
Indeed, in real applications, we need a reliable decision and it is the final
results that matter. This chapter shows that a fine proportional conflict
redistribution rule must be preferred for the combination in the belief
function theory.",N/A
"Une nouvelle règle de combinaison répartissant le conflit -
  Applications en imagerie Sonar et classification de cibles Radar","These last years, there were many studies on the problem of the conflict
coming from information combination, especially in evidence theory. We can
summarise the solutions for manage the conflict into three different
approaches: first, we can try to suppress or reduce the conflict before the
combination step, secondly, we can manage the conflict in order to give no
influence of the conflict in the combination step, and then take into account
the conflict in the decision step, thirdly, we can take into account the
conflict in the combination step. The first approach is certainly the better,
but not always feasible. It is difficult to say which approach is the best
between the second and the third. However, the most important is the produced
results in applications. We propose here a new combination rule that
distributes the conflict proportionally on the element given this conflict. We
compare these different combination rules on real data in Sonar imagery and
Radar target classification.",N/A
Perfect Derived Propagators,"When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear equations both with and without coefficients? Constraint
variants are ubiquitous: implementing them requires considerable (if not
prohibitive) effort and decreases maintainability, but will deliver better
performance.
  This paper shows how to use variable views, previously introduced for an
implementation architecture, to derive perfect propagator variants. A model for
views and derived propagators is introduced. Derived propagators are proved to
be indeed perfect in that they inherit essential properties such as correctness
and domain and bounds consistency. Techniques for systematically deriving
propagators such as transformation, generalization, specialization, and
channeling are developed for several variable domains. We evaluate the massive
impact of derived propagators. Without derived propagators, Gecode would
require 140000 rather than 40000 lines of code for propagators.","17 pages, 2 tables"
Defaults and Normality in Causal Structures,"A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.",N/A
The model of quantum evolution,"This paper has been withdrawn by the author due to extremely unscientific
errors.",This paper has been withdrawn
Belief decision support and reject for textured images characterization,"The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment, it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover, on the areas that are the classification units, we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images, which contributes to an illustration.",N/A
"The Correspondence Analysis Platform for Uncovering Deep Structure in
  Data and Information","We study two aspects of information semantics: (i) the collection of all
relationships, (ii) tracking and spotting anomaly and change. The first is
implemented by endowing all relevant information spaces with a Euclidean metric
in a common projected space. The second is modelled by an induced ultrametric.
A very general way to achieve a Euclidean embedding of different information
spaces based on cross-tabulation counts (and from other input data formats) is
provided by Correspondence Analysis. From there, the induced ultrametric that
we are particularly interested in takes a sequential - e.g. temporal - ordering
of the data into account. We employ such a perspective to look at narrative,
""the flow of thought and the flow of language"" (Chafe). In application to
policy decision making, we show how we can focus analysis in a small number of
dimensions.","Sixth Annual Boole Lecture in Informatics, Boole Centre for Research
  in Informatics, Cork, Ireland, 29 April 2008. 28 pp., 17 figures. To appear,
  Computer Journal. This version: 3 typos corrected"
"Extension of Inagaki General Weighted Operators and A New Fusion Rule
  Class of Proportional Redistribution of Intersection Masses","In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.","6 pages; SWIFT 2008 - Skovde Workshop on Information Fusion Topics,
  Sweden;"
"Implementing general belief function framework with a practical
  codification for low complexity","In this chapter, we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity, the eventual constraints must be integrated in the
codification at the beginning. Hence, we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied, indeed, when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence, two approaches are proposed, an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory.","Advances and Applications of DSmT for Information Fusion, Florentin
  Smarandache & Jean Dezert (Ed.) (2008) Pnd"
A new probabilistic transformation of belief mass assignment,"In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a
new probabilistic transformation, called DSmP, in order to build a subjective
probability measure from any basic belief assignment defined on any model of
the frame of discernment. Several examples are given to show how the DSmP
transformation works and we compare it to main existing transformations
proposed in the literature so far. We show the advantages of DSmP over
classical transformations in term of Probabilistic Information Content (PIC).
The direct extension of this transformation for dealing with qualitative belief
assignments is also presented.",N/A
"On Introspection, Metacognitive Control and Augmented Data Mining Live
  Cycles","We discuss metacognitive modelling as an enhancement to cognitive modelling
and computing. Metacognitive control mechanisms should enable AI systems to
self-reflect, reason about their actions, and to adapt to new situations. In
this respect, we propose implementation details of a knowledge taxonomy and an
augmented data mining life cycle which supports a live integration of obtained
models.","10 pages, 3 figures"
Hacia una teoria de unificacion para los comportamientos cognitivos,"Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found, and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book, Philosophiae Naturalis Principia
Mathematica, to the cognitive behaviors. The third is to propose a scientific
theory, currently developing, that follows the rules established by Newton to
make sense of nature, and could be the theory to explain all the cognitive
behaviors.","63 pages, 4 figures, Spanish, mistakes erased"
Verified Null-Move Pruning,"In this article we review standard null-move pruning and introduce our
extended version of it, which we call verified null-move pruning. In verified
null-move pruning, whenever the shallow null-move search indicates a fail-high,
instead of cutting off the search from the current node, the search is
continued with reduced depth.
  Our experiments with verified null-move pruning show that on average, it
constructs a smaller search tree with greater tactical strength in comparison
to standard null-move pruning. Moreover, unlike standard null-move pruning,
which fails badly in zugzwang positions, verified null-move pruning manages to
detect most zugzwangs and in such cases conducts a re-search to obtain the
correct result. In addition, verified null-move pruning is very easy to
implement, and any standard null-move pruning program can use verified
null-move pruning by modifying only a few lines of code.",9 pages
n-ary Fuzzy Logic and Neutrosophic Logic Operators,"We extend Knuth's 16 Boolean binary logic operators to fuzzy logic and
neutrosophic logic binary operators. Then we generalize them to n-ary fuzzy
logic and neutrosophic logic operators using the smarandache codification of
the Venn diagram and a defined vector neutrosophic law. In such way, new
operators in neutrosophic logic/set/probability are built.","15 pages, 2 fuzzy and neutrosophic value tables, many diagrams"
"Randomised Variable Neighbourhood Search for Multi Objective
  Optimisation","Various local search approaches have recently been applied to machine
scheduling problems under multiple objectives. Their foremost consideration is
the identification of the set of Pareto optimal alternatives. An important
aspect of successfully solving these problems lies in the definition of an
appropriate neighbourhood structure. Unclear in this context remains, how
interdependencies within the fitness landscape affect the resolution of the
problem.
  The paper presents a study of neighbourhood search operators for multiple
objective flow shop scheduling. Experiments have been carried out with twelve
different combinations of criteria. To derive exact conclusions, small problem
instances, for which the optimal solutions are known, have been chosen.
Statistical tests show that no single neighbourhood operator is able to equally
identify all Pareto optimal alternatives. Significant improvements however have
been obtained by hybridising the solution algorithm using a randomised variable
neighbourhood search technique.",N/A
Foundations of the Pareto Iterated Local Search Metaheuristic,"The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality, another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby, Sweden
(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)","Proceedings of the 18th International Conference on Multiple Criteria
  Decision Making, Chania, Greece, June 19-23, 2006"
"A Computational Study of Genetic Crossover Operators for Multi-Objective
  Vehicle Routing Problem with Soft Time Windows","The article describes an investigation of the effectiveness of genetic
algorithms for multi-objective combinatorial optimization (MOCO) by presenting
an application for the vehicle routing problem with soft time windows. The work
is motivated by the question, if and how the problem structure influences the
effectiveness of different configurations of the genetic algorithm.
Computational results are presented for different classes of vehicle routing
problems, varying in their coverage with time windows, time window size,
distribution and number of customers. The results are compared with a simple,
but effective local search approach for multi-objective combinatorial
optimization problems.",N/A
Genetic Algorithms for multiple objective vehicle routing,"The talk describes a general approach of a genetic algorithm for multiple
objective optimization problems. A particular dominance relation between the
individuals of the population is used to define a fitness operator, enabling
the genetic algorithm to adress even problems with efficient, but
convex-dominated alternatives. The algorithm is implemented in a multilingual
computer program, solving vehicle routing problems with time windows under
multiple objectives. The graphical user interface of the program shows the
progress of the genetic algorithm and the main parameters of the approach can
be easily modified. In addition to that, the program provides powerful decision
support to the decision maker. The software has proved it's excellence at the
finals of the European Academic Software Award EASA, held at the Keble college/
University of Oxford/ Great Britain.",N/A
"A framework for the interactive resolution of multi-objective vehicle
  routing problems","The article presents a framework for the resolution of rich vehicle routing
problems which are difficult to address with standard optimization techniques.
We use local search on the basis on variable neighborhood search for the
construction of the solutions, but embed the techniques in a flexible framework
that allows the consideration of complex side constraints of the problem such
as time windows, multiple depots, heterogeneous fleets, and, in particular,
multiple optimization criteria. In order to identify a compromise alternative
that meets the requirements of the decision maker, an interactive procedure is
integrated in the resolution of the problem, allowing the modification of the
preference information articulated by the decision maker. The framework is
prototypically implemented in a computer system. First results of test runs on
multiple depot vehicle routing problems with time windows are reported.","Proceedings of the 7th EU/ME Workshop: Adaptive, Self-Adaptive, and
  Multi-Level Metaheuristics, Malaga, Spain, November 16-17, 2006"
Improving Local Search for Fuzzy Scheduling Problems,"The integration of fuzzy set theory and fuzzy logic into scheduling is a
rather new aspect with growing importance for manufacturing applications,
resulting in various unsolved aspects. In the current paper, we investigate an
improved local search technique for fuzzy scheduling problems with fitness
plateaus, using a multi criteria formulation of the problem. We especially
address the problem of changing job priorities over time as studied at the
Sherwood Press Ltd, a Nottingham based printing company, who is a collaborator
on the project.",N/A
"Bin Packing Under Multiple Objectives - a Heuristic Approximation
  Approach","The article proposes a heuristic approximation approach to the bin packing
problem under multiple objectives. In addition to the traditional objective of
minimizing the number of bins, the heterogeneousness of the elements in each
bin is minimized, leading to a biobjective formulation of the problem with a
tradeoff between the number of bins and their heterogeneousness. An extension
of the Best-Fit approximation algorithm is presented to solve the problem.
Experimental investigations have been carried out on benchmark instances of
different size, ranging from 100 to 1000 items. Encouraging results have been
obtained, showing the applicability of the heuristic approach to the described
problem.",N/A
"An application of the Threshold Accepting metaheuristic for curriculum
  based course timetabling","The article presents a local search approach for the solution of timetabling
problems in general, with a particular implementation for competition track 3
of the International Timetabling Competition 2007 (ITC 2007). The heuristic
search procedure is based on Threshold Accepting to overcome local optima. A
stochastic neighborhood is proposed and implemented, randomly removing and
reassigning events from the current solution.
  The overall concept has been incrementally obtained from a series of
experiments, which we describe in each (sub)section of the paper. In result, we
successfully derived a potential candidate solution approach for the finals of
track 3 of the ITC 2007.",N/A
"Variable Neighborhood Search for the University Lecturer-Student
  Assignment Problem","The paper presents a study of local search heuristics in general and variable
neighborhood search in particular for the resolution of an assignment problem
studied in the practical work of universities. Here, students have to be
assigned to scientific topics which are proposed and supported by members of
staff. The problem involves the optimization under given preferences of
students which may be expressed when applying for certain topics.
  It is possible to observe that variable neighborhood search leads to superior
results for the tested problem instances. One instance is taken from an actual
case, while others have been generated based on the real world data to support
the analysis with a deeper analysis.
  An extension of the problem has been formulated by integrating a second
objective function that simultaneously balances the workload of the members of
staff while maximizing utility of the students. The algorithmic approach has
been prototypically implemented in a computer system. One important aspect in
this context is the application of the research work to problems of other
scientific institutions, and therefore the provision of decision support
functionalities.","Proceedings of the 18th Mini Euro Conference on Variable Neighborhood
  Search, November 23-25, 2005, Puerto de La Cruz, Tenerife, Spain, ISBN
  84-689-5679-1"
Extended ASP tableaux and rule redundancy in normal logic programs,"We introduce an extended tableau calculus for answer set programming (ASP).
The proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP
2006], with an added extension rule. We investigate the power of Extended ASP
Tableaux both theoretically and empirically. We study the relationship of
Extended ASP Tableaux with the Extended Resolution proof system defined by
Tseitin for sets of clauses, and separate Extended ASP Tableaux from ASP
Tableaux by giving a polynomial-length proof for a family of normal logic
programs P_n for which ASP Tableaux has exponential-length minimal proofs with
respect to n. Additionally, Extended ASP Tableaux imply interesting insight
into the effect of program simplification on the lengths of proofs in ASP.
Closely related to Extended ASP Tableaux, we empirically investigate the effect
of redundant rules on the efficiency of ASP solving.
  To appear in Theory and Practice of Logic Programming (TPLP).","27 pages, 5 figures, 1 table"
"Achieving compositionality of the stable model semantics for Smodels
  programs","In this paper, a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future, the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover,
the respective notion of module-level equivalence, namely modular equivalence,
is shown to be a proper congruence relation: it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming.","44 pages, 2 tables"
"Determining the Unithood of Word Sequences using a Probabilistic
  Approach","Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently, novelties are rare in this
small sub-field of term extraction. In addition, existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure, independent of any influences of termhood, that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1,825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision,
recall and accuracy.","More information is available at
  http://explorer.csse.uwa.edu.au/reference/"
"Determining the Unithood of Word Sequences using Mutual Information and
  Independence Measure","Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently, the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach, independent
of any influences of termhood, that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases.","More information is available at
  http://explorer.csse.uwa.edu.au/reference/"
Enhanced Integrated Scoring for Cleaning Dirty Texts,"An increasing number of approaches for ontology engineering from text are
gearing towards the use of online sources such as company intranet and the
World Wide Web. Despite such rise, not much work can be found in aspects of
preprocessing and cleaning dirty texts from online sources. This paper presents
an enhancement of an Integrated Scoring for Spelling error correction,
Abbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as
part of a text preprocessing phase in an ontology engineering system. New
evaluations performed on the enhanced ISSAC using 700 chat records reveal an
improved accuracy of 98% as compared to 96.5% and 71% based on the use of only
basic ISSAC and of Aspell, respectively.","More information is available at
  http://explorer.csse.uwa.edu.au/reference/"
On-the-fly Macros,"We present a domain-independent algorithm that computes macros in a novel
way. Our algorithm computes macros ""on-the-fly"" for a given set of states and
does not require previously learned or inferred information, nor prior domain
knowledge. The algorithm is used to define new domain-independent tractable
classes of classical planning that are proved to include \emph{Blocksworld-arm}
and \emph{Towers of Hanoi}.",N/A
Modeling of Social Transitions Using Intelligent Systems,"In this study, we reproduce two new hybrid intelligent systems, involve three
prominent intelligent computing and approximate reasoning methods: Self
Organizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory
(RST),called SONFIS and SORST. We show how our algorithms can be construed as a
linkage of government-society interactions, where government catches various
states of behaviors: solid (absolute) or flexible. So, transition of society,
by changing of connectivity parameters (noise) from order to disorder is
inferred.",N/A
"Relationship between Diversity and Perfomance of Multiple Classifiers
  for Decision Support","The paper presents the investigation and implementation of the relationship
between diversity and the performance of multiple classifiers on classification
accuracy. The study is critical as to build classifiers that are strong and can
generalize better. The parameters of the neural network within the committee
were varied to induce diversity; hence structural diversity is the focus for
this study. The hidden nodes and the activation function are the parameters
that were varied. The diversity measures that were adopted from ecology such as
Shannon and Simpson were used to quantify diversity. Genetic algorithm is used
to find the optimal ensemble by using the accuracy as the cost function. The
results observed shows that there is a relationship between structural
diversity and accuracy. It is observed that the classification accuracy of an
ensemble increases as the diversity increases. There was an increase of 3%-6%
in the classification accuracy.",6 pages
"Balancing Exploration and Exploitation by an Elitist Ant System with
  Exponential Pheromone Deposition Rule","The paper presents an exponential pheromone deposition rule to modify the
basic ant system algorithm which employs constant deposition rule. A stability
analysis using differential equation is carried out to find out the values of
parameters that make the ant system dynamics stable for both kinds of
deposition rule. A roadmap of connected cities is chosen as the problem
environment where the shortest route between two given cities is required to be
discovered. Simulations performed with both forms of deposition approach using
Elitist Ant System model reveal that the exponential deposition approach
outperforms the classical one by a large extent. Exhaustive experiments are
also carried out to find out the optimum setting of different controlling
parameters for exponential deposition approach and an empirical relationship
between the major controlling parameters of the algorithm and some features of
problem environment.","2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA.
  Paper ID: 250"
A Novel Parser Design Algorithm Based on Artificial Ants,"This article presents a unique design for a parser using the Ant Colony
Optimization algorithm. The paper implements the intuitive thought process of
human mind through the activities of artificial ants. The scheme presented here
uses a bottom-up approach and the parsing program can directly use ambiguous or
redundant grammars. We allocate a node corresponding to each production rule
present in the given grammar. Each node is connected to all other nodes
(representing other production rules), thereby establishing a completely
connected graph susceptible to the movement of artificial ants. Each ant tries
to modify this sentential form by the production rule present in the node and
upgrades its position until the sentential form reduces to the start symbol S.
Successful ants deposit pheromone on the links that they have traversed
through. Eventually, the optimum path is discovered by the links carrying
maximum amount of pheromone concentration. The design is simple, versatile,
robust and effective and obviates the calculation of the above mentioned sets
and precedence relation tables. Further advantages of our scheme lie in i)
ascertaining whether a given string belongs to the language represented by the
grammar, and ii) finding out the shortest possible path from the given string
to the start symbol S in case multiple routes exist.","4th IEEE International Conference on Information and Automation for
  Sustainability, 2008"
"Extension of Max-Min Ant System with Exponential Pheromone Deposition
  Rule","The paper presents an exponential pheromone deposition approach to improve
the performance of classical Ant System algorithm which employs uniform
deposition rule. A simplified analysis using differential equations is carried
out to study the stability of basic ant system dynamics with both exponential
and constant deposition rules. A roadmap of connected cities, where the
shortest path between two specified cities are to be found out, is taken as a
platform to compare Max-Min Ant System model (an improved and popular model of
Ant System algorithm) with exponential and constant deposition rules. Extensive
simulations are performed to find the best parameter settings for non-uniform
deposition approach and experiments with these parameter settings revealed that
the above approach outstripped the traditional one by a large extent in terms
of both solution quality and convergence time.","16th IEEE International Conference on Advanced Computing and
  Communication, 2008"
"Document stream clustering: experimenting an incremental algorithm and
  AR-based tools for highlighting dynamic trends","We address here two major challenges presented by dynamic data mining: 1) the
stability challenge: we have implemented a rigorous incremental density-based
clustering algorithm, independent from any initial conditions and ordering of
the data-vectors stream, 2) the cognitive challenge: we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database.",N/A
"Classification dynamique d'un flux documentaire : une évaluation
  statique préalable de l'algorithme GERMEN","Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement, mainly for ""weak signals"" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t, by identifying the local perturbations induced by the current
document vector, such as changing cluster borders, or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter, 2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics.",N/A
"Étude longitudinale d'une procédure de modélisation de
  connaissances en matière de gestion du territoire agricole","This paper gives an introduction to this issue, and presents the framework
and the main steps of the Rosa project. Four teams of researchers, agronomists,
computer scientists, psychologists and linguists were involved during five
years within this project that aimed at the development of a knowledge based
system. The purpose of the Rosa system is the modelling and the comparison of
farm spatial organizations. It relies on a formalization of agronomical
knowledge and thus induces a joint knowledge building process involving both
the agronomists and the computer scientists. The paper describes the steps of
the modelling process as well as the filming procedures set up by the
psychologists and linguists in order to make explicit and to analyze the
underlying knowledge building process.",N/A
Modeling Social Annotation: a Bayesian Approach,"Collaborative tagging systems, such as Delicious, CiteULike, and others,
allow users to annotate resources, e.g., Web pages or scientific papers, with
descriptive labels called tags. The social annotations contributed by thousands
of users, can potentially be used to infer categorical knowledge, classify
documents or recommend new relevant information. Traditional text inference
methods do not make best use of social annotation, since they do not take into
account variations in individual users' perspectives and vocabulary. In a
previous work, we introduced a simple probabilistic model that takes interests
of individual annotators into account in order to find hidden topics of
annotated resources. Unfortunately, that approach had one major shortcoming:
the number of topics and interests must be specified a priori. To address this
drawback, we extend the model to a fully Bayesian framework, which offers a way
to automatically estimate these numbers. In particular, the model allows the
number of interests and topics to change as suggested by the structure of the
data. We evaluate the proposed model in detail on the synthetic and real-world
data by comparing its performance to Latent Dirichlet Allocation on the topic
extraction task. For the latter evaluation, we apply the model to infer topics
of Web resources from social annotations obtained from Delicious in order to
discover new resources similar to a specified one. Our empirical results
demonstrate that the proposed model is a promising method for exploiting social
knowledge contained in user-generated annotations.","29 Pages, Accepted for publication at ACM Transactions on Knowledge
  Discovery from Data(TKDD) on March 2, 2010"
Airport Gate Assignment: New Model and Implementation,"Airport gate assignment is of great importance in airport operations. In this
paper, we study the Airport Gate Assignment Problem (AGAP), propose a new model
and implement the model with Optimization Programming language (OPL). With the
objective to minimize the number of conflicts of any two adjacent aircrafts
assigned to the same gate, we build a mathematical model with logical
constraints and the binary constraints, which can provide an efficient
evaluation criterion for the Airlines to estimate the current gate assignment.
To illustrate the feasibility of the model we construct experiments with the
data obtained from Continental Airlines, Houston Gorge Bush Intercontinental
Airport IAH, which indicate that our model is both energetic and effective.
Moreover, we interpret experimental results, which further demonstrate that our
proposed model can provide a powerful tool for airline companies to estimate
the efficiency of their current work of gate assignment.","5 pages, 2 figures, 1 table. Accepted by ICOR 2008"
Artificial Intelligence Techniques for Steam Generator Modelling,"This paper investigates the use of different Artificial Intelligence methods
to predict the values of several continuous variables from a Steam Generator.
The objective was to determine how the different artificial intelligence
methods performed in making predictions on the given dataset. The artificial
intelligence methods evaluated were Neural Networks, Support Vector Machines,
and Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks
investigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian
and committee techniques were applied to these neural networks. Each of the AI
methods considered was simulated in Matlab. The results of the simulations
showed that all the AI methods were capable of predicting the Steam Generator
data reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system
out performed the other methods in terms of accuracy and ease of
implementation, while still achieving a fast execution time as well as a
reasonable training time.",23 pages
Elementary epistemological features of machine intelligence,"Theoretical analysis of machine intelligence (MI) is useful for defining a
common platform in both theoretical and applied artificial intelligence (AI).
The goal of this paper is to set canonical definitions that can assist
pragmatic research in both strong and weak AI. Described epistemological
features of machine intelligence include relationship between intelligent
behavior, intelligent and unintelligent machine characteristics, observable and
unobservable entities and classification of intelligence. The paper also
establishes algebraic definitions of efficiency and accuracy of MI tests as
their quality measure. The last part of the paper addresses the learning
process with respect to the traditional epistemology and the epistemology of MI
described here. The proposed views on MI positively correlate to the Hegelian
monistic epistemology and contribute towards amalgamating idealistic
deliberations with the AI theory, particularly in a local frame of reference.",The paper needs to be redesigned
Logic programs with propositional connectives and aggregates,"Answer set programming (ASP) is a logic programming paradigm that can be used
to solve complex combinatorial search problems. Aggregates are an ASP construct
that plays an important role in many applications. Defining a satisfactory
semantics of aggregates turned out to be a difficult problem, and in this paper
we propose a new approach, based on an analogy between aggregates and
propositional connectives. First, we extend the definition of an answer
set/stable model to cover arbitrary propositional theories; then we define
aggregates on top of them both as primitive constructs and as abbreviations for
formulas. Our definition of an aggregate combines expressiveness and
simplicity, and it inherits many theorems about programs with nested
expressions, such as theorems about strong equivalence and splitting.",N/A
"Identification of parameters underlying emotions and a classification of
  emotions","The standard classification of emotions involves categorizing the expression
of emotions. In this paper, parameters underlying some emotions are identified
and a new classification based on these parameters is suggested.",6 pages
"Prediction of Platinum Prices Using Dynamically Weighted Mixture of
  Experts","Neural networks are powerful tools for classification and regression in
static environments. This paper describes a technique for creating an ensemble
of neural networks that adapts dynamically to changing conditions. The model
separates the input space into four regions and each network is given a weight
in each region based on its performance on samples from that region. The
ensemble adapts dynamically by constantly adjusting these weights based on the
current performance of the networks. The data set used is a collection of
financial indicators with the goal of predicting the future platinum price. An
ensemble with no weightings does not improve on the naive estimate of no weekly
change; our weighting algorithm gives an average percentage error of 63% for
twenty weeks of prediction.",N/A
"Analyse et structuration automatique des guides de bonnes pratiques
  cliniques : essai d'évaluation","Health Practice Guideliens are supposed to unify practices and propose
recommendations to physicians. This paper describes GemFrame, a system capable
of semi-automatically filling an XML template from free texts in the clinical
domain. The XML template includes semantic information not explicitly encoded
in the text (pairs of conditions and ac-tions/recommendations). Therefore,
there is a need to compute the exact scope of condi-tions over text sequences
expressing the re-quired actions. We present a system developped for this task.
We show that it yields good performance when applied to the analysis of French
practice guidelines. We conclude with a precise evaluation of the tool.",N/A
"Automatic Construction of Lightweight Domain Ontologies for Chemical
  Engineering Risk Management","The need for domain ontologies in mission critical applications such as risk
management and hazard identification is becoming more and more pressing. Most
research on ontology learning conducted in the academia remains unrealistic for
real-world applications. One of the main problems is the dependence on
non-incremental, rare knowledge and textual resources, and manually-crafted
patterns and rules. This paper reports work in progress aiming to address such
undesirable dependencies during ontology construction. Initial experiments
using a working prototype of the system revealed promising potentials in
automatically constructing high-quality domain ontologies using real-world
texts.","In the Proceedings of the 11th Conference on Process Integration,
  Modelling and Optimisation for Energy Saving and Pollution Reduction (PRES),
  Prague, Czech Rep., August, 2008"
"Approximate inference on planar graphs using Loop Calculus and Belief
  Propagation","We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak, 2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general, full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al., 2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general, intractable planar model, can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference.","23 pages, 10 figures. Submitted to Journal of Machine Learning
  Research. Proceedings version accepted for UAI 2009"
"N-norm and N-conorm in Neutrosophic Logic and Set, and the Neutrosophic
  Topologies","In this paper we present the N-norms/N-conorms in neutrosophic logic and set
as extensions of T-norms/T-conorms in fuzzy logic and set. Also, as an
extension of the Intuitionistic Fuzzy Topology we present the Neutrosophic
Topologies.","11 pages, 3 diagrams"
Deceptiveness and Neutrality - the ND family of fitness landscapes,"When a considerable number of mutations have no effects on fitness values,
the fitness landscape is said neutral. In order to study the interplay between
neutrality, which exists in many real-world applications, and performances of
metaheuristics, it is useful to design landscapes which make it possible to
tune precisely neutral degree distribution. Even though many neutral landscape
models have already been designed, none of them are general enough to create
landscapes with specific neutral degree distributions. We propose three steps
to design such landscapes: first using an algorithm we construct a landscape
whose distribution roughly fits the target one, then we use a simulated
annealing heuristic to bring closer the two distributions and finally we affect
fitness values to each neutral network. Then using this new family of fitness
landscapes we are able to highlight the interplay between deceptiveness and
neutrality.","Genetic And Evolutionary Computation Conference, Seatle :
  \'Etats-Unis d'Am\'erique (2006)"
Mining for adverse drug events with formal concept analysis,"The pharmacovigilance databases consist of several case reports involving
drugs and adverse events (AEs). Some methods are applied consistently to
highlight all signals, i.e. all statistically significant associations between
a drug and an AE. These methods are appropriate for verification of more
complex relationships involving one or several drug(s) and AE(s) (e.g;
syndromes or interactions) but do not address the identification of them. We
propose a method for the extraction of these relationships based on Formal
Concept Analysis (FCA) associated with disproportionality measures. This method
identifies all sets of drugs and AEs which are potential signals, syndromes or
interactions. Compared to a previous experience of disproportionality analysis
without FCA, the addition of FCA was more efficient for identifying false
positives related to concomitant drugs.",N/A
"A Knowledge Discovery Framework for Learning Task Models from User
  Interactions in Intelligent Tutoring Systems","Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system.","Proceedings of the 7th Mexican International Conference on Artificial
  Intelligence (MICAI 2008), Springer, pp. 765-778"
How Emotional Mechanism Helps Episodic Learning in a Cognitive Agent,"In this paper we propose the CTS (Concious Tutoring System) technology, a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures, stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience, because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover, the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm.",N/A
Alleviating Media Bias Through Intelligent Agent Blogging,"Consumers of mass media must have a comprehensive, balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging, laborious and time consuming. News stories development
over time, its (in)consistency, and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process, while holding media outlets accountable for their work.",N/A
"Comparative concept similarity over Minspaces: Axiomatisation and
  Tableaux Calculus","We study the logic of comparative concept similarity $\CSL$ introduced by
Sheremet, Tishkovsky, Wolter and Zakharyaschev to capture a form of qualitative
similarity comparison. In this logic we can formulate assertions of the form ""
objects A are more similar to B than to C"". The semantics of this logic is
defined by structures equipped by distance functions evaluating the similarity
degree of objects. We consider here the particular case of the semantics
induced by \emph{minspaces}, the latter being distance spaces where the minimum
of a set of distances always exists. It turns out that the semantics over
arbitrary minspaces can be equivalently specified in terms of preferential
structures, typical of conditional logics. We first give a direct
axiomatisation of this logic over Minspaces. We next define a decision
procedure in the form of a tableaux calculus. Both the calculus and the
axiomatisation take advantage of the reformulation of the semantics in terms of
preferential structures.",25 pages
A Model for Managing Collections of Patterns,"Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains, end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless, those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover, some privacy issues, or some commercial one may lead the users not to
be able to mine the data by themselves. Thus, the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus, in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases.",N/A
Feature Hashing for Large Scale Multitask Learning,"Empirical evidence suggests that hashing is an effective strategy for
dimensionality reduction and practical nonparametric estimation. In this paper
we provide exponential tail bounds for feature hashing and show that the
interaction between random subspaces is negligible with high probability. We
demonstrate the feasibility of this approach with experimental results for a
new use case -- multitask learning with hundreds of thousands of tasks.",Fixed broken theorem
XML Representation of Constraint Networks: Format XCSP 2.1,"We propose a new extended format to represent constraint networks using XML.
This format allows us to represent constraints defined either in extension or
in intension. It also allows us to reference global constraints. Any instance
of the problems CSP (Constraint Satisfaction Problem), QCSP (Quantified CSP)
and WCSP (Weighted CSP) can be represented using this format.",N/A
The Semantics of Kalah Game,"The present work consisted in developing a plateau game. There are the
traditional ones (monopoly, cluedo, ect.) but those which interest us leave
less place at the chance (luck) than to the strategy such that the chess game.
Kallah is an old African game, its rules are simple but the strategies to be
used are very complex to implement. Of course, they are based on a strongly
mathematical basis as in the film ""Rain-Man"" where one can see that gambling
can be payed with strategies based on mathematical theories. The Artificial
Intelligence gives the possibility ""of thinking"" to a machine and, therefore,
allows it to make decisions. In our work, we use it to give the means to the
computer choosing its best movement.",N/A
Learning DTW Global Constraint for Time Series Classification,"1-Nearest Neighbor with the Dynamic Time Warping (DTW) distance is one of the
most effective classifiers on time series domain. Since the global constraint
has been introduced in speech community, many global constraint models have
been proposed including Sakoe-Chiba (S-C) band, Itakura Parallelogram, and
Ratanamahatana-Keogh (R-K) band. The R-K band is a general global constraint
model that can represent any global constraints with arbitrary shape and size
effectively. However, we need a good learning algorithm to discover the most
suitable set of R-K bands, and the current R-K band learning algorithm still
suffers from an 'overfitting' phenomenon. In this paper, we propose two new
learning algorithms, i.e., band boundary extraction algorithm and iterative
learning algorithm. The band boundary extraction is calculated from the bound
of all possible warping paths in each class, and the iterative learning is
adjusted from the original R-K band learning. We also use a Silhouette index, a
well-known clustering validation technique, as a heuristic function, and the
lower bound function, LB_Keogh, to enhance the prediction speed. Twenty
datasets, from the Workshop and Challenge on Time Series Classification, held
in conjunction of the SIGKDD 2007, are used to evaluate our approach.","The first runner up of Workshop and Challenge on Time Series
  Classification held in conjunction with SIGKDD 2007. 8 pages, 5 figures"
"Range and Roots: Two Common Patterns for Specifying and Propagating
  Counting and Occurrence Constraints","We propose Range and Roots which are two common patterns useful for
specifying a wide range of counting and occurrence constraints. We design
specialised propagation algorithms for these two patterns. Counting and
occurrence constraints specified using these patterns thus directly inherit a
propagation algorithm. To illustrate the capabilities of the Range and Roots
constraints, we specify a number of global constraints taken from the
literature. Preliminary experiments demonstrate that propagating counting and
occurrence constraints using these two patterns leads to a small loss in
performance when compared to specialised global constraints and is competitive
with alternative decompositions using elementary constraints.","41 pages, 7 figures"
An introduction to DSmT,"The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been, and
still remains today, of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this
introduction, we present a survey of our recent theory of plausible and
paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT), developed for
dealing with imprecise, uncertain and conflicting sources of information. We
focus our presentation on the foundations of DSmT and on its most important
rules of combination, rather than on browsing specific applications of DSmT
available in literature. Several simple examples are given throughout this
presentation to show the efficiency and the generality of this new approach.",N/A
Granularity-Adaptive Proof Presentation,"When mathematicians present proofs they usually adapt their explanations to
their didactic goals and to the (assumed) knowledge of their addressees. Modern
automated theorem provers, in contrast, present proofs usually at a fixed level
of detail (also called granularity). Often these presentations are neither
intended nor suitable for human use. A challenge therefore is to develop user-
and goal-adaptive proof presentation techniques that obey common mathematical
practice. We present a flexible and adaptive approach to proof presentation
that exploits machine learning techniques to extract a model of the specific
granularity of proof examples and employs this model for the automated
generation of further proofs at an adapted level of granularity.","Extended Version. This SEKI Working-Paper refines and extends the
  following publication: Granularity-Adaptive Proof Presentation. Proceedings
  of the 14th International Conference on Artificial Intelligence in Education;
  Brighton, UK, 2009. Submitted"
Breaking Value Symmetry,"Symmetry is an important factor in solving many constraint satisfaction
problems. One common type of symmetry is when we have symmetric values. In a
recent series of papers, we have studied methods to break value symmetries. Our
results identify computational limits on eliminating value symmetry. For
instance, we prove that pruning all symmetric values is NP-hard in general.
Nevertheless, experiments show that much value symmetry can be broken in
practice. These results may be useful to researchers in planning, scheduling
and other areas as value symmetry occurs in many different domains.","Proceedings of the Twenty-Third AAAI Conference on Artificial
  Intelligence"
Reformulating Global Grammar Constraints,"An attractive mechanism to specify global constraints in rostering and other
domains is via formal languages. For instance, the Regular and Grammar
constraints specify constraints in terms of the languages accepted by an
automaton and a context-free grammar respectively. Taking advantage of the
fixed length of the constraint, we give an algorithm to transform a
context-free grammar into an automaton. We then study the use of minimization
techniques to reduce the size of such automata and speed up propagation. We
show that minimizing such automata after they have been unfolded and domains
initially reduced can give automata that are more compact than minimizing
before unfolding and reducing. Experimental results show that such
transformations can improve the size of rostering problems that we can 'model
and run'.","15 pages, 4 figures"
Combining Symmetry Breaking and Global Constraints,"We propose a new family of constraints which combine together lexicographical
ordering constraints for symmetry breaking with other common global
constraints. We give a general purpose propagator for this family of
constraints, and show how to improve its complexity by exploiting properties of
the included global constraints.","15 pages, 4 figures"
Online Estimation of SAT Solving Runtime,"We present an online method for estimating the cost of solving SAT problems.
Modern SAT solvers present several challenges to estimate search cost including
non-chronological backtracking, learning and restarts. Our method uses a linear
model trained on data gathered at the start of search. We show the
effectiveness of this method using random and structured problems. We
demonstrate that predictions made in early restarts can be used to improve
later predictions. We also show that we can use such cost estimations to select
a solver from a portfolio.","6 pages, 3 figures. Proc. of the 11th International Conf. on Theory
  and Applications of Satisfiability Testing, Guangzhou, China, May 2008"
On Requirements for Programming Exercises from an E-learning Perspective,"In this work, we deal with the question of modeling programming exercises for
novices pointing to an e-learning scenario. Our purpose is to identify basic
requirements, raise some key questions and propose potential answers from a
conceptual perspective. Presented as a general picture, we hypothetically
situate our work in a general context where e-learning instructional material
needs to be adapted to form part of an introductory Computer Science (CS)
e-learning course at the CS1-level. Meant is a potential course which aims at
improving novices skills and knowledge on the essentials of programming by
using e-learning based approaches in connection (at least conceptually) with a
general host framework like Activemath (www.activemath.org). Our elaboration
covers contextual and, particularly, cognitive elements preparing the terrain
for eventual research stages in a derived project, as indicated. We concentrate
our main efforts on reasoning mechanisms about exercise complexity that can
eventually offer tool support for the task of exercise authoring. We base our
requirements analysis on our own perception of the exercise subsystem provided
by Activemath especially within the domain reasoner area. We enrich the
analysis by bringing to the discussion several relevant contextual elements
from the CS1 courses, its definition and implementation. Concerning cognitive
models and exercises, we build upon the principles of Bloom's Taxonomy as a
relatively standardized basis and use them as a framework for study and
analysis of complexity in basic programming exercises. Our analysis includes
requirements for the domain reasoner which are necessary for the exercise
analysis. We propose for such a purpose a three-layered conceptual model
considering exercise evaluation, programming and metaprogramming.",ii + 31 pages
Tagging multimedia stimuli with ontologies,"Successful management of emotional stimuli is a pivotal issue concerning
Affective Computing (AC) and the related research. As a subfield of Artificial
Intelligence, AC is concerned not only with the design of computer systems and
the accompanying hardware that can recognize, interpret, and process human
emotions, but also with the development of systems that can trigger human
emotional response in an ordered and controlled manner. This requires the
maximum attainable precision and efficiency in the extraction of data from
emotionally annotated databases While these databases do use keywords or tags
for description of the semantic content, they do not provide either the
necessary flexibility or leverage needed to efficiently extract the pertinent
emotional content. Therefore, to this extent we propose an introduction of
ontologies as a new paradigm for description of emotionally annotated data. The
ability to select and sequence data based on their semantic attributes is vital
for any study involving metadata, semantics and ontological sorting like the
Semantic Web or the Social Semantic Desktop, and the approach described in the
paper facilitates reuse in these areas as well.","7 pages, 7 figures, 1 table, submitted for publication (MIPRO 2009)"
Stochastic Constraint Programming: A Scenario-Based Approach,"To model combinatorial decision problems involving uncertainty and
probability, we introduce scenario based stochastic constraint programming.
Stochastic constraint programs contain both decision variables, which we can
set, and stochastic variables, which follow a discrete probability
distribution. We provide a semantics for stochastic constraint programs based
on scenario trees. Using this semantics, we can compile stochastic constraint
programs down into conventional (non-stochastic) constraint programs. This
allows us to exploit the full power of existing constraint solvers. We have
implemented this framework for decision making under uncertainty in stochastic
OPL, a language which is based on the OPL constraint modelling language
[Hentenryck et al., 1999]. To illustrate the potential of this framework, we
model a wide range of problems in areas as diverse as portfolio
diversification, agricultural planning and production/inventory management.",N/A
Stochastic Constraint Programming,"To model combinatorial decision problems involving uncertainty and
probability, we introduce stochastic constraint programming. Stochastic
constraint programs contain both decision variables (which we can set) and
stochastic variables (which follow a probability distribution). They combine
together the best features of traditional constraint satisfaction, stochastic
integer programming, and stochastic satisfiability. We give a semantics for
stochastic constraint programs, and propose a number of complete algorithms and
approximation procedures. Finally, we discuss a number of extensions of
stochastic constraint programming to relax various assumptions like the
independence between stochastic variables, and compare with other approaches
for decision making under uncertainty.","Proceedings of the 15th Eureopean Conference on Artificial
  Intelligence"
Designing a GUI for Proofs - Evaluation of an HCI Experiment,"Often user interfaces of theorem proving systems focus on assisting
particularly trained and skilled users, i.e., proof experts. As a result, the
systems are difficult to use for non-expert users. This paper describes a paper
and pencil HCI experiment, in which (non-expert) students were asked to make
suggestions for a GUI for an interactive system for mathematical proofs. They
had to explain the usage of the GUI by applying it to construct a proof sketch
for a given theorem. The evaluation of the experiment provides insights for the
interaction design for non-expert users and the needs and wants of this user
group.",N/A
Flow of Activity in the Ouroboros Model,"The Ouroboros Model is a new conceptual proposal for an algorithmic structure
for efficient data processing in living beings as well as for artificial
agents. Its central feature is a general repetitive loop where one iteration
cycle sets the stage for the next. Sensory input activates data structures
(schemata) with similar constituents encountered before, thus expectations are
kindled. This corresponds to the highlighting of empty slots in the selected
schema, and these expectations are compared with the actually encountered
input. Depending on the outcome of this consumption analysis different next
steps like search for further data or a reset, i.e. a new attempt employing
another schema, are triggered. Monitoring of the whole process, and in
particular of the flow of activation directed by the consumption analysis,
yields valuable feedback for the optimum allocation of attention and resources
including the selective establishment of useful new memory entries.","6 pages, 4 figures"
"Heterogeneous knowledge representation using a finite automaton and
  first order logic: a case study in electromyography","In a certain number of situations, human cognitive functioning is difficult
to represent with classical artificial intelligence structures. Such a
difficulty arises in the polyneuropathy diagnosis which is based on the spatial
distribution, along the nerve fibres, of lesions, together with the synthesis
of several partial diagnoses. Faced with this problem while building up an
expert system (NEUROP), we developed a heterogeneous knowledge representation
associating a finite automaton with first order logic. A number of knowledge
representation problems raised by the electromyography test features are
examined in this study and the expert system architecture allowing such a
knowledge modeling are laid out.",N/A
Learning for Dynamic subsumption,"In this paper a new dynamic subsumption technique for Boolean CNF formulae is
proposed. It exploits simple and sufficient conditions to detect during
conflict analysis, clauses from the original formula that can be reduced by
subsumption. During the learnt clause derivation, and at each step of the
resolution process, we simply check for backward subsumption between the
current resolvent and clauses from the original formula and encoded in the
implication graph. Our approach give rise to a strong and dynamic
simplification technique that exploits learning to eliminate literals from the
original clauses. Experimental results show that the integration of our dynamic
subsumption approach within the state-of-the-art SAT solvers Minisat and Rsat
achieves interesting improvements particularly on crafted instances.",N/A
Principle of development,"Today, science have a powerful tool for the description of reality - the
numbers. However, the concept of number was not immediately, lets try to trace
the evolution of the concept. The numbers emerged as the need for accurate
estimates of the amount in order to permit a comparison of some objects. So if
you see to it how many times a day a person uses the numbers and compare, it
becomes evident that the comparison is used much more frequently. However, the
comparison is not possible without two opposite basic standards. Thus, to
introduce the concept of comparison, must have two opposing standards, in turn,
the operation of comparison is necessary to introduce the concept of number.
Arguably, the scientific description of reality is impossible without the
concept of opposites.
  In this paper analyzes the concept of opposites, as the basis for the
introduction of the principle of development.",This paper has been withdrawn by the author
Semantic Social Network Analysis,"Social Network Analysis (SNA) tries to understand and exploit the key
features of social networks in order to manage their life cycle and predict
their evolution. Increasingly popular web 2.0 sites are forming huge social
network. Classical methods from social network analysis (SNA) have been applied
to such online networks. In this paper, we propose leveraging semantic web
technologies to merge and exploit the best features of each domain. We present
how to facilitate and enhance the analysis of online social networks,
exploiting the power of semantic social network analysis.",published in Web Science (2009)
Guarded resolution for answer set programming,"We describe a variant of resolution rule of proof and show that it is
complete for stable semantics of logic programs. We show applications of this
result.","13 pages, some results added. Accepted for publication at TPLP"
Fuzzy Mnesors,"A fuzzy mnesor space is a semimodule over the positive real numbers. It can
be used as theoretical framework for fuzzy sets. Hence we can prove a great
number of properties for fuzzy sets without refering to the membership
functions.",N/A
An Application of Proof-Theory in Answer Set Programming,"We apply proof-theoretic techniques in answer Set Programming. The main
results include: 1. A characterization of continuity properties of
Gelfond-Lifschitz operator for logic program. 2. A propositional
characterization of stable models of logic programs (without referring to loop
formulas.","22 pages. Short version was published in ICLP08. New version slightly
  shorter than the previous version"
"Decompositions of All Different, Global Cardinality and Related
  Constraints","We show that some common and important global constraints like ALL-DIFFERENT
and GCC can be decomposed into simple arithmetic constraints on which we
achieve bound or range consistency, and in some cases even greater pruning.
These decompositions can be easily added to new solvers. They also provide
other constraints with access to the state of the propagator by sharing of
variables. Such sharing can be used to improve propagation between constraints.
We report experiments with our decomposition in a pseudo-Boolean solver.","Proceedings of the Twenty-first International Joint Conference on
  Artificial Intelligence (IJCAI-09)"
Scenario-based Stochastic Constraint Programming,"To model combinatorial decision problems involving uncertainty and
probability, we extend the stochastic constraint programming framework proposed
in [Walsh, 2002] along a number of important dimensions (e.g. to multiple
chance constraints and to a range of new objectives). We also provide a new
(but equivalent) semantics based on scenarios. Using this semantics, we can
compile stochastic constraint programs down into conventional (nonstochastic)
constraint programs. This allows us to exploit the full power of existing
constraint solvers. We have implemented this framework for decision making
under uncertainty in stochastic OPL, a language which is based on the OPL
constraint modelling language [Hentenryck et al., 1999]. To illustrate the
potential of this framework, we model a wide range of problems in areas as
diverse as finance, agriculture and production.","Proceedings of the Eighteenth International Joint Conference on
  Artificial Intelligence (IJCAI-03)"
"Reasoning about soft constraints and conditional preferences: complexity
  results and approximation techniques","Many real life optimization problems contain both hard and soft constraints,
as well as qualitative conditional preferences. However, there is no single
formalism to specify all three kinds of information. We therefore propose a
framework, based on both CP-nets and soft constraints, that handles both hard
and soft constraints as well as conditional preferences efficiently and
uniformly. We study the complexity of testing the consistency of preference
statements, and show how soft constraints can faithfully approximate the
semantics of conditional preference statements whilst improving the
computational complexity","Proceedings of the Eighteenth International Joint Conference on
  Artificial Intelligence (IJCAI-03)"
Multiset Ordering Constraints,"We identify a new and important global (or non-binary) constraint. This
constraint ensures that the values taken by two vectors of variables, when
viewed as multisets, are ordered. This constraint is useful for a number of
different applications including breaking symmetry and fuzzy constraint
satisfaction. We propose and implement an efficient linear time algorithm for
enforcing generalised arc consistency on such a multiset ordering constraint.
Experimental results on several problem domains show considerable promise.","Proceedings of the Eighteenth International Joint Conference on
  Artificial Intelligence (IJCAI-03)"
Tag Clouds for Displaying Semantics: The Case of Filmscripts,"We relate tag clouds to other forms of visualization, including planar or
reduced dimensionality mapping, and Kohonen self-organizing maps. Using a
modified tag cloud visualization, we incorporate other information into it,
including text sequence and most pertinent words. Our notion of word pertinence
goes beyond just word frequency and instead takes a word in a mathematical
sense as located at the average of all of its pairwise relationships. We
capture semantics through context, taken as all pairwise relationships. Our
domain of application is that of filmscript analysis. The analysis of
filmscripts, always important for cinema, is experiencing a major gain in
importance in the context of television. Our objective in this work is to
visualize the semantics of filmscript, and beyond filmscript any other
partially structured, time-ordered, sequence of text segments. In particular we
develop an innovative approach to plot characterization.","23 pages, 7 figures"
Considerations on Construction Ontologies,"The paper proposes an analysis on some existent ontologies, in order to point
out ways to resolve semantic heterogeneity in information systems. Authors are
highlighting the tasks in a Knowledge Acquisiton System and identifying aspects
related to the addition of new information to an intelligent system. A solution
is proposed, as a combination of ontology reasoning services and natural
languages generation. A multi-agent system will be conceived with an extractor
agent, a reasoner agent and a competence management agent.","10 pages, exposed on 5th International Conference ""Actualities and
  Perspectives on Hardware and Software"" - APHS2009, Timisoara, Romania"
A Logic Programming Approach to Activity Recognition,"We have been developing a system for recognising human activity given a
symbolic representation of video content. The input of our system is a set of
time-stamped short-term activities detected on video frames. The output of our
system is a set of recognised long-term activities, which are pre-defined
temporal combinations of short-term activities. The constraints on the
short-term activities that, if satisfied, lead to the recognition of a
long-term activity, are expressed using a dialect of the Event Calculus. We
illustrate the expressiveness of the dialect by showing the representation of
several typical complex activities. Furthermore, we present a detailed
evaluation of the system through experimentation on a benchmark dataset of
surveillance videos.","The original publication is available in the Proceedings of the 2nd
  ACM international workshop on Events in multimedia, 2010"
"Knowledge Management in Economic Intelligence with Reasoning on Temporal
  Attributes","People have to make important decisions within a time frame. Hence, it is
imperative to employ means or strategy to aid effective decision making.
Consequently, Economic Intelligence (EI) has emerged as a field to aid
strategic and timely decision making in an organization. In the course of
attaining this goal: it is indispensable to be more optimistic towards
provision for conservation of intellectual resource invested into the process
of decision making. This intellectual resource is nothing else but the
knowledge of the actors as well as that of the various processes for effecting
decision making. Knowledge has been recognized as a strategic economic resource
for enhancing productivity and a key for innovation in any organization or
community. Thus, its adequate management with cognizance of its temporal
properties is highly indispensable. Temporal properties of knowledge refer to
the date and time (known as timestamp) such knowledge is created as well as the
duration or interval between related knowledge. This paper focuses on the needs
for a user-centered knowledge management approach as well as exploitation of
associated temporal properties. Our perspective of knowledge is with respect to
decision-problems projects in EI. Our hypothesis is that the possibility of
reasoning about temporal properties in exploitation of knowledge in EI projects
should foster timely decision making through generation of useful inferences
from available and reusable knowledge for a new project.",N/A
Toward a Category Theory Design of Ontological Knowledge Bases,"I discuss (ontologies_and_ontological_knowledge_bases /
formal_methods_and_theories) duality and its category theory extensions as a
step toward a solution to Knowledge-Based Systems Theory. In particular I focus
on the example of the design of elements of ontologies and ontological
knowledge bases of next three electronic courses: Foundations of Research
Activities, Virtual Modeling of Complex Systems and Introduction to String
Theory.","10 pages, Preliminary results to International Joint Conference on
  Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K
  2009)"
Mnesors for automatic control,"Mnesors are defined as elements of a semimodule over the min-plus integers.
This two-sorted structure is able to merge graduation properties of vectors and
idempotent properties of boolean numbers, which makes it appropriate for hybrid
systems. We apply it to the control of an inverted pendulum and design a full
logical controller, that is, without the usual algebra of real numbers.",N/A
Semi-Myopic Sensing Plans for Value Optimization,"We consider the following sequential decision problem. Given a set of items
of unknown utility, we need to select one of as high a utility as possible
(``the selection problem''). Measurements (possibly noisy) of item values prior
to selection are allowed, at a known cost. The goal is to optimize the overall
sequential decision process of measurements and selection.
  Value of information (VOI) is a well-known scheme for selecting measurements,
but the intractability of the problem typically leads to using myopic VOI
estimates. In the selection problem, myopic VOI frequently badly underestimates
the value of information, leading to inferior sensing plans. We relax the
strict myopic assumption into a scheme we term semi-myopic, providing a
spectrum of methods that can improve the performance of sensing plans. In
particular, we propose the efficiently computable method of ``blinkered'' VOI,
and examine theoretical bounds for special cases. Empirical evaluation of
``blinkered'' VOI in the selection problem with normally distributed item
values shows that is performs much better than pure myopic VOI.","9 pages, 4 figures, presented at BISFAI 2009"
Updating Sets of Probabilities,"There are several well-known justifications for conditioning as the
appropriate method for updating a single probability measure, given an
observation. However, there is a significant body of work arguing for sets of
probability measures, rather than single measures, as a more realistic model of
uncertainty. Conditioning still makes sense in this context--we can simply
condition each measure in the set individually, then combine the results--and,
indeed, it seems to be the preferred updating procedure in the literature. But
how justified is conditioning in this richer setting? Here we show, by
considering an axiomatic account of conditioning given by van Fraassen, that
the single-measure and sets-of-measures cases are very different. We show that
van Fraassen's axiomatization for the former case is nowhere near sufficient
for updating sets of measures. We give a considerably longer (and not as
compelling) list of axioms that together force conditioning in this setting,
and describe other update methods that are allowed once any of these axioms is
dropped.","In Proceedings of the Fourteenth Conference on Uncertainty in AI,
  1998, pp. 173-182"
"A Novel Two-Stage Dynamic Decision Support based Optimal Threat
  Evaluation and Defensive Resource Scheduling Algorithm for Multi Air-borne
  threats","This paper presents a novel two-stage flexible dynamic decision support based
optimal threat evaluation and defensive resource scheduling algorithm for
multi-target air-borne threats. The algorithm provides flexibility and
optimality by swapping between two objective functions, i.e. the preferential
and subtractive defense strategies as and when required. To further enhance the
solution quality, it outlines and divides the critical parameters used in
Threat Evaluation and Weapon Assignment (TEWA) into three broad categories
(Triggering, Scheduling and Ranking parameters). Proposed algorithm uses a
variant of many-to-many Stable Marriage Algorithm (SMA) to solve Threat
Evaluation (TE) and Weapon Assignment (WA) problem. In TE stage, Threat Ranking
and Threat-Asset pairing is done. Stage two is based on a new flexible dynamic
weapon scheduling algorithm, allowing multiple engagements using
shoot-look-shoot strategy, to compute near-optimal solution for a range of
scenarios. Analysis part of this paper presents the strengths and weaknesses of
the proposed algorithm over an alternative greedy algorithm as applied to
different offline scenarios.","8 Pages, International Journal of Computer Science and Information
  Security, IJCSIS"
General combination rules for qualitative and quantitative beliefs,"Martin and Osswald \cite{Martin07} have recently proposed many
generalizations of combination rules on quantitative beliefs in order to manage
the conflict and to consider the specificity of the responses of the experts.
Since the experts express themselves usually in natural language with
linguistic labels, Smarandache and Dezert \cite{Li07} have introduced a
mathematical framework for dealing directly also with qualitative beliefs. In
this paper we recall some element of our previous works and propose the new
combination rules, developed for the fusion of both qualitative or quantitative
beliefs.",N/A
"A Novel Two-Staged Decision Support based Threat Evaluation and Weapon
  Assignment Algorithm, Asset-based Dynamic Weapon Scheduling using Artificial
  Intelligence Techinques","Surveillance control and reporting (SCR) system for air threats play an
important role in the defense of a country. SCR system corresponds to air and
ground situation management/processing along with information fusion,
communication, coordination, simulation and other critical defense oriented
tasks. Threat Evaluation and Weapon Assignment (TEWA) sits at the core of SCR
system. In such a system, maximal or near maximal utilization of constrained
resources is of extreme importance. Manual TEWA systems cannot provide
optimality because of different limitations e.g.surface to air missile (SAM)
can fire from a distance of 5Km, but manual TEWA systems are constrained by
human vision range and other constraints. Current TEWA systems usually work on
target-by-target basis using some type of greedy algorithm thus affecting the
optimality of the solution and failing in multi-target scenario. his paper
relates to a novel two-staged flexible dynamic decision support based optimal
threat evaluation and weapon assignment algorithm for multi-target air-borne
threats.","7 Pages, International Journal of Computer Science and Information
  Security (IJCSIS)"
Generalized Collective Inference with Symmetric Clique Potentials,"Collective graphical models exploit inter-instance associative dependence to
output more accurate labelings. However existing models support very limited
kind of associativity which restricts accuracy gains. This paper makes two
major contributions. First, we propose a general collective inference framework
that biases data instances to agree on a set of {\em properties} of their
labelings. Agreement is encouraged through symmetric clique potentials. We show
that rich properties leads to bigger gains, and present a systematic inference
procedure for a large class of such properties. The procedure performs message
passing on the cluster graph, where property-aware messages are computed with
cluster specific algorithms. This provides an inference-only solution for
domain adaptation. Our experiments on bibliographic information extraction
illustrate significant test error reduction over unseen domains. Our second
major contribution consists of algorithms for computing outgoing messages from
clique clusters with symmetric clique potentials. Our algorithms are exact for
arbitrary symmetric potentials on binary labels and for max-like and
majority-like potentials on multiple labels. For majority potentials, we also
provide an efficient Lagrangian Relaxation based algorithm that compares
favorably with the exact algorithm. We present a 13/15-approximation algorithm
for the NP-hard Potts potential, with runtime sub-quadratic in the clique size.
In contrast, the best known previous guarantee for graphs with Potts potentials
is only 1/2. We empirically show that our method for Potts potentials is an
order of magnitude faster than the best alternatives, and our Lagrangian
Relaxation based algorithm for majority potentials beats the best applicable
heuristic -- ICM.",30 pages
The Soft Cumulative Constraint,"This research report presents an extension of Cumulative of Choco constraint
solver, which is useful to encode over-constrained cumulative problems. This
new global constraint uses sweep and task interval violation-based algorithms.",N/A
Modelling Concurrent Behaviors in the Process Specification Language,"In this paper, we propose a first-order ontology for generalized stratified
order structure. We then classify the models of the theory using
model-theoretic techniques. An ontology mapping from this ontology to the core
theory of Process Specification Language is also discussed.",N/A
"The Single Machine Total Weighted Tardiness Problem - Is it (for
  Metaheuristics) a Solved Problem ?","The article presents a study of rather simple local search heuristics for the
single machine total weighted tardiness problem (SMTWTP), namely hillclimbing
and Variable Neighborhood Search. In particular, we revisit these approaches
for the SMTWTP as there appears to be a lack of appropriate/challenging
benchmark instances in this case. The obtained results are impressive indeed.
Only few instances remain unsolved, and even those are approximated within 1%
of the optimal/best known solutions. Our experiments support the claim that
metaheuristics for the SMTWTP are very likely to lead to good results, and
that, before refining search strategies, more work must be done with regard to
the proposition of benchmark data. Some recommendations for the construction of
such data sets are derived from our investigations.",N/A
"Improvements for multi-objective flow shop scheduling by Pareto Iterated
  Local Search","The article describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search, intensification through variable
neighborhoods, and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives and compared to other local search approaches. While the obtained
results are encouraging in terms of their quality, another positive attribute
of the approach is its simplicity as it does require the setting of only very
few parameters.",N/A
Beyond Turing Machines,"This paper discusses ""computational"" systems capable of ""computing"" functions
not computable by predefined Turing machines if the systems are not isolated
from their environment. Roughly speaking, these systems can change their finite
descriptions by interacting with their environment.",N/A
Pattern Recognition Theory of Mind,"I propose that pattern recognition, memorization and processing are key
concepts that can be a principle set for the theoretical modeling of the mind
function. Most of the questions about the mind functioning can be answered by a
descriptive modeling and definitions from these principles. An understandable
consciousness definition can be drawn based on the assumption that a pattern
recognition system can recognize its own patterns of activity. The principles,
descriptive modeling and definitions can be a basis for theoretical and applied
research on cognitive sciences, particularly at artificial intelligence
studies.",N/A
Fact Sheet on Semantic Web,"The report gives an overview about activities on the topic Semantic Web. It
has been released as technical report for the project ""KTweb -- Connecting
Knowledge Technologies Communities"" in 2003.",N/A
Restart Strategy Selection using Machine Learning Techniques,"Restart strategies are an important factor in the performance of
conflict-driven Davis Putnam style SAT solvers. Selecting a good restart
strategy for a problem instance can enhance the performance of a solver.
Inspired by recent success applying machine learning techniques to predict the
runtime of SAT solvers, we present a method which uses machine learning to
boost solver performance through a smart selection of the restart strategy.
Based on easy to compute features, we train both a satisfiability classifier
and runtime models. We use these models to choose between restart strategies.
We present experimental results comparing this technique with the most commonly
used restart strategies. Our results demonstrate that machine learning is
effective in improving solver performance.","14 pages, 4 figures"
Online Search Cost Estimation for SAT Solvers,"We present two different methods for estimating the cost of solving SAT
problems. The methods focus on the online behaviour of the backtracking solver,
as well as the structure of the problem. Modern SAT solvers present several
challenges to estimate search cost including coping with nonchronological
backtracking, learning and restarts. Our first method adapt an existing
algorithm for estimating the size of a search tree to deal with these
challenges. We then suggest a second method that uses a linear model trained on
data gathered online at the start of search. We compare the effectiveness of
these two methods using random and structured problems. We also demonstrate
that predictions made in early restarts can be used to improve later
predictions. We conclude by showing that the cost of solving a set of problems
can be reduced by selecting a solver from a portfolio based on such cost
estimations.","8 pages, 9 figures"
On Classification from Outlier View,"Classification is the basis of cognition. Unlike other solutions, this study
approaches it from the view of outliers. We present an expanding algorithm to
detect outliers in univariate datasets, together with the underlying
foundation. The expanding algorithm runs in a holistic way, making it a rather
robust solution. Synthetic and real data experiments show its power.
Furthermore, an application for multi-class problems leads to the introduction
of the oscillator algorithm. The corresponding result implies the potential
wide use of the expanding algorithm.","Conclusion renewed; IAENG International Journal of Computer Science,
  Volume 37, Issue 4, Nov, 2010"
Convergence of Expected Utility for Universal AI,"We consider a sequence of repeated interactions between an agent and an
environment. Uncertainty about the environment is captured by a probability
distribution over a space of hypotheses, which includes all computable
functions. Given a utility function, we can evaluate the expected utility of
any computational policy for interaction with the environment. After making
some plausible assumptions (and maybe one not-so-plausible assumption), we show
that if the utility function is unbounded, then the expected utility of any
policy is undefined.",N/A
Knowledge Discovery of Hydrocyclone s Circuit Based on SONFIS and SORST,"This study describes application of some approximate reasoning methods to
analysis of hydrocyclone performance. In this manner, using a combining of Self
Organizing Map (SOM), Neuro-Fuzzy Inference System (NFIS)-SONFIS- and Rough Set
Theory (RST)-SORST-crisp and fuzzy granules are obtained. Balancing of crisp
granules and non-crisp granules can be implemented in close-open iteration.
Using different criteria and based on granulation level balance point
(interval) or a pseudo-balance point is estimated. Validation of the proposed
methods, on the data set of the hydrocyclone is rendered.","Proceedings of the 11th International Mineral Processing Symposium
  21-23 October 2008, Belek-Antalya, Turkey"
A Class of DSm Conditional Rules,"In this paper we introduce two new DSm fusion conditioning rules with
example, and as a generalization of them a class of DSm fusion conditioning
rules, and then extend them to a class of DSm conditioning rules.",9 pages. SUbmitted to COGIS 2009 International Conference in PAris
View-based Propagator Derivation,"When implementing a propagator for a constraint, one must decide about
variants: When implementing min, should one also implement max? Should one
implement linear constraints both with unit and non-unit coefficients?
Constraint variants are ubiquitous: implementing them requires considerable (if
not prohibitive) effort and decreases maintainability, but will deliver better
performance than resorting to constraint decomposition.
  This paper shows how to use views to derive perfect propagator variants. A
model for views and derived propagators is introduced. Derived propagators are
proved to be indeed perfect in that they inherit essential properties such as
correctness and domain and bounds consistency. Techniques for systematically
deriving propagators such as transformation, generalization, specialization,
and type conversion are developed. The paper introduces an implementation
architecture for views that is independent of the underlying constraint
programming system. A detailed evaluation of views implemented in Gecode shows
that derived propagators are efficient and that views often incur no overhead.
Without views, Gecode would either require 180 000 rather than 40 000 lines of
propagator code, or would lack many efficient propagator variants. Compared to
8 000 lines of code for views, the reduction in code for propagators yields a
1750% return on investment.","28 pages, 7 tables, 3 figures"
A Cognitive Mind-map Framework to Foster Trust,"The explorative mind-map is a dynamic framework, that emerges automatically
from the input, it gets. It is unlike a verificative modeling system where
existing (human) thoughts are placed and connected together. In this regard,
explorative mind-maps change their size continuously, being adaptive with
connectionist cells inside; mind-maps process data input incrementally and
offer lots of possibilities to interact with the user through an appropriate
communication interface. With respect to a cognitive motivated situation like a
conversation between partners, mind-maps become interesting as they are able to
process stimulating signals whenever they occur. If these signals are close to
an own understanding of the world, then the conversational partner becomes
automatically more trustful than if the signals do not or less match the own
knowledge scheme. In this (position) paper, we therefore motivate explorative
mind-maps as a cognitive engine and propose these as a decision support engine
to foster trust.","5 pages, 4 Figures, Extended Version, presented at the 5th
  International Conference on Natural Computation, 2009"
An improved axiomatic definition of information granulation,"To capture the uncertainty of information or knowledge in information
systems, various information granulations, also known as knowledge
granulations, have been proposed. Recently, several axiomatic definitions of
information granulation have been introduced. In this paper, we try to improve
these axiomatic definitions and give a universal construction of information
granulation by relating information granulations with a class of functions of
multiple variables. We show that the improved axiomatic definition has some
concrete information granulations in the literature as instances.",10 pages
Reasoning with Topological and Directional Spatial Information,"Current research on qualitative spatial representation and reasoning mainly
focuses on one single aspect of space. In real world applications, however,
multiple spatial aspects are often involved simultaneously.
  This paper investigates problems arising in reasoning with combined
topological and directional information. We use the RCC8 algebra and the
Rectangle Algebra (RA) for expressing topological and directional information
respectively. We give examples to show that the bipath-consistency algorithm
BIPATH is incomplete for solving even basic RCC8 and RA constraints. If
topological constraints are taken from some maximal tractable subclasses of
RCC8, and directional constraints are taken from a subalgebra, termed DIR49, of
RA, then we show that BIPATH is able to separate topological constraints from
directional ones. This means, given a set of hybrid topological and directional
constraints from the above subclasses of RCC8 and RA, we can transfer the joint
satisfaction problem in polynomial time to two independent satisfaction
problems in RCC8 and RA. For general RA constraints, we give a method to
compute solutions that satisfy all topological constraints and approximately
satisfy each RA constraint to any prescribed precision.",N/A
Reasoning about Cardinal Directions between Extended Objects,"Direction relations between extended spatial objects are important
commonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model,
known as Cardinal Direction Calculus (CDC), for representing direction
relations between connected plane regions. CDC is perhaps the most expressive
qualitative calculus for directional information, and has attracted increasing
interest from areas such as artificial intelligence, geographical information
science, and image retrieval. Given a network of CDC constraints, the
consistency problem is deciding if the network is realizable by connected
regions in the real plane. This paper provides a cubic algorithm for checking
consistency of basic CDC constraint networks, and proves that reasoning with
CDC is in general an NP-Complete problem. For a consistent network of basic CDC
constraints, our algorithm also returns a 'canonical' solution in cubic time.
This cubic algorithm is also adapted to cope with cardinal directions between
possibly disconnected regions, in which case currently the best algorithm is of
time complexity O(n^5).",N/A
On Planning with Preferences in HTN,"In this paper, we address the problem of generating preferred plans by
combining the procedural control knowledge specified by Hierarchical Task
Networks (HTNs) with rich qualitative user preferences. The outcome of our work
is a language for specifyin user preferences, tailored to HTN planning,
together with a provably optimal preference-based planner, HTNPLAN, that is
implemented as an extension of SHOP2. To compute preferred plans, we propose an
approach based on forward-chaining heuristic search. Our heuristic uses an
admissible evaluation function measuring the satisfaction of preferences over
partial plans. Our empirical evaluation demonstrates the effectiveness of our
HTNPLAN heuristics. We prove our approach sound and optimal with respect to the
plans it generates by appealing to a situation calculus semantics of our
preference language and of HTN planning. While our implementation builds on
SHOP2, the language and techniques proposed here are relevant to a broad range
of HTN planners.","This paper appears in Twelfth International Workshop on Non-Monotonic
  Reasoning (NMR08). An earlier version of this paper appears in Fourth
  Multidisciplinary Workshop on Advances in Preference Handling (M-Pref08) at
  AAAI-08"
Assessing the Impact of Informedness on a Consultant's Profit,"We study the notion of informedness in a client-consultant setting. Using a
software simulator, we examine the extent to which it pays off for consultants
to provide their clients with advice that is well-informed, or with advice that
is merely meant to appear to be well-informed. The latter strategy is
beneficial in that it costs less resources to keep up-to-date, but carries the
risk of a decreased reputation if the clients discover the low level of
informedness of the consultant. Our experimental results indicate that under
different circumstances, different strategies yield the optimal results (net
profit) for the consultants.","20 pages, 42 figures, Technical Report, University of Luxembourg"
A multiagent urban traffic simulation Part I: dealing with the ordinary,"We describe in this article a multiagent urban traffic simulation, as we
believe individual-based modeling is necessary to encompass the complex
influence the actions of an individual vehicle can have on the overall flow of
vehicles. We first describe how we build a graph description of the network
from purely geometric data, ESRI shapefiles. We then explain how we include
traffic related data to this graph. We go on after that with the model of the
vehicle agents: origin and destination, driving behavior, multiple lanes,
crossroads, and interactions with the other vehicles in day-to-day, ?ordinary?
traffic. We conclude with the presentation of the resulting simulation of this
model on the Rouen agglomeration.",N/A
n-Opposition theory to structure debates,"2007 was the first international congress on the ?square of oppositions?. A
first attempt to structure debate using n-opposition theory was presented along
with the results of a first experiment on the web. Our proposal for this paper
is to define relations between arguments through a structure of opposition
(square of oppositions is one structure of opposition). We will be trying to
answer the following questions: How to organize debates on the web 2.0? How to
structure them in a logical way? What is the role of n-opposition theory, in
this context? We present in this paper results of three experiments
(Betapolitique 2007, ECAP 2008, Intermed 2008).",N/A
Paired Comparisons-based Interactive Differential Evolution,"We propose Interactive Differential Evolution (IDE) based on paired
comparisons for reducing user fatigue and evaluate its convergence speed in
comparison with Interactive Genetic Algorithms (IGA) and tournament IGA. User
interface and convergence performance are two big keys for reducing Interactive
Evolutionary Computation (IEC) user fatigue. Unlike IGA and conventional IDE,
users of the proposed IDE and tournament IGA do not need to compare whole
individuals each other but compare pairs of individuals, which largely
decreases user fatigue. In this paper, we design a pseudo-IEC user and evaluate
another factor, IEC convergence performance, using IEC simulators and show that
our proposed IDE converges significantly faster than IGA and tournament IGA,
i.e. our proposed one is superior to others from both user interface and
convergence performance points of view.",N/A
Back analysis based on SOM-RST system,"This paper describes application of information granulation theory, on the
back analysis of Jeffrey mine southeast wall Quebec. In this manner, using a
combining of Self Organizing Map (SOM) and rough set theory (RST), crisp and
rough granules are obtained. Balancing of crisp granules and sub rough granules
is rendered in close-open iteration. Combining of hard and soft computing,
namely finite difference method (FDM) and computational intelligence and taking
in to account missing information are two main benefits of the proposed method.
As a practical example, reverse analysis on the failure of the southeast wall
Jeffrey mine is accomplished.","10th. International Symposium on Landslides and Engineering and.
  Engineered Slopes, Xi'an, China"
"Similarity Matching Techniques for Fault Diagnosis in Automotive
  Infotainment Electronics","Fault diagnosis has become a very important area of research during the last
decade due to the advancement of mechanical and electrical systems in
industries. The automobile is a crucial field where fault diagnosis is given a
special attention. Due to the increasing complexity and newly added features in
vehicles, a comprehensive study has to be performed in order to achieve an
appropriate diagnosis model. A diagnosis system is capable of identifying the
faults of a system by investigating the observable effects (or symptoms). The
system categorizes the fault into a diagnosis class and identifies a probable
cause based on the supplied fault symptoms. Fault categorization and
identification are done using similarity matching techniques. The development
of diagnosis classes is done by making use of previous experience, knowledge or
information within an application area. The necessary information used may come
from several sources of knowledge, such as from system analysis. In this paper
similarity matching techniques for fault diagnosis in automotive infotainment
applications are discussed.","International Journal of Computer Science Issues(IJCSI), Volume 3,
  pp14-19, August 2009"
"Performing Hybrid Recommendation in Intermodal Transportation-the
  FTMarket System's Recommendation Module","Diverse recommendation techniques have been already proposed and encapsulated
into several e-business applications, aiming to perform a more accurate
evaluation of the existing information and accordingly augment the assistance
provided to the users involved. This paper reports on the development and
integration of a recommendation module in an agent-based transportation
transactions management system. The module is built according to a novel hybrid
recommendation technique, which combines the advantages of collaborative
filtering and knowledge-based approaches. The proposed technique and supporting
module assist customers in considering in detail alternative transportation
transactions that satisfy their requests, as well as in evaluating completed
transactions. The related services are invoked through a software agent that
constructs the appropriate knowledge rules and performs a synthesis of the
recommendation policy.","International Journal of Computer Science Issues (IJCSI), Volume 3,
  pp24-34, August 2009"
Decomposition of the NVALUE constraint,"We study decompositions of NVALUE, a global constraint that can be used to
model a wide range of problems where values need to be counted. Whilst
decomposition typically hinders propagation, we identify one decomposition that
maintains a global view as enforcing bound consistency on the decomposition
achieves bound consistency on the original global NVALUE constraint. Such
decompositions offer the prospect for advanced solving techniques like nogood
learning and impact based branching heuristics. They may also help SAT and IP
solvers take advantage of the propagation of global constraints.","To appear in Proceedings of the Eighth International Workshop on
  Constraint Modelling and Reformulation, held alongside the 15th International
  Conference on Principles and Practice of Constraint Programming (CP 2009),
  Lisbon, Portugal"
Symmetries of Symmetry Breaking Constraints,"Symmetry is an important feature of many constraint programs. We show that
any symmetry acting on a set of symmetry breaking constraints can be used to
break symmetry. Different symmetries pick out different solutions in each
symmetry class. We use these observations in two methods for eliminating
symmetry from a problem. These methods are designed to have many of the
advantages of symmetry breaking methods that post static symmetry breaking
constraint without some of the disadvantages. In particular, the two methods
prune the search space using fast and efficient propagation of posted
constraints, whilst reducing the conflict between symmetry breaking and
branching heuristics. Experimental results show that the two methods perform
well on some standard benchmarks.","To appear in the Proceedings of the Ninth International Workshop on
  Symmetry and Constraint Satisfaction Problems, held alongside the 15th
  International Conference on Principles and Practice of Constraint Programming
  (CP 2009), Lisbon, Portugal"
"Elicitation strategies for fuzzy constraint problems with missing
  preferences: algorithms and experimental studies","Fuzzy constraints are a popular approach to handle preferences and
over-constrained problems in scenarios where one needs to be cautious, such as
in medical or space applications. We consider here fuzzy constraint problems
where some of the preferences may be missing. This models, for example,
settings where agents are distributed and have privacy issues, or where there
is an ongoing preference elicitation process. In this setting, we study how to
find a solution which is optimal irrespective of the missing preferences. In
the process of finding such a solution, we may elicit preferences from the user
if necessary. However, our goal is to ask the user as little as possible. We
define a combined solving and preference elicitation scheme with a large number
of different instantiations, each corresponding to a concrete algorithm which
we compare experimentally. We compute both the number of elicited preferences
and the ""user effort"", which may be larger, as it contains all the preference
values the user has to compute to be able to respond to the elicitation
requests. While the number of elicited preferences is important when the
concern is to communicate as little information as possible, the user effort
measures also the hidden work the user has to do to be able to communicate the
elicited preferences. Our experimental results show that some of our algorithms
are very good at finding a necessarily optimal solution while asking the user
for only a very small fraction of the missing preferences. The user effort is
also very small for the best algorithms. Finally, we test these algorithms on
hard constraint problems with possibly missing constraints, where the aim is to
find feasible solutions irrespective of the missing constraints.","Principles and Practice of Constraint Programming, 14th International
  Conference, CP 2008, Sydney, Australia, September 14-18, 2008. Proceedings"
Flow-Based Propagators for the SEQUENCE and Related Global Constraints,"We propose new filtering algorithms for the SEQUENCE constraint and some
extensions of the SEQUENCE constraint based on network flows. We enforce domain
consistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the
search tree. This improves upon the best existing domain consistency algorithm
by a factor of $O(\log n)$. The flows used in these algorithms are derived from
a linear program. Some of them differ from the flows used to propagate global
constraints like GCC since the domains of the variables are encoded as costs on
the edges rather than capacities. Such flows are efficient for maintaining
bounds consistency over large domains and may be useful for other global
constraints.","Principles and Practice of Constraint Programming, 14th International
  Conference, CP 2008, Sydney, Australia, September 14-18, 2008. Proceedings"
The Weighted CFG Constraint,"We introduce the weighted CFG constraint and propose a propagation algorithm
that enforces domain consistency in $O(n^3|G|)$ time. We show that this
algorithm can be decomposed into a set of primitive arithmetic constraints
without hindering propagation.","Integration of AI and OR Techniques in Constraint Programming for
  Combinatorial Optimization Problems, 5th International Conference, CPAIOR
  2008, Paris, France, May 20-23, 2008, Proceedings"
Building upon Fast Multipole Methods to Detect and Model Organizations,"Many models in natural and social sciences are comprised of sets of
inter-acting entities whose intensity of interaction decreases with distance.
This often leads to structures of interest in these models composed of dense
packs of entities. Fast Multipole Methods are a family of methods developed to
help with the calculation of a number of computable models such as described
above. We propose a method that builds upon FMM to detect and model the dense
structures of these systems.",N/A
"A multiagent urban traffic simulation. Part II: dealing with the
  extraordinary","In Probabilistic Risk Management, risk is characterized by two quantities:
the magnitude (or severity) of the adverse consequences that can potentially
result from the given activity or action, and by the likelihood of occurrence
of the given adverse consequences. But a risk seldom exists in isolation: chain
of consequences must be examined, as the outcome of one risk can increase the
likelihood of other risks. Systemic theory must complement classic PRM. Indeed
these chains are composed of many different elements, all of which may have a
critical importance at many different levels. Furthermore, when urban
catastrophes are envisioned, space and time constraints are key determinants of
the workings and dynamics of these chains of catastrophes: models must include
a correct spatial topology of the studied risk. Finally, literature insists on
the importance small events can have on the risk on a greater scale: urban
risks management models belong to self-organized criticality theory. We chose
multiagent systems to incorporate this property in our model: the behavior of
an agent can transform the dynamics of important groups of them.",N/A
"A Local Search Modeling for Constrained Optimum Paths Problems (Extended
  Abstract)","Constrained Optimum Path (COP) problems appear in many real-life
applications, especially on communication networks. Some of these problems have
been considered and solved by specific techniques which are usually difficult
to extend. In this paper, we introduce a novel local search modeling for
solving some COPs by local search. The modeling features the compositionality,
modularity, reuse and strengthens the benefits of Constrained-Based Local
Search. We also apply the modeling to the edge-disjoint paths problem (EDP). We
show that side constraints can easily be added in the model. Computational
results show the significance of the approach.",N/A
"Dynamic Demand-Capacity Balancing for Air Traffic Management Using
  Constraint-Based Local Search: First Results","Using constraint-based local search, we effectively model and efficiently
solve the problem of balancing the traffic demands on portions of the European
airspace while ensuring that their capacity constraints are satisfied. The
traffic demand of a portion of airspace is the hourly number of flights planned
to enter it, and its capacity is the upper bound on this number under which
air-traffic controllers can work. Currently, the only form of demand-capacity
balancing we allow is ground holding, that is the changing of the take-off
times of not yet airborne flights. Experiments with projected European flight
plans of the year 2030 show that already this first form of demand-capacity
balancing is feasible without incurring too much total delay and that it can
lead to a significantly better demand-capacity balance.",N/A
On Improving Local Search for Unsatisfiability,"Stochastic local search (SLS) has been an active field of research in the
last few years, with new techniques and procedures being developed at an
astonishing rate. SLS has been traditionally associated with satisfiability
solving, that is, finding a solution for a given problem instance, as its
intrinsic nature does not address unsatisfiable problems. Unsatisfiable
instances were therefore commonly solved using backtrack search solvers. For
this reason, in the late 90s Selman, Kautz and McAllester proposed a challenge
to use local search instead to prove unsatisfiability. More recently, two SLS
solvers - Ranger and Gunsat - have been developed, which are able to prove
unsatisfiability albeit being SLS solvers. In this paper, we first compare
Ranger with Gunsat and then propose to improve Ranger performance using some of
Gunsat's techniques, namely unit propagation look-ahead and extended
resolution.",N/A
Integrating Conflict Driven Clause Learning to Local Search,"This article introduces SatHyS (SAT HYbrid Solver), a novel hybrid approach
for propositional satisfiability. It combines local search and conflict driven
clause learning (CDCL) scheme. Each time the local search part reaches a local
minimum, the CDCL is launched. For SAT problems it behaves like a tabu list,
whereas for UNSAT ones, the CDCL part tries to focus on minimum unsatisfiable
sub-formula (MUS). Experimental results show good performances on many classes
of SAT instances from the last SAT competitions.",N/A
A Constraint-directed Local Search Approach to Nurse Rostering Problems,"In this paper, we investigate the hybridization of constraint programming and
local search techniques within a large neighbourhood search scheme for solving
highly constrained nurse rostering problems. As identified by the research, a
crucial part of the large neighbourhood search is the selection of the fragment
(neighbourhood, i.e. the set of variables), to be relaxed and re-optimized
iteratively. The success of the large neighbourhood search depends on the
adequacy of this identified neighbourhood with regard to the problematic part
of the solution assignment and the choice of the neighbourhood size. We
investigate three strategies to choose the fragment of different sizes within
the large neighbourhood search scheme. The first two strategies are tailored
concerning the problem properties. The third strategy is more general, using
the information of the cost from the soft constraint violations and their
propagation as the indicator to choose the variables added into the fragment.
The three strategies are analyzed and compared upon a benchmark nurse rostering
problem. Promising results demonstrate the possibility of future work in the
hybrid approach.",N/A
Sonet Network Design Problems,"This paper presents a new method and a constraint-based objective function to
solve two problems related to the design of optical telecommunication networks,
namely the Synchronous Optical Network Ring Assignment Problem (SRAP) and the
Intra-ring Synchronous Optical Network Design Problem (IDP). These network
topology problems can be represented as a graph partitioning with capacity
constraints as shown in previous works. We present here a new objective
function and a new local search algorithm to solve these problems. Experiments
conducted in Comet allow us to compare our method to previous ones and show
that we obtain better results.",N/A
"Parallel local search for solving Constraint Problems on the Cell
  Broadband Engine (Preliminary Results)","We explore the use of the Cell Broadband Engine (Cell/BE for short) for
combinatorial optimization applications: we present a parallel version of a
constraint-based local search algorithm that has been implemented on a
multiprocessor BladeCenter machine with twin Cell/BE processors (total of 16
SPUs per blade). This algorithm was chosen because it fits very well the
Cell/BE architecture and requires neither shared memory nor communication
between processors, while retaining a compact memory footprint. We study the
performance on several large optimization benchmarks and show that this
achieves mostly linear time speedups, even sometimes super-linear. This is
possible because the parallel implementation might explore simultaneously
different parts of the search space and therefore converge faster towards the
best sub-space and thus towards a solution. Besides getting speedups, the
resulting times exhibit a much smaller variance, which benefits applications
where a timely reply is critical.",N/A
Toward an automaton Constraint for Local Search,"We explore the idea of using finite automata to implement new constraints for
local search (this is already a successful technique in constraint-based global
search). We show how it is possible to maintain incrementally the violations of
a constraint and its decision variables from an automaton that describes a
ground checker for that constraint. We establish the practicality of our
approach idea on real-life personnel rostering problems, and show that it is
competitive with the approach of [Pralong, 2007].",N/A
"Proceedings 6th International Workshop on Local Search Techniques in
  Constraint Satisfaction","LSCS is a satellite workshop of the international conference on principles
and practice of Constraint Programming (CP), since 2004. It is devoted to local
search techniques in constraint satisfaction, and focuses on all aspects of
local search techniques, including: design and implementation of new
algorithms, hybrid stochastic-systematic search, reactive search optimization,
adaptive search, modeling for local-search, global constraints, flexibility and
robustness, learning methods, and specific applications.",N/A
Tracking object's type changes with fuzzy based fusion rule,"In this paper the behavior of three combinational rules for
temporal/sequential attribute data fusion for target type estimation are
analyzed. The comparative analysis is based on: Dempster's fusion rule proposed
in Dempster-Shafer Theory; Proportional Conflict Redistribution rule no. 5
(PCR5), proposed in Dezert-Smarandache Theory and one alternative class fusion
rule, connecting the combination rules for information fusion with particular
fuzzy operators, focusing on the t-norm based Conjunctive rule as an analog of
the ordinary conjunctive rule and t-conorm based Disjunctive rule as an analog
of the ordinary disjunctive rule. The way how different t-conorms and t-norms
functions within TCN fusion rule influence over target type estimation
performance is studied and estimated.",N/A
Finite element model selection using Particle Swarm Optimization,"This paper proposes the application of particle swarm optimization (PSO) to
the problem of finite element model (FEM) selection. This problem arises when a
choice of the best model for a system has to be made from set of competing
models, each developed a priori from engineering judgment. PSO is a
population-based stochastic search algorithm inspired by the behaviour of
biological entities in nature when they are foraging for resources. Each
potentially correct model is represented as a particle that exhibits both
individualistic and group behaviour. Each particle moves within the model
search space looking for the best solution by updating the parameters values
that define it. The most important step in the particle swarm algorithm is the
method of representing models which should take into account the number,
location and variables of parameters to be updated. One example structural
system is used to show the applicability of PSO in finding an optimal FEM. An
optimal model is defined as the model that has the least number of updated
parameters and has the smallest parameter variable variation from the mean
material properties. Two different objective functions are used to compare
performance of the PSO algorithm.","Accepted for the Proceedings of the International Modal Analysis
  Conference 2010"
A Fuzzy Petri Nets Model for Computing With Words,"Motivated by Zadeh's paradigm of computing with words rather than numbers,
several formal models of computing with words have recently been proposed.
These models are based on automata and thus are not well-suited for concurrent
computing. In this paper, we incorporate the well-known model of concurrent
computing, Petri nets, together with fuzzy set theory and thereby establish a
concurrency model of computing with words--fuzzy Petri nets for computing with
words (FPNCWs). The new feature of such fuzzy Petri nets is that the labels of
transitions are some special words modeled by fuzzy sets. By employing the
methodology of fuzzy reasoning, we give a faithful extension of an FPNCW which
makes it possible for computing with more words. The language expressiveness of
the two formal models of computing with words, fuzzy automata for computing
with words and FPNCWs, is compared as well. A few small examples are provided
to illustrate the theoretical development.","double columns 14 pages, 8 figures"
"Emotion: Appraisal-coping model for the ""Cascades"" problem","Modelling emotion has become a challenge nowadays. Therefore, several models
have been produced in order to express human emotional activity. However, only
a few of them are currently able to express the close relationship existing
between emotion and cognition. An appraisal-coping model is presented here,
with the aim to simulate the emotional impact caused by the evaluation of a
particular situation (appraisal), along with the consequent cognitive reaction
intended to face the situation (coping). This model is applied to the
""Cascades"" problem, a small arithmetical exercise designed for ten-year-old
pupils. The goal is to create a model corresponding to a child's behaviour when
solving the problem using his own strategies.",6 pages
Emotion : modèle d'appraisal-coping pour le problème des Cascades,"Modeling emotion has become a challenge nowadays. Therefore, several models
have been produced in order to express human emotional activity. However, only
a few of them are currently able to express the close relationship existing
between emotion and cognition. An appraisal-coping model is presented here,
with the aim to simulate the emotional impact caused by the evaluation of a
particular situation (appraisal), along with the consequent cognitive reaction
intended to face the situation (coping). This model is applied to the
?Cascades? problem, a small arithmetical exercise designed for ten-year-old
pupils. The goal is to create a model corresponding to a child's behavior when
solving the problem using his own strategies.",N/A
"Covering rough sets based on neighborhoods: An approach without using
  neighborhoods","Rough set theory, a mathematical tool to deal with inexact or uncertain
knowledge in information systems, has originally described the indiscernibility
of elements by equivalence relations. Covering rough sets are a natural
extension of classical rough sets by relaxing the partitions arising from
equivalence relations to coverings. Recently, some topological concepts such as
neighborhood have been applied to covering rough sets. In this paper, we
further investigate the covering rough sets based on neighborhoods by
approximation operations. We show that the upper approximation based on
neighborhoods can be defined equivalently without using neighborhoods. To
analyze the coverings themselves, we introduce unary and composition operations
on coverings. A notion of homomorphismis provided to relate two covering
approximation spaces. We also examine the properties of approximations
preserved by the operations and homomorphisms, respectively.",13 pages; to appear in International Journal of Approximate Reasoning
An axiomatic approach to the roughness measure of rough sets,"In Pawlak's rough set theory, a set is approximated by a pair of lower and
upper approximations. To measure numerically the roughness of an approximation,
Pawlak introduced a quantitative measure of roughness by using the ratio of the
cardinalities of the lower and upper approximations. Although the roughness
measure is effective, it has the drawback of not being strictly monotonic with
respect to the standard ordering on partitions. Recently, some improvements
have been made by taking into account the granularity of partitions. In this
paper, we approach the roughness measure in an axiomatic way. After
axiomatically defining roughness measure and partition measure, we provide a
unified construction of roughness measure, called strong Pawlak roughness
measure, and then explore the properties of this measure. We show that the
improved roughness measures in the literature are special instances of our
strong Pawlak roughness measure and introduce three more strong Pawlak
roughness measures as well. The advantage of our axiomatic approach is that
some properties of a roughness measure follow immediately as soon as the
measure satisfies the relevant axiomatic definition.",to appear in the Fundamenta Informaticae
On a Model for Integrated Information,"In this paper we give a thorough presentation of a model proposed by Tononi
et al. for modeling \emph{integrated information}, i.e. how much information is
generated in a system transitioning from one state to the next one by the
causal interaction of its parts and \emph{above and beyond} the information
given by the sum of its parts. We also provides a more general formulation of
such a model, independent from the time chosen for the analysis and from the
uniformity of the probability distribution at the initial time instant.
Finally, we prove that integrated information is null for disconnected systems.",N/A
Graph Quantization,"Vector quantization(VQ) is a lossy data compression technique from signal
processing, which is restricted to feature vectors and therefore inapplicable
for combinatorial structures. This contribution presents a theoretical
foundation of graph quantization (GQ) that extends VQ to the domain of
attributed graphs. We present the necessary Lloyd-Max conditions for optimality
of a graph quantizer and consistency results for optimal GQ design based on
empirical distortion measures and stochastic optimization. These results
statistically justify existing clustering algorithms in the domain of graphs.
The proposed approach provides a template of how to link structural pattern
recognition methods other than GQ to statistical pattern recognition.",24 pages; submitted to CVIU
"Decisional Processes with Boolean Neural Network: the Emergence of
  Mental Schemes","Human decisional processes result from the employment of selected quantities
of relevant information, generally synthesized from environmental incoming data
and stored memories. Their main goal is the production of an appropriate and
adaptive response to a cognitive or behavioral task. Different strategies of
response production can be adopted, among which haphazard trials, formation of
mental schemes and heuristics. In this paper, we propose a model of Boolean
neural network that incorporates these strategies by recurring to global
optimization strategies during the learning session. The model characterizes as
well the passage from an unstructured/chaotic attractor neural network typical
of data-driven processes to a faster one, forward-only and representative of
schema-driven processes. Moreover, a simplified version of the Iowa Gambling
Task (IGT) is introduced in order to test the model. Our results match with
experimental data and point out some relevant knowledge coming from
psychological domain.","11 pages, 7 figures"
Web-Based Expert System for Civil Service Regulations: RCSES,"Internet and expert systems have offered new ways of sharing and distributing
knowledge, but there is a lack of researches in the area of web based expert
systems. This paper introduces a development of a web-based expert system for
the regulations of civil service in the Kingdom of Saudi Arabia named as RCSES.
It is the first time to develop such system (application of civil service
regulations) as well the development of it using web based approach. The
proposed system considers 17 regulations of the civil service system. The
different phases of developing the RCSES system are presented, as knowledge
acquiring and selection, ontology and knowledge representations using XML
format. XML Rule-based knowledge sources and the inference mechanisms were
implemented using ASP.net technique. An interactive tool for entering the
ontology and knowledge base, and the inferencing was built. It gives the
ability to use, modify, update, and extend the existing knowledge base in an
easy way. The knowledge was validated by experts in the domain of civil service
regulations, and the proposed RCSES was tested, verified, and validated by
different technical users and the developers staff. The RCSES system is
compared with other related web based expert systems, that comparison proved
the goodness, usability, and high performance of RCSES.","10 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
"Application of a Fuzzy Programming Technique to Production Planning in
  the Textile Industry","Many engineering optimization problems can be considered as linear
programming problems where all or some of the parameters involved are
linguistic in nature. These can only be quantified using fuzzy sets. The aim of
this paper is to solve a fuzzy linear programming problem in which the
parameters involved are fuzzy quantities with logistic membership functions. To
explore the applicability of the method a numerical example is considered to
determine the monthly production planning quotas and profit of a home textile
group.","6 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
"The Application of Mamdani Fuzzy Model for Auto Zoom Function of a
  Digital Camera","Mamdani Fuzzy Model is an important technique in Computational Intelligence
(CI) study. This paper presents an implementation of a supervised learning
method based on membership function training in the context of Mamdani fuzzy
models. Specifically, auto zoom function of a digital camera is modelled using
Mamdani technique. The performance of control method is verified through a
series of simulation and numerical results are provided as illustrations.","6 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
$α$-Discounting Multi-Criteria Decision Making ($α$-D MCDM),"In this book we introduce a new procedure called \alpha-Discounting Method
for Multi-Criteria Decision Making (\alpha-D MCDM), which is as an alternative
and extension of Saaty Analytical Hierarchy Process (AHP). It works for any
number of preferences that can be transformed into a system of homogeneous
linear equations. A degree of consistency (and implicitly a degree of
inconsistency) of a decision-making problem are defined. \alpha-D MCDM is
afterwards generalized to a set of preferences that can be transformed into a
system of linear and or non-linear homogeneous and or non-homogeneous equations
and or inequalities. The general idea of \alpha-D MCDM is to assign non-null
positive parameters \alpha_1, \alpha_2, and so on \alpha_p to the coefficients
in the right-hand side of each preference that diminish or increase them in
order to transform the above linear homogeneous system of equations which has
only the null-solution, into a system having a particular non-null solution.
After finding the general solution of this system, the principles used to
assign particular values to all parameters \alpha is the second important part
of \alpha-D, yet to be deeper investigated in the future. In the current book
we propose the Fairness Principle, i.e. each coefficient should be discounted
with the same percentage (we think this is fair: not making any favoritism or
unfairness to any coefficient), but the reader can propose other principles.
For consistent decision-making problems with pairwise comparisons,
\alpha-Discounting Method together with the Fairness Principle give the same
result as AHP. But for weak inconsistent decision-making problem,
\alpha-Discounting together with the Fairness Principle give a different result
from AHP. Many consistent, weak inconsistent, and strong inconsistent examples
are given in this book.",62 pages
Dominion -- A constraint solver generator,"This paper proposes a design for a system to generate constraint solvers that
are specialised for specific problem models. It describes the design in detail
and gives preliminary experimental results showing the feasibility and
effectiveness of the approach.",N/A
"Logical Evaluation of Consciousness: For Incorporating Consciousness
  into Machine Architecture","Machine Consciousness is the study of consciousness in a biological,
philosophical, mathematical and physical perspective and designing a model that
can fit into a programmable system architecture. Prime objective of the study
is to make the system architecture behave consciously like a biological model
does. Present work has developed a feasible definition of consciousness, that
characterizes consciousness with four parameters i.e., parasitic, symbiotic,
self referral and reproduction. Present work has also developed a biologically
inspired consciousness architecture that has following layers: quantum layer,
cellular layer, organ layer and behavioral layer and traced the characteristics
of consciousness at each layer. Finally, the work has estimated physical and
algorithmic architecture to devise a system that can behave consciously.",N/A
Some improved results on communication between information systems,"To study the communication between information systems, Wang et al. [C. Wang,
C. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,
Information Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and
type-2 consistent functions. Some properties of such functions and induced
relation mappings have been investigated there. In this paper, we provide an
improvement of the aforementioned work by disclosing the symmetric relationship
between type-1 and type-2 consistent functions. We present more properties of
consistent functions and induced relation mappings and improve upon several
deficient assertions in the original work. In particular, we unify and extend
type-1 and type-2 consistent functions into the so-called
neighborhood-consistent functions. This provides a convenient means for
studying the communication between information systems based on various
neighborhoods.",12 pages
Homomorphisms between fuzzy information systems revisited,"Recently, Wang et al. discussed the properties of fuzzy information systems
under homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms
between fuzzy information systems, Applied Mathematics Letters 22 (2009)
1045-1050], where homomorphisms are based upon the concepts of consistent
functions and fuzzy relation mappings. In this paper, we classify consistent
functions as predecessor-consistent and successor-consistent, and then proceed
to present more properties of consistent functions. In addition, we improve
some characterizations of fuzzy relation mappings provided by Wang et al.",10 pages
"Establishment of Relationships between Material Design and Product
  Design Domains by Hybrid FEM-ANN Technique","In this paper, research on AI based modeling technique to optimize
development of new alloys with necessitated improvements in properties and
chemical mixture over existing alloys as per functional requirements of product
is done. The current research work novels AI in lieu of predictions to
establish association between material and product customary. Advanced
computational simulation techniques like CFD, FEA interrogations are made
viable to authenticate product dynamics in context to experimental
investigations. Accordingly, the current research is focused towards binding
relationships between material design and product design domains. The input to
feed forward back propagation prediction network model constitutes of material
design features. Parameters relevant to product design strategies are furnished
as target outputs. The outcomes of ANN shows good sign of correlation between
material and product design domains. The study enriches a new path to
illustrate material factors at the time of new product development.","International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 1, January 2010,
  http://ijcsi.org/articles/Establishment-of-Relationships-between-Material-Design-and-Product-Design-Domains-by-Hybrid-FEM-ANN-Technique.php"
Modeling of Human Criminal Behavior using Probabilistic Networks,"Currently, criminals profile (CP) is obtained from investigators or forensic
psychologists interpretation, linking crime scene characteristics and an
offenders behavior to his or her characteristics and psychological profile.
This paper seeks an efficient and systematic discovery of nonobvious and
valuable patterns between variables from a large database of solved cases via a
probabilistic network (PN) modeling approach. The PN structure can be used to
extract behavioral patterns and to gain insight into what factors influence
these behaviors. Thus, when a new case is being investigated and the profile
variables are unknown because the offender has yet to be identified, the
observed crime scene variables are used to infer the unknown variables based on
their connections in the structure and the corresponding numerical
(probabilistic) weights. The objective is to produce a more systematic and
empirical approach to profiling, and to use the resulting PN model as a
decision tool.","IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS January 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
Model-Driven Constraint Programming,"Constraint programming can definitely be seen as a model-driven paradigm. The
users write programs for modeling problems. These programs are mapped to
executable models to calculate the solutions. This paper focuses on efficient
model management (definition and transformation). From this point of view, we
propose to revisit the design of constraint-programming systems. A model-driven
architecture is introduced to map solving-independent constraint models to
solving-dependent decision models. Several important questions are examined,
such as the need for a visual highlevel modeling language, and the quality of
metamodeling techniques to implement the transformations. A main result is the
s-COMMA platform that efficiently implements the chain from modeling to solving
constraint problems",N/A
Rewriting Constraint Models with Metamodels,"An important challenge in constraint programming is to rewrite constraint
models into executable programs calculat- ing the solutions. This phase of
constraint processing may require translations between constraint programming
lan- guages, transformations of constraint representations, model
optimizations, and tuning of solving strategies. In this paper, we introduce a
pivot metamodel describing the common fea- tures of constraint models including
different kinds of con- straints, statements like conditionals and loops, and
other first-class elements like object classes and predicates. This metamodel
is general enough to cope with the constructions of many languages, from
object-oriented modeling languages to logic languages, but it is independent
from them. The rewriting operations manipulate metamodel instances apart from
languages. As a consequence, the rewriting operations apply whatever languages
are selected and they are able to manage model semantic information. A bridge
is created between the metamodel space and languages using parsing techniques.
Tools from the software engineering world can be useful to implement this
framework.",N/A
"Using ATL to define advanced and flexible constraint model
  transformations","Transforming constraint models is an important task in re- cent constraint
programming systems. User-understandable models are defined during the modeling
phase but rewriting or tuning them is manda- tory to get solving-efficient
models. We propose a new architecture al- lowing to define bridges between any
(modeling or solver) languages and to implement model optimizations. This
architecture follows a model- driven approach where the constraint modeling
process is seen as a set of model transformations. Among others, an interesting
feature is the def- inition of transformations as concept-oriented rules, i.e.
based on types of model elements where the types are organized into a hierarchy
called a metamodel.",N/A
"Feature Importance in Bayesian Assessment of Newborn Brain Maturity from
  EEG","The methodology of Bayesian Model Averaging (BMA) is applied for assessment
of newborn brain maturity from sleep EEG. In theory this methodology provides
the most accurate assessments of uncertainty in decisions. However, the
existing BMA techniques have been shown providing biased assessments in the
absence of some prior information enabling to explore model parameter space in
details within a reasonable time. The lack in details leads to disproportional
sampling from the posterior distribution. In case of the EEG assessment of
brain maturity, BMA results can be biased because of the absence of information
about EEG feature importance. In this paper we explore how the posterior
information about EEG features can be used in order to reduce a negative impact
of disproportional sampling on BMA performance. We use EEG data recorded from
sleeping newborns to test the efficiency of the proposed BMA technique.","Proceedings of the 9th WSEAS International Conference on Artificial
  Intelligence, Knowledge Engineering and Data Bases (AIKED), University of
  Cambridge, UK, 2010, edited by L. A. Zadeh et al, pp 191 - 195"
A new model for solution of complex distributed constrained problems,"In this paper we describe an original computational model for solving
different types of Distributed Constraint Satisfaction Problems (DCSP). The
proposed model is called Controller-Agents for Constraints Solving (CACS). This
model is intended to be used which is an emerged field from the integration
between two paradigms of different nature: Multi-Agent Systems (MAS) and the
Constraint Satisfaction Problem paradigm (CSP) where all constraints are
treated in central manner as a black-box. This model allows grouping
constraints to form a subset that will be treated together as a local problem
inside the controller. Using this model allows also handling non-binary
constraints easily and directly so that no translating of constraints into
binary ones is needed. This paper presents the implementation outlines of a
prototype of DCSP solver, its usage methodology and overview of the CACS
application for timetabling problems.",N/A
Automatically Discovering Hidden Transformation Chaining Constraints,"Model transformations operate on models conforming to precisely defined
metamodels. Consequently, it often seems relatively easy to chain them: the
output of a transformation may be given as input to a second one if metamodels
match. However, this simple rule has some obvious limitations. For instance, a
transformation may only use a subset of a metamodel. Therefore, chaining
transformations appropriately requires more information. We present here an
approach that automatically discovers more detailed information about actual
chaining constraints by statically analyzing transformations. The objective is
to provide developers who decide to chain transformations with more data on
which to base their choices. This approach has been successfully applied to the
case of a library of endogenous transformations. They all have the same source
and target metamodel but have some hidden chaining constraints. In such a case,
the simple metamodel matching rule given above does not provide any useful
information.",N/A
"Integration of Rule Based Expert Systems and Case Based Reasoning in an
  Acute Bacterial Meningitis Clinical Decision Support System","This article presents the results of the research carried out on the
development of a medical diagnostic system applied to the Acute Bacterial
Meningitis, using the Case Based Reasoning methodology. The research was
focused on the implementation of the adaptation stage, from the integration of
Case Based Reasoning and Rule Based Expert Systems. In this adaptation stage we
use a higher level RBC that stores and allows reutilizing change experiences,
combined with a classic rule-based inference engine. In order to take into
account the most evident clinical situation, a pre-diagnosis stage is
implemented using a rule engine that, given an evident situation, emits the
corresponding diagnosis and avoids the complete process.","Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/"
Indexer Based Dynamic Web Services Discovery,"Recent advancement in web services plays an important role in business to
business and business to consumer interaction. Discovery mechanism is not only
used to find a suitable service but also provides collaboration between service
providers and consumers by using standard protocols. A static web service
discovery mechanism is not only time consuming but requires continuous human
interaction. This paper proposed an efficient dynamic web services discovery
mechanism that can locate relevant and updated web services from service
registries and repositories with timestamp based on indexing value and
categorization for faster and efficient discovery of service. The proposed
prototype focuses on quality of service issues and introduces concept of local
cache, categorization of services, indexing mechanism, CSP (Constraint
Satisfaction Problem) solver, aging and usage of translator. Performance of
proposed framework is evaluated by implementing the algorithm and correctness
of our method is shown. The results of proposed framework shows greater
performance and accuracy in dynamic discovery mechanism of web services
resolving the existing issues of flexibility, scalability, based on quality of
service, and discovers updated and most relevant services with ease of usage.","Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/"
"On the Failure of the Finite Model Property in some Fuzzy Description
  Logics","Fuzzy Description Logics (DLs) are a family of logics which allow the
representation of (and the reasoning with) structured knowledge affected by
vagueness. Although most of the not very expressive crisp DLs, such as ALC,
enjoy the Finite Model Property (FMP), this is not the case once we move into
the fuzzy case. In this paper we show that if we allow arbitrary knowledge
bases, then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not
verify the FMP even if we restrict to witnessed models; in other words, finite
satisfiability and witnessed satisfiability are different for arbitrary
knowledge bases. The aim of this paper is to point out the failure of FMP
because it affects several algorithms published in the literature for reasoning
under fuzzy ALC.",N/A
A multivalued knowledge-base model,"The basic aim of our study is to give a possible model for handling uncertain
information. This model is worked out in the framework of DATALOG. At first the
concept of fuzzy Datalog will be summarized, then its extensions for
intuitionistic- and interval-valued fuzzy logic is given and the concept of
bipolar fuzzy Datalog is introduced. Based on these ideas the concept of
multivalued knowledge-base will be defined as a quadruple of any background
knowledge; a deduction mechanism; a connecting algorithm, and a function set of
the program, which help us to determine the uncertainty levels of the results.
At last a possible evaluation strategy is given.",N/A
Release ZERO.0.1 of package RefereeToolbox,"RefereeToolbox is a java package implementing combination operators for
fusing evidences. It is downloadable from:
http://refereefunction.fredericdambreville.com/releases RefereeToolbox is based
on an interpretation of the fusion rules by means of Referee Functions. This
approach implies a dissociation between the definition of the combination and
its actual implementation, which is common to all referee-based combinations.
As a result, RefereeToolbox is designed with the aim to be generic and
evolutive.",N/A
LEXSYS: Architecture and Implication for Intelligent Agent systems,"LEXSYS, (Legume Expert System) was a project conceived at IITA (International
Institute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the
COMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was
meant for a general framework for characterizing on-farm testing for technology
design for sustainable cereal-based cropping system. LEXSYS is not a true
expert system as the name would imply, but simply a user-friendly information
system. This work is an attempt to give a formal representation of the existing
system and then present areas where intelligent agent can be applied.",N/A
Rational Value of Information Estimation for Measurement Selection,"Computing value of information (VOI) is a crucial task in various aspects of
decision-making under uncertainty, such as in meta-reasoning for search; in
selecting measurements to make, prior to choosing a course of action; and in
managing the exploration vs. exploitation tradeoff. Since such applications
typically require numerous VOI computations during a single run, it is
essential that VOI be computed efficiently. We examine the issue of anytime
estimation of VOI, as frequently it suffices to get a crude estimate of the
VOI, thus saving considerable computational resources. As a case study, we
examine VOI estimation in the measurement selection problem. Empirical
evaluation of the proposed scheme in this domain shows that computational
resources can indeed be significantly reduced, at little cost in expected
rewards achieved in the overall decision problem.","7 pages, 2 figures, presented at URPDM2010; plots fixed"
Geometric Algebra Model of Distributed Representations,"Formalism based on GA is an alternative to distributed representation models
developed so far --- Smolensky's tensor product, Holographic Reduced
Representations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced
by geometric products, interpretable in terms of geometry which seems to be the
most natural language for visualization of higher concepts. This paper recalls
the main ideas behind the GA model and investigates recognition test results
using both inner product and a clipped version of matrix representation. The
influence of accidental blade equality on recognition is also studied. Finally,
the efficiency of the GA model is compared to that of previously developed
models.","30 pages, 19 figures"
"Importance of Sources using the Repeated Fusion Method and the
  Proportional Conflict Redistribution Rules #5 and #6","We present in this paper some examples of how to compute by hand the PCR5
fusion rule for three sources, so the reader will better understand its
mechanism. We also take into consideration the importance of sources, which is
different from the classical discounting of sources.",N/A
Terrorism Event Classification Using Fuzzy Inference Systems,"Terrorism has led to many problems in Thai societies, not only property
damage but also civilian casualties. Predicting terrorism activities in advance
can help prepare and manage risk from sabotage by these activities. This paper
proposes a framework focusing on event classification in terrorism domain using
fuzzy inference systems (FISs). Each FIS is a decision-making model combining
fuzzy logic and approximate reasoning. It is generated in five main parts: the
input interface, the fuzzification interface, knowledge base unit, decision
making unit and output defuzzification interface. Adaptive neuro-fuzzy
inference system (ANFIS) is a FIS model adapted by combining the fuzzy logic
and neural network. The ANFIS utilizes automatic identification of fuzzy logic
rules and adjustment of membership function (MF). Moreover, neural network can
directly learn from data set to construct fuzzy logic rules and MF implemented
in various applications. FIS settings are evaluated based on two comparisons.
The first evaluation is the comparison between unstructured and structured
events using the same FIS setting. The second comparison is the model settings
between FIS and ANFIS for classifying structured events. The data set consists
of news articles related to terrorism events in three southern provinces of
Thailand. The experimental results show that the classification performance of
the FIS resulting from structured events achieves satisfactory accuracy and is
better than the unstructured events. In addition, the classification of
structured events using ANFIS gives higher performance than the events using
only FIS in the prediction of terrorism events.","IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
Probabilistic Semantic Web Mining Using Artificial Neural Analysis,"Most of the web user's requirements are search or navigation time and getting
correctly matched result. These constrains can be satisfied with some
additional modules attached to the existing search engines and web servers.
This paper proposes that powerful architecture for search engines with the
title of Probabilistic Semantic Web Mining named from the methods used. With
the increase of larger and larger collection of various data resources on the
World Wide Web (WWW), Web Mining has become one of the most important
requirements for the web users. Web servers will store various formats of data
including text, image, audio, video etc., but servers can not identify the
contents of the data. These search techniques can be improved by adding some
special techniques including semantic web mining and probabilistic analysis to
get more accurate results. Semantic web mining technique can provide meaningful
search of data resources by eliminating useless information with mining
process. In this technique web servers will maintain Meta information of each
and every data resources available in that particular web server. This will
help the search engine to retrieve information that is relevant to user given
input string. This paper proposing the idea of combing these two techniques
Semantic web mining and Probabilistic analysis for efficient and accurate
search results of web mining. SPF can be calculated by considering both
semantic accuracy and syntactic accuracy of data with the input string. This
will be the deciding factor for producing results.","IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/"
Matrix Coherence and the Nystrom Method,"The Nystrom method is an efficient technique to speed up large-scale learning
applications by generating low-rank approximations. Crucial to the performance
of this technique is the assumption that a matrix can be well approximated by
working exclusively with a subset of its columns. In this work we relate this
assumption to the concept of matrix coherence and connect matrix coherence to
the performance of the Nystrom method. Making use of related work in the
compressed sensing and the matrix completion literature, we derive novel
coherence-based bounds for the Nystrom method in the low-rank setting. We then
present empirical results that corroborate these theoretical bounds. Finally,
we present more general empirical results for the full-rank setting that
convincingly demonstrate the ability of matrix coherence to measure the degree
to which information can be extracted from a subset of columns.",N/A
Symmetry within Solutions,"We define the concept of an internal symmetry. This is a symmety within a
solution of a constraint satisfaction problem. We compare this to solution
symmetry, which is a mapping between different solutions of the same problem.
We argue that we may be able to exploit both types of symmetry when finding
solutions. We illustrate the potential of exploiting internal symmetries on two
benchmark domains: Van der Waerden numbers and graceful graphs. By identifying
internal symmetries we are able to extend the state of the art in both cases.","AAAI 2010, Proceedings of Twenty-Fourth AAAI Conference on Artificial
  Intelligence"
Propagating Conjunctions of AllDifferent Constraints,"We study propagation algorithms for the conjunction of two AllDifferent
constraints. Solutions of an AllDifferent constraint can be seen as perfect
matchings on the variable/value bipartite graph. Therefore, we investigate the
problem of finding simultaneous bipartite matchings. We present an extension of
the famous Hall theorem which characterizes when simultaneous bipartite
matchings exists. Unfortunately, finding such matchings is NP-hard in general.
However, we prove a surprising result that finding a simultaneous matching on a
convex bipartite graph takes just polynomial time. Based on this theoretical
result, we provide the first polynomial time bound consistency algorithm for
the conjunction of two AllDifferent constraints. We identify a pathological
problem on which this propagator is exponentially faster compared to existing
propagators. Our experiments show that this new propagator can offer
significant benefits over existing methods.","AAAI 2010, Proceedings of the Twenty-Fourth AAAI Conference on
  Artificial Intelligence"
Decision Support Systems (DSS) in Construction Tendering Processes,"The successful execution of a construction project is heavily impacted by
making the right decision during tendering processes. Managing tender
procedures is very complex and uncertain involving coordination of many tasks
and individuals with different priorities and objectives. Bias and inconsistent
decision are inevitable if the decision-making process is totally depends on
intuition, subjective judgement or emotion. In making transparent decision and
healthy competition tendering, there exists a need for flexible guidance tool
for decision support. Aim of this paper is to give a review on current
practices of Decision Support Systems (DSS) technology in construction
tendering processes. Current practices of general tendering processes as
applied to the most countries in different regions such as United States,
Europe, Middle East and Asia are comprehensively discussed. Applications of
Web-based tendering processes is also summarised in terms of its properties.
Besides that, a summary of Decision Support System (DSS) components is included
in the next section. Furthermore, prior researches on implementation of DSS
approaches in tendering processes are discussed in details. Current issues
arise from both of paper-based and Web-based tendering processes are outlined.
Finally, conclusion is included at the end of this paper.","International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Decision-Support-Systems-DSS-in-Construction-Tendering-Processes.php"
Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version),"The need for integration of ontologies with nonmonotonic rules has been
gaining importance in a number of areas, such as the Semantic Web. A number of
researchers addressed this problem by proposing a unified semantics for hybrid
knowledge bases composed of both an ontology (expressed in a fragment of
first-order logic) and nonmonotonic rules. These semantics have matured over
the years, but only provide solutions for the static case when knowledge does
not need to evolve. In this paper we take a first step towards addressing the
dynamics of hybrid knowledge bases. We focus on knowledge updates and,
considering the state of the art of belief update, ontology update and rule
update, we show that current solutions are only partial and difficult to
combine. Then we extend the existing work on ABox updates with rules, provide a
semantics for such evolving hybrid knowledge bases and study its basic
properties. To the best of our knowledge, this is the first time that an update
operator is proposed for hybrid knowledge bases.","40 pages; an extended version of the article published in Theory and
  Practice of Logic Programming, 10 (4-6): 547 - 564, July. Copyright 2010
  Cambridge University Press"
"On the comparison of plans: Proposition of an instability measure for
  dynamic machine scheduling","On the basis of an analysis of previous research, we present a generalized
approach for measuring the difference of plans with an exemplary application to
machine scheduling. Our work is motivated by the need for such measures, which
are used in dynamic scheduling and planning situations. In this context,
quantitative approaches are needed for the assessment of the robustness and
stability of schedules. Obviously, any `robustness' or `stability' of plans has
to be defined w. r. t. the particular situation and the requirements of the
human decision maker. Besides the proposition of an instability measure, we
therefore discuss possibilities of obtaining meaningful information from the
decision maker for the implementation of the introduced approach.",N/A
Ontology-based inference for causal explanation,"We define an inference system to capture explanations based on causal
statements, using an ontology in the form of an IS-A hierarchy. We first
introduce a simple logical language which makes it possible to express that a
fact causes another fact and that a fact explains another fact. We present a
set of formal inference patterns from causal statements to explanation
statements. We introduce an elementary ontology which gives greater
expressiveness to the system while staying close to propositional reasoning. We
provide an inference system that captures the patterns discussed, firstly in a
purely propositional framework, then in a datalog (limited predicate)
framework.",N/A
The Exact Closest String Problem as a Constraint Satisfaction Problem,"We report (to our knowledge) the first evaluation of Constraint Satisfaction
as a computational framework for solving closest string problems. We show that
careful consideration of symbol occurrences can provide search heuristics that
provide several orders of magnitude speedup at and above the optimal distance.
We also report (to our knowledge) the first analysis and evaluation -- using
any technique -- of the computational difficulties involved in the
identification of all closest strings for a given input set. We describe
algorithms for web-scale distributed solution of closest string problems, both
purely based on AI backtrack search and also hybrid numeric-AI methods.",N/A
Joint Structured Models for Extraction from Overlapping Sources,"We consider the problem of jointly training structured models for extraction
from sources whose instances enjoy partial overlap. This has important
applications like user-driven ad-hoc information extraction on the web. Such
applications present new challenges in terms of the number of sources and their
arbitrary pattern of overlap not seen by earlier collective training schemes
applied on two sources. We present an agreement-based learning framework and
alternatives within it to trade-off tractability, robustness to noise, and
extent of agreement. We provide a principled scheme to discover low-noise
agreement sets in unlabeled data across the sources. Through extensive
experiments over 58 real datasets, we establish that our method of additively
rewarding agreement over maximal segments of text provides the best trade-offs,
and also scores over alternatives such as collective inference, staged
training, and multi-view learning.",N/A
"An approach to visualize the course of solving of a research task in
  humans","A technique to study the dynamics of solving of a research task is suggested.
The research task was based on specially developed software Right- Wrong
Responder (RWR), with the participants having to reveal the response logic of
the program. The participants interacted with the program in the form of a
semi-binary dialogue, which implies the feedback responses of only two kinds -
""right"" or ""wrong"". The technique has been applied to a small pilot group of
volunteer participants. Some of them have successfully solved the task
(solvers) and some have not (non-solvers). In the beginning of the work, the
solvers did more wrong moves than non-solvers, and they did less wrong moves
closer to the finish of the work. A phase portrait of the work both in solvers
and non-solvers showed definite cycles that may correspond to sequences of
partially true hypotheses that may be formulated by the participants during the
solving of the task.","20 pages, 9 figures"
Informal Concepts in Machines,"This paper constructively proves the existence of an effective procedure
generating a computable (total) function that is not contained in any given
effectively enumerable set of such functions. The proof implies the existence
of machines that process informal concepts such as computable (total) functions
beyond the limits of any given Turing machine or formal system, that is, these
machines can, in a certain sense, ""compute"" function values beyond these
limits. We call these machines creative. We argue that any ""intelligent""
machine should be capable of processing informal concepts such as computable
(total) functions, that is, it should be creative. Finally, we introduce
hypotheses on creative machines which were developed on the basis of
theoretical investigations and experiments with computer programs. The
hypotheses say that machine intelligence is the execution of a self-developing
procedure starting from any universal programming language and any input.",N/A
"A two-step fusion process for multi-criteria decision applied to natural
  hazards in mountains","Mountain river torrents and snow avalanches generate human and material
damages with dramatic consequences. Knowledge about natural phenomenona is
often lacking and expertise is required for decision and risk management
purposes using multi-disciplinary quantitative or qualitative approaches.
Expertise is considered as a decision process based on imperfect information
coming from more or less reliable and conflicting sources. A methodology mixing
the Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and
information fusion using Belief Function Theory is described. Fuzzy Sets and
Possibilities theories allow to transform quantitative and qualitative criteria
into a common frame of discernment for decision in Dempster-Shafer Theory (DST
) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic
belief assignments elicitation, conflict identification and management, fusion
rule choices, results validation but also in specific needs to make a
difference between importance and reliability and uncertainty in the fusion
process.",N/A
On Building a Knowledge Base for Stability Theory,"A lot of mathematical knowledge has been formalized and stored in
repositories by now: different mathematical theorems and theories have been
taken into consideration and included in mathematical repositories.
Applications more distant from pure mathematics, however --- though based on
these theories --- often need more detailed knowledge about the underlying
theories. In this paper we present an example Mizar formalization from the area
of electrical engineering focusing on stability theory which is based on
complex analysis. We discuss what kind of special knowledge is necessary here
and which amount of this knowledge is included in existing repositories.","To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010"
"Recognizability of Individual Creative Style Within and Across Domains:
  Preliminary Studies","It is hypothesized that creativity arises from the self-mending capacity of
an internal model of the world, or worldview. The uniquely honed worldview of a
creative individual results in a distinctive style that is recognizable within
and across domains. It is further hypothesized that creativity is domaingeneral
in the sense that there exist multiple avenues by which the distinctiveness of
one's worldview can be expressed. These hypotheses were tested using art
students and creative writing students. Art students guessed significantly
above chance both which painting was done by which of five famous artists, and
which artwork was done by which of their peers. Similarly, creative writing
students guessed significantly above chance both which passage was written by
which of five famous writers, and which passage was written by which of their
peers. These findings support the hypothesis that creative style is
recognizable. Moreover, creative writing students guessed significantly above
chance which of their peers produced particular works of art, supporting the
hypothesis that creative style is recognizable not just within but across
domains.","6 pages, submitted to Annual Meeting of the Cognitive Science
  Society. August 11-14, 2010, Portland, Oregon"
"Feature Selection Using Regularization in Approximate Linear Programs
  for Markov Decision Processes","Approximate dynamic programming has been used successfully in a large variety
of domains, but it relies on a small set of provided approximation features to
calculate solutions reliably. Large and rich sets of features can cause
existing algorithms to overfit because of a limited number of samples. We
address this shortcoming using $L_1$ regularization in approximate linear
programming. Because the proposed method can automatically select the
appropriate richness of features, its performance does not degrade with an
increasing number of features. These results rely on new and stronger sampling
bounds for regularized approximate linear programs. We also propose a
computationally efficient homotopy method. The empirical evaluation of the
approach shows that the proposed method performs well on simple MDPs and
standard benchmark problems.","Technical report corresponding to the ICML2010 submission of the same
  name"
Evolving Genes to Balance a Pole,"We discuss how to use a Genetic Regulatory Network as an evolutionary
representation to solve a typical GP reinforcement problem, the pole balancing.
The network is a modified version of an Artificial Regulatory Network proposed
a few years ago, and the task could be solved only by finding a proper way of
connecting inputs and outputs to the network. We show that the representation
is able to generalize well over the problem domain, and discuss the performance
of different models of this kind.",N/A
"Using machine learning to make constraint solver implementation
  decisions","Programs to solve so-called constraint problems are complex pieces of
software which require many design decisions to be made more or less
arbitrarily by the implementer. These decisions affect the performance of the
finished solver significantly. Once a design decision has been made, it cannot
easily be reversed, although a different decision may be more appropriate for a
particular problem.
  We investigate using machine learning to make these decisions automatically
depending on the problem to solve with the alldifferent constraint as an
example. Our system is capable of making non-trivial, multi-level decisions
that improve over always making a default choice.",N/A
A Soft Computing Model for Physicians' Decision Process,"In this paper the author presents a kind of Soft Computing Technique, mainly
an application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical
Experts Systems. The choosen problem is on design of a physician's decision
model which can take crisp as well as fuzzy data as input, unlike the
traditional models. The author presents a mathematical model based on fuzzy set
theory for physician aided evaluation of a complete representation of
information emanating from the initial interview including patient past
history, present symptoms, and signs observed upon physical examination and
results of clinical and diagnostic tests.",http://www.journalofcomputing.org
The Complexity of Manipulating $k$-Approval Elections,"An important problem in computational social choice theory is the complexity
of undesirable behavior among agents, such as control, manipulation, and
bribery in election systems. These kinds of voting strategies are often
tempting at the individual level but disastrous for the agents as a whole.
Creating election systems where the determination of such strategies is
difficult is thus an important goal.
  An interesting set of elections is that of scoring protocols. Previous work
in this area has demonstrated the complexity of misuse in cases involving a
fixed number of candidates, and of specific election systems on unbounded
number of candidates such as Borda. In contrast, we take the first step in
generalizing the results of computational complexity of election misuse to
cases of infinitely many scoring protocols on an unbounded number of
candidates. Interesting families of systems include $k$-approval and $k$-veto
elections, in which voters distinguish $k$ candidates from the candidate set.
  Our main result is to partition the problems of these families based on their
complexity. We do so by showing they are polynomial-time computable, NP-hard,
or polynomial-time equivalent to another problem of interest. We also
demonstrate a surprising connection between manipulation in election systems
and some graph theory problems.",N/A
"Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of
  Analystical Methodology","In the last two decades, a number of methods have been proposed for
forecasting based on fuzzy time series. Most of the fuzzy time series methods
are presented for forecasting of car road accidents. However, the forecasting
accuracy rates of the existing methods are not good enough. In this paper, we
compared our proposed new method of fuzzy time series forecasting with existing
methods. Our method is based on means based partitioning of the historical data
of car road accidents. The proposed method belongs to the kth order and
time-variant methods. The proposed method can get the best forecasting accuracy
rate for forecasting the car road accidents than the existing methods.","IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/"
Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection,"In this paper, a new learning algorithm for adaptive network intrusion
detection using naive Bayesian classifier and decision tree is presented, which
performs balance detections and keeps false positives at acceptable level for
different types of network attacks, and eliminates redundant attributes as well
as contradictory examples from training data that make the detection model
complex. The proposed algorithm also addresses some difficulties of data mining
such as handling continuous attribute, dealing with missing attribute values,
and reducing noise in training data. Due to the large volumes of security audit
data as well as the complex and dynamic properties of intrusion behaviours,
several data miningbased intrusion detection techniques have been applied to
network-based traffic data and host-based data in the last decades. However,
there remain various issues needed to be examined towards current intrusion
detection systems (IDS). We tested the performance of our proposed algorithm
with existing learning algorithms by employing on the KDD99 benchmark intrusion
detection dataset. The experimental results prove that the proposed algorithm
achieved high detection rates (DR) and significant reduce false positives (FP)
for different types of network intrusions using limited computational
resources.","14 Pages, IJNSA"
"Automated Reasoning and Presentation Support for Formalizing Mathematics
  in Mizar","This paper presents a combination of several automated reasoning and proof
presentation tools with the Mizar system for formalization of mathematics. The
combination forms an online service called MizAR, similar to the SystemOnTPTP
service for first-order automated reasoning. The main differences to
SystemOnTPTP are the use of the Mizar language that is oriented towards human
mathematicians (rather than the pure first-order logic used in SystemOnTPTP),
and setting the service in the context of the large Mizar Mathematical Library
of previous theorems,definitions, and proofs (rather than the isolated problems
that are solved in SystemOnTPTP). These differences poses new challenges and
new opportunities for automated reasoning and for proof presentation tools.
This paper describes the overall structure of MizAR, and presents the automated
reasoning systems and proof presentation tools that are combined to make MizAR
a useful mathematical service.","To appear in 10th International Conference on. Artificial
  Intelligence and Symbolic Computation AISC 2010"
Integrating Structured Metadata with Relational Affinity Propagation,"Structured and semi-structured data describing entities, taxonomies and
ontologies appears in many domains. There is a huge interest in integrating
structured information from multiple sources; however integrating structured
data to infer complex common structures is a difficult task because the
integration must aggregate similar structures while avoiding structural
inconsistencies that may appear when the data is combined. In this work, we
study the integration of structured social metadata: shallow personal
hierarchies specified by many individual users on the SocialWeb, and focus on
inferring a collection of integrated, consistent taxonomies. We frame this task
as an optimization problem with structural constraints. We propose a new
inference algorithm, which we refer to as Relational Affinity Propagation (RAP)
that extends affinity propagation (Frey and Dueck 2007) by introducing
structural constraints. We validate the approach on a real-world social media
dataset, collected from the photosharing website Flickr. Our empirical results
show that our proposed approach is able to construct deeper and denser
structures compared to an approach using only the standard affinity propagation
algorithm.","6 Pages, To appear at AAAI Workshop on Statistical Relational AI"
A Formalization of the Turing Test,"The paper offers a mathematical formalization of the Turing test. This
formalization makes it possible to establish the conditions under which some
Turing machine will pass the Turing test and the conditions under which every
Turing machine (or every Turing machine of the special class) will fail the
Turing test.",10 pages
"Growing a Tree in the Forest: Constructing Folksonomies by Integrating
  Structured Metadata","Many social Web sites allow users to annotate the content with descriptive
metadata, such as tags, and more recently to organize content hierarchically.
These types of structured metadata provide valuable evidence for learning how a
community organizes knowledge. For instance, we can aggregate many personal
hierarchies into a common taxonomy, also known as a folksonomy, that will aid
users in visualizing and browsing social content, and also to help them in
organizing their own content. However, learning from social metadata presents
several challenges, since it is sparse, shallow, ambiguous, noisy, and
inconsistent. We describe an approach to folksonomy learning based on
relational clustering, which exploits structured metadata contained in personal
hierarchies. Our approach clusters similar hierarchies using their structure
and tag statistics, then incrementally weaves them into a deeper, bushier tree.
We study folksonomy learning using social metadata extracted from the
photo-sharing site Flickr, and demonstrate that the proposed approach addresses
the challenges. Moreover, comparing to previous work, the approach produces
larger, more accurate folksonomies, and in addition, scales better.","10 pages, To appear in the Proceedings of ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining(KDD) 2010"
Symmetries of Symmetry Breaking Constraints,"Symmetry is an important feature of many constraint programs. We show that
any problem symmetry acting on a set of symmetry breaking constraints can be
used to break symmetry. Different symmetries pick out different solutions in
each symmetry class. This simple but powerful idea can be used in a number of
different ways. We describe one application within model restarts, a search
technique designed to reduce the conflict between symmetry breaking and the
branching heuristic. In model restarts, we restart search periodically with a
random symmetry of the symmetry breaking constraints. Experimental results show
that this symmetry breaking technique is effective in practice on some standard
benchmark problems.","To appear in Proceedings of the 19th European Conference on
  Artificial Intelligence (ECAI 2010). Revises workshop paper that appears at
  SymCon 2009"
"Learning Probabilistic Hierarchical Task Networks to Capture User
  Preferences","We propose automatically learning probabilistic Hierarchical Task Networks
(pHTNs) in order to capture a user's preferences on plans, by observing only
the user's behavior. HTNs are a common choice of representation for a variety
of purposes in planning, including work on learning in planning. Our
contributions are (a) learning structure and (b) representing preferences. In
contrast, prior work employing HTNs considers learning method preconditions
(instead of structure) and representing domain physics or search control
knowledge (rather than preferences). Initially we will assume that the observed
distribution of plans is an accurate representation of user preference, and
then generalize to the situation where feasibility constraints frequently
prevent the execution of preferred plans. In order to learn a distribution on
plans we adapt an Expectation-Maximization (EM) technique from the discipline
of (probabilistic) grammar induction, taking the perspective of task reductions
as productions in a context-free grammar over primitive actions. To account for
the difference between the distributions of possible and preferred plans we
subsequently modify this core EM technique, in short, by rescaling its input.",30 pages
"Brain-Like Stochastic Search: A Research Challenge and Funding
  Opportunity","Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of
utility functions U(u,A), where u is a vector of parameters or task
descriptors, maximize or minimize U with respect to u, using networks (Option
Nets) which input A and learn to generate good options u stochastically. This
paper discusses why this is crucial to brain-like intelligence (an area funded
by NSF) and to many applications, and discusses various possibilities for
network design and training. The appendix discusses recent research, relations
to work on stochastic optimization in operations research, and relations to
engineering-based approaches to understanding neocortex.","Plenary talk at IEEE Conference on Evolutionary Computing 1999,
  extended in 2010 with new appendix"
Variational Program Inference,"We introduce a framework for representing a variety of interesting problems
as inference over the execution of probabilistic model programs. We represent a
""solution"" to such a problem as a guide program which runs alongside the model
program and influences the model program's random choices, leading the model
program to sample from a different distribution than from its priors. Ideally
the guide program influences the model program to sample from the posteriors
given the evidence. We show how the KL- divergence between the true posterior
distribution and the distribution induced by the guided model program can be
efficiently estimated (up to an additive constant) by sampling multiple
executions of the guided model program. In addition, we show how to use the
guide program as a proposal distribution in importance sampling to
statistically prove lower bounds on the probability of the evidence and on the
probability of a hypothesis and the evidence. We can use the quotient of these
two bounds as an estimate of the conditional probability of the hypothesis
given the evidence. We thus turn the inference problem into a heuristic search
for better guide programs.",N/A
The Dilated Triple,"The basic unit of meaning on the Semantic Web is the RDF statement, or
triple, which combines a distinct subject, predicate and object to make a
definite assertion about the world. A set of triples constitutes a graph, to
which they give a collective meaning. It is upon this simple foundation that
the rich, complex knowledge structures of the Semantic Web are built. Yet the
very expressiveness of RDF, by inviting comparison with real-world knowledge,
highlights a fundamental shortcoming, in that RDF is limited to statements of
absolute fact, independent of the context in which a statement is asserted.
This is in stark contrast with the thoroughly context-sensitive nature of human
thought. The model presented here provides a particularly simple means of
contextualizing an RDF triple by associating it with related statements in the
same graph. This approach, in combination with a notion of graph similarity, is
sufficient to select only those statements from an RDF graph which are
subjectively most relevant to the context of the requesting process.",N/A
Game Information System,"In this Information system age many organizations consider information system
as their weapon to compete or gain competitive advantage or give the best
services for non profit organizations. Game Information System as combining
Information System and game is breakthrough to achieve organizations'
performance. The Game Information System will run the Information System with
game and how game can be implemented to run the Information System. Game is not
only for fun and entertainment, but will be a challenge to combine fun and
entertainment with Information System. The Challenge to run the information
system with entertainment, deliver the entertainment with information system
all at once. Game information system can be implemented in many sectors as like
the information system itself but in difference's view. A view of game which
people can joy and happy and do their transaction as a fun things.",14 pages
Virtual information system on working area,"In order to get strategic positioning for competition in business
organization, the information system must be ahead in this information age
where the information as one of the weapons to win the competition and in the
right hand the information will become a right bullet. The information system
with the information technology support isn't enough if just only on internet
or implemented with internet technology. The growth of information technology
as tools for helping and making people easy to use must be accompanied by
wanting to make fun and happy when they make contact with the information
technology itself. Basically human like to play, since childhood human have
been playing, free and happy and when human grow up they can't play as much as
when human was in their childhood. We have to develop the information system
which is not perform information system itself but can help human to explore
their natural instinct for playing, making fun and happiness when they interact
with the information system. Virtual information system is the way to present
playing and having fun atmosphere on working area.","6 pages, 3 figures"
Indonesian Earthquake Decision Support System,"Earthquake DSS is an information technology environment which can be used by
government to sharpen, make faster and better the earthquake mitigation
decision. Earthquake DSS can be delivered as E-government which is not only for
government itself but in order to guarantee each citizen's rights for
education, training and information about earthquake and how to overcome the
earthquake. Knowledge can be managed for future use and would become mining by
saving and maintain all the data and information about earthquake and
earthquake mitigation in Indonesia. Using Web technology will enhance global
access and easy to use. Datawarehouse as unNormalized database for
multidimensional analysis will speed the query process and increase reports
variation. Link with other Disaster DSS in one national disaster DSS, link with
other government information system and international will enhance the
knowledge and sharpen the reports.","8 pages, 7 figures"
MDPs with Unawareness,"Markov decision processes (MDPs) are widely used for modeling decision-making
problems in robotics, automated control, and economics. Traditional MDPs assume
that the decision maker (DM) knows all states and actions. However, this may
not be true in many situations of interest. We define a new framework, MDPs
with unawareness (MDPUs) to deal with the possibilities that a DM may not be
aware of all possible actions. We provide a complete characterization of when a
DM can learn to play near-optimally in an MDPU, and give an algorithm that
learns to play near-optimally when it is possible to do so, as efficiently as
possible. In particular, we characterize when a near-optimal solution can be
found in polynomial time.",11 pages
Global Optimization for Value Function Approximation,"Existing value function approximation methods have been successfully used in
many applications, but they often lack useful a priori error bounds. We propose
a new approximate bilinear programming formulation of value function
approximation, which employs global optimization. The formulation provides
strong a priori guarantees on both robust and expected policy loss by
minimizing specific norms of the Bellman residual. Solving a bilinear program
optimally is NP-hard, but this is unavoidable because the Bellman-residual
minimization itself is NP-hard. We describe and analyze both optimal and
approximate algorithms for solving bilinear programs. The analysis shows that
this algorithm offers a convergent generalization of approximate policy
iteration. We also briefly analyze the behavior of bilinear programming
algorithms under incomplete samples. Finally, we demonstrate that the proposed
approach can consistently minimize the Bellman residual on simple benchmark
problems.",N/A
"A General Framework for Equivalences in Answer-Set Programming by
  Countermodels in the Logic of Here-and-There","Different notions of equivalence, such as the prominent notions of strong and
uniform equivalence, have been studied in Answer-Set Programming, mainly for
the purpose of identifying programs that can serve as substitutes without
altering the semantics, for instance in program optimization. Such semantic
comparisons are usually characterized by various selections of models in the
logic of Here-and-There (HT). For uniform equivalence however, correct
characterizations in terms of HT-models can only be obtained for finite
theories, respectively programs. In this article, we show that a selection of
countermodels in HT captures uniform equivalence also for infinite theories.
This result is turned into coherent characterizations of the different notions
of equivalence by countermodels, as well as by a mixture of HT-models and
countermodels (so-called equivalence interpretations). Moreover, we generalize
the so-called notion of relativized hyperequivalence for programs to
propositional theories, and apply the same methodology in order to obtain a
semantic characterization which is amenable to infinite settings. This allows
for a lifting of the results to first-order theories under a very general
semantics given in terms of a quantified version of HT. We thus obtain a
general framework for the study of various notions of equivalence for theories
under answer-set semantics. Moreover, we prove an expedient property that
allows for a simplified treatment of extended signatures, and provide further
results for non-ground logic programs. In particular, uniform equivalence
coincides under open and ordinary answer-set semantics, and for finite
non-ground programs under these semantics, also the usual characterization of
uniform equivalence in terms of maximal and total HT-models of the grounding is
correct, even for infinite domains, when corresponding ground programs are
infinite.","32 pages; to appear in Theory and Practice of Logic Programming
  (TPLP)"
Human Disease Diagnosis Using a Fuzzy Expert System,"Human disease diagnosis is a complicated process and requires high level of
expertise. Any attempt of developing a web-based expert system dealing with
human disease diagnosis has to overcome various difficulties. This paper
describes a project work aiming to develop a web-based fuzzy expert system for
diagnosing human diseases. Now a days fuzzy systems are being used successfully
in an increasing number of application areas; they use linguistic rules to
describe systems. This research project focuses on the research and development
of a web-based clinical tool designed to improve the quality of the exchange of
health information between health care professionals and patients.
Practitioners can also use this web-based tool to corroborate diagnosis. The
proposed system is experimented on various scenarios in order to evaluate it's
performance. In all the cases, proposed system exhibits satisfactory results.","IEEE Publication Format,
  https://sites.google.com/site/journalofcomputing/"
Vagueness of Linguistic variable,"In the area of computer science focusing on creating machines that can engage
on behaviors that humans consider intelligent. The ability to create
intelligent machines has intrigued humans since ancient times and today with
the advent of the computer and 50 years of research into various programming
techniques, the dream of smart machines is becoming a reality. Researchers are
creating systems which can mimic human thought, understand speech, beat the
best human chessplayer, and countless other feats never before possible.
Ability of the human to estimate the information is most brightly shown in
using of natural languages. Using words of a natural language for valuation
qualitative attributes, for example, the person pawns uncertainty in form of
vagueness in itself estimations. Vague sets, vague judgments, vague conclusions
takes place there and then, where and when the reasonable subject exists and
also is interested in something. The vague sets theory has arisen as the answer
to an illegibility of language the reasonable subject speaks. Language of a
reasonable subject is generated by vague events which are created by the reason
and which are operated by the mind. The theory of vague sets represents an
attempt to find such approximation of vague grouping which would be more
convenient, than the classical theory of sets in situations where the natural
language plays a significant role. Such theory has been offered by known
American mathematician Gau and Buehrer .In our paper we are describing how
vagueness of linguistic variables can be solved by using the vague set
theory.This paper is mainly designed for one of directions of the eventology
(the theory of the random vague events), which has arisen within the limits of
the probability theory and which pursue the unique purpose to describe
eventologically a movement of reason.","IEEE Publication Format,
  https://sites.google.com/site/journalofcomputing/"
An Efficient Technique for Similarity Identification between Ontologies,"Ontologies usually suffer from the semantic heterogeneity when simultaneously
used in information sharing, merging, integrating and querying processes.
Therefore, the similarity identification between ontologies being used becomes
a mandatory task for all these processes to handle the problem of semantic
heterogeneity. In this paper, we propose an efficient technique for similarity
measurement between two ontologies. The proposed technique identifies all
candidate pairs of similar concepts without omitting any similar pair. The
proposed technique can be used in different types of operations on ontologies
such as merging, mapping and aligning. By analyzing its results a reasonable
improvement in terms of completeness, correctness and overall quality of the
results has been found.","IEEE Publication Format,
  https://sites.google.com/site/journalofcomputing/"
The State of the Art: Ontology Web-Based Languages: XML Based,"Many formal languages have been proposed to express or represent Ontologies,
including RDF, RDFS, DAML+OIL and OWL. Most of these languages are based on XML
syntax, but with various terminologies and expressiveness. Therefore, choosing
a language for building an Ontology is the main step. The main point of
choosing language to represent Ontology is based mainly on what the Ontology
will represent or be used for. That language should have a range of quality
support features such as ease of use, expressive power, compatibility, sharing
and versioning, internationalisation. This is because different kinds of
knowledge-based applications need different language features. The main
objective of these languages is to add semantics to the existing information on
the web. The aims of this paper is to provide a good knowledge of existing
language and understanding of these languages and how could be used.","IEEE Publication Format,
  https://sites.google.com/site/journalofcomputing/"
Understanding Semantic Web and Ontologies: Theory and Applications,"Semantic Web is actually an extension of the current one in that it
represents information more meaningfully for humans and computers alike. It
enables the description of contents and services in machine-readable form, and
enables annotating, discovering, publishing, advertising and composing services
to be automated. It was developed based on Ontology, which is considered as the
backbone of the Semantic Web. In other words, the current Web is transformed
from being machine-readable to machine-understandable. In fact, Ontology is a
key technique with which to annotate semantics and provide a common,
comprehensible foundation for resources on the Semantic Web. Moreover, Ontology
can provide a common vocabulary, a grammar for publishing data, and can supply
a semantic description of data which can be used to preserve the Ontologies and
keep them ready for inference. This paper provides basic concepts of web
services and the Semantic Web, defines the structure and the main applications
of ontology, and provides many relevant terms are explained in order to provide
a basic understanding of ontologies.","IEEE Publication Format,
  https://sites.google.com/site/journalofcomputing/"
GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables,"Finding the structure of a graphical model has been received much attention
in many fields. Recently, it is reported that the non-Gaussianity of data
enables us to identify the structure of a directed acyclic graph without any
prior knowledge on the structure. In this paper, we propose a novel
non-Gaussianity based algorithm for more general type of models; chain graphs.
The algorithm finds an ordering of the disjoint subsets of variables by
iteratively evaluating the independence between the variable subset and the
residuals when the remaining variables are regressed on those. However, its
computational cost grows exponentially according to the number of variables.
Therefore, we further discuss an efficient approximate approach for applying
the algorithm to large sized graphs. We illustrate the algorithm with
artificial and real-world datasets.",N/A
Soft Approximations and uni-int Decision Making,"Notions of core, support and inversion of a soft set have been defined and
studied. Soft approximations are soft sets developed through core and support,
and are used for granulating the soft space. Membership structure of a soft set
has been probed in and many interesting properties presented. The mathematical
apparatus developed so far in this paper yields a detailed analysis of two
works viz. [N. Cagman, S. Enginoglu, Soft set theory and uni-int decision
making, European Jr. of Operational Research (article in press, available
online 12 May 2010)] and [N. Cagman, S. Enginoglu, Soft matrix theory and its
decision making, Computers and Mathematics with Applications 59 (2010) 3308 -
3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a
core-support expression which is computationally far less expansive than
uni-int. This also highlights some shortcomings in Cagman's uni-int method and
thus motivates us to improve the method. We first suggest an improvement in
uni-int method and then present a new conjecture to solve the optimum choice
problem given by Cagman and Enginoglu. Our Example 8.6 presents a case where
the optimum choice is intuitively clear yet both uni-int methods (Cagman's and
our improved one) give wrong answer but the new conjecture solves the problem
correctly.","This paper has been withdrawn by the author due to further expansion
  of this work. Work is also submitted to a peer reviewed journal and is
  expected to be published very soon"
"Reasoning Support for Risk Prediction and Prevention in Independent
  Living","In recent years there has been growing interest in solutions for the delivery
of clinical care for the elderly, due to the large increase in aging
population. Monitoring a patient in his home environment is necessary to ensure
continuity of care in home settings, but, to be useful, this activity must not
be too invasive for patients and a burden for caregivers. We prototyped a
system called SINDI (Secure and INDependent lIving), focused on i) collecting a
limited amount of data about the person and the environment through Wireless
Sensor Networks (WSN), and ii) inferring from these data enough information to
support caregivers in understanding patients' well being and in predicting
possible evolutions of their health. Our hierarchical logic-based model of
health combines data from different sources, sensor data, tests results,
common-sense knowledge and patient's clinical profile at the lower level, and
correlation rules between health conditions across upper levels. The logical
formalization and the reasoning process are based on Answer Set Programming.
The expressive power of this logic programming paradigm makes it possible to
reason about health evolution even when the available information is incomplete
and potentially incoherent, while declarativity simplifies rules specification
by caregivers and allows automatic encoding of knowledge. This paper describes
how these issues have been targeted in the application scenario of the SINDI
system.","36 pages, 5 figures, 10 tables. To appear in Theory and Practice of
  Logic Programming (TPLP)"
Improving Iris Recognition Accuracy By Score Based Fusion Method,"Iris recognition technology, used to identify individuals by photographing
the iris of their eye, has become popular in security applications because of
its ease of use, accuracy, and safety in controlling access to high-security
areas. Fusion of multiple algorithms for biometric verification performance
improvement has received considerable attention. The proposed method combines
the zero-crossing 1 D wavelet Euler number, and genetic algorithm based for
feature extraction. The output from these three algorithms is normalized and
their score are fused to decide whether the user is genuine or imposter. This
new strategies is discussed in this paper, in order to compute a multimodal
combined score.",http://ijict.org/index.php/ijoat/article/view/improving-iris-recognition
Decomposition of the NVALUE constraint,"We study decompositions of the global NVALUE constraint. Our main
contribution is theoretical: we show that there are propagators for global
constraints like NVALUE which decomposition can simulate with the same time
complexity but with a much greater space complexity. This suggests that the
benefit of a global propagator may often not be in saving time but in saving
space. Our other theoretical contribution is to show for the first time that
range consistency can be enforced on NVALUE with the same worst-case time
complexity as bound consistency. Finally, the decompositions we study are
readily encoded as linear inequalities. We are therefore able to use them in
integer linear programs.","To appear in the Proceedings of the 16th International Conference on
  Principles and Practice of Constraint Programming 2010 (CP 2010). An earlier
  version appeared in the Proceedings of the Eighth International Workshop on
  Constraint Modelling and Reformulation, held alongside the 15th International
  Conference on Principles and Practice of Constraint Programming (CP 2009)"
Symmetry within and between solutions,"Symmetry can be used to help solve many problems. For instance, Einstein's
famous 1905 paper (""On the Electrodynamics of Moving Bodies"") uses symmetry to
help derive the laws of special relativity. In artificial intelligence,
symmetry has played an important role in both problem representation and
reasoning. I describe recent work on using symmetry to help solve constraint
satisfaction problems. Symmetries occur within individual solutions of problems
as well as between different solutions of the same problem. Symmetry can also
be applied to the constraints in a problem to give new symmetric constraints.
Reasoning about symmetry can speed up problem solving, and has led to the
discovery of new results in both graph and number theory.","Keynote talk to appear in the Proceedings of the Eleventh Pacific Rim
  International Conference on Artificial Intelligence (PRICAI-10)"
Online Cake Cutting,"We propose an online form of the cake cutting problem. This models situations
where players arrive and depart during the process of dividing a resource. We
show that well known fair division procedures like cut-and-choose and the
Dubins-Spanier moving knife procedure can be adapted to apply to such online
problems. We propose some desirable properties that online cake cutting
procedures might possess like online forms of proportionality and
envy-freeness, and identify which properties are in fact possessed by the
different online cake procedures.","To appear in Proceedings of the Third International Workshop on
  Computational Social Choice (COMSOC-2010)"
Local search for stable marriage problems with ties and incomplete lists,"The stable marriage problem has a wide variety of practical applications,
ranging from matching resident doctors to hospitals, to matching students to
schools, or more generally to any two-sided market. We consider a useful
variation of the stable marriage problem, where the men and women express their
preferences using a preference list with ties over a subset of the members of
the other sex. Matchings are permitted only with people who appear in these
preference lists. In this setting, we study the problem of finding a stable
matching that marries as many people as possible. Stability is an envy-free
notion: no man and woman who are not married to each other would both prefer
each other to their partners or to being single. This problem is NP-hard. We
tackle this problem using local search, exploiting properties of the problem to
reduce the size of the neighborhood and to make local moves efficiently.
Experimental results show that this approach is able to solve large problems,
quickly returning stable matchings of large and often optimal size.","12 pages, Proc. PRICAI 2010 (11th Pacific Rim International
  Conference on Artificial Intelligence), Byoung-Tak Zhang and Mehmet A. Orgun
  eds., Springer LNAI"
"A unified view of Automata-based algorithms for Frequent Episode
  Discovery","Frequent Episode Discovery framework is a popular framework in Temporal Data
Mining with many applications. Over the years many different notions of
frequencies of episodes have been proposed along with different algorithms for
episode discovery. In this paper we present a unified view of all such
frequency counting algorithms. We present a generic algorithm such that all
current algorithms are special cases of it. This unified view allows one to
gain insights into different frequencies and we present quantitative
relationships among different frequencies. Our unified view also helps in
obtaining correctness proofs for various algorithms as we show here. We also
point out how this unified view helps us to consider generalization of the
algorithm so that they can discover episodes with general partial orders.",N/A
Local search for stable marriage problems,"The stable marriage (SM) problem has a wide variety of practical
applications, ranging from matching resident doctors to hospitals, to matching
students to schools, or more generally to any two-sided market. In the
classical formulation, n men and n women express their preferences (via a
strict total order) over the members of the other sex. Solving a SM problem
means finding a stable marriage where stability is an envy-free notion: no man
and woman who are not married to each other would both prefer each other to
their partners or to being single. We consider both the classical stable
marriage problem and one of its useful variations (denoted SMTI) where the men
and women express their preferences in the form of an incomplete preference
list with ties over a subset of the members of the other sex. Matchings are
permitted only with people who appear in these lists, an we try to find a
stable matching that marries as many people as possible. Whilst the SM problem
is polynomial to solve, the SMTI problem is NP-hard. We propose to tackle both
problems via a local search approach, which exploits properties of the problems
to reduce the size of the neighborhood and to make local moves efficiently. We
evaluate empirically our algorithm for SM problems by measuring its runtime
behaviour and its ability to sample the lattice of all possible stable
marriages. We evaluate our algorithm for SMTI problems in terms of both its
runtime behaviour and its ability to find a maximum cardinality stable
marriage.For SM problems, the number of steps of our algorithm grows only as
O(nlog(n)), and that it samples very well the set of all stable marriages. It
is thus a fair and efficient approach to generate stable marriages.Furthermore,
our approach for SMTI problems is able to solve large problems, quickly
returning stable matchings of large and often optimal size despite the
NP-hardness of this problem.","12 pages, Proc. COMSOC 2010 (Third International Workshop on
  Computational Social Choice)"
An svm multiclassifier approach to land cover mapping,"From the advent of the application of satellite imagery to land cover
mapping, one of the growing areas of research interest has been in the area of
image classification. Image classifiers are algorithms used to extract land
cover information from satellite imagery. Most of the initial research has
focussed on the development and application of algorithms to better existing
and emerging classifiers. In this paper, a paradigm shift is proposed whereby a
committee of classifiers is used to determine the final classification output.
Two of the key components of an ensemble system are that there should be
diversity among the classifiers and that there should be a mechanism through
which the results are combined. In this paper, the members of the ensemble
system include: Linear SVM, Gaussian SVM and Quadratic SVM. The final output
was determined through a simple majority vote of the individual classifiers.
From the results obtained it was observed that the final derived map generated
by an ensemble system can potentially improve on the results derived from the
individual classifiers making up the ensemble system. The ensemble system
classification accuracy was, in this case, better than the linear and quadratic
SVM result. It was however less than that of the RBF SVM. Areas for further
research could focus on improving the diversity of the ensemble system used in
this research.","ASPRS 2008 Annual Conference Portland, Oregon. April 28 - May 2, 2008"
A general method for deciding about logically constrained issues,"A general method is given for revising degrees of belief and arriving at
consistent decisions about a system of logically constrained issues. In
contrast to other works about belief revision, here the constraints are assumed
to be fixed. The method has two variants, dual of each other, whose revised
degrees of belief are respectively above and below the original ones. The upper
[resp. lower] revised degrees of belief are uniquely characterized as the
lowest [resp. highest] ones that are invariant by a certain max-min [resp.
min-max] operation determined by the logical constraints. In both variants,
making balance between the revised degree of belief of a proposition and that
of its negation leads to decisions that are ensured to be consistent with the
logical constraints. These decisions are ensured to agree with the majority
criterion as applied to the original degrees of belief whenever this gives a
consistent result. They are also also ensured to satisfy a property of respect
for unanimity about any particular issue, as well as a property of monotonicity
with respect to the original degrees of belief. The application of the method
to certain special domains comes down to well established or increasingly
accepted methods, such as the single-link method of cluster analysis and the
method of paths in preferential voting.","Several substantial improvements have been included. The outline
  structure of the article has also undergone some changes"
Logic-Based Decision Support for Strategic Environmental Assessment,"Strategic Environmental Assessment is a procedure aimed at introducing
systematic assessment of the environmental effects of plans and programs. This
procedure is based on the so-called coaxial matrices that define dependencies
between plan activities (infrastructures, plants, resource extractions,
buildings, etc.) and positive and negative environmental impacts, and
dependencies between these impacts and environmental receptors. Up to now, this
procedure is manually implemented by environmental experts for checking the
environmental effects of a given plan or program, but it is never applied
during the plan/program construction. A decision support system, based on a
clear logic semantics, would be an invaluable tool not only in assessing a
single, already defined plan, but also during the planning process in order to
produce an optimized, environmentally assessed plan and to study possible
alternative scenarios. We propose two logic-based approaches to the problem,
one based on Constraint Logic Programming and one on Probabilistic Logic
Programming that could be, in the future, conveniently merged to exploit the
advantages of both. We test the proposed approaches on a real energy plan and
we discuss their limitations and advantages.","17 pages, 1 figure, 26th Int'l. Conference on Logic Programming
  (ICLP'10)"
Query-driven Procedures for Hybrid MKNF Knowledge Bases,"Hybrid MKNF knowledge bases are one of the most prominent tightly integrated
combinations of open-world ontology languages with closed-world (non-monotonic)
rule paradigms. The definition of Hybrid MKNF is parametric on the description
logic (DL) underlying the ontology language, in the sense that non-monotonic
rules can extend any decidable DL language. Two related semantics have been
defined for Hybrid MKNF: one that is based on the Stable Model Semantics for
logic programs and one on the Well-Founded Semantics (WFS). Under WFS, the
definition of Hybrid MKNF relies on a bottom-up computation that has polynomial
data complexity whenever the DL language is tractable. Here we define a general
query-driven procedure for Hybrid MKNF that is sound with respect to the stable
model-based semantics, and sound and complete with respect to its WFS variant.
This procedure is able to answer a slightly restricted form of conjunctive
queries, and is based on tabled rule evaluation extended with an external
oracle that captures reasoning within the ontology. Such an (abstract) oracle
receives as input a query along with knowledge already derived, and replies
with a (possibly empty) set of atoms, defined in the rules, whose truth would
suffice to prove the initial query. With appropriate assumptions on the
complexity of the abstract oracle, the general procedure maintains the data
complexity of the WFS for Hybrid MKNF knowledge bases.
  To illustrate this approach, we provide a concrete oracle for EL+, a fragment
of the light-weight DL EL++. Such an oracle has practical use, as EL++ is the
language underlying OWL 2 EL, which is part of the W3C recommendations for the
Semantic Web, and is tractable for reasoning tasks such as subsumption. We show
that query-driven Hybrid MKNF preserves polynomial data complexity when using
the EL+ oracle and WFS.","48 pages with 1 figures, submitted to ACM TOCL"
A decidable subclass of finitary programs,"Answer set programming - the most popular problem solving paradigm based on
logic programs - has been recently extended to support uninterpreted function
symbols. All of these approaches have some limitation. In this paper we propose
a class of programs called FP2 that enjoys a different trade-off between
expressiveness and complexity. FP2 programs enjoy the following unique
combination of properties: (i) the ability of expressing predicates with
infinite extensions; (ii) full support for predicates with arbitrary arity;
(iii) decidability of FP2 membership checking; (iv) decidability of skeptical
and credulous stable model reasoning for call-safe queries. Odd cycles are
supported by composing FP2 programs with argument restricted programs.",N/A
Predicting Suicide Attacks: A Fuzzy Soft Set Approach,"This paper models a decision support system to predict the occurance of
suicide attack in a given collection of cities. The system comprises two parts.
First part analyzes and identifies the factors which affect the prediction.
Admitting incomplete information and use of linguistic terms by experts, as two
characteristic features of this peculiar prediction problem we exploit the
Theory of Fuzzy Soft Sets. Hence the Part 2 of the model is an algorithm vz.
FSP which takes the assessment of factors given in Part 1 as its input and
produces a possibility profile of cities likely to receive the accident. The
algorithm is of O(2^n) complexity. It has been illustrated by an example solved
in detail. Simulation results for the algorithm have been presented which give
insight into the strengths and weaknesses of FSP. Three different decision
making measures have been simulated and compared in our discussion.",Submitted manuscript
"A Program-Level Approach to Revising Logic Programs under the Answer Set
  Semantics","An approach to the revision of logic programs under the answer set semantics
is presented. For programs P and Q, the goal is to determine the answer sets
that correspond to the revision of P by Q, denoted P * Q. A fundamental
principle of classical (AGM) revision, and the one that guides the approach
here, is the success postulate. In AGM revision, this stipulates that A is in K
* A. By analogy with the success postulate, for programs P and Q, this means
that the answer sets of Q will in some sense be contained in those of P * Q.
The essential idea is that for P * Q, a three-valued answer set for Q,
consisting of positive and negative literals, is first determined. The positive
literals constitute a regular answer set, while the negated literals make up a
minimal set of naf literals required to produce the answer set from Q. These
literals are propagated to the program P, along with those rules of Q that are
not decided by these literals. The approach differs from work in update logic
programs in two main respects. First, we ensure that the revising logic program
has higher priority, and so we satisfy the success postulate; second, for the
preference implicit in a revision P * Q, the program Q as a whole takes
precedence over P, unlike update logic programs, since answer sets of Q are
propagated to P. We show that a core group of the AGM postulates are satisfied,
as are the postulates that have been proposed for update logic programs.",N/A
An Empirical Study of Borda Manipulation,"We study the problem of coalitional manipulation in elections using the
unweighted Borda rule. We provide empirical evidence of the manipulability of
Borda elections in the form of two new greedy manipulation algorithms based on
intuitions from the bin-packing and multiprocessor scheduling domains. Although
we have not been able to show that these algorithms beat existing methods in
the worst-case, our empirical evaluation shows that they significantly
outperform the existing method and are able to find optimal manipulations in
the vast majority of the randomly generated elections that we tested. These
empirical results provide further evidence that the Borda rule provides little
defense against coalitional manipulation.","To appear in Proceedings of the Third International Workshop on
  Computational Social Choice"
Resource-Optimal Planning For An Autonomous Planetary Vehicle,"Autonomous planetary vehicles, also known as rovers, are small autonomous
vehicles equipped with a variety of sensors used to perform exploration and
experiments on a planet's surface. Rovers work in a partially unknown
environment, with narrow energy/time/movement constraints and, typically, small
computational resources that limit the complexity of on-line planning and
scheduling, thus they represent a great challenge in the field of autonomous
vehicles. Indeed, formal models for such vehicles usually involve hybrid
systems with nonlinear dynamics, which are difficult to handle by most of the
current planning algorithms and tools. Therefore, when offline planning of the
vehicle activities is required, for example for rovers that operate without a
continuous Earth supervision, such planning is often performed on simplified
models that are not completely realistic. In this paper we show how the
UPMurphi model checking based planning tool can be used to generate
resource-optimal plans to control the engine of an autonomous planetary
vehicle, working directly on its hybrid model and taking into account several
safety constraints, thus achieving very accurate results.","15 pages, 4 figures"
"Solving the Resource Constrained Project Scheduling Problem with
  Generalized Precedences by Lazy Clause Generation","The technical report presents a generic exact solution approach for
minimizing the project duration of the resource-constrained project scheduling
problem with generalized precedences (Rcpsp/max). The approach uses lazy clause
generation, i.e., a hybrid of finite domain and Boolean satisfiability solving,
in order to apply nogood learning and conflict-driven search on the solution
generation. Our experiments show the benefit of lazy clause generation for
finding an optimal solutions and proving its optimality in comparison to other
state-of-the-art exact and non-exact methods. The method is highly robust: it
matched or bettered the best known results on all of the 2340 instances we
examined except 3, according to the currently available data on the PSPLib. Of
the 631 open instances in this set it closed 573 and improved the bounds of 51
of the remaining 58 instances.","37 pages, 3 figures, 16 tables"
Experimental Evaluation of Branching Schemes for the CSP,"The search strategy of a CP solver is determined by the variable and value
ordering heuristics it employs and by the branching scheme it follows. Although
the effects of variable and value ordering heuristics on search effort have
been widely studied, the effects of different branching schemes have received
less attention. In this paper we study this effect through an experimental
evaluation that includes standard branching schemes such as 2-way, d-way, and
dichotomic domain splitting, as well as variations of set branching where
branching is performed on sets of values. We also propose and evaluate a
generic approach to set branching where the partition of a domain into sets is
created using the scores assigned to values by a value ordering heuristic, and
a clustering algorithm from machine learning. Experimental results demonstrate
that although exponential differences between branching schemes, as predicted
in theory between 2-way and d-way branching, are not very common, still the
choice of branching scheme can make quite a difference on certain classes of
problems. Set branching methods are very competitive with 2-way branching and
outperform it on some problem classes. A statistical analysis of the results
reveals that our generic clustering-based set branching method is the best
among the methods compared.","To appear in the 3rd workshop on techniques for implementing
  constraint programming systems (TRICS workshop at the 16th CP Conference),
  St. Andrews, Scotland 2010"
"The Challenge of Believability in Video Games: Definitions, Agents
  Models and Imitation Learning","In this paper, we address the problem of creating believable agents (virtual
characters) in video games. We consider only one meaning of believability,
``giving the feeling of being controlled by a player'', and outline the problem
of its evaluation. We present several models for agents in games which can
produce believable behaviours, both from industry and research. For high level
of believability, learning and especially imitation learning seems to be the
way to go. We make a quick overview of different approaches to make video
games' agents learn from players. To conclude we propose a two-step method to
develop new models for believable agents. First we must find the criteria for
believability for our application and define an evaluation method. Then the
model and the learning algorithm can be designed.",N/A
"Automatable Evaluation Method Oriented toward Behaviour Believability
  for Video Games","Classic evaluation methods of believable agents are time-consuming because
they involve many human to judge agents. They are well suited to validate work
on new believable behaviours models. However, during the implementation,
numerous experiments can help to improve agents' believability. We propose a
method which aim at assessing how much an agent's behaviour looks like humans'
behaviours. By representing behaviours with vectors, we can store data computed
for humans and then evaluate as many agents as needed without further need of
humans. We present a test experiment which shows that even a simple evaluation
following our method can reveal differences between quite believable agents and
humans. This method seems promising although, as shown in our experiment,
results' analysis can be difficult.","GAME-ON 2008, France (2008)"
AI 3D Cybug Gaming,"In this short paper I briefly discuss 3D war Game based on artificial
intelligence concepts called AI WAR. Going in to the details, I present the
importance of CAICL language and how this language is used in AI WAR. Moreover
I also present a designed and implemented 3D War Cybug for AI WAR using CAICL
and discus the implemented strategy to defeat its enemies during the game life.","In the proceedings of 9th National Research Conference on Management
  and Computer Sciences, SZABIST Institute of Science and Technology, Pakistan"
Multi-Agent Only-Knowing Revisited,"Levesque introduced the notion of only-knowing to precisely capture the
beliefs of a knowledge base. He also showed how only-knowing can be used to
formalize non-monotonic behavior within a monotonic logic. Despite its appeal,
all attempts to extend only-knowing to the many agent case have undesirable
properties. A belief model by Halpern and Lakemeyer, for instance, appeals to
proof-theoretic constructs in the semantics and needs to axiomatize validity as
part of the logic. It is also not clear how to generalize their ideas to a
first-order case. In this paper, we propose a new account of multi-agent
only-knowing which, for the first time, has a natural possible-world semantics
for a quantified language with equality. We then provide, for the propositional
fragment, a sound and complete axiomatization that faithfully lifts Levesque's
proof theory to the many agent case. We also discuss comparisons to the earlier
approach by Halpern and Lakemeyer.",Appears in Principles of Knowledge Representation and Reasoning 2010
Optimal Bangla Keyboard Layout using Association Rule of Data Mining,"In this paper we present an optimal Bangla Keyboard Layout, which distributes
the load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters, for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First, we analyze the frequencies of data consisting of monograph,
digraph and trigraph, which are derived from data wire-house, and then used
association rule of data mining to distribute the Bangla characters in the
layout. Finally, we propose a Bangla Keyboard Layout. Experimental results on
several keyboard layout shows the effectiveness of the proposed approach with
better performance.","3 Pages, International Conference"
Optimal Bangla Keyboard Layout using Data Mining Technique,"This paper presents an optimal Bangla Keyboard Layout, which distributes the
load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters, for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First, we analyze the frequencies of data consisting of monograph,
digraph and trigraph, which are derived from data wire-house, and then used
association rule of data mining to distribute the Bangla characters in the
layout. Experimental results on several data show the effectiveness of the
proposed approach with better performance.","9 Pages, International Conference"
The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique,"Bangla alphabet has a large number of letters, for this it is complicated to
type faster using Bangla keyboard. The proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Association rule
of data mining to distribute the Bangla characters in the keyboard is used
here. The frequencies of data consisting of monograph, digraph and trigraph are
analyzed, which are derived from data wire-house, and then used association
rule of data mining to distribute the Bangla characters in the layout.
Experimental results on several data show the effectiveness of the proposed
approach with better performance. This paper presents an optimal Bangla
Keyboard Layout, which distributes the load equally on both hands so that
maximizing the ease and minimizing the effort.","10 Pages, International Journal"
General Scaled Support Vector Machines,"Support Vector Machines (SVMs) are popular tools for data mining tasks such
as classification, regression, and density estimation. However, original SVM
(C-SVM) only considers local information of data points on or over the margin.
Therefore, C-SVM loses robustness. To solve this problem, one approach is to
translate (i.e., to move without rotation or change of shape) the hyperplane
according to the distribution of the entire data. But existing work can only be
applied for 1-D case. In this paper, we propose a simple and efficient method
called General Scaled SVM (GS-SVM) to extend the existing approach to
multi-dimensional case. Our method translates the hyperplane according to the
distribution of data projected on the normal vector of the hyperplane. Compared
with C-SVM, GS-SVM has better performance on several data sets.","5 pages, 4 figures"
Measuring Similarity of Graphs and their Nodes by Neighbor Matching,"The problem of measuring similarity of graphs and their nodes is important in
a range of practical problems. There is a number of proposed measures, some of
them being based on iterative calculation of similarity between two graphs and
the principle that two nodes are as similar as their neighbors are. In our
work, we propose one novel method of that sort, with a refined concept of
similarity of two nodes that involves matching of their neighbors. We prove
convergence of the proposed method and show that it has some additional
desirable properties that, to our knowledge, the existing methods lack. We
illustrate the method on two specific problems and empirically compare it to
other methods.",N/A
Steepest Ascent Hill Climbing For A Mathematical Problem,"The paper proposes artificial intelligence technique called hill climbing to
find numerical solutions of Diophantine Equations. Such equations are important
as they have many applications in fields like public key cryptography, integer
factorization, algebraic curves, projective curves and data dependency in super
computers. Importantly, it has been proved that there is no general method to
find solutions of such equations. This paper is an attempt to find numerical
solutions of Diophantine equations using steepest ascent version of Hill
Climbing. The method, which uses tree representation to depict possible
solutions of Diophantine equations, adopts a novel methodology to generate
successors. The heuristic function used help to make the process of finding
solution as a minimization process. The work illustrates the effectiveness of
the proposed methodology using a class of Diophantine equations given by a1. x1
p1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The
experimental results validate that the procedure proposed is successful in
finding solutions of Diophantine Equations with sufficiently large powers and
large number of variables.","8 Pages, 3 Figures, 2 Tables, International Symposium on Advanced
  Engineering and Applied Management 40th Anniversary in Higher Education -
  Informatics & Computer Science, University Politehnica, Timisoara, 4-5
  November, 2010, Hunedoara, ROMANIA"
"Hierarchical Multiclass Decompositions with Application to Authorship
  Determination","This paper is mainly concerned with the question of how to decompose
multiclass classification problems into binary subproblems. We extend known
Jensen-Shannon bounds on the Bayes risk of binary problems to hierarchical
multiclass problems and use these bounds to develop a heuristic procedure for
constructing hierarchical multiclass decomposition for multinomials. We test
our method and compare it to the well known ""all-pairs"" decomposition. Our
tests are performed using a new authorship determination benchmark test of
machine learning authors. The new method consistently outperforms the all-pairs
decomposition when the number of classes is small and breaks even on larger
multiclass problems. Using both methods, the classification accuracy we
achieve, using an SVM over a feature set consisting of both high frequency
single tokens and high frequency token-pairs, appears to be exceptionally high
compared to known results in authorship determination.",N/A
Introduction to the iDian,"The iDian (previously named as the Operation Agent System) is a framework
designed to enable computer users to operate software in natural language.
Distinct from current speech-recognition systems, our solution supports
format-free combinations of orders, and is open to both developers and
customers. We used a multi-layer structure to build the entire framework,
approached rule-based natural language processing, and implemented demos
narrowing down to Windows, text-editing and a few other applications. This
essay will firstly give an overview of the entire system, and then scrutinize
the functions and structure of the system, and finally discuss the prospective
de-velopment, esp. on-line interaction functions.",4 pages
"A Protocol for Self-Synchronized Duty-Cycling in Sensor Networks:
  Generic Implementation in Wiselib","In this work we present a protocol for self-synchronized duty-cycling in
wireless sensor networks with energy harvesting capabilities. The protocol is
implemented in Wiselib, a library of generic algorithms for sensor networks.
Simulations are conducted with the sensor network simulator Shawn. They are
based on the specifications of real hardware known as iSense sensor nodes. The
experimental results show that the proposed mechanism is able to adapt to
changing energy availabilities. Moreover, it is shown that the system is very
robust against packet loss.","Accepted for the proceedings of MSN 2010 (The 6th International
  Conference on Mobile Ad-hoc and Sensor Networks)"
New S-norm and T-norm Operators for Active Learning Method,"Active Learning Method (ALM) is a soft computing method used for modeling and
control based on fuzzy logic. All operators defined for fuzzy sets must serve
as either fuzzy S-norm or fuzzy T-norm. Despite being a powerful modeling
method, ALM does not possess operators which serve as S-norms and T-norms which
deprive it of a profound analytical expression/form. This paper introduces two
new operators based on morphology which satisfy the following conditions:
First, they serve as fuzzy S-norm and T-norm. Second, they satisfy Demorgans
law, so they complement each other perfectly. These operators are investigated
via three viewpoints: Mathematics, Geometry and fuzzy logic.","11 pages, 20 figures, under review of SPRINGER (Fuzzy Optimization
  and Decision Making)"
A Partial Taxonomy of Substitutability and Interchangeability,"Substitutability, interchangeability and related concepts in Constraint
Programming were introduced approximately twenty years ago and have given rise
to considerable subsequent research. We survey this work, classify, and relate
the different concepts, and indicate directions for future work, in particular
with respect to making connections with research into symmetry breaking. This
paper is a condensed version of a larger work in progress.","18 pages, The 10th International Workshop on Symmetry in Constraint
  Satisfaction Problems (SymCon'10)"
Learning under Concept Drift: an Overview,"Concept drift refers to a non stationary learning problem over time. The
training and the application data often mismatch in real life problems. In this
report we present a context of concept drift problem 1. We focus on the issues
relevant to adaptive training set formation. We present the framework and
terminology, and formulate a global picture of concept drift learners design.
We start with formalizing the framework for the concept drifting data in
Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept
drift learners. In Section 3 we overview the principle mechanisms of concept
drift learners. In this chapter we give a general picture of the available
algorithms and categorize them based on their properties. Section 5 discusses
the related research fields and Section 5 groups and presents major concept
drift applications. This report is intended to give a bird's view of concept
drift research field, provide a context of the research and position it within
broad spectrum of research fields and applications.","Technical report, Vilnius University, 2009 techniques, related areas,
  applications"
"A Unifying Probabilistic Perspective for Spectral Dimensionality
  Reduction: Insights and New Models","We introduce a new perspective on spectral dimensionality reduction which
views these methods as Gaussian Markov random fields (GRFs). Our unifying
perspective is based on the maximum entropy principle which is in turn inspired
by maximum variance unfolding. The resulting model, which we call maximum
entropy unfolding (MEU) is a nonlinear generalization of principal component
analysis. We relate the model to Laplacian eigenmaps and isomap. We show that
parameter fitting in the locally linear embedding (LLE) is approximate maximum
likelihood MEU. We introduce a variant of LLE that performs maximum likelihood
exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the
leading spectral approaches on a robot navigation visualization and a human
motion capture data set. Finally the maximum likelihood perspective allows us
to introduce a new approach to dimensionality reduction based on L1
regularization of the Gaussian random field via the graphical lasso.","26 pages,11 figures"
Translation-Invariant Representation for Cumulative Foot Pressure Images,"Human can be distinguished by different limb movements and unique ground
reaction force. Cumulative foot pressure image is a 2-D cumulative ground
reaction force during one gait cycle. Although it contains pressure spatial
distribution information and pressure temporal distribution information, it
suffers from several problems including different shoes and noise, when putting
it into practice as a new biometric for pedestrian identification. In this
paper, we propose a hierarchical translation-invariant representation for
cumulative foot pressure images, inspired by the success of Convolutional deep
belief network for digital classification. Key contribution in our approach is
discriminative hierarchical sparse coding scheme which helps to learn useful
discriminative high-level visual features. Based on the feature representation
of cumulative foot pressure images, we develop a pedestrian recognition system
which is invariant to three different shoes and slight local shape change.
Experiments are conducted on a proposed open dataset that contains more than
2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the
effectiveness of the proposed method and the potential of cumulative foot
pressure images as a biometric.",6 pages
"Qualitative Reasoning about Relative Direction on Adjustable Levels of
  Granularity","An important issue in Qualitative Spatial Reasoning is the representation of
relative direction. In this paper we present simple geometric rules that enable
reasoning about relative direction between oriented points. This framework, the
Oriented Point Algebra OPRA_m, has a scalable granularity m. We develop a
simple algorithm for computing the OPRA_m composition tables and prove its
correctness. Using a composition table, algebraic closure for a set of OPRA
statements is sufficient to solve spatial navigation tasks. And it turns out
that scalable granularity is useful in these navigation tasks.",N/A
A Distributed AI Aided 3D Domino Game,"In the article a turn-based game played on four computers connected via
network is investigated. There are three computers with natural intelligence
and one with artificial intelligence. Game table is seen by each player's own
view point in all players' monitors. Domino pieces are three dimensional. For
distributed systems TCP/IP protocol is used. In order to get 3D image,
Microsoft XNA technology is applied. Domino 101 game is nondeterministic game
that is result of the game depends on the initial random distribution of the
pieces. Number of the distributions is equal to the multiplication of following
combinations: . Moreover, in this game that is played by four people, players
are divided into 2 pairs. Accordingly, we cannot predict how the player uses
the dominoes that is according to the dominoes of his/her partner or according
to his/her own dominoes. The fact that the natural intelligence can be a player
in any level affects the outcome. These reasons make it difficult to develop an
AI. In the article four levels of AI are developed. The AI in the first level
is equivalent to the intelligence of a child who knows the rules of the game
and recognizes the numbers. The AI in this level plays if it has any domino,
suitable to play or says pass. In most of the games which can be played on the
internet, the AI does the same. But the AI in the last level is a master
player, and it can develop itself according to its competitors' levels.",N/A
"Prunnig Algorithm of Generation a Minimal Set of Rule Reducts Based on
  Rough Set Theory","In this paper it is considered rule reduct generation problem, based on Rough
Set Theory. Rule Reduct Generation (RG) and Modified Rule Generation (MRG)
algorithms are well-known. Alternative to these algorithms Pruning Algorithm of
Generation A Minimal Set of Rule Reducts, or briefly Pruning Rule Generation
(PRG) algorithm is developed. PRG algorithm uses tree structured data type. PRG
algorithm is compared with RG and MRG algorithms",N/A
"Reasoning about Cardinal Directions between Extended Objects: The
  Hardness Result","The cardinal direction calculus (CDC) proposed by Goyal and Egenhofer is a
very expressive qualitative calculus for directional information of extended
objects. Early work has shown that consistency checking of complete networks of
basic CDC constraints is tractable while reasoning with the CDC in general is
NP-hard. This paper shows, however, if allowing some constraints unspecified,
then consistency checking of possibly incomplete networks of basic CDC
constraints is already intractable. This draws a sharp boundary between the
tractable and intractable subclasses of the CDC. The result is achieved by a
reduction from the well-known 3-SAT problem.","24 pages, 24 figures"
"Imitation learning of motor primitives and language bootstrapping in
  robots","Imitation learning in robots, also called programing by demonstration, has
made important advances in recent years, allowing humans to teach context
dependant motor skills/tasks to robots. We propose to extend the usual contexts
investigated to also include acoustic linguistic expressions that might denote
a given motor skill, and thus we target joint learning of the motor skills and
their potential acoustic linguistic name. In addition to this, a modification
of a class of existing algorithms within the imitation learning framework is
made so that they can handle the unlabeled demonstration of several tasks/motor
primitives without having to inform the imitator of what task is being
demonstrated or what the number of tasks are, which is a necessity for language
learning, i.e; if one wants to teach naturally an open number of new motor
skills together with their acoustic names. Finally, a mechanism for detecting
whether or not linguistic input is relevant to the task is also proposed, and
our architecture also allows the robot to find the right framing for a given
identified motor primitive. With these additions it becomes possible to build
an imitator that bridges the gap between imitation learning and language
learning by being able to learn linguistic expressions using methods from the
imitation learning community. In this sense the imitator can learn a word by
guessing whether a certain speech pattern present in the context means that a
specific task is to be executed. The imitator is however not assumed to know
that speech is relevant and has to figure this out on its own by looking at the
demonstrations: indeed, the architecture allows the robot to transparently also
learn tasks which should not be triggered by an acoustic word, but for example
by the color or position of an object or a gesture made by someone in the
environment. To demonstrate this ability to find the ...","This paper has been withdrawn by the author due to several issues
  regarding the clarity of presentation"
"Significance of Classification Techniques in Prediction of Learning
  Disabilities","The aim of this study is to show the importance of two classification
techniques, viz. decision tree and clustering, in prediction of learning
disabilities (LD) of school-age children. LDs affect about 10 percent of all
children enrolled in schools. The problems of children with specific learning
disabilities have been a cause of concern to parents and teachers for some
time. Decision trees and clustering are powerful and popular tools used for
classification and prediction in Data mining. Different rules extracted from
the decision tree are used for prediction of learning disabilities. Clustering
is the assignment of a set of observations into subsets, called clusters, which
are useful in finding the different signs and symptoms (attributes) present in
the LD affected child. In this paper, J48 algorithm is used for constructing
the decision tree and K-means algorithm is used for creating the clusters. By
applying these classification techniques, LD in any child can be identified.","10 pages, 3 tables and 2 figures"
"Detecting Ontological Conflicts in Protocols between Semantic Web
  Services","The task of verifying the compatibility between interacting web services has
traditionally been limited to checking the compatibility of the interaction
protocol in terms of message sequences and the type of data being exchanged.
Since web services are developed largely in an uncoordinated way, different
services often use independently developed ontologies for the same domain
instead of adhering to a single ontology as standard. In this work we
investigate the approaches that can be taken by the server to verify the
possibility to reach a state with semantically inconsistent results during the
execution of a protocol with a client, if the client ontology is published.
Often database is used to store the actual data along with the ontologies
instead of storing the actual data as a part of the ontology description. It is
important to observe that at the current state of the database the semantic
conflict state may not be reached even if the verification done by the server
indicates the possibility of reaching a conflict state. A relational algebra
based decision procedure is also developed to incorporate the current state of
the client and the server databases in the overall verification procedure.",N/A
"Gradient Computation In Linear-Chain Conditional Random Fields Using The
  Entropy Message Passing Algorithm","The paper proposes a numerically stable recursive algorithm for the exact
computation of the linear-chain conditional random field gradient. It operates
as a forward algorithm over the log-domain expectation semiring and has the
purpose of enhancing memory efficiency when applied to long observation
sequences. Unlike the traditional algorithm based on the forward-backward
recursions, the memory complexity of our algorithm does not depend on the
sequence length. The experiments on real data show that it can be useful for
the problems which deal with long sequences.","11 pages, 2 tables, 3 figures, 2 algorithms"
Reinforcement Learning Based on Active Learning Method,"In this paper, a new reinforcement learning approach is proposed which is
based on a powerful concept named Active Learning Method (ALM) in modeling. ALM
expresses any multi-input-single-output system as a fuzzy combination of some
single-input-singleoutput systems. The proposed method is an actor-critic
system similar to Generalized Approximate Reasoning based Intelligent Control
(GARIC) structure to adapt the ALM by delayed reinforcement signals. Our system
uses Temporal Difference (TD) learning to model the behavior of useful actions
of a control system. The goodness of an action is modeled on Reward-
Penalty-Plane. IDS planes will be updated according to this plane. It is shown
that the system can learn with a predefined fuzzy system or without it (through
random actions).","5 pages, 11 figures, 1 table"
A New Sufficient Condition for 1-Coverage to Imply Connectivity,"An effective approach for energy conservation in wireless sensor networks is
scheduling sleep intervals for extraneous nodes while the remaining nodes stay
active to provide continuous service. For the sensor network to operate
successfully the active nodes must maintain both sensing coverage and network
connectivity, It proved before if the communication range of nodes is at least
twice the sensing range, complete coverage of a convex area implies
connectivity among the working set of nodes. In this paper we consider a
rectangular region A = a *b, such that R a R b s s {\pounds}, {\pounds}, where
s R is the sensing range of nodes. and put a constraint on minimum allowed
distance between nodes(s). according to this constraint we present a new lower
bound for communication range relative to sensing range of sensors(s 2 + 3 *R)
that complete coverage of considered area implies connectivity among the
working set of nodes; also we present a new distribution method, that satisfy
our constraint.",N/A
"Target tracking in the recommender space: Toward a new recommender
  system based on Kalman filtering","In this paper, we propose a new approach for recommender systems based on
target tracking by Kalman filtering. We assume that users and their seen
resources are vectors in the multidimensional space of the categories of the
resources. Knowing this space, we propose an algorithm based on a Kalman filter
to track users and to predict the best prediction of their future position in
the recommendation space.",N/A
"Should one compute the Temporal Difference fix point or minimize the
  Bellman Residual? The unified oblique projection view","We investigate projection methods, for evaluating a linear approximation of
the value function of a policy in a Markov Decision Process context. We
consider two popular approaches, the one-step Temporal Difference fix-point
computation (TD(0)) and the Bellman Residual (BR) minimization. We describe
examples, where each method outperforms the other. We highlight a simple
relation between the objective function they minimize, and show that while BR
enjoys a performance guarantee, TD(0) does not in general. We then propose a
unified view in terms of oblique projections of the Bellman equation, which
substantially simplifies and extends the characterization of (schoknecht,2002)
and the recent analysis of (Yu & Bertsekas, 2008). Eventually, we describe some
simulations that suggest that if the TD(0) solution is usually slightly better
than the BR solution, its inherent numerical instability makes it very bad in
some cases, and thus worse on average.",N/A
"Distributed Graph Coloring: An Approach Based on the Calling Behavior of
  Japanese Tree Frogs","Graph coloring, also known as vertex coloring, considers the problem of
assigning colors to the nodes of a graph such that adjacent nodes do not share
the same color. The optimization version of the problem concerns the
minimization of the number of used colors. In this paper we deal with the
problem of finding valid colorings of graphs in a distributed way, that is, by
means of an algorithm that only uses local information for deciding the color
of the nodes. Such algorithms prescind from any central control. Due to the
fact that quite a few practical applications require to find colorings in a
distributed way, the interest in distributed algorithms for graph coloring has
been growing during the last decade. As an example consider wireless ad-hoc and
sensor networks, where tasks such as the assignment of frequencies or the
assignment of TDMA slots are strongly related to graph coloring.
  The algorithm proposed in this paper is inspired by the calling behavior of
Japanese tree frogs. Male frogs use their calls to attract females.
Interestingly, groups of males that are located nearby each other desynchronize
their calls. This is because female frogs are only able to correctly localize
the male frogs when their calls are not too close in time. We experimentally
show that our algorithm is very competitive with the current state of the art,
using different sets of problem instances and comparing to one of the most
competitive algorithms from the literature.",N/A
Bayesian Modeling of a Human MMORPG Player,"This paper describes an application of Bayesian programming to the control of
an autonomous avatar in a multiplayer role-playing game (the example is based
on World of Warcraft). We model a particular task, which consists of choosing
what to do and to select which target in a situation where allies and foes are
present. We explain the model in Bayesian programming and show how we could
learn the conditional probabilities from data gathered during human-played
sessions.","30th international workshop on Bayesian Inference and Maximum
  Entropy, Chamonix : France (2010)"
"Reinforcement Learning in Partially Observable Markov Decision Processes
  using Hybrid Probabilistic Logic Programs","We present a probabilistic logic programming framework to reinforcement
learning, by integrating reinforce-ment learning, in POMDP environments, with
normal hybrid probabilistic logic programs with probabilistic answer set
seman-tics, that is capable of representing domain-specific knowledge. We
formally prove the correctness of our approach. We show that the complexity of
finding a policy for a reinforcement learning problem in our approach is
NP-complete. In addition, we show that any reinforcement learning problem can
be encoded as a classical logic program with answer set semantics. We also show
that a reinforcement learning problem can be encoded as a SAT problem. We
present a new high level action description language that allows the factored
representation of POMDP. Moreover, we modify the original model of POMDP so
that it be able to distinguish between knowledge producing actions and actions
that change the environment.",N/A
Multimodal Biometric Systems - Study to Improve Accuracy and Performance,"Biometrics is the science and technology of measuring and analyzing
biological data of human body, extracting a feature set from the acquired data,
and comparing this set against to the template set in the database.
Experimental studies show that Unimodal biometric systems had many
disadvantages regarding performance and accuracy. Multimodal biometric systems
perform better than unimodal biometric systems and are popular even more
complex also. We examine the accuracy and performance of multimodal biometric
authentication systems using state of the art Commercial Off- The-Shelf (COTS)
products. Here we discuss fingerprint and face biometric systems, decision and
fusion techniques used in these systems. We also discuss their advantage over
unimodal biometric systems.","8 pages,5 figures, published in International Journal of Computer
  Science & Engineering Survey (IJCSES) Vol.1, No.2, November 2010"
"A Bayesian Methodology for Estimating Uncertainty of Decisions in
  Safety-Critical Systems","Uncertainty of decisions in safety-critical engineering applications can be
estimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC)
technique of averaging over decision models. The use of decision tree (DT)
models assists experts to interpret causal relations and find factors of the
uncertainty. Bayesian averaging also allows experts to estimate the uncertainty
accurately when a priori information on the favored structure of DTs is
available. Then an expert can select a single DT model, typically the Maximum a
Posteriori model, for interpretation purposes. Unfortunately, a priori
information on favored structure of DTs is not always available. For this
reason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also
suggest a new procedure of selecting a single DT and describe an application
scenario. In our experiments on the Short-Term Conflict Alert data our
technique outperforms the existing Bayesian techniques in predictive accuracy
of the selected single DTs.",N/A
Using ASP with recent extensions for causal explanations,"We examine the practicality for a user of using Answer Set Programming (ASP)
for representing logical formalisms. We choose as an example a formalism aiming
at capturing causal explanations from causal information. We provide an
implementation, showing the naturalness and relative efficiency of this
translation job. We are interested in the ease for writing an ASP program, in
accordance with the claimed ``declarative'' aspect of ASP. Limitations of the
earlier systems (poor data structure and difficulty in reusing pieces of
programs) made that in practice, the ``declarative aspect'' was more
theoretical than practical. We show how recent improvements in working ASP
systems facilitate a lot the translation, even if a few improvements could
still be useful.",N/A
URSA: A System for Uniform Reduction to SAT,"There are a huge number of problems, from various areas, being solved by
reducing them to SAT. However, for many applications, translation into SAT is
performed by specialized, problem-specific tools. In this paper we describe a
new system for uniform solving of a wide class of problems by reducing them to
SAT. The system uses a new specification language URSA that combines imperative
and declarative programming paradigms. The reduction to SAT is defined
precisely by the semantics of the specification language. The domain of the
approach is wide (e.g., many NP-complete problems can be simply specified and
then solved by the system) and there are problems easily solvable by the
proposed system, while they can be hardly solved by using other programming
languages or constraint programming systems. So, the system can be seen not
only as a tool for solving problems by reducing them to SAT, but also as a
general-purpose constraint solving system (for finite domains). In this paper,
we also describe an open-source implementation of the described approach. The
performed experiments suggest that the system is competitive to
state-of-the-art related modelling systems.","39 pages, uses tikz.sty"
Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM,"SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in
the life sciences, with more than 300,000 concepts and relationships, but is
distributed with no associated software tools. In this paper we present MySNOM,
a web-based SNOMED CT browser. MySNOM allows organizations to browse their own
distribution of SNOMED CT under a controlled environment, focuses on navigating
using the structure of SNOMED CT, and has diagramming capabilities.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
"A study on the relation between linguistics-oriented and domain-specific
  semantics","In this paper we dealt with the comparison and linking between lexical
resources with domain knowledge provided by ontologies. It is one of the issues
for the combination of the Semantic Web Ontologies and Text Mining. We
investigated the relations between the linguistics oriented and domain-specific
semantics, by associating the GO biological process concepts to the FrameNet
semantic frames. The result shows the gaps between the linguistics-oriented and
domain-specific semantics on the classification of events and the grouping of
target words. The result provides valuable information for the improvement of
domain ontologies supporting for text mining systems. And also, it will result
in benefits to language understanding technology.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
Process Makna - A Semantic Wiki for Scientific Workflows,"Virtual e-Science infrastructures supporting Web-based scientific workflows
are an example for knowledge-intensive collaborative and weakly-structured
processes where the interaction with the human scientists during process
execution plays a central role. In this paper we propose the lightweight
dynamic user-friendly interaction with humans during execution of scientific
workflows via the low-barrier approach of Semantic Wikis as an intuitive
interface for non-technical scientists. Our Process Makna Semantic Wiki system
is a novel combination of an business process management system adapted for
scientific workflows with a Corporate Semantic Web Wiki user interface
supporting knowledge intensive human interaction tasks during scientific
workflow execution.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
"Use of semantic technologies for the development of a dynamic
  trajectories generator in a Semantic Chemistry eLearning platform","ChemgaPedia is a multimedia, webbased eLearning service platform that
currently contains about 18.000 pages organized in 1.700 chapters covering the
complete bachelor studies in chemistry and related topics of chemistry,
pharmacy, and life sciences. The eLearning encyclopedia contains some 25.000
media objects and the eLearning platform provides services such as virtual and
remote labs for experiments. With up to 350.000 users per month the platform is
the most frequently used scientific educational service in the German spoken
Internet. In this demo we show the benefit of mapping the static eLearning
contents of ChemgaPedia to a Linked Data representation for Semantic Chemistry
which allows for generating dynamic eLearning paths tailored to the semantic
profiles of the users.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
Using Semantic Wikis for Structured Argument in Medical Domain,"This research applies ideas from argumentation theory in the context of
semantic wikis, aiming to provide support for structured-large scale
argumentation between human agents. The implemented prototype is exemplified by
modelling the MMR vaccine controversy.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
Creating a new Ontology: a Modular Approach,Creating a new Ontology: a Modular Approach,"in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
"A semantic approach for the requirement-driven discovery of web services
  in the Life Sciences","Research in the Life Sciences depends on the integration of large,
distributed and heterogeneous data sources and web services. The discovery of
which of these resources are the most appropriate to solve a given task is a
complex research question, since there is a large amount of plausible
candidates and there is little, mostly unstructured, metadata to be able to
decide among them.We contribute a semi-automatic approach,based on semantic
techniques, to assist researchers in the discovery of the most appropriate web
services to full a set of given requirements.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
Scientific Collaborations: principles of WikiBridge Design,"Semantic wikis, wikis enhanced with Semantic Web technologies, are
appropriate systems for community-authored knowledge models. They are
particularly suitable for scientific collaboration. This paper details the
design principles ofWikiBridge, a semantic wiki.","in Adrian Paschke, Albert Burger begin_of_the_skype_highlighting
  end_of_the_skype_highlighting, Andrea Splendiani, M. Scott Marshall, Paolo
  Romano: Proceedings of the 3rd International Workshop on Semantic Web
  Applications and Tools for the Life Sciences, Berlin,Germany, December 8-10,
  2010"
Populous: A tool for populating ontology templates,"We present Populous, a tool for gathering content with which to populate an
ontology. Domain experts need to add content, that is often repetitive in its
form, but without having to tackle the underlying ontological representation.
Populous presents users with a table based form in which columns are
constrained to take values from particular ontologies; the user can select a
concept from an ontology via its meaningful label to give a value for a given
entity attribute. Populated tables are mapped to patterns that can then be used
to automatically generate the ontology's content. Populous's contribution is in
the knowledge gathering stage of ontology development. It separates knowledge
gathering from the conceptualisation and also separates the user from the
standard ontology authoring environments. As a result, Populous can allow
knowledge to be gathered in a straight-forward manner that can then be used to
do mass production of ontology content.","in Adrian Paschke, Albert Burger begin_of_the_skype_highlighting
  end_of_the_skype_highlighting, Andrea Splendiani, M. Scott Marshall, Paolo
  Romano: Proceedings of the 3rd International Workshop on Semantic Web
  Applications and Tools for the Life Sciences, Berlin,Germany, December 8-10,
  2010"
Querying Biomedical Ontologies in Natural Language using Answer Set,"In this work, we develop an intelligent user interface that allows users to
enter biomedical queries in a natural language, and that presents the answers
(possibly with explanations if requested) in a natural language. We develop a
rule layer over biomedical ontologies and databases, and use automated
reasoners to answer queries considering relevant parts of the rule layer.","in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010"
Bisimulations for fuzzy transition systems,"There has been a long history of using fuzzy language equivalence to compare
the behavior of fuzzy systems, but the comparison at this level is too coarse.
Recently, a finer behavioral measure, bisimulation, has been introduced to
fuzzy finite automata. However, the results obtained are applicable only to
finite-state systems. In this paper, we consider bisimulation for general fuzzy
systems which may be infinite-state or infinite-event, by modeling them as
fuzzy transition systems. To help understand and check bisimulation, we
characterize it in three ways by enumerating whole transitions, comparing
individual transitions, and using a monotonic function. In addition, we address
composition operations, subsystems, quotients, and homomorphisms of fuzzy
transition systems and discuss their properties connected with bisimulation.
The results presented here are useful for comparing the behavior of general
fuzzy systems. In particular, this makes it possible to relate an infinite
fuzzy system to a finite one, which is easier to analyze, with the same
behavior.",13 double column pages
Nondeterministic fuzzy automata,"Fuzzy automata have long been accepted as a generalization of
nondeterministic finite automata. A closer examination, however, shows that the
fundamental property---nondeterminism---in nondeterministic finite automata has
not been well embodied in the generalization. In this paper, we introduce
nondeterministic fuzzy automata with or without $\el$-moves and fuzzy languages
recognized by them. Furthermore, we prove that (deterministic) fuzzy automata,
nondeterministic fuzzy automata, and nondeterministic fuzzy automata with
$\el$-moves are all equivalent in the sense that they recognize the same class
of fuzzy languages.","14 pages, 3 figures"
"Experimental Comparison of Representation Methods and Distance Measures
  for Time Series Data","The previous decade has brought a remarkable increase of the interest in
applications that deal with querying and mining of time series data. Many of
the research efforts in this context have focused on introducing new
representation methods for dimensionality reduction or novel similarity
measures for the underlying data. In the vast majority of cases, each
individual work introducing a particular method has made specific claims and,
aside from the occasional theoretical justifications, provided quantitative
experimental observations. However, for the most part, the comparative aspects
of these experiments were too narrowly focused on demonstrating the benefits of
the proposed methods over some of the previously introduced ones. In order to
provide a comprehensive validation, we conducted an extensive experimental
study re-implementing eight different time series representations and nine
similarity measures and their variants, and testing their effectiveness on
thirty-eight time series data sets from a wide variety of application domains.
In this paper, we give an overview of these different techniques and present
our comparative experimental findings regarding their effectiveness. In
addition to providing a unified validation of some of the existing
achievements, our experiments also indicate that, in some cases, certain claims
in the literature may be unduly optimistic.",N/A
"A new Recommender system based on target tracking: a Kalman Filter
  approach","In this paper, we propose a new approach for recommender systems based on
target tracking by Kalman filtering. We assume that users and their seen
resources are vectors in the multidimensional space of the categories of the
resources. Knowing this space, we propose an algorithm based on a Kalman filter
to track users and to predict the best prediction of their future position in
the recommendation space.",N/A
"Dynamic Capitalization and Visualization Strategy in Collaborative
  Knowledge Management System for EI Process","Knowledge is attributed to human whose problem-solving behavior is subjective
and complex. In today's knowledge economy, the need to manage knowledge
produced by a community of actors cannot be overemphasized. This is due to the
fact that actors possess some level of tacit knowledge which is generally
difficult to articulate. Problem-solving requires searching and sharing of
knowledge among a group of actors in a particular context. Knowledge expressed
within the context of a problem resolution must be capitalized for future
reuse. In this paper, an approach that permits dynamic capitalization of
relevant and reliable actors' knowledge in solving decision problem following
Economic Intelligence process is proposed. Knowledge annotation method and
temporal attributes are used for handling the complexity in the communication
among actors and in contextualizing expressed knowledge. A prototype is built
to demonstrate the functionalities of a collaborative Knowledge Management
system based on this approach. It is tested with sample cases and the result
showed that dynamic capitalization leads to knowledge validation hence
increasing reliability of captured knowledge for reuse. The system can be
adapted to various domains",N/A
"Dynamic Knowledge Capitalization through Annotation among Economic
  Intelligence Actors in a Collaborative Environment","The shift from industrial economy to knowledge economy in today's world has
revolutionalized strategic planning in organizations as well as their problem
solving approaches. The point of focus today is knowledge and service
production with more emphasis been laid on knowledge capital. Many
organizations are investing on tools that facilitate knowledge sharing among
their employees and they are as well promoting and encouraging collaboration
among their staff in order to build the organization's knowledge capital with
the ultimate goal of creating a lasting competitive advantage for their
organizations. One of the current leading approaches used for solving
organization's decision problem is the Economic Intelligence (EI) approach
which involves interactions among various actors called EI actors. These actors
collaborate to ensure the overall success of the decision problem solving
process. In the course of the collaboration, the actors express knowledge which
could be capitalized for future reuse. In this paper, we propose in the first
place, an annotation model for knowledge elicitation among EI actors. Because
of the need to build a knowledge capital, we also propose a dynamic knowledge
capitalisation approach for managing knowledge produced by the actors. Finally,
the need to manage the interactions and the interdependencies among
collaborating EI actors, led to our third proposition which constitute an
awareness mechanism for group work management.",N/A
Descriptive-complexity based distance for fuzzy sets,"A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is
based on the descriptive complexity, i.e., the number of bits (on average) that
are needed to describe an element in the symmetric difference of the two sets.
The distance gives the amount of additional information needed to describe any
one of the two sets given the other. We prove its mathematical properties and
perform pattern clustering on data based on this distance.",N/A
"Artificial Intelligence in Reverse Supply Chain Management: The State of
  the Art","Product take-back legislation forces manufacturers to bear the costs of
collection and disposal of products that have reached the end of their useful
lives. In order to reduce these costs, manufacturers can consider reuse,
remanufacturing and/or recycling of components as an alternative to disposal.
The implementation of such alternatives usually requires an appropriate reverse
supply chain management. With the concepts of reverse supply chain are gaining
popularity in practice, the use of artificial intelligence approaches in these
areas is also becoming popular. As a result, the purpose of this paper is to
give an overview of the recent publications concerning the application of
artificial intelligence techniques to reverse supply chain with emphasis on
certain types of product returns.","Proceedings of the Twenty-First Annual Symposium of the Pattern
  Recognition Association of South Africa 22-23 November 2010 Stellenbosch,
  South Africa, pp. 305-310"
"Automatic Estimation of the Exposure to Lateral Collision in Signalized
  Intersections using Video Sensors","Intersections constitute one of the most dangerous elements in road systems.
Traffic signals remain the most common way to control traffic at high-volume
intersections and offer many opportunities to apply intelligent transportation
systems to make traffic more efficient and safe. This paper describes an
automated method to estimate the temporal exposure of road users crossing the
conflict zone to lateral collision with road users originating from a different
approach. This component is part of a larger system relying on video sensors to
provide queue lengths and spatial occupancy that are used for real time traffic
control and monitoring. The method is evaluated on data collected during a real
world experiment.",N/A
Symmetry Breaking with Polynomial Delay,"A conservative class of constraint satisfaction problems CSPs is a class for
which membership is preserved under arbitrary domain reductions. Many
well-known tractable classes of CSPs are conservative. It is well known that
lexleader constraints may significantly reduce the number of solutions by
excluding symmetric solutions of CSPs. We show that adding certain lexleader
constraints to any instance of any conservative class of CSPs still allows us
to find all solutions with a time which is polynomial between successive
solutions. The time is polynomial in the total size of the instance and the
additional lexleader constraints. It is well known that for complete symmetry
breaking one may need an exponential number of lexleader constraints. However,
in practice, the number of additional lexleader constraints is typically
polynomial number in the size of the instance. For polynomially many lexleader
constraints, we may in general not have complete symmetry breaking but
polynomially many lexleader constraints may provide practically useful symmetry
breaking -- and they sometimes exclude super-exponentially many solutions. We
prove that for any instance from a conservative class, the time between finding
successive solutions of the instance with polynomially many additional
lexleader constraints is polynomial even in the size of the instance without
lexleaderconstraints.",N/A
Looking for plausibility,"In the interpretation of experimental data, one is actually looking for
plausible explanations. We look for a measure of plausibility, with which we
can compare different possible explanations, and which can be combined when
there are different sets of data. This is contrasted to the conventional
measure for probabilities as well as to the proposed measure of possibilities.
We define what characteristics this measure of plausibility should have.
  In getting to the conception of this measure, we explore the relation of
plausibility to abductive reasoning, and to Bayesian probabilities. We also
compare with the Dempster-Schaefer theory of evidence, which also has its own
definition for plausibility. Abduction can be associated with biconditionality
in inference rules, and this provides a platform to relate to the
Collins-Michalski theory of plausibility. Finally, using a formalism for wiring
logic onto Hopfield neural networks, we ask if this is relevant in obtaining
this measure.","6 pages, invited paper presented at the International Conference on
  Advanced Computer Science and Information Systems 2010 (ICACSIS2010), Bali,
  Indonesia, 20-22 November 2010"
"SAPFOCS: a metaheuristic based approach to part family formation
  problems in group technology","This article deals with Part family formation problem which is believed to be
moderately complicated to be solved in polynomial time in the vicinity of Group
Technology (GT). In the past literature researchers investigated that the part
family formation techniques are principally based on production flow analysis
(PFA) which usually considers operational requirements, sequences and time.
Part Coding Analysis (PCA) is merely considered in GT which is believed to be
the proficient method to identify the part families. PCA classifies parts by
allotting them to different families based on their resemblances in: (1) design
characteristics such as shape and size, and/or (2) manufacturing
characteristics (machining requirements). A novel approach based on simulated
annealing namely SAPFOCS is adopted in this study to develop effective part
families exploiting the PCA technique. Thereafter Taguchi's orthogonal design
method is employed to solve the critical issues on the subject of parameters
selection for the proposed metaheuristic algorithm. The adopted technique is
therefore tested on 5 different datasets of size 5 {\times} 9 to 27 {\times} 9
and the obtained results are compared with C-Linkage clustering technique. The
experimental results reported that the proposed metaheuristic algorithm is
extremely effective in terms of the quality of the solution obtained and has
outperformed C-Linkage algorithm in most instances.",10 pages; 6 figures; 12 tables
On Elementary Loops of Logic Programs,"Using the notion of an elementary loop, Gebser and Schaub refined the theorem
on loop formulas due to Lin and Zhao by considering loop formulas of elementary
loops only. In this article, we reformulate their definition of an elementary
loop, extend it to disjunctive programs, and study several properties of
elementary loops, including how maximal elementary loops are related to minimal
unfounded sets. The results provide useful insights into the stable model
semantics in terms of elementary loops. For a nondisjunctive program, using a
graph-theoretic characterization of an elementary loop, we show that the
problem of recognizing an elementary loop is tractable. On the other hand, we
show that the corresponding problem is {\sf coNP}-complete for a disjunctive
program. Based on the notion of an elementary loop, we present the class of
Head-Elementary-loop-Free (HEF) programs, which strictly generalizes the class
of Head-Cycle-Free (HCF) programs due to Ben-Eliyahu and Dechter. Like an HCF
program, an HEF program can be turned into an equivalent nondisjunctive program
in polynomial time by shifting head atoms into the body.","36 pages, 2 figures"
"Extending Binary Qualitative Direction Calculi with a Granular Distance
  Concept: Hidden Feature Attachment","In this paper we introduce a method for extending binary qualitative
direction calculi with adjustable granularity like OPRAm or the star calculus
with a granular distance concept. This method is similar to the concept of
extending points with an internal reference direction to get oriented points
which are the basic entities in the OPRAm calculus. Even if the spatial objects
are from a geometrical point of view infinitesimal small points locally
available reference measures are attached. In the case of OPRAm, a reference
direction is attached. The same principle works also with local reference
distances which are called elevations. The principle of attaching references
features to a point is called hidden feature attachment.",N/A
"Learning a Representation of a Believable Virtual Character's
  Environment with an Imitation Algorithm","In video games, virtual characters' decision systems often use a simplified
representation of the world. To increase both their autonomy and believability
we want those characters to be able to learn this representation from human
players. We propose to use a model called growing neural gas to learn by
imitation the topology of the environment. The implementation of the model, the
modifications and the parameters we used are detailed. Then, the quality of the
learned representations and their evolution during the learning are studied
using different measures. Improvements for the growing neural gas to give more
information to the character's model are given in the conclusion.",N/A
Planning with Partial Preference Models,"Current work in planning with preferences assume that the user's preference
models are completely specified and aim to search for a single solution plan.
In many real-world planning scenarios, however, the user probably cannot
provide any information about her desired plans, or in some cases can only
express partial preferences. In such situations, the planner has to present not
only one but a set of plans to the user, with the hope that some of them are
similar to the plan she prefers. We first propose the usage of different
measures to capture quality of plan sets that are suitable for such scenarios:
domain-independent distance measures defined based on plan elements (actions,
states, causal links) if no knowledge of the user's preferences is given, and
the Integrated Convex Preference measure in case the user's partial preference
is provided. We then investigate various heuristic approaches to find set of
plans according to these measures, and present empirical results demonstrating
the promise of our approach.","38 pages, submitted to Artificial Intelligence Journal"
Extracting Features from Ratings: The Role of Factor Models,"Performing effective preference-based data retrieval requires detailed and
preferentially meaningful structurized information about the current user as
well as the items under consideration. A common problem is that representations
of items often only consist of mere technical attributes, which do not resemble
human perception. This is particularly true for integral items such as movies
or songs. It is often claimed that meaningful item features could be extracted
from collaborative rating data, which is becoming available through social
networking services. However, there is only anecdotal evidence supporting this
claim; but if it is true, the extracted information could very valuable for
preference-based data retrieval. In this paper, we propose a methodology to
systematically check this common claim. We performed a preliminary
investigation on a large collection of movie ratings and present initial
evidence.",N/A
"The ""psychological map of the brain"", as a personal information card
  (file), - a project for the student of the 21st century","We suggest a procedure that is relevant both to electronic performance and
human psychology, so that the creative logic and the respect for human nature
appear in a good agreement. The idea is to create an electronic card containing
basic information about a person's psychological behavior in order to make it
possible to quickly decide about the suitability of one for another. This
""psychological electronics"" approach could be tested via student projects.","This is an unusual work, not easy for classification. I beg the
  readers' pardon for the excessive topical originality, but I tried to close
  the gap between the ""accelerating"" specialization causing one to forget the
  true Educational Side/Meaning that still can be found behind the modern
  science and technology. There are a lot of points to be developed, - one more
  disadvantage. 4 pages, 1 figure"
Meaning Negotiation as Inference,"Meaning negotiation (MN) is the general process with which agents reach an
agreement about the meaning of a set of terms. Artificial Intelligence scholars
have dealt with the problem of MN by means of argumentations schemes, beliefs
merging and information fusion operators, and ontology alignment but the
proposed approaches depend upon the number of participants. In this paper, we
give a general model of MN for an arbitrary number of agents, in which each
participant discusses with the others her viewpoint by exhibiting it in an
actual set of constraints on the meaning of the negotiated terms. We call this
presentation of individual viewpoints an angle. The agents do not aim at
forming a common viewpoint but, instead, at agreeing about an acceptable common
angle. We analyze separately the process of MN by two agents (\emph{bilateral}
or \emph{pairwise} MN) and by more than two agents (\emph{multiparty} MN), and
we use game theoretic models to understand how the process develops in both
cases: the models are Bargaining Game for bilateral MN and English Auction for
multiparty MN. We formalize the process of reaching such an agreement by giving
a deduction system that comprises of rules that are consistent and adequate for
representing MN.",N/A
Information-theoretic measures associated with rough set approximations,"Although some information-theoretic measures of uncertainty or granularity
have been proposed in rough set theory, these measures are only dependent on
the underlying partition and the cardinality of the universe, independent of
the lower and upper approximations. It seems somewhat unreasonable since the
basic idea of rough set theory aims at describing vague concepts by the lower
and upper approximations. In this paper, we thus define new
information-theoretic entropy and co-entropy functions associated to the
partition and the approximations to measure the uncertainty and granularity of
an approximation space. After introducing the novel notions of entropy and
co-entropy, we then examine their properties. In particular, we discuss the
relationship of co-entropies between different universes. The theoretical
development is accompanied by illustrative numerical examples.",N/A
An architecture for the evaluation of intelligent systems,"One of the main research areas in Artificial Intelligence is the coding of
agents (programs) which are able to learn by themselves in any situation. This
means that agents must be useful for purposes other than those they were
created for, as, for example, playing chess. In this way we try to get closer
to the pristine goal of Artificial Intelligence. One of the problems to decide
whether an agent is really intelligent or not is the measurement of its
intelligence, since there is currently no way to measure it in a reliable way.
The purpose of this project is to create an interpreter that allows for the
execution of several environments, including those which are generated
randomly, so that an agent (a person or a program) can interact with them. Once
the interaction between the agent and the environment is over, the interpreter
will measure the intelligence of the agent according to the actions, states and
rewards the agent has undergone inside the environment during the test. As a
result we will be able to measure agents' intelligence in any possible
environment, and to make comparisons between several agents, in order to
determine which of them is the most intelligent. In order to perform the tests,
the interpreter must be able to randomly generate environments that are really
useful to measure agents' intelligence, since not any randomly generated
environment will serve that purpose.",112 pages. In Spanish. Final Project Thesis
Intelligent Semantic Web Search Engines: A Brief Survey,"The World Wide Web (WWW) allows the people to share the information (data)
from the large database repositories globally. The amount of information grows
billions of databases. We need to search the information will specialize tools
known generically search engine. There are many of search engines available
today, retrieving meaningful information is difficult. However to overcome this
problem in search engines to retrieve meaningful information intelligently,
semantic web technologies are playing a major role. In this paper we present
survey on the search engine generations and the role of search engines in
intelligent web and semantic search technologies.",N/A
"Online Least Squares Estimation with Self-Normalized Processes: An
  Application to Bandit Problems","The analysis of online least squares estimation is at the heart of many
stochastic sequential decision making problems. We employ tools from the
self-normalized processes to provide a simple and self-contained proof of a
tail bound of a vector-valued martingale. We use the bound to construct a new
tighter confidence sets for the least squares estimate.
  We apply the confidence sets to several online decision problems, such as the
multi-armed and the linearly parametrized bandit problems. The confidence sets
are potentially applicable to other problems such as sleeping bandits,
generalized linear bandits, and other linear control problems.
  We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of
Auer et al. (2002) and show that its regret is with high-probability a problem
dependent constant. In the case of linear bandits (Dani et al., 2008), we
improve the problem dependent bound in the dimension and number of time steps.
Furthermore, as opposed to the previous result, we prove that our bound holds
for small sample sizes, and at the same time the worst case bound is improved
by a logarithmic factor and the constant is improved.","Submitted to the 24th Annual Conference on Learning Theory (COLT
  2011)"
"Hybrid Model for Solving Multi-Objective Problems Using Evolutionary
  Algorithm and Tabu Search","This paper presents a new multi-objective hybrid model that makes cooperation
between the strength of research of neighborhood methods presented by the tabu
search (TS) and the important exploration capacity of evolutionary algorithm.
This model was implemented and tested in benchmark functions (ZDT1, ZDT2, and
ZDT3), using a network of computers.",5 pages
New Worst-Case Upper Bound for #XSAT,"An algorithm running in O(1.1995n) is presented for counting models for exact
satisfiability formulae(#XSAT). This is faster than the previously best
algorithm which runs in O(1.2190n). In order to improve the efficiency of the
algorithm, a new principle, i.e. the common literals principle, is addressed to
simplify formulae. This allows us to eliminate more common literals. In
addition, we firstly inject the resolution principles into solving #XSAT
problem, and therefore this further improves the efficiency of the algorithm.",submitted to AAAI-10
Back and Forth Between Rules and SE-Models (Extended Version),"Rules in logic programming encode information about mutual interdependencies
between literals that is not captured by any of the commonly used semantics.
This information becomes essential as soon as a program needs to be modified or
further manipulated.
  We argue that, in these cases, a program should not be viewed solely as the
set of its models. Instead, it should be viewed and manipulated as the set of
sets of models of each rule inside it. With this in mind, we investigate and
highlight relations between the SE-model semantics and individual rules. We
identify a set of representatives of rule equivalence classes induced by
SE-models, and so pinpoint the exact expressivity of this semantics with
respect to a single rule. We also characterise the class of sets of
SE-interpretations representable by a single rule. Finally, we discuss the
introduction of two notions of equivalence, both stronger than strong
equivalence [1] and weaker than strong update equivalence [2], which seem more
suitable whenever the dependency information found in rules is of interest.",25 pages; extended version of the paper accepted for LPNMR 2011
"Practical inventory routing: A problem definition and an optimization
  method","The global objective of this work is to provide practical optimization
methods to companies involved in inventory routing problems, taking into
account this new type of data. Also, companies are sometimes not able to deal
with changing plans every period and would like to adopt regular structures for
serving customers.",N/A
"Fuzzy Approach to Critical Bus Ranking under Normal and Line Outage
  Contingencies","Identification of critical or weak buses for a given operating condition is
an important task in the load dispatch centre. It has become more vital in view
of the threat of voltage instability leading to voltage collapse. This paper
presents a fuzzy approach for ranking critical buses in a power system under
normal and network contingencies based on Line Flow index and voltage profiles
at load buses. The Line Flow index determines the maximum load that is possible
to be connected to a bus in order to maintain stability before the system
reaches its bifurcation point. Line Flow index (LF index) along with voltage
profiles at the load buses are represented in Fuzzy Set notation. Further they
are evaluated using fuzzy rules to compute Criticality Index. Based on this
index, critical buses are ranked. The bus with highest rank is the weakest bus
as it can withstand a small amount of load before causing voltage collapse. The
proposed method is tested on Five Bus Test System.","12 pages, 7 figures, CCSIT Conference"
"An Agent Based Architecture (Using Planning) for Dynamic and Semantic
  Web Services Composition in an EBXML Context","The process-based semantic composition of Web Services is gaining a
considerable momentum as an approach for the effective integration of
distributed, heterogeneous, and autonomous applications. To compose Web
Services semantically, we need an ontology. There are several ways of inserting
semantics in Web Services. One of them consists of using description languages
like OWL-S. In this paper, we introduce our work which consists in the
proposition of a new model and the use of semantic matching technology for
semantic and dynamic composition of ebXML business processes.","22 pages, 11 figures, 1 table"
"A Wiki for Business Rules in Open Vocabulary, Executable English","The problem of business-IT alignment is of widespread economic concern.
  As one way of addressing the problem, this paper describes an online system
that functions as a kind of Wiki -- one that supports the collaborative writing
and running of business and scientific applications, as rules in open
vocabulary, executable English, using a browser.
  Since the rules are in English, they are indexed by Google and other search
engines. This is useful when looking for rules for a task that one has in mind.
  The design of the system integrates the semantics of data, with a semantics
of an inference method, and also with the meanings of English sentences. As
such, the system has functionality that may be useful for the Rules, Logic,
Proof and Trust requirements of the Semantic Web.
  The system accepts rules, and small numbers of facts, typed or copy-pasted
directly into a browser. One can then run the rules, again using a browser. For
larger amounts of data, the system uses information in the rules to
automatically generate and run SQL over networked databases. From a few highly
declarative rules, the system typically generates SQL that would be too
complicated to write reliably by hand. However, the system can explain its
results in step-by-step hypertexted English, at the business or scientific
level
  As befits a Wiki, shared use of the system is free.",9 pages
Teraflop-scale Incremental Machine Learning,"We propose a long-term memory design for artificial general intelligence
based on Solomonoff's incremental machine learning methods. We use R5RS Scheme
and its standard library with a few omissions as the reference machine. We
introduce a Levin Search variant based on Stochastic Context Free Grammar
together with four synergistic update algorithms that use the same grammar as a
guiding probability distribution of programs. The update algorithms include
adjusting production probabilities, re-using previous solutions, learning
programming idioms and discovery of frequent subprograms. Experiments with two
training sequences demonstrate that our approach to incremental learning is
effective.",N/A
GRASP and path-relinking for Coalition Structure Generation,"In Artificial Intelligence with Coalition Structure Generation (CSG) one
refers to those cooperative complex problems that require to find an optimal
partition, maximising a social welfare, of a set of entities involved in a
system into exhaustive and disjoint coalitions. The solution of the CSG problem
finds applications in many fields such as Machine Learning (covering machines,
clustering), Data Mining (decision tree, discretization), Graph Theory, Natural
Language Processing (aggregation), Semantic Web (service composition), and
Bioinformatics. The problem of finding the optimal coalition structure is
NP-complete. In this paper we present a greedy adaptive search procedure
(GRASP) with path-relinking to efficiently search the space of coalition
structures. Experiments and comparisons to other algorithms prove the validity
of the proposed method in solving this hard combinatorial problem.",N/A
"A Directional Feature with Energy based Offline Signature Verification
  Network","Signature used as a biometric is implemented in various systems as well as
every signature signed by each person is distinct at the same time. So, it is
very important to have a computerized signature verification system. In offline
signature verification system dynamic features are not available obviously, but
one can use a signature as an image and apply image processing techniques to
make an effective offline signature verification system. Author proposes a
intelligent network used directional feature and energy density both as inputs
to the same network and classifies the signature. Neural network is used as a
classifier for this system. The results are compared with both the very basic
energy density method and a simple directional feature method of offline
signature verification system and this proposed new network is found very
effective as compared to the above two methods, specially for less number of
training samples, which can be implemented practically.","10 pages, 6 figures"
Planning Graph Heuristics for Belief Space Search,"Some recent works in conditional planning have proposed reachability
heuristics to improve planner scalability, but many lack a formal description
of the properties of their distance estimates. To place previous work in
context and extend work on heuristics for conditional planning, we provide a
formal basis for distance estimates between belief states. We give a definition
for the distance between belief states that relies on aggregating underlying
state distance measures. We give several techniques to aggregate state
distances and their associated properties. Many existing heuristics exhibit a
subset of the properties, but in order to provide a standardized comparison we
present several generalizations of planning graph heuristics that are used in a
single planner. We compliment our belief state distance estimate framework by
also investigating efficient planning graph data structures that incorporate
BDDs to compute the most effective heuristics.
  We developed two planners to serve as test-beds for our investigation. The
first, CAltAlt, is a conformant regression planner that uses A* search. The
second, POND, is a conditional progression planner that uses AO* search. We
show the relative effectiveness of our heuristic techniques within these
planners. We also compare the performance of these planners with several state
of the art approaches in conditional planning.",N/A
"An Artificial Immune System Model for Multi-Agents Resource Sharing in
  Distributed Environments","Natural Immune system plays a vital role in the survival of the all living
being. It provides a mechanism to defend itself from external predates making
it consistent systems, capable of adapting itself for survival incase of
changes. The human immune system has motivated scientists and engineers for
finding powerful information processing algorithms that has solved complex
engineering tasks. This paper explores one of the various possibilities for
solving problem in a Multiagent scenario wherein multiple robots are deployed
to achieve a goal collectively. The final goal is dependent on the performance
of individual robot and its survival without having to lose its energy beyond a
predetermined threshold value by deploying an evolutionary computational
technique otherwise called the artificial immune system that imitates the
biological immune system.",N/A
SPPAM - Statistical PreProcessing AlgorithM,"Most machine learning tools work with a single table where each row is an
instance and each column is an attribute. Each cell of the table contains an
attribute value for an instance. This representation prevents one important
form of learning, which is, classification based on groups of correlated
records, such as multiple exams of a single patient, internet customer
preferences, weather forecast or prediction of sea conditions for a given day.
To some extent, relational learning methods, such as inductive logic
programming, can capture this correlation through the use of intensional
predicates added to the background knowledge. In this work, we propose SPPAM,
an algorithm that aggregates past observations in one single record. We show
that applying SPPAM to the original correlated data, before the learning task,
can produce classifiers that are better than the ones trained using all
records.","Submited to IJCAI11 conference on January 25, 2011"
"Language, Emotions, and Cultures: Emotional Sapir-Whorf Hypothesis","An emotional version of Sapir-Whorf hypothesis suggests that differences in
language emotionalities influence differences among cultures no less than
conceptual differences. Conceptual contents of languages and cultures to
significant extent are determined by words and their semantic differences;
these could be borrowed among languages and exchanged among cultures. Emotional
differences, as suggested in the paper, are related to grammar and mostly
cannot be borrowed. Conceptual and emotional mechanisms of languages are
considered here along with their functions in the mind and cultural evolution.
A fundamental contradiction in human mind is considered: language evolution
requires reduced emotionality, but ""too low"" emotionality makes language
""irrelevant to life,"" disconnected from sensory-motor experience. Neural
mechanisms of these processes are suggested as well as their mathematical
models: the knowledge instinct, the language instinct, the dual model
connecting language and cognition, dynamic logic, neural modeling fields.
Mathematical results are related to cognitive science, linguistics, and
psychology. Experimental evidence and theoretical arguments are discussed.
Approximate equations for evolution of human minds and cultures are obtained.
Their solutions identify three types of cultures: ""conceptual""-pragmatic
cultures, in which emotionality of language is reduced and differentiation
overtakes synthesis resulting in fast evolution at the price of uncertainty of
values, self doubts, and internal crises; ""traditional-emotional"" cultures
where differentiation lags behind synthesis, resulting in cultural stability at
the price of stagnation; and ""multi-cultural"" societies combining fast cultural
evolution and stability. Unsolved problems and future theoretical and
experimental directions are discussed.","16p, 2 figs"
"Reduced Ordered Binary Decision Diagram with Implied Literals: A New
  knowledge Compilation Approach","Knowledge compilation is an approach to tackle the computational
intractability of general reasoning problems. According to this approach,
knowledge bases are converted off-line into a target compilation language which
is tractable for on-line querying. Reduced ordered binary decision diagram
(ROBDD) is one of the most influential target languages. We generalize ROBDD by
associating some implied literals in each node and the new language is called
reduced ordered binary decision diagram with implied literals (ROBDD-L). Then
we discuss a kind of subsets of ROBDD-L called ROBDD-i with precisely i implied
literals (0 \leq i \leq \infty). In particular, ROBDD-0 is isomorphic to ROBDD;
ROBDD-\infty requires that each node should be associated by the implied
literals as many as possible. We show that ROBDD-i has uniqueness over some
specific variables order, and ROBDD-\infty is the most succinct subset in
ROBDD-L and can meet most of the querying requirements involved in the
knowledge compilation map. Finally, we propose an ROBDD-i compilation algorithm
for any i and a ROBDD-\infty compilation algorithm. Based on them, we implement
a ROBDD-L package called BDDjLu and then get some conclusions from preliminary
experimental results: ROBDD-\infty is obviously smaller than ROBDD for all
benchmarks; ROBDD-\infty is smaller than the d-DNNF the benchmarks whose
compilation results are relatively small; it seems that it is better to
transform ROBDDs-\infty into FBDDs and ROBDDs rather than straight compile the
benchmarks.","18 pages, 13 figures"
"Using Soft Computer Techniques on Smart Devices for Monitoring Chronic
  Diseases: the CHRONIOUS case","CHRONIOUS is an Open, Ubiquitous and Adaptive Chronic Disease Management
Platform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease
(CKD) and Renal Insufficiency. It consists of several modules: an ontology
based literature search engine, a rule based decision support system, remote
sensors interacting with lifestyle interfaces (PDA, monitor touchscreen) and a
machine learning module. All these modules interact each other to allow the
monitoring of two types of chronic diseases and to help clinician in taking
decision for cure purpose. This paper illustrates how some machine learning
algorithms and a rule based decision support system can be used in smart
devices, to monitor chronic patient. We will analyse how a set of machine
learning algorithms can be used in smart devices to alert the clinician in case
of a patient health condition worsening trend.","presented at ""The Third International Conference on eHealth,
  Telemedicine, and Social Medicine (eTELEMED 2011)"""
Decentralized Constraint Satisfaction,"We show that several important resource allocation problems in wireless
networks fit within the common framework of Constraint Satisfaction Problems
(CSPs). Inspired by the requirements of these applications, where variables are
located at distinct network devices that may not be able to communicate but may
interfere, we define natural criteria that a CSP solver must possess in order
to be practical. We term these algorithms decentralized CSP solvers. The best
known CSP solvers were designed for centralized problems and do not meet these
criteria. We introduce a stochastic decentralized CSP solver and prove that it
will find a solution in almost surely finite time, should one exist, also
showing it has many practically desirable properties. We benchmark the
algorithm's performance on a well-studied class of CSPs, random k-SAT,
illustrating that the time the algorithm takes to find a satisfying assignment
is competitive with stochastic centralized solvers on problems with order a
thousand variables despite its decentralized nature. We demonstrate the
solver's practical utility for the problems that motivated its introduction by
using it to find a non-interfering channel allocation for a network formed from
data from downtown Manhattan.",N/A
Finding Shortest Path for Developed Cognitive Map Using Medial Axis,"this paper presents an enhancement of the medial axis algorithm to be used
for finding the optimal shortest path for developed cognitive map. The
cognitive map has been developed, based on the architectural blueprint maps.
The idea for using the medial-axis is to find main path central pixels; each
center pixel represents the center distance between two side boarder pixels.
The need for these pixels in the algorithm comes from the need of building a
network of nodes for the path, where each node represents a turning in the real
world (left, right, critical left, critical right...). The algorithm also
ignores from finding the center pixels paths that are too small for intelligent
robot navigation. The Idea of this algorithm is to find the possible shortest
path between start and end points. The goal of this research is to extract a
simple, robust representation of the shape of the cognitive map together with
the optimal shortest path between start and end points. The intelligent robot
will use this algorithm in order to decrease the time that is needed for
sweeping the targeted building.",9 pages
"Extraction of handwritten areas from colored image of bank checks by an
  hybrid method","One of the first step in the realization of an automatic system of check
recognition is the extraction of the handwritten area. We propose in this paper
an hybrid method to extract these areas. This method is based on digit
recognition by Fourier descriptors and different steps of colored image
processing . It requires the bank recognition of its code which is located in
the check marking band as well as the handwritten color recognition by the
method of difference of histograms. The areas extraction is then carried out by
the use of some mathematical morphology tools.","International Conference on Machine Intelligence (ACIDCA-ICIM),
  Tozeur, Tunisia, November 2005"
Cost Based Satisficing Search Considered Harmful,"Recently, several researchers have found that cost-based satisficing search
with A* often runs into problems. Although some ""work arounds"" have been
proposed to ameliorate the problem, there has not been any concerted effort to
pinpoint its origin. In this paper, we argue that the origins can be traced
back to the wide variance in action costs that is observed in most planning
domains. We show that such cost variance misleads A* search, and that this is
no trifling detail or accidental phenomenon, but a systemic weakness of the
very concept of ""cost-based evaluation functions + systematic search +
combinatorial graphs"". We show that satisficing search with sized-based
evaluation functions is largely immune to this problem.",Longer version of an extended abstract from SOCS 2010
The AllDifferent Constraint with Precedences,"We propose AllDiffPrecedence, a new global constraint that combines together
an AllDifferent constraint with precedence constraints that strictly order
given pairs of variables. We identify a number of applications for this global
constraint including instruction scheduling and symmetry breaking. We give an
efficient propagation algorithm that enforces bounds consistency on this global
constraint. We show how to implement this propagator using a decomposition that
extends the bounds consistency enforcing decomposition proposed for the
AllDifferent constraint. Finally, we prove that enforcing domain consistency on
this global constraint is NP-hard in general.",N/A
"A Goal-Directed Implementation of Query Answering for Hybrid MKNF
  Knowledge Bases","Ontologies and rules are usually loosely coupled in knowledge representation
formalisms. In fact, ontologies use open-world reasoning while the leading
semantics for rules use non-monotonic, closed-world reasoning. One exception is
the tightly-coupled framework of Minimal Knowledge and Negation as Failure
(MKNF), which allows statements about individuals to be jointly derived via
entailment from an ontology and inferences from rules. Nonetheless, the
practical usefulness of MKNF has not always been clear, although recent work
has formalized a general resolution-based method for querying MKNF when rules
are taken to have the well-founded semantics, and the ontology is modeled by a
general oracle. That work leaves open what algorithms should be used to relate
the entailments of the ontology and the inferences of rules. In this paper we
provide such algorithms, and describe the implementation of a query-driven
system, CDF-Rules, for hybrid knowledge bases combining both (non-monotonic)
rules under the well-founded semantics and a (monotonic) ontology, represented
by a CDF Type-1 (ALQ) theory. To appear in Theory and Practice of Logic
Programming (TPLP)",N/A
"BoolVar/PB v1.0, a java library for translating pseudo-Boolean
  constraints into CNF formulae","BoolVar/PB is an open source java library dedicated to the translation of
pseudo-Boolean constraints into CNF formulae. Input constraints can be
categorized with tags. Several encoding schemes are implemented in a way that
each input constraint can be translated using one or several encoders,
according to the related tags. The library can be easily extended by adding new
encoders and / or new output formats.",N/A
On Understanding and Machine Understanding,"In the present paper, we try to propose a self-similar network theory for the
basic understanding. By extending the natural languages to a kind of so called
idealy sufficient language, we can proceed a few steps to the investigation of
the language searching and the language understanding of AI.
  Image understanding, and the familiarity of the brain to the surrounding
environment are also discussed. Group effects are discussed by addressing the
essense of the power of influences, and constructing the influence network of a
society. We also give a discussion of inspirations.","due to some serious errors on page 2,3 and 5"
Phase Transitions in Knowledge Compilation: an Experimental Study,"Phase transitions in many complex combinational problems have been widely
studied in the past decade. In this paper, we investigate phase transitions in
the knowledge compilation empirically, where DFA, OBDD and d-DNNF are chosen as
the target languages to compile random k-SAT instances. We perform intensive
experiments to analyze the sizes of compilation results and draw the following
conclusions: there exists an easy-hard-easy pattern in compilations; the peak
point of sizes in the pattern is only related to the ratio of the number of
clauses to that of variables when k is fixed, regardless of target languages;
most sizes of compilation results increase exponentially with the number of
variables growing, but there also exists a phase transition that separates a
polynomial-increment region from the exponential-increment region; Moreover, we
explain why the phase transition in compilations occurs by analyzing
microstructures of DFAs, and conclude that a kind of solution
interchangeability with more than 2 variables has a sharp transition near the
peak point of the easy-hard-easy pattern, and thus it has a great impact on
sizes of DFAs.",N/A
Automatic Vehicle Checking Agent (VCA),"A definition of intelligence is given in terms of performance that can be
quantitatively measured. In this study, we have presented a conceptual model of
Intelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve
this goal, we have introduced several kinds of agents that exhibit intelligent
features. These are the Management agent, internal agent, External Agent,
Watcher agent and Report agent. Metrics and measurements are suggested for
evaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate
data and test facilities are suggested to facilitate the development of
intelligent systems.","5 pages, 2 figures"
"A Proposed Decision Support System/Expert System for Guiding Fresh
  Students in Selecting a Faculty in Gomal University, Pakistan","This paper presents the design and development of a proposed rule based
Decision Support System that will help students in selecting the best suitable
faculty/major decision while taking admission in Gomal University, Dera Ismail
Khan, Pakistan. The basic idea of our approach is to design a model for testing
and measuring the student capabilities like intelligence, understanding,
comprehension, mathematical concepts plus his/her past academic record plus
his/her intelligence level, and applying the module results to a rule-based
decision support system to determine the compatibility of those capabilities
with the available faculties/majors in Gomal University. The result is shown as
a list of suggested faculties/majors with the student capabilities and
abilities.",I have withdrawn for some changes
Rational Deployment of CSP Heuristics,"Heuristics are crucial tools in decreasing search effort in varied fields of
AI. In order to be effective, a heuristic must be efficient to compute, as well
as provide useful information to the search algorithm. However, some well-known
heuristics which do well in reducing backtracking are so heavy that the gain of
deploying them in a search algorithm might be outweighed by their overhead.
  We propose a rational metareasoning approach to decide when to deploy
heuristics, using CSP backtracking search as a case study. In particular, a
value of information approach is taken to adaptive deployment of solution-count
estimation heuristics for value ordering. Empirical results show that indeed
the proposed mechanism successfully balances the tradeoff between decreasing
backtracking and heuristic computational overhead, resulting in a significant
overall search time reduction.","7 pages, 2 figures, to appear in IJCAI-2011, http://www.ijcai.org/"
"Adding noise to the input of a model trained with a regularized
  objective","Regularization is a well studied problem in the context of neural networks.
It is usually used to improve the generalization performance when the number of
input samples is relatively small or heavily contaminated with noise. The
regularization of a parametric model can be achieved in different manners some
of which are early stopping (Morgan and Bourlard, 1990), weight decay, output
smoothing that are used to avoid overfitting during the training of the
considered model. From a Bayesian point of view, many regularization techniques
correspond to imposing certain prior distributions on model parameters (Krogh
and Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective
function when a restricted type of noise is added to the input of a parametric
function, we derive the higher order terms of the Taylor expansion and analyze
the coefficients of the regularization terms induced by the noisy input. In
particular we study the effect of penalizing the Hessian of the mapping
function with respect to the input in terms of generalization performance. We
also show how we can control independently this coefficient by explicitly
penalizing the Jacobian of the mapping function on corrupted inputs.",N/A
Translation-based Constraint Answer Set Solving,"We solve constraint satisfaction problems through translation to answer set
programming (ASP). Our reformulations have the property that unit-propagation
in the ASP solver achieves well defined local consistency properties like arc,
bound and range consistency. Experiments demonstrate the computational value of
this approach.",Self-archived version for IJCAI'11 Best Paper Track submission
On the evolution of the instance level of DL-lite knowledge bases,"Recent papers address the issue of updating the instance level of knowledge
bases expressed in Description Logic following a model-based approach. One of
the outcomes of these papers is that the result of updating a knowledge base K
is generally not expressible in the Description Logic used to express K. In
this paper we introduce a formula-based approach to this problem, by revisiting
some research work on formula-based updates developed in the '80s, in
particular the WIDTIO (When In Doubt, Throw It Out) approach. We show that our
operator enjoys desirable properties, including that both insertions and
deletions according to such operator can be expressed in the DL used for the
original KB. Also, we present polynomial time algorithms for the evolution of
the instance level knowledge bases expressed in the most expressive Description
Logics of the DL-lite family.",N/A
Learning invariant features through local space contraction,"We present in this paper a novel approach for training deterministic
auto-encoders. We show that by adding a well chosen penalty term to the
classical reconstruction cost function, we can achieve results that equal or
surpass those attained by other regularized auto-encoders as well as denoising
auto-encoders on a range of datasets. This penalty term corresponds to the
Frobenius norm of the Jacobian matrix of the encoder activations with respect
to the input. We show that this penalty term results in a localized space
contraction which in turn yields robust features on the activation layer.
Furthermore, we show how this penalty term is related to both regularized
auto-encoders and denoising encoders and how it can be seen as a link between
deterministic and non-deterministic auto-encoders. We find empirically that
this penalty helps to carve a representation that better captures the local
directions of variation dictated by the data, corresponding to a
lower-dimensional non-linear manifold, while being more invariant to the vast
majority of directions orthogonal to the manifold. Finally, we show that by
using the learned features to initialize a MLP, we achieve state of the art
classification error on a range of datasets, surpassing other methods of
pre-training.",N/A
Algorithms and Complexity Results for Persuasive Argumentation,"The study of arguments as abstract entities and their interaction as
introduced by Dung (Artificial Intelligence 177, 1995) has become one of the
most active research branches within Artificial Intelligence and Reasoning. A
main issue for abstract argumentation systems is the selection of acceptable
sets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.
Logic Comput. 13, 2003), extends Dung's framework. It takes into account the
relative strength of arguments with respect to some ranking representing an
audience: an argument is subjectively accepted if it is accepted with respect
to some audience, it is objectively accepted if it is accepted with respect to
all audiences. Deciding whether an argument is subjectively or objectively
accepted, respectively, are computationally intractable problems. In fact, the
problems remain intractable under structural restrictions that render the main
computational problems for non-value-based argumentation systems tractable. In
this paper we identify nontrivial classes of value-based argumentation systems
for which the acceptance problems are polynomial-time tractable. The classes
are defined by means of structural restrictions in terms of the underlying
graphical structure of the value-based system. Furthermore we show that the
acceptance problems are intractable for two classes of value-based systems that
where conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007).",N/A
"Hybrid Tractable Classes of Binary Quantified Constraint Satisfaction
  Problems","In this paper, we investigate the hybrid tractability of binary Quantified
Constraint Satisfaction Problems (QCSPs). First, a basic tractable class of
binary QCSPs is identified by using the broken-triangle property. In this
class, the variable ordering for the broken-triangle property must be same as
that in the prefix of the QCSP. Second, we break this restriction to allow that
existentially quantified variables can be shifted within or out of their
blocks, and thus identify some novel tractable classes by introducing the
broken-angle property. Finally, we identify a more generalized tractable class,
i.e., the min-of-max extendable class for QCSPs.",N/A
Synthesizing Robust Plans under Incomplete Domain Models,"Most current planners assume complete domain models and focus on generating
correct plans. Unfortunately, domain modeling is a laborious and error-prone
task. While domain experts cannot guarantee completeness, often they are able
to circumscribe the incompleteness of the model by providing annotations as to
which parts of the domain model may be incomplete. In such cases, the goal
should be to generate plans that are robust with respect to any known
incompleteness of the domain. In this paper, we first introduce annotations
expressing the knowledge of the domain incompleteness, and formalize the notion
of plan robustness with respect to an incomplete domain model. We then propose
an approach to compiling the problem of finding robust plans to the conformant
probabilistic planning problem. We present experimental results with
Probabilistic-FF, a state-of-the-art planner, showing the promise of our
approach.",N/A
Splitting and Updating Hybrid Knowledge Bases (Extended Version),"Over the years, nonmonotonic rules have proven to be a very expressive and
useful knowledge representation paradigm. They have recently been used to
complement the expressive power of Description Logics (DLs), leading to the
study of integrative formal frameworks, generally referred to as hybrid
knowledge bases, where both DL axioms and rules can be used to represent
knowledge. The need to use these hybrid knowledge bases in dynamic domains has
called for the development of update operators, which, given the substantially
different way Description Logics and rules are usually updated, has turned out
to be an extremely difficult task.
  In [SL10], a first step towards addressing this problem was taken, and an
update operator for hybrid knowledge bases was proposed. Despite its
significance -- not only for being the first update operator for hybrid
knowledge bases in the literature, but also because it has some applications -
this operator was defined for a restricted class of problems where only the
ABox was allowed to change, which considerably diminished its applicability.
Many applications that use hybrid knowledge bases in dynamic scenarios require
both DL axioms and rules to be updated.
  In this paper, motivated by real world applications, we introduce an update
operator for a large class of hybrid knowledge bases where both the DL
component as well as the rule component are allowed to dynamically change. We
introduce splitting sequences and splitting theorem for hybrid knowledge bases,
use them to define a modular update semantics, investigate its basic
properties, and illustrate its use on a realistic example about cargo imports.",64 pages; extended version of the paper accepted for ICLP 2011
Transition Systems for Model Generators - A Unifying Approach,"A fundamental task for propositional logic is to compute models of
propositional formulas. Programs developed for this task are called
satisfiability solvers. We show that transition systems introduced by
Nieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers
can be adapted for solvers developed for two other propositional formalisms:
logic programming under the answer-set semantics, and the logic PC(ID). We show
that in each case the task of computing models can be seen as ""satisfiability
modulo answer-set programming,"" where the goal is to find a model of a theory
that also is an answer set of a certain program. The unifying perspective we
develop shows, in particular, that solvers CLASP and MINISATID are closely
related despite being developed for different formalisms, one for answer-set
programming and the latter for the logic PC(ID).","30 pages; Accepted for presentation at ICLP 2011 and for publication
  in Theory and Practice of Logic Programming; contains the appendix with
  proofs"
GANC: Greedy Agglomerative Normalized Cut,"This paper describes a graph clustering algorithm that aims to minimize the
normalized cut criterion and has a model order selection procedure. The
performance of the proposed algorithm is comparable to spectral approaches in
terms of minimizing normalized cut. However, unlike spectral approaches, the
proposed algorithm scales to graphs with millions of nodes and edges. The
algorithm consists of three components that are processed sequentially: a
greedy agglomerative hierarchical clustering procedure, model order selection,
and a local refinement.
  For a graph of n nodes and O(n) edges, the computational complexity of the
algorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of
spectral methods. Experiments are performed on real and synthetic networks to
demonstrate the scalability of the proposed approach, the effectiveness of the
model order selection procedure, and the performance of the proposed algorithm
in terms of minimizing the normalized cut metric.","Submitted to Pattern Recognition. 27 pages, 5 figures"
"Machine-Part cell formation through visual decipherable clustering of
  Self Organizing Map","Machine-part cell formation is used in cellular manufacturing in order to
process a large variety, quality, lower work in process levels, reducing
manufacturing lead-time and customer response time while retaining flexibility
for new products. This paper presents a new and novel approach for obtaining
machine cells and part families. In the cellular manufacturing the fundamental
problem is the formation of part families and machine cells. The present paper
deals with the Self Organising Map (SOM) method an unsupervised learning
algorithm in Artificial Intelligence, and has been used as a visually
decipherable clustering tool of machine-part cell formation. The objective of
the paper is to cluster the binary machine-part matrix through visually
decipherable cluster of SOM color-coding and labelling via the SOM map nodes in
such a way that the part families are processed in that machine cells. The
Umatrix, component plane, principal component projection, scatter plot and
histogram of SOM have been reported in the present work for the successful
visualization of the machine-part cell formation. Computational result with the
proposed algorithm on a set of group technology problems available in the
literature is also presented. The proposed SOM approach produced solutions with
a grouping efficacy that is at least as good as any results earlier reported in
the literature and improved the grouping efficacy for 70% of the problems and
found immensely useful to both industry practitioners and researchers.","18 pages,3 table, 4 figures"
Solving Rubik's Cube Using SAT Solvers,"Rubik's Cube is an easily-understood puzzle, which is originally called the
""magic cube"". It is a well-known planning problem, which has been studied for a
long time. Yet many simple properties remain unknown. This paper studies
whether modern SAT solvers are applicable to this puzzle. To our best
knowledge, we are the first to translate Rubik's Cube to a SAT problem. To
reduce the number of variables and clauses needed for the encoding, we replace
a naive approach of 6 Boolean variables to represent each color on each facelet
with a new approach of 3 or 2 Boolean variables. In order to be able to solve
quickly Rubik's Cube, we replace the direct encoding of 18 turns with the layer
encoding of 18-subtype turns based on 6-type turns. To speed up the solving
further, we encode some properties of two-phase algorithm as an additional
constraint, and restrict some move sequences by adding some constraint clauses.
Using only efficient encoding cannot solve this puzzle. For this reason, we
improve the existing SAT solvers, and develop a new SAT solver based on
PrecoSAT, though it is suited only for Rubik's Cube. The new SAT solver
replaces the lookahead solving strategy with an ALO (\emph{at-least-one})
solving strategy, and decomposes the original problem into sub-problems. Each
sub-problem is solved by PrecoSAT. The empirical results demonstrate both our
SAT translation and new solving technique are efficient. Without the efficient
SAT encoding and the new solving technique, Rubik's Cube will not be able to be
solved still by any SAT solver. Using the improved SAT solver, we can find
always a solution of length 20 in a reasonable time. Although our solver is
slower than Kociemba's algorithm using lookup tables, but does not require a
huge lookup table.",13 pages
"The Hidden Web, XML and Semantic Web: A Scientific Data Management
  Perspective","The World Wide Web no longer consists just of HTML pages. Our work sheds
light on a number of trends on the Internet that go beyond simple Web pages.
The hidden Web provides a wealth of data in semi-structured form, accessible
through Web forms and Web services. These services, as well as numerous other
applications on the Web, commonly use XML, the eXtensible Markup Language. XML
has become the lingua franca of the Internet that allows customized markups to
be defined for specific domains. On top of XML, the Semantic Web grows as a
common structured data source. In this work, we first explain each of these
developments in detail. Using real-world examples from scientific domains of
great interest today, we then demonstrate how these new developments can assist
the managing, harvesting, and organization of data on the Web. On the way, we
also illustrate the current research avenues in these domains. We believe that
this effort would help bridge multiple database tracks, thereby attracting
researchers with a view to extend database technology.",EDBT - Tutorial (2011)
A Multi-Purpose Scenario-based Simulator for Smart House Environments,"Developing smart house systems has been a great challenge for researchers and
engineers in this area because of the high cost of implementation and
evaluation process of these systems, while being very time consuming. Testing a
designed smart house before actually building it is considered as an obstacle
towards an efficient smart house project. This is because of the variety of
sensors, home appliances and devices available for a real smart environment. In
this paper, we present the design and implementation of a multi-purpose smart
house simulation system for designing and simulating all aspects of a smart
house environment. This simulator provides the ability to design the house plan
and different virtual sensors and appliances in a two dimensional model of the
virtual house environment. This simulator can connect to any external smart
house remote controlling system, providing evaluation capabilities to their
system much easier than before. It also supports detailed adding of new
emerging sensors and devices to help maintain its compatibility with future
simulation needs. Scenarios can also be defined for testing various possible
combinations of device states; so different criteria and variables can be
simply evaluated without the need of experimenting on a real environment.",N/A
Xapagy: a cognitive architecture for narrative reasoning,"We introduce the Xapagy cognitive architecture: a software system designed to
perform narrative reasoning. The architecture has been designed from scratch to
model and mimic the activities performed by humans when witnessing, reading,
recalling, narrating and talking about stories.",N/A
"Probabilistic Inference from Arbitrary Uncertainty using Mixtures of
  Factorized Generalized Gaussians","This paper presents a general and efficient framework for probabilistic
inference and learning from arbitrary uncertain information. It exploits the
calculation properties of finite mixture models, conjugate families and
factorization. Both the joint probability density of the variables and the
likelihood function of the (objective or subjective) observation are
approximated by a special mixture model, in such a way that any desired
conditional distribution can be directly obtained without numerical
integration. We have developed an extended version of the expectation
maximization (EM) algorithm to estimate the parameters of mixture models from
uncertain training examples (indirect observations). As a consequence, any
piece of exact or uncertain information about both input and output values is
consistently handled in the inference and learning stages. This ability,
extremely useful in certain situations, is not found in most alternative
methods. The proposed framework is formally justified from standard
probabilistic principles and illustrative examples are provided in the fields
of nonparametric pattern classification, nonlinear regression and pattern
completion. Finally, experiments on a real application and comparative results
over standard databases provide empirical evidence of the utility of the method
in a wide range of applications.",N/A
Ontological Crises in Artificial Agents' Value Systems,"Decision-theoretic agents predict and evaluate the results of their actions
using a model, or ontology, of their environment. An agent's goal, or utility
function, may also be specified in terms of the states of, or entities within,
its ontology. If the agent may upgrade or replace its ontology, it faces a
crisis: the agent's original goal may not be well-defined with respect to its
new ontology. This crisis must be resolved before the agent can make plans
towards achieving its goals.
  We discuss in this paper which sorts of agents will undergo ontological
crises and why we may want to create such agents. We present some concrete
examples, and argue that a well-defined procedure for resolving ontological
crises is needed. We point to some possible approaches to solving this problem,
and evaluate these methods on our examples.",N/A
Typical models: minimizing false beliefs,"A knowledge system S describing a part of real world does in general not
contain complete information. Reasoning with incomplete information is prone to
errors since any belief derived from S may be false in the present state of the
world. A false belief may suggest wrong decisions and lead to harmful actions.
So an important goal is to make false beliefs as unlikely as possible. This
work introduces the notions of ""typical atoms"" and ""typical models"", and shows
that reasoning with typical models minimizes the expected number of false
beliefs over all ways of using incomplete information. Various properties of
typical models are studied, in particular, correctness and stability of beliefs
suggested by typical models, and their connection to oblivious reasoning.",N/A
The Ariadne's Clew Algorithm,"We present a new approach to path planning, called the ""Ariadne's clew
algorithm"". It is designed to find paths in high-dimensional continuous spaces
and applies to robots with many degrees of freedom in static, as well as
dynamic environments - ones where obstacles may move. The Ariadne's clew
algorithm comprises two sub-algorithms, called Search and Explore, applied in
an interleaved manner. Explore builds a representation of the accessible space
while Search looks for the target. Both are posed as optimization problems. We
describe a real implementation of the algorithm to plan paths for a six degrees
of freedom arm in a dynamic environment where another six degrees of freedom
arm is used as a moving obstacle. Experimental results show that a path is
found in about one second without any pre-processing.",N/A
Computational Aspects of Reordering Plans,"This article studies the problem of modifying the action ordering of a plan
in order to optimise the plan according to various criteria. One of these
criteria is to make a plan less constrained and the other is to minimize its
parallel execution time. Three candidate definitions are proposed for the first
of these criteria, constituting a sequence of increasing optimality guarantees.
Two of these are based on deordering plans, which means that ordering relations
may only be removed, not added, while the third one uses reordering, where
arbitrary modifications to the ordering are allowed. It is shown that only the
weakest one of the three criteria is tractable to achieve, the other two being
NP-hard and even difficult to approximate. Similarly, optimising the parallel
execution time of a plan is studied both for deordering and reordering of
plans. In the general case, both of these computations are NP-hard. However, it
is shown that optimal deorderings can be computed in polynomial time for a
class of planning languages based on the notions of producers, consumers and
threats, which includes most of the commonly used planning languages. Computing
optimal reorderings can potentially lead to even faster parallel executions,
but this problem remains NP-hard and difficult to approximate even under quite
severe restrictions.",N/A
"The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic
  Inference","It is common to view programs as a combination of logic and control: the
logic part defines what the program must do, the control part -- how to do it.
The Logic Programming paradigm was developed with the intention of separating
the logic from the control. Recently, extensive research has been conducted on
automatic generation of control for logic programs. Only a few of these works
considered the issue of automatic generation of control for improving the
efficiency of logic programs. In this paper we present a novel algorithm for
automatic finding of lowest-cost subgoal orderings. The algorithm works using
the divide-and-conquer strategy. The given set of subgoals is partitioned into
smaller sets, based on co-occurrence of free variables. The subsets are ordered
recursively and merged, yielding a provably optimal order. We experimentally
demonstrate the utility of the algorithm by testing it in several domains, and
discuss the possibilities of its cooperation with other existing methods.",N/A
"The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem","Using an improved backtrack algorithm with sophisticated pruning techniques,
we revise previous observations correlating a high frequency of hard to solve
Hamiltonian Cycle instances with the Gn,m phase transition between
Hamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500
vertices are easily solved. When we artificially restrict the degree sequence
with a bounded maximum degree, although there is some increase in difficulty,
the frequency of hard graphs is still low. When we consider more regular graphs
based on a generalization of knight's tours, we observe frequent instances of
really hard graphs, but on these the average degree is bounded by a constant.
We design a set of graphs with a feature our algorithm is unable to detect and
so are very hard for our algorithm, but in these we can vary the average degree
from O(1) to O(n). We have so far found no class of graphs correlated with the
Gn,m phase transition which asymptotically produces a high frequency of hard
instances.",N/A
"Semantic Similarity in a Taxonomy: An Information-Based Measure and its
  Application to Problems of Ambiguity in Natural Language","This article presents a measure of semantic similarity in an IS-A taxonomy
based on the notion of shared information content. Experimental evaluation
against a benchmark set of human similarity judgments demonstrates that the
measure performs better than the traditional edge-counting approach. The
article presents algorithms that take advantage of taxonomic similarity in
resolving syntactic and semantic ambiguity, along with experimental results
demonstrating their effectiveness.",N/A
A Temporal Description Logic for Reasoning about Actions and Plans,"A class of interval-based temporal languages for uniformly representing and
reasoning about actions and plans is presented. Actions are represented by
describing what is true while the action itself is occurring, and plans are
constructed by temporally relating actions and world states. The temporal
languages are members of the family of Description Logics, which are
characterized by high expressivity combined with good computational properties.
The subsumption problem for a class of temporal Description Logics is
investigated and sound and complete decision procedures are given. The basic
language TL-F is considered first: it is the composition of a temporal logic TL
-- able to express interval temporal networks -- together with the non-temporal
logic F -- a Feature Description Logic. It is proven that subsumption in this
language is an NP-complete problem. Then it is shown how to reason with the
more expressive languages TLU-FU and TL-ALCF. The former adds disjunction both
at the temporal and non-temporal sides of the language, the latter extends the
non-temporal side with set-valued features (i.e., roles) and a propositionally
complete language.",N/A
Adaptive Parallel Iterative Deepening Search,"Many of the artificial intelligence techniques developed to date rely on
heuristic search through large spaces. Unfortunately, the size of these spaces
and the corresponding computational effort reduce the applicability of
otherwise novel and effective algorithms. A number of parallel and distributed
approaches to search have considerably improved the performance of the search
process. Our goal is to develop an architecture that automatically selects
parallel search strategies for optimal performance on a variety of search
problems. In this paper we describe one such architecture realized in the
Eureka system, which combines the benefits of many different approaches to
parallel heuristic search. Through empirical and theoretical analyses we
observe that features of the problem space directly affect the choice of
optimal parallel search strategy. We then employ machine learning techniques to
select the optimal parallel search strategy for a given problem space. When a
new search task is input to the system, Eureka uses features describing the
search space and the chosen architecture to automatically select the
appropriate search strategy. Eureka has been tested on a MIMD parallel
processor, a distributed network of workstations, and a single workstation
using multithreading. Results generated from fifteen puzzle problems, robot arm
motion problems, artificial search spaces, and planning problems indicate that
Eureka outperforms any of the tested strategies used exclusively for all
problem instances and is able to greatly reduce the search time for these
applications.",N/A
Order of Magnitude Comparisons of Distance,"Order of magnitude reasoning - reasoning by rough comparisons of the sizes of
quantities - is often called 'back of the envelope calculation', with the
implication that the calculations are quick though approximate. This paper
exhibits an interesting class of constraint sets in which order of magnitude
reasoning is demonstrably fast. Specifically, we present a polynomial-time
algorithm that can solve a set of constraints of the form 'Points a and b are
much closer together than points c and d.' We prove that this algorithm can be
applied if `much closer together' is interpreted either as referring to an
infinite difference in scale or as referring to a finite difference in scale,
as long as the difference in scale is greater than the number of variables in
the constraint set. We also prove that the first-order theory over such
constraints is decidable.",N/A
AntNet: Distributed Stigmergetic Control for Communications Networks,"This paper introduces AntNet, a novel approach to the adaptive learning of
routing tables in communications networks. AntNet is a distributed, mobile
agents based Monte Carlo system that was inspired by recent work on the ant
colony metaphor for solving optimization problems. AntNet's agents concurrently
explore the network and exchange collected information. The communication among
the agents is indirect and asynchronous, mediated by the network itself. This
form of communication is typical of social insects and is called stigmergy. We
compare our algorithm with six state-of-the-art routing algorithms coming from
the telecommunications and machine learning fields. The algorithms' performance
is evaluated over a set of realistic testbeds. We run many experiments over
real and artificial IP datagram networks with increasing number of nodes and
under several paradigmatic spatial and temporal traffic distributions. Results
are very encouraging. AntNet showed superior performance under all the
experimental conditions with respect to its competitors. We analyze the main
characteristics of the algorithm and try to explain the reasons for its
superiority.",N/A
A Counter Example to Theorems of Cox and Fine,"Cox's well-known theorem justifying the use of probability is shown not to
hold in finite domains. The counterexample also suggests that Cox's assumptions
are insufficient to prove the result even in infinite domains. The same
counterexample is used to disprove a result of Fine on comparative conditional
probability.",N/A
The Automatic Inference of State Invariants in TIM,"As planning is applied to larger and richer domains the effort involved in
constructing domain descriptions increases and becomes a significant burden on
the human application designer. If general planners are to be applied
successfully to large and complex domains it is necessary to provide the domain
designer with some assistance in building correctly encoded domains. One way of
doing this is to provide domain-independent techniques for extracting, from a
domain description, knowledge that is implicit in that description and that can
assist domain designers in debugging domain descriptions. This knowledge can
also be exploited to improve the performance of planners: several researchers
have explored the potential of state invariants in speeding up the performance
of domain-independent planners. In this paper we describe a process by which
state invariants can be extracted from the automatically inferred type
structure of a domain. These techniques are being developed for exploitation by
STAN, a Graphplan based planner that employs state analysis techniques to
enhance its performance.",N/A
Unifying Class-Based Representation Formalisms,"The notion of class is ubiquitous in computer science and is central in many
formalisms for the representation of structured knowledge used both in
knowledge representation and in databases. In this paper we study the basic
issues underlying such representation formalisms and single out both their
common characteristics and their distinguishing features. Such investigation
leads us to propose a unifying framework in which we are able to capture the
fundamental aspects of several representation languages used in different
contexts. The proposed formalism is expressed in the style of description
logics, which have been introduced in knowledge representation as a means to
provide a semantically well-founded basis for the structural aspects of
knowledge representation systems. The description logic considered in this
paper is a subset of first order logic with nice computational characteristics.
It is quite expressive and features a novel combination of constructs that has
not been studied before. The distinguishing constructs are number restrictions,
which generalize existence and functional dependencies, inverse roles, which
allow one to refer to the inverse of a relationship, and possibly cyclic
assertions, which are necessary for capturing real world domains. We are able
to show that it is precisely such combination of constructs that makes our
logic powerful enough to model the essential set of features for defining class
structures that are common to frame systems, object-oriented database
languages, and semantic data models. As a consequence of the established
correspondences, several significant extensions of each of the above formalisms
become available. The high expressiveness of the logic we propose and the need
for capturing the reasoning in different contexts forces us to distinguish
between unrestricted and finite model reasoning. A notable feature of our
proposal is that reasoning in both cases is decidable. We argue that, by virtue
of the high expressive power and of the associated reasoning capabilities on
both unrestricted and finite models, our logic provides a common core for
class-based representation formalisms.",N/A
Complexity of Prioritized Default Logics,"In default reasoning, usually not all possible ways of resolving conflicts
between default rules are acceptable. Criteria expressing acceptable ways of
resolving the conflicts may be hardwired in the inference mechanism, for
example specificity in inheritance reasoning can be handled this way, or they
may be given abstractly as an ordering on the default rules. In this article we
investigate formalizations of the latter approach in Reiter's default logic.
Our goal is to analyze and compare the computational properties of three such
formalizations in terms of their computational complexity: the prioritized
default logics of Baader and Hollunder, and Brewka, and a prioritized default
logic that is based on lexicographic comparison. The analysis locates the
propositional variants of these logics on the second and third levels of the
polynomial hierarchy, and identifies the boundary between tractable and
intractable inference for restricted classes of prioritized default theories.",N/A
Squeaky Wheel Optimization,"We describe a general approach to optimization which we term `Squeaky Wheel'
Optimization (SWO). In SWO, a greedy algorithm is used to construct a solution
which is then analyzed to find the trouble spots, i.e., those elements, that,
if improved, are likely to improve the objective function score. The results of
the analysis are used to generate new priorities that determine the order in
which the greedy algorithm constructs the next solution. This
Construct/Analyze/Prioritize cycle continues until some limit is reached, or an
acceptable solution is found. SWO can be viewed as operating on two search
spaces: solutions and prioritizations. Successive solutions are only indirectly
related, via the re-prioritization that results from analyzing the prior
solution. Similarly, successive prioritizations are generated by constructing
and analyzing solutions. This `coupled search' has some interesting properties,
which we discuss. We report encouraging experimental results on two domains,
scheduling problems that arise in fiber-optic cable manufacturing, and graph
coloring problems. The fact that these domains are very different supports our
claim that SWO is a general technique for optimization.",N/A
Variational Cumulant Expansions for Intractable Distributions,"Intractable distributions present a common difficulty in inference within the
probabilistic knowledge representation framework and variational methods have
recently been popular in providing an approximate solution. In this article, we
describe a perturbational approach in the form of a cumulant expansion which,
to lowest order, recovers the standard Kullback-Leibler variational bound.
Higher-order terms describe corrections on the variational approach without
incurring much further computational cost. The relationship to other
perturbational approaches such as TAP is also elucidated. We demonstrate the
method on a particular class of undirected graphical models, Boltzmann
machines, for which our simulation results confirm improved accuracy and
enhanced stability during learning.",N/A
Efficient Implementation of the Plan Graph in STAN,"STAN is a Graphplan-based planner, so-called because it uses a variety of
STate ANalysis techniques to enhance its performance. STAN competed in the
AIPS-98 planning competition where it compared well with the other competitors
in terms of speed, finding solutions fastest to many of the problems posed.
Although the domain analysis techniques STAN exploits are an important factor
in its overall performance, we believe that the speed at which STAN solved the
competition problems is largely due to the implementation of its plan graph.
The implementation is based on two insights: that many of the graph
construction operations can be implemented as bit-level logical operations on
bit vectors, and that the graph should not be explicitly constructed beyond the
fix point. This paper describes the implementation of STAN's plan graph and
provides experimental results which demonstrate the circumstances under which
advantages can be obtained from using this implementation.",N/A
Cooperation between Top-Down and Bottom-Up Theorem Provers,"Top-down and bottom-up theorem proving approaches each have specific
advantages and disadvantages. Bottom-up provers profit from strong redundancy
control but suffer from the lack of goal-orientation, whereas top-down provers
are goal-oriented but often have weak calculi when their proof lengths are
considered. In order to integrate both approaches, we try to achieve
cooperation between a top-down and a bottom-up prover in two different ways:
The first technique aims at supporting a bottom-up with a top-down prover. A
top-down prover generates subgoal clauses, they are then processed by a
bottom-up prover. The second technique deals with the use of bottom-up
generated lemmas in a top-down prover. We apply our concept to the areas of
model elimination and superposition. We discuss the ability of our techniques
to shorten proofs as well as to reorder the search space in an appropriate
manner. Furthermore, in order to identify subgoal clauses and lemmas which are
actually relevant for the proof task, we develop methods for a relevancy-based
filtering. Experiments with the provers SETHEO and SPASS performed in the
problem library TPTP reveal the high potential of our cooperation approaches.",N/A
Solving Highly Constrained Search Problems with Quantum Computers,"A previously developed quantum search algorithm for solving 1-SAT problems in
a single step is generalized to apply to a range of highly constrained k-SAT
problems. We identify a bound on the number of clauses in satisfiability
problems for which the generalized algorithm can find a solution in a constant
number of steps as the number of variables increases. This performance
contrasts with the linear growth in the number of steps required by the best
classical algorithms, and the exponential number required by classical and
quantum methods that ignore the problem structure. In some cases, the algorithm
can also guarantee that insoluble problems in fact have no solutions, unlike
previously proposed quantum search algorithms.",N/A
"Decision-Theoretic Planning: Structural Assumptions and Computational
  Leverage","Planning under uncertainty is a central problem in the study of automated
sequential decision making, and has been addressed by researchers in many
different fields, including AI planning, decision analysis, operations
research, control theory and economics. While the assumptions and perspectives
adopted in these areas often differ in substantial ways, many planning problems
of interest to researchers in these fields can be modeled as Markov decision
processes (MDPs) and analyzed using the techniques of decision theory. This
paper presents an overview and synthesis of MDP-related methods, showing how
they provide a unifying framework for modeling many classes of planning
problems studied in AI. It also describes structural properties of MDPs that,
when exhibited by particular classes of problems, can be exploited in the
construction of optimal or approximately optimal policies or plans. Planning
problems commonly possess structure in the reward and value functions used to
describe performance criteria, in the functions used to describe state
transitions and observations, and in the relationships among features used to
describe states, actions, rewards, and observations. Specialized
representations, and algorithms employing these representations, can achieve
computational leverage by exploiting these various forms of structure. Certain
AI techniques -- in particular those based on the use of structured,
intensional representations -- can be viewed in this way. This paper surveys
several types of representations for both classical and decision-theoretic
planning problems, and planning algorithms that exploit these representations
in a number of different ways to ease the computational burden of constructing
policies or plans. It focuses primarily on abstraction, aggregation and
decomposition techniques based on AI-style representations.",N/A
Probabilistic Deduction with Conditional Constraints over Basic Events,"We study the problem of probabilistic deduction with conditional constraints
over basic events. We show that globally complete probabilistic deduction with
conditional constraints over basic events is NP-hard. We then concentrate on
the special case of probabilistic deduction in conditional constraint trees. We
elaborate very efficient techniques for globally complete probabilistic
deduction. In detail, for conditional constraint trees with point
probabilities, we present a local approach to globally complete probabilistic
deduction, which runs in linear time in the size of the conditional constraint
trees. For conditional constraint trees with interval probabilities, we show
that globally complete probabilistic deduction can be done in a global approach
by solving nonlinear programs. We show how these nonlinear programs can be
transformed into equivalent linear programs, which are solvable in polynomial
time in the size of the conditional constraint trees.",N/A
Variational Probabilistic Inference and the QMR-DT Network,"We describe a variational approximation method for efficient inference in
large-scale probabilistic models. Variational methods are deterministic
procedures that provide approximations to marginal and conditional
probabilities of interest. They provide alternatives to approximate inference
methods based on stochastic sampling or search. We describe a variational
approach to the problem of diagnostic inference in the `Quick Medical
Reference' (QMR) network. The QMR network is a large-scale probabilistic
graphical model built on statistical and expert knowledge. Exact probabilistic
inference is infeasible in this model for all but a small set of cases. We
evaluate our variational inference algorithm on a large set of diagnostic test
cases, comparing the algorithm to a state-of-the-art stochastic sampling
method.",N/A
Extensible Knowledge Representation: the Case of Description Reasoners,"This paper offers an approach to extensible knowledge representation and
reasoning for a family of formalisms known as Description Logics. The approach
is based on the notion of adding new concept constructors, and includes a
heuristic methodology for specifying the desired extensions, as well as a
modularized software architecture that supports implementing extensions. The
architecture detailed here falls in the normalize-compared paradigm, and
supports both intentional reasoning (subsumption) involving concepts, and
extensional reasoning involving individuals after incremental updates to the
knowledge base. The resulting approach can be used to extend the reasoner with
specialized notions that are motivated by specific problems or application
areas, such as reasoning about dates, plans, etc. In addition, it provides an
opportunity to implement constructors that are not currently yet sufficiently
well understood theoretically, but are needed in practice. Also, for
constructors that are provably hard to reason with (e.g., ones whose presence
would lead to undecidability), it allows the implementation of incomplete
reasoners where the incompleteness is tailored to be acceptable for the
application at hand.",N/A
Constructing Conditional Plans by a Theorem-Prover,"The research on conditional planning rejects the assumptions that there is no
uncertainty or incompleteness of knowledge with respect to the state and
changes of the system the plans operate on. Without these assumptions the
sequences of operations that achieve the goals depend on the initial state and
the outcomes of nondeterministic changes in the system. This setting raises the
questions of how to represent the plans and how to perform plan search. The
answers are quite different from those in the simpler classical framework. In
this paper, we approach conditional planning from a new viewpoint that is
motivated by the use of satisfiability algorithms in classical planning.
Translating conditional planning to formulae in the propositional logic is not
feasible because of inherent computational limitations. Instead, we translate
conditional planning to quantified Boolean formulae. We discuss three
formalizations of conditional planning as quantified Boolean formulae, and
present experimental results obtained with a theorem-prover.",N/A
Issues in Stacked Generalization,"Stacked generalization is a general method of using a high-level model to
combine lower-level models to achieve greater predictive accuracy. In this
paper we address two crucial issues which have been considered to be a `black
art' in classification tasks ever since the introduction of stacked
generalization in 1992 by Wolpert: the type of generalizer that is suitable to
derive the higher-level model, and the kind of attributes that should be used
as its input. We find that best results are obtained when the higher-level
model combines the confidence (and not just the predictions) of the lower-level
ones. We demonstrate the effectiveness of stacked generalization for combining
three different types of learning algorithms for classification tasks. We also
compare the performance of stacked generalization with majority vote and
published results of arcing and bagging.",N/A
Ontology Alignment at the Instance and Schema Level,"We present PARIS, an approach for the automatic alignment of ontologies.
PARIS aligns not only instances, but also relations and classes. Alignments at
the instance-level cross-fertilize with alignments at the schema-level.
Thereby, our system provides a truly holistic solution to the problem of
ontology alignment. The heart of the approach is probabilistic. This allows
PARIS to run without any parameter tuning. We demonstrate the efficiency of the
algorithm and its precision through extensive experiments. In particular, we
obtain a precision of around 90% in experiments with two of the world's largest
ontologies.",Technical Report at INRIA RT-0408
Complexity of and Algorithms for Borda Manipulation,"We prove that it is NP-hard for a coalition of two manipulators to compute
how to manipulate the Borda voting rule. This resolves one of the last open
problems in the computational complexity of manipulating common voting rules.
Because of this NP-hardness, we treat computing a manipulation as an
approximation problem where we try to minimize the number of manipulators.
Based on ideas from bin packing and multiprocessor scheduling, we propose two
new approximation methods to compute manipulations of the Borda rule.
Experiments show that these methods significantly outperform the previous best
known %existing approximation method. We are able to find optimal manipulations
in almost all the randomly generated elections tested. Our results suggest
that, whilst computing a manipulation of the Borda rule by a coalition is
NP-hard, computational complexity may provide only a weak barrier against
manipulation in practice.",N/A
"Reasoning on Interval and Point-based Disjunctive Metric Constraints in
  Temporal Contexts","We introduce a temporal model for reasoning on disjunctive metric constraints
on intervals and time points in temporal contexts. This temporal model is
composed of a labeled temporal algebra and its reasoning algorithms. The
labeled temporal algebra defines labeled disjunctive metric point-based
constraints, where each disjunct in each input disjunctive constraint is
univocally associated to a label. Reasoning algorithms manage labeled
constraints, associated label lists, and sets of mutually inconsistent
disjuncts. These algorithms guarantee consistency and obtain a minimal network.
Additionally, constraints can be organized in a hierarchy of alternative
temporal contexts. Therefore, we can reason on context-dependent disjunctive
metric constraints on intervals and points. Moreover, the model is able to
represent non-binary constraints, such that logical dependencies on disjuncts
in constraints can be handled. The computational cost of reasoning algorithms
is exponential in accordance with the underlying problem complexity, although
some improvements are proposed.",N/A
Overcoming Misleads In Logic Programs by Redefining Negation,"Negation as failure and incomplete information in logic programs have been
studied by many researchers In order to explains HOW a negated conclusion was
reached, we introduce and proof a different way for negating facts to
overcoming misleads in logic programs. Negating facts can be achieved by asking
the user for constants that do not appear elsewhere in the knowledge base.","8 pages, 1 figure"
"Proposal of Pattern Recognition as a necessary and sufficient Principle
  to Cognitive Science","Despite the prevalence of the Computational Theory of Mind and the
Connectionist Model, the establishing of the key principles of the Cognitive
Science are still controversy and inconclusive. This paper proposes the concept
of Pattern Recognition as Necessary and Sufficient Principle for a general
cognitive science modeling, in a very ambitious scientific proposal. A formal
physical definition of the pattern recognition concept is also proposed to
solve many key conceptual gaps on the field.",N/A
The Good Old Davis-Putnam Procedure Helps Counting Models,"As was shown recently, many important AI problems require counting the number
of models of propositional formulas. The problem of counting models of such
formulas is, according to present knowledge, computationally intractable in a
worst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP,
that computes the exact number of models of a propositional CNF or DNF formula
F. Let m and n be the number of clauses and variables of F, respectively, and
let p denote the probability that a literal l of F occurs in a clause C of F,
then the average running time of CDP is shown to be O(nm^d), where
d=-1/log(1-p). The practical performance of CDP has been estimated in a series
of experiments on a wide variety of CNF formulas.",N/A
Identifying Mislabeled Training Data,"This paper presents a new approach to identifying and eliminating mislabeled
training instances for supervised learning. The goal of this approach is to
improve classification accuracies produced by learning algorithms by improving
the quality of the training data. Our approach uses a set of learning
algorithms to create classifiers that serve as noise filters for the training
data. We evaluate single algorithm, majority vote and consensus filters on five
datasets that are prone to labeling errors. Our experiments illustrate that
filtering significantly improves classification accuracy for noise levels up to
30 percent. An analytical and empirical evaluation of the precision of our
approach shows that consensus filters are conservative at throwing away good
data at the expense of retaining bad data and that majority filters are better
at detecting bad data at the expense of throwing away good data. This suggests
that for situations in which there is a paucity of data, consensus filters are
preferable, whereas majority vote filters are preferable for situations with an
abundance of data.",N/A
Committee-Based Sample Selection for Probabilistic Classifiers,"In many real-world learning tasks, it is expensive to acquire a sufficient
number of labeled examples for training. This paper investigates methods for
reducing annotation cost by `sample selection'. In this approach, during
training the learning program examines many unlabeled examples and selects for
labeling only those that are most informative at each stage. This avoids
redundantly labeling examples that contribute little new information. Our work
follows on previous research on Query By Committee, extending the
committee-based paradigm to the context of probabilistic classification. We
describe a family of empirical methods for committee-based sample selection in
probabilistic classification models, which evaluate the informativeness of an
example by measuring the degree of disagreement between several model variants.
These variants (the committee) are drawn randomly from a probability
distribution conditioned by the training set labeled so far. The method was
applied to the real-world natural language processing task of stochastic
part-of-speech tagging. We find that all variants of the method achieve a
significant reduction in annotation cost, although their computational
efficiency differs. In particular, the simplest variant, a two member committee
with no parameters to tune, gives excellent results. We also show that sample
selection yields a significant reduction in the size of the model used by the
tagger.",N/A
Reasoning about Minimal Belief and Negation as Failure,"We investigate the problem of reasoning in the propositional fragment of
MBNF, the logic of minimal belief and negation as failure introduced by
Lifschitz, which can be considered as a unifying framework for several
nonmonotonic formalisms, including default logic, autoepistemic logic,
circumscription, epistemic queries, and logic programming. We characterize the
complexity and provide algorithms for reasoning in propositional MBNF. In
particular, we show that entailment in propositional MBNF lies at the third
level of the polynomial hierarchy, hence it is harder than reasoning in all the
above mentioned propositional formalisms for nonmonotonic reasoning. We also
prove the exact correspondence between negation as failure in MBNF and negative
introspection in Moore's autoepistemic logic.",N/A
Randomized Algorithms for the Loop Cutset Problem,"We show how to find a minimum weight loop cutset in a Bayesian network with
high probability. Finding such a loop cutset is the first step in the method of
conditioning for inference. Our randomized algorithm for finding a loop cutset
outputs a minimum loop cutset after O(c 6^k kn) steps with probability at least
1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is
the minimal size of a minimum weight loop cutset, and n is the number of
vertices. We also show empirically that a variant of this algorithm often finds
a loop cutset that is closer to the minimum weight loop cutset than the ones
found by the best deterministic algorithms known.",N/A
"OBDD-based Universal Planning for Synchronized Agents in
  Non-Deterministic Domains","Recently model checking representation and search techniques were shown to be
efficiently applicable to planning, in particular to non-deterministic
planning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDs)
to encode a planning domain as a non-deterministic finite automaton and then
apply fast algorithms from model checking to search for a solution. OBDDs can
effectively scale and can provide universal plans for complex planning domains.
We are particularly interested in addressing the complexities arising in
non-deterministic, multi-agent domains. In this article, we present UMOP, a new
universal OBDD-based planning framework for non-deterministic, multi-agent
domains. We introduce a new planning domain description language, NADL, to
specify non-deterministic, multi-agent domains. The language contributes the
explicit definition of controllable agents and uncontrollable environment
agents. We describe the syntax and semantics of NADL and show how to build an
efficient OBDD-based representation of an NADL description. The UMOP planning
system uses NADL and different OBDD-based universal planning algorithms. It
includes the previously developed strong and strong cyclic planning algorithms.
In addition, we introduce our new optimistic planning algorithm that relaxes
optimality guarantees and generates plausible universal plans in some domains
where no strong nor strong cyclic solution exists. We present empirical results
applying UMOP to domains ranging from deterministic and single-agent with no
environment actions to non-deterministic and multi-agent with complex
environment actions. UMOP is shown to be a rich and efficient planning system.",N/A
"Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP
  Search Techniques in Graphplan","This paper reviews the connections between Graphplan's planning-graph and the
dynamic constraint satisfaction problem and motivates the need for adapting CSP
search techniques to the Graphplan algorithm. It then describes how explanation
based learning, dependency directed backtracking, dynamic variable ordering,
forward checking, sticky values and random-restart search strategies can be
adapted to Graphplan. Empirical results are provided to demonstrate that these
augmentations improve Graphplan's performance significantly (up to 1000x
speedups) on several benchmark problems. Special attention is paid to the
explanation-based learning and dependency directed backtracking techniques as
they are empirically found to be most useful in improving the performance of
Graphplan.",N/A
Space Efficiency of Propositional Knowledge Representation Formalisms,"We investigate the space efficiency of a Propositional Knowledge
Representation (PKR) formalism. Intuitively, the space efficiency of a
formalism F in representing a certain piece of knowledge A, is the size of the
shortest formula of F that represents A. In this paper we assume that knowledge
is either a set of propositional interpretations (models) or a set of
propositional formulae (theorems). We provide a formal way of talking about the
relative ability of PKR formalisms to compactly represent a set of models or a
set of theorems. We introduce two new compactness measures, the corresponding
classes, and show that the relative space efficiency of a PKR formalism in
representing models/theorems is directly related to such classes. In
particular, we consider formalisms for nonmonotonic reasoning, such as
circumscription and default logic, as well as belief revision operators and the
stable model semantics for logic programs with negation. One interesting result
is that formalisms with the same time complexity do not necessarily belong to
the same space efficiency class.",N/A
"Value-Function Approximations for Partially Observable Markov Decision
  Processes","Partially observable Markov decision processes (POMDPs) provide an elegant
mathematical framework for modeling complex decision and planning problems in
stochastic domains in which states of the system are observable only
indirectly, via a set of imperfect or noisy observations. The modeling
advantage of POMDPs, however, comes at a price -- exact methods for solving
them are computationally very expensive and thus applicable in practice only to
very simple problems. We focus on efficient approximation (heuristic) methods
that attempt to alleviate the computational problem and trade off accuracy for
speed. We have two objectives here. First, we survey various approximation
methods, analyze their properties and relations and provide some new insights
into their differences. Second, we present a number of new approximation
methods and novel refinements of existing techniques. The theoretical results
are supported by experiments on a problem from the agent navigation domain.",N/A
"On Deducing Conditional Independence from d-Separation in Causal Graphs
  with Feedback (Research Note)","Pearl and Dechter (1996) claimed that the d-separation criterion for
conditional independence in acyclic causal networks also applies to networks of
discrete variables that have feedback cycles, provided that the variables of
the system are uniquely determined by the random disturbances. I show by
example that this is not true in general. Some condition stronger than
uniqueness is needed, such as the existence of a causal dynamics guaranteed to
lead to the unique solution.",N/A
What's in an Attribute? Consequences for the Least Common Subsumer,"Functional relationships between objects, called `attributes', are of
considerable importance in knowledge representation languages, including
Description Logics (DLs). A study of the literature indicates that papers have
made, often implicitly, different assumptions about the nature of attributes:
whether they are always required to have a value, or whether they can be
partial functions. The work presented here is the first explicit study of this
difference for subclasses of the CLASSIC DL, involving the same-as concept
constructor. It is shown that although determining subsumption between concept
descriptions has the same complexity (though requiring different algorithms),
the story is different in the case of determining the least common subsumer
(lcs). For attributes interpreted as partial functions, the lcs exists and can
be computed relatively easily; even in this case our results correct and extend
three previous papers about the lcs of DLs. In the case where attributes must
have a value, the lcs may not exist, and even if it exists it may be of
exponential size. Interestingly, it is possible to decide in polynomial time if
the lcs exists.",N/A
"The Complexity of Reasoning with Cardinality Restrictions and Nominals
  in Expressive Description Logics","We study the complexity of the combination of the Description Logics ALCQ and
ALCQI with a terminological formalism based on cardinality restrictions on
concepts. These combinations can naturally be embedded into C^2, the two
variable fragment of predicate logic with counting quantifiers, which yields
decidability in NExpTime. We show that this approach leads to an optimal
solution for ALCQI, as ALCQI with cardinality restrictions has the same
complexity as C^2 (NExpTime-complete). In contrast, we show that for ALCQ, the
problem can be solved in ExpTime. This result is obtained by a reduction of
reasoning with cardinality restrictions to reasoning with the (in general
weaker) terminological formalism of general axioms for ALCQ extended with
nominals. Using the same reduction, we show that, for the extension of ALCQI
with nominals, reasoning with general axioms is a NExpTime-complete problem.
Finally, we sharpen this result and show that pure concept satisfiability for
ALCQI with nominals is NExpTime-complete. Without nominals, this problem is
known to be PSpace-complete.",N/A
Backbone Fragility and the Local Search Cost Peak,"The local search algorithm WSat is one of the most successful algorithms for
solving the satisfiability (SAT) problem. It is notably effective at solving
hard Random 3-SAT instances near the so-called `satisfiability threshold', but
still shows a peak in search cost near the threshold and large variations in
cost over different instances. We make a number of significant contributions to
the analysis of WSat on high-cost random instances, using the
recently-introduced concept of the backbone of a SAT instance. The backbone is
the set of literals which are entailed by an instance. We find that the number
of solutions predicts the cost well for small-backbone instances but is much
less relevant for the large-backbone instances which appear near the threshold
and dominate in the overconstrained region. We show a very strong correlation
between search cost and the Hamming distance to the nearest solution early in
WSat's search. This pattern leads us to introduce a measure of the backbone
fragility of an instance, which indicates how persistent the backbone is as
clauses are removed. We propose that high-cost random instances for local
search are those with very large backbones which are also backbone-fragile. We
suggest that the decay in cost beyond the satisfiability threshold is due to
increasing backbone robustness (the opposite of backbone fragility). Our
hypothesis makes three correct predictions. First, that the backbone robustness
of an instance is negatively correlated with the local search cost when other
factors are controlled for. Second, that backbone-minimal instances (which are
3-SAT instances altered so as to be more backbone-fragile) are unusually hard
for WSat. Third, that the clauses most often unsatisfied during search are
those whose deletion has the most effect on the backbone. In understanding the
pathologies of local search methods, we hope to contribute to the development
of new and better techniques.",N/A
"An Application of Reinforcement Learning to Dialogue Strategy Selection
  in a Spoken Dialogue System for Email","This paper describes a novel method by which a spoken dialogue system can
learn to choose an optimal dialogue strategy from its experience interacting
with human users. The method is based on a combination of reinforcement
learning and performance modeling of spoken dialogue systems. The reinforcement
learning component applies Q-learning (Watkins, 1989), while the performance
modeling component applies the PARADISE evaluation framework (Walker et al.,
1997) to learn the performance function (reward) used in reinforcement
learning. We illustrate the method with a spoken dialogue system named ELVIS
(EmaiL Voice Interactive System), that supports access to email over the phone.
We conduct a set of experiments for training an optimal dialogue strategy on a
corpus of 219 dialogues in which human users interact with ELVIS over the
phone. We then test that strategy on a corpus of 18 dialogues. We show that
ELVIS can learn to optimize its strategy selection for agent initiative, for
reading messages, and for summarizing email folders.",N/A
"Nonapproximability Results for Partially Observable Markov Decision
  Processes","We show that for several variations of partially observable Markov decision
processes, polynomial-time algorithms for finding control policies are unlikely
to or simply don't have guarantees of finding policies within a constant factor
or a constant summand of optimal. Here ""unlikely"" means ""unless some complexity
classes collapse,"" where the collapses considered are P=NP, P=PSPACE, or P=EXP.
Until or unless these collapses are shown to hold, any control-policy designer
must choose between such performance guarantees and efficient computation.",N/A
"On Reasonable and Forced Goal Orderings and their Use in an
  Agenda-Driven Planning Algorithm","The paper addresses the problem of computing goal orderings, which is one of
the longstanding issues in AI planning. It makes two new contributions. First,
it formally defines and discusses two different goal orderings, which are
called the reasonable and the forced ordering. Both orderings are defined for
simple STRIPS operators as well as for more complex ADL operators supporting
negation and conditional effects. The complexity of these orderings is
investigated and their practical relevance is discussed. Secondly, two
different methods to compute reasonable goal orderings are developed. One of
them is based on planning graphs, while the other investigates the set of
actions directly. Finally, it is shown how the ordering relations, which have
been derived for a given set of goals G, can be used to compute a so-called
goal agenda that divides G into an ordered set of subgoals. Any planner can
then, in principle, use the goal agenda to plan for increasing sets of
subgoals. This can lead to an exponential complexity reduction, as the solution
to a complex planning problem is found by solving easier subproblems. Since
only a polynomial overhead is caused by the goal agenda computation, a
potential exists to dramatically speed up planning algorithms as we demonstrate
in the empirical evaluation, where we use this method in the IPP planner.",N/A
Asimovian Adaptive Agents,"The goal of this research is to develop agents that are adaptive and
predictable and timely. At first blush, these three requirements seem
contradictory. For example, adaptation risks introducing undesirable side
effects, thereby making agents' behavior less predictable. Furthermore,
although formal verification can assist in ensuring behavioral predictability,
it is known to be time-consuming. Our solution to the challenge of satisfying
all three requirements is the following. Agents have finite-state automaton
plans, which are adapted online via evolutionary learning (perturbation)
operators. To ensure that critical behavioral constraints are always satisfied,
agents' plans are first formally verified. They are then reverified after every
adaptation. If reverification concludes that constraints are violated, the
plans are repaired. The main objective of this paper is to improve the
efficiency of reverification after learning, so that agents have a sufficiently
rapid response time. We present two solutions: positive results that certain
learning operators are a priori guaranteed to preserve useful classes of
behavioral assurance constraints (which implies that no reverification is
needed for these operators), and efficient incremental reverification
algorithms for those learning operators that have negative a priori results.",N/A
A Model of Inductive Bias Learning,"A major problem in machine learning is that of inductive bias: how to choose
a learner's hypothesis space so that it is large enough to contain a solution
to the problem being learnt, yet small enough to ensure reliable generalization
from reasonably-sized training sets. Typically such bias is supplied by hand
through the skill and insights of experts. In this paper a model for
automatically learning bias is investigated. The central assumption of the
model is that the learner is embedded within an environment of related learning
tasks. Within such an environment the learner can sample from multiple tasks,
and hence it can search for a hypothesis space that contains good solutions to
many of the problems in the environment. Under certain restrictions on the set
of all hypothesis spaces available to the learner, we show that a hypothesis
space that performs well on a sufficiently large number of training tasks will
also perform well when learning novel tasks in the same environment. Explicit
bounds are also derived demonstrating that learning multiple tasks within an
environment of related tasks can potentially give much better generalization
than learning a single task.",N/A
Mean Field Methods for a Special Class of Belief Networks,"The chief aim of this paper is to propose mean-field approximations for a
broad class of Belief networks, of which sigmoid and noisy-or networks can be
seen as special cases. The approximations are based on a powerful mean-field
theory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach
is the first order approximation in Plefka's approach, via a variational
derivation. The application of Plefka's theory to belief networks is not
computationally tractable. To tackle this problem we propose new approximations
based on Taylor series. Small scale experiments show that the proposed schemes
are attractive.",N/A
"On the Compilability and Expressive Power of Propositional Planning
  Formalisms","The recent approaches of extending the GRAPHPLAN algorithm to handle more
expressive planning formalisms raise the question of what the formal meaning of
""expressive power"" is. We formalize the intuition that expressive power is a
measure of how concisely planning domains and plans can be expressed in a
particular formalism by introducing the notion of ""compilation schemes"" between
planning formalisms. Using this notion, we analyze the expressiveness of a
large family of propositional planning formalisms, ranging from basic STRIPS to
a formalism with conditional effects, partial state specifications, and
propositional formulae in the preconditions. One of the results is that
conditional effects cannot be compiled away if plan size should grow only
linearly but can be compiled away if we allow for polynomial growth of the
resulting plans. This result confirms that the recently proposed extensions to
the GRAPHPLAN algorithm concerning conditional effects are optimal with respect
to the ""compilability"" framework. Another result is that general propositional
formulae cannot be compiled into conditional effects if the plan size should be
preserved linearly. This implies that allowing general propositional formulae
in preconditions and effect conditions adds another level of difficulty in
generating a plan.",N/A
Partial-Order Planning with Concurrent Interacting Actions,"In order to generate plans for agents with multiple actuators, agent teams,
or distributed controllers, we must be able to represent and plan using
concurrent actions with interacting effects. This has historically been
considered a challenging task requiring a temporal planner with the ability to
reason explicitly about time. We show that with simple modifications, the
STRIPS action representation language can be used to represent interacting
actions. Moreover, algorithms for partial-order planning require only small
modifications in order to be applied in such multiagent domains. We demonstrate
this fact by developing a sound and complete partial-order planner for planning
with concurrent interacting actions, POMP, that extends existing partial-order
planners in a straightforward way. These results open the way to the use of
partial-order planners for the centralized control of cooperative multiagent
systems.",N/A
Planning by Rewriting,"Domain-independent planning is a hard combinatorial problem. Taking into
account plan quality makes the task even more difficult. This article
introduces Planning by Rewriting (PbR), a new paradigm for efficient
high-quality domain-independent planning. PbR exploits declarative
plan-rewriting rules and efficient local search techniques to transform an
easy-to-generate, but possibly suboptimal, initial plan into a high-quality
plan. In addition to addressing the issues of planning efficiency and plan
quality, this framework offers a new anytime planning algorithm. We have
implemented this planner and applied it to several existing domains. The
experimental results show that the PbR approach provides significant savings in
planning effort while generating high-quality plans.",N/A
"Speeding Up the Convergence of Value Iteration in Partially Observable
  Markov Decision Processes","Partially observable Markov decision processes (POMDPs) have recently become
popular among many AI researchers because they serve as a natural model for
planning under uncertainty. Value iteration is a well-known algorithm for
finding optimal policies for POMDPs. It typically takes a large number of
iterations to converge. This paper proposes a method for accelerating the
convergence of value iteration. The method has been evaluated on an array of
benchmark problems and was found to be very effective: It enabled value
iteration to converge after only a few iterations on all the test problems.",N/A
Conformant Planning via Symbolic Model Checking,"We tackle the problem of planning in nondeterministic domains, by presenting
a new approach to conformant planning. Conformant planning is the problem of
finding a sequence of actions that is guaranteed to achieve the goal despite
the nondeterminism of the domain. Our approach is based on the representation
of the planning domain as a finite state automaton. We use Symbolic Model
Checking techniques, in particular Binary Decision Diagrams, to compactly
represent and efficiently search the automaton. In this paper we make the
following contributions. First, we present a general planning algorithm for
conformant planning, which applies to fully nondeterministic domains, with
uncertainty in the initial condition and in action effects. The algorithm is
based on a breadth-first, backward search, and returns conformant plans of
minimal length, if a solution to the planning problem exists, otherwise it
terminates concluding that the problem admits no conformant solution. Second,
we provide a symbolic representation of the search space based on Binary
Decision Diagrams (BDDs), which is the basis for search techniques derived from
symbolic model checking. The symbolic representation makes it possible to
analyze potentially large sets of states and transitions in a single
computation step, thus providing for an efficient implementation. Third, we
present CMBP (Conformant Model Based Planner), an efficient implementation of
the data structures and algorithm described above, directly based on BDD
manipulations, which allows for a compact representation of the search layers
and an efficient implementation of the search steps. Finally, we present an
experimental comparison of our approach with the state-of-the-art conformant
planners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems
from the distribution packages of these systems, plus other problems defined to
stress a number of specific factors. Our approach appears to be the most
effective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all
the problems where a comparison is possible, CMBP outperforms its competitors,
sometimes by orders of magnitude.",N/A
"AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential
  Reasoning in Large Bayesian Networks","Stochastic sampling algorithms, while an attractive alternative to exact
algorithms in very large Bayesian network models, have been observed to perform
poorly in evidential reasoning with extremely unlikely evidence. To address
this problem, we propose an adaptive importance sampling algorithm, AIS-BN,
that shows promising convergence rates even under extreme conditions and seems
to outperform the existing sampling algorithms consistently. Three sources of
this performance improvement are (1) two heuristics for initialization of the
importance function that are based on the theoretical properties of importance
sampling in finite-dimensional integrals and the structural advantages of
Bayesian networks, (2) a smooth learning method for the importance function,
and (3) a dynamic weighting function for combining samples from different
stages of the algorithm. We tested the performance of the AIS-BN algorithm
along with two state of the art general purpose sampling algorithms, likelihood
weighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance
sampling (Shachter and Peot, 1989). We used in our tests three large real
Bayesian network models available to the scientific community: the CPCS network
(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and
Nathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,
1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always
performed better than the other two algorithms, in the majority of the test
cases it achieved orders of magnitude improvement in precision of the results.
Improvement in speed given a desired precision is even more dramatic, although
we are unable to report numerical results here, as the other algorithms almost
never achieved the precision reached even by the first few iterations of the
AIS-BN algorithm.",N/A
Conflict-Directed Backjumping Revisited,"In recent years, many improvements to backtracking algorithms for solving
constraint satisfaction problems have been proposed. The techniques for
improving backtracking algorithms can be conveniently classified as look-ahead
schemes and look-back schemes. Unfortunately, look-ahead and look-back schemes
are not entirely orthogonal as it has been observed empirically that the
enhancement of look-ahead techniques is sometimes counterproductive to the
effects of look-back techniques. In this paper, we focus on the relationship
between the two most important look-ahead techniques---using a variable
ordering heuristic and maintaining a level of local consistency during the
backtracking search---and the look-back technique of conflict-directed
backjumping (CBJ). We show that there exists a ""perfect"" dynamic variable
ordering such that CBJ becomes redundant. We also show theoretically that as
the level of local consistency that is maintained in the backtracking search is
increased, the less that backjumping will be an improvement. Our theoretical
results partially explain why a backtracking algorithm doing more in the
look-ahead phase cannot benefit more from the backjumping look-back scheme.
Finally, we show empirically that adding CBJ to a backtracking algorithm that
maintains generalized arc consistency (GAC), an algorithm that we refer to as
GAC-CBJ, can still provide orders of magnitude speedups. Our empirical results
contrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an
algorithm that maintains arc consistency.",N/A
"Grounding the Lexical Semantics of Verbs in Visual Perception using
  Force Dynamics and Event Logic","This paper presents an implemented system for recognizing the occurrence of
events described by simple spatial-motion verbs in short image sequences. The
semantics of these verbs is specified with event-logic expressions that
describe changes in the state of force-dynamic relations between the
participants of the event. An efficient finite representation is introduced for
the infinite sets of intervals that occur when describing liquid and
semi-liquid events. Additionally, an efficient procedure using this
representation is presented for inferring occurrences of compound events,
described with event-logic expressions, from occurrences of primitive events.
Using force dynamics and event logic to specify the lexical semantics of events
allows the system to be more robust than prior systems based on motion profile.",N/A
Popular Ensemble Methods: An Empirical Study,"An ensemble consists of a set of individually trained classifiers (such as
neural networks or decision trees) whose predictions are combined when
classifying novel instances. Previous research has shown that an ensemble is
often more accurate than any of the single classifiers in the ensemble. Bagging
(Breiman, 1996c) and Boosting (Freund and Shapire, 1996; Shapire, 1990) are two
relatively new but popular methods for producing ensembles. In this paper we
evaluate these methods on 23 data sets using both neural networks and decision
trees as our classification algorithm. Our results clearly indicate a number of
conclusions. First, while Bagging is almost always more accurate than a single
classifier, it is sometimes much less accurate than Boosting. On the other
hand, Boosting can create ensembles that are less accurate than a single
classifier -- especially when using neural networks. Analysis indicates that
the performance of the Boosting methods is dependent on the characteristics of
the data set being examined. In fact, further results show that Boosting
ensembles may overfit noisy data sets, thus decreasing its performance.
Finally, consistent with previous studies, our work suggests that most of the
gain in an ensemble's performance comes in the first few classifiers combined;
however, relatively large gains can be seen up to 25 classifiers when Boosting
decision trees.",N/A
"An Evolutionary Algorithm with Advanced Goal and Priority Specification
  for Multi-objective Optimization","This paper presents an evolutionary algorithm with a new goal-sequence
domination scheme for better decision support in multi-objective optimization.
The approach allows the inclusion of advanced hard/soft priority and constraint
information on each objective component, and is capable of incorporating
multiple specifications with overlapping or non-overlapping objective functions
via logical 'OR' and 'AND' connectives to drive the search towards multiple
regions of trade-off. In addition, we propose a dynamic sharing scheme that is
simple and adaptively estimated according to the on-line population
distribution without needing any a priori parameter setting. Each feature in
the proposed algorithm is examined to show its respective contribution, and the
performance of the algorithm is compared with other evolutionary optimization
methods. It is shown that the proposed algorithm has performed well in the
diversity of evolutionary search and uniform distribution of non-dominated
individuals along the final trade-offs, without significant computational
effort. The algorithm is also applied to the design optimization of a practical
servo control system for hard disk drives with a single voice-coil-motor
actuator. Results of the evolutionary designed servo control system show a
superior closed-loop performance compared to classical PID or RPT approaches.",N/A
"The GRT Planning System: Backward Heuristic Construction in Forward
  State-Space Planning","This paper presents GRT, a domain-independent heuristic planning system for
STRIPS worlds. GRT solves problems in two phases. In the pre-processing phase,
it estimates the distance between each fact and the goals of the problem, in a
backward direction. Then, in the search phase, these estimates are used in
order to further estimate the distance between each intermediate state and the
goals, guiding so the search process in a forward direction and on a best-first
basis. The paper presents the benefits from the adoption of opposite directions
between the preprocessing and the search phases, discusses some difficulties
that arise in the pre-processing phase and introduces techniques to cope with
them. Moreover, it presents several methods of improving the efficiency of the
heuristic, by enriching the representation and by reducing the size of the
problem. Finally, a method of overcoming local optimal states, based on domain
axioms, is proposed. According to it, difficult problems are decomposed into
easier sub-problems that have to be solved sequentially. The performance
results from various domains, including those of the recent planning
competitions, show that GRT is among the fastest planners.",N/A
The Complexity of Reasoning about Spatial Congruence,"In the recent literature of Artificial Intelligence, an intensive research
effort has been spent, for various algebras of qualitative relations used in
the representation of temporal and spatial knowledge, on the problem of
classifying the computational complexity of reasoning problems for subsets of
algebras. The main purpose of these researches is to describe a restricted set
of maximal tractable subalgebras, ideally in an exhaustive fashion with respect
to the hosting algebras. In this paper we introduce a novel algebra for
reasoning about Spatial Congruence, show that the satisfiability problem in the
spatial algebra MC-4 is NP-complete, and present a complete classification of
tractability in the algebra, based on the individuation of three maximal
tractable subclasses, one containing the basic relations. The three algebras
are formed by 14, 10 and 9 relations out of 16 which form the full algebra.",N/A
Infinite-Horizon Policy-Gradient Estimation,"Gradient-based approaches to direct policy search in reinforcement learning
have received much recent attention as a means to solve problems of partial
observability and to avoid some of the problems associated with policy
degradation in value-function methods. In this paper we introduce GPOMDP, a
simulation-based algorithm for generating a {\em biased} estimate of the
gradient of the {\em average reward} in Partially Observable Markov Decision
Processes (POMDPs) controlled by parameterized stochastic policies. A similar
algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The
algorithm's chief advantages are that it requires storage of only twice the
number of policy parameters, uses one free parameter $\beta\in [0,1)$ (which
has a natural interpretation in terms of bias-variance trade-off), and requires
no knowledge of the underlying state. We prove convergence of GPOMDP, and show
how the correct choice of the parameter $\beta$ is related to the {\em mixing
time} of the controlled POMDP. We briefly describe extensions of GPOMDP to
controlled Markov chains, continuous state, observation and control spaces,
multiple-agents, higher-order derivatives, and a version for training
stochastic policies with internal states. In a companion paper (Baxter,
Bartlett, & Weaver, 2001) we show how the gradient estimates generated by
GPOMDP can be used in both a traditional stochastic gradient algorithm and a
conjugate-gradient procedure to find local optima of the average reward",N/A
Reasoning within Fuzzy Description Logics,"Description Logics (DLs) are suitable, well-known, logics for managing
structured knowledge. They allow reasoning about individuals and well defined
concepts, i.e., set of individuals with common properties. The experience in
using DLs in applications has shown that in many cases we would like to extend
their capabilities. In particular, their use in the context of Multimedia
Information Retrieval (MIR) leads to the convincement that such DLs should
allow the treatment of the inherent imprecision in multimedia object content
representation and retrieval. In this paper we will present a fuzzy extension
of ALC, combining Zadeh's fuzzy logic with a classical DL. In particular,
concepts becomes fuzzy and, thus, reasoning about imprecise concepts is
supported. We will define its syntax, its semantics, describe its properties
and present a constraint propagation calculus for reasoning in it.",N/A
An Analysis of Reduced Error Pruning,"Top-down induction of decision trees has been observed to suffer from the
inadequate functioning of the pruning phase. In particular, it is known that
the size of the resulting tree grows linearly with the sample size, even though
the accuracy of the tree does not improve. Reduced Error Pruning is an
algorithm that has been used as a representative technique in attempts to
explain the problems of decision tree learning. In this paper we present
analyses of Reduced Error Pruning in three different settings. First we study
the basic algorithmic properties of the method, properties that hold
independent of the input decision tree and pruning examples. Then we examine a
situation that intuitively should lead to the subtree under consideration to be
replaced by a leaf node, one in which the class label and attribute values of
the pruning examples are independent of each other. This analysis is conducted
under two different assumptions. The general analysis shows that the pruning
probability of a node fitting pure noise is bounded by a function that
decreases exponentially as the size of the tree grows. In a specific analysis
we assume that the examples are distributed uniformly to the tree. This
assumption lets us approximate the number of subtrees that are pruned because
they do not receive any pruning examples. This paper clarifies the different
variants of the Reduced Error Pruning algorithm, brings new insight to its
algorithmic properties, analyses the algorithm with less imposed assumptions
than before, and includes the previously overlooked empty subtrees to the
analysis.",N/A
GIB: Imperfect Information in a Computationally Challenging Game,"This paper investigates the problems arising in the construction of a program
to play the game of contract bridge. These problems include both the difficulty
of solving the game's perfect information variant, and techniques needed to
address the fact that bridge is not, in fact, a perfect information game. GIB,
the program being described, involves five separate technical advances:
partition search, the practical application of Monte Carlo techniques to
realistic problems, a focus on achievable sets to solve problems inherent in
the Monte Carlo approach, an extension of alpha-beta pruning from total orders
to arbitrary distributive lattices, and the use of squeaky wheel optimization
to find approximately optimal solutions to cardplay problems. GIB is currently
believed to be of approximately expert caliber, and is currently the strongest
computer bridge program in the world.",N/A
Domain Filtering Consistencies,"Enforcing local consistencies is one of the main features of constraint
reasoning. Which level of local consistency should be used when searching for
solutions in a constraint network is a basic question. Arc consistency and
partial forms of arc consistency have been widely studied, and have been known
for sometime through the forward checking or the MAC search algorithms. Until
recently, stronger forms of local consistency remained limited to those that
change the structure of the constraint graph, and thus, could not be used in
practice, especially on large networks. This paper focuses on the local
consistencies that are stronger than arc consistency, without changing the
structure of the network, i.e., only removing inconsistent values from the
domains. In the last five years, several such local consistencies have been
proposed by us or by others. We make an overview of all of them, and highlight
some relations between them. We compare them both theoretically and
experimentally, considering their pruning efficiency and the time required to
enforce them.",N/A
Policy Recognition in the Abstract Hidden Markov Model,"In this paper, we present a method for recognising an agent's behaviour in
dynamic, noisy, uncertain domains, and across multiple levels of abstraction.
We term this problem on-line plan recognition under uncertainty and view it
generally as probabilistic inference on the stochastic process representing the
execution of the agent's plan. Our contributions in this paper are twofold. In
terms of probabilistic inference, we introduce the Abstract Hidden Markov Model
(AHMM), a novel type of stochastic processes, provide its dynamic Bayesian
network (DBN) structure and analyse the properties of this network. We then
describe an application of the Rao-Blackwellised Particle Filter to the AHMM
which allows us to construct an efficient, hybrid inference method for this
model. In terms of plan recognition, we propose a novel plan recognition
framework based on the AHMM as the plan execution model. The Rao-Blackwellised
hybrid inference for AHMM can take advantage of the independence properties
inherent in a model of plan execution, leading to an algorithm for online
probabilistic plan recognition that scales well with the number of levels in
the plan hierarchy. This illustrates that while stochastic models for plan
execution can be complex, they exhibit special structures which, if exploited,
can lead to efficient plan recognition algorithms. We demonstrate the
usefulness of the AHMM framework via a behaviour recognition system in a
complex spatial environment using distributed video surveillance data.",N/A
The FF Planning System: Fast Plan Generation Through Heuristic Search,"We describe and evaluate the algorithmic techniques that are used in the FF
planning system. Like the HSP system, FF relies on forward state space search,
using a heuristic that estimates goal distances by ignoring delete lists.
Unlike HSP's heuristic, our method does not assume facts to be independent. We
introduce a novel search strategy that combines hill-climbing with systematic
search, and we show how other powerful heuristic information can be extracted
and used to prune the search space. FF was the most successful automatic
planner at the recent AIPS-2000 planning competition. We review the results of
the competition, give data for other benchmark domains, and investigate the
reasons for the runtime performance of FF compared to HSP.",N/A
ATTac-2000: An Adaptive Autonomous Bidding Agent,"The First Trading Agent Competition (TAC) was held from June 22nd to July
8th, 2000. TAC was designed to create a benchmark problem in the complex domain
of e-marketplaces and to motivate researchers to apply unique approaches to a
common task. This article describes ATTac-2000, the first-place finisher in
TAC. ATTac-2000 uses a principled bidding strategy that includes several
elements of adaptivity. In addition to the success at the competition, isolated
empirical results are presented indicating the robustness and effectiveness of
ATTac-2000's adaptive strategy.",N/A
Efficient Methods for Qualitative Spatial Reasoning,"The theoretical properties of qualitative spatial reasoning in the RCC8
framework have been analyzed extensively. However, no empirical investigation
has been made yet. Our experiments show that the adaption of the algorithms
used for qualitative temporal reasoning can solve large RCC8 instances, even if
they are in the phase transition region -- provided that one uses the maximal
tractable subsets of RCC8 that have been identified by us. In particular, we
demonstrate that the orthogonal combination of heuristic methods is successful
in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time.",N/A
Towards OWL-based Knowledge Representation in Petrology,"This paper presents our work on development of OWL-driven systems for formal
representation and reasoning about terminological knowledge and facts in
petrology. The long-term aim of our project is to provide solid foundations for
a large-scale integration of various kinds of knowledge, including basic terms,
rock classification algorithms, findings and reports. We describe three steps
we have taken towards that goal here. First, we develop a semi-automated
procedure for transforming a database of igneous rock samples to texts in a
controlled natural language (CNL), and then a collection of OWL ontologies.
Second, we create an OWL ontology of important petrology terms currently
described in natural language thesauri. We describe a prototype of a tool for
collecting definitions from domain experts. Third, we present an approach to
formalization of current industrial standards for classification of rock
samples, which requires linear equations in OWL 2. In conclusion, we discuss a
range of opportunities arising from the use of semantic technologies in
petrology and outline the future work in this area.","10 pages. The paper has been accepted by OWLED2011 as a long
  presentation"
"Accelerating Reinforcement Learning by Composing Solutions of
  Automatically Identified Subtasks","This paper discusses a system that accelerates reinforcement learning by
using transfer from related tasks. Without such transfer, even if two tasks are
very similar at some abstract level, an extensive re-learning effort is
required. The system achieves much of its power by transferring parts of
previously learned solutions rather than a single complete solution. The system
exploits strong features in the multi-dimensional function produced by
reinforcement learning in solving a particular task. These features are stable
and easy to recognize early in the learning process. They generate a
partitioning of the state space and thus the function. The partition is
represented as a graph. This is used to index and compose functions stored in a
case base to form a close approximation to the solution of the new task.
Experiments demonstrate that function composition often produces more than an
order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm.",N/A
Parameter Learning of Logic Programs for Symbolic-Statistical Modeling,"We propose a logical/mathematical framework for statistical parameter
learning of parameterized logic programs, i.e. definite clause programs
containing probabilistic facts with a parameterized distribution. It extends
the traditional least Herbrand model semantics in logic programming to
distribution semantics, possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs
including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM
algorithm, the graphical EM algorithm, that runs for a class of parameterized
logic programs representing sequential decision processes where each decision
is exclusive and independent. It runs on a new data structure called support
graphs describing the logical relationship between observations and their
explanations, and learns parameters by computing inside and outside probability
generalized for logic programs. The complexity analysis shows that when
combined with OLDT search for all explanations for observations, the graphical
EM algorithm, despite its generality, has the same time complexity as existing
EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside
algorithm for PCFGs, and the one for singly connected Bayesian networks that
have been developed independently in each research field. Learning experiments
with PCFGs using two corpora of moderate size indicate that the graphical EM
algorithm can significantly outperform the Inside-Outside algorithm.",N/A
Finding a Path is Harder than Finding a Tree,"I consider the problem of learning an optimal path graphical model from data
and show the problem to be NP-hard for the maximum likelihood and minimum
description length approaches and a Bayesian approach. This hardness result
holds despite the fact that the problem is a restriction of the polynomially
solvable problem of finding the optimal tree graphical model.",N/A
"Extensions of Simple Conceptual Graphs: the Complexity of Rules and
  Constraints","Simple conceptual graphs are considered as the kernel of most knowledge
representation formalisms built upon Sowa's model. Reasoning in this model can
be expressed by a graph homomorphism called projection, whose semantics is
usually given in terms of positive, conjunctive, existential FOL. We present
here a family of extensions of this model, based on rules and constraints,
keeping graph homomorphism as the basic operation. We focus on the formal
definitions of the different models obtained, including their operational
semantics and relationships with FOL, and we analyze the decidability and
complexity of the associated problems (consistency and deduction). As soon as
rules are involved in reasonings, these problems are not decidable, but we
exhibit a condition under which they fall in the polynomial hierarchy. These
results extend and complete the ones already published by the authors. Moreover
we systematically study the complexity of some particular cases obtained by
restricting the form of constraints and/or rules.",N/A
Fusions of Description Logics and Abstract Description Systems,"Fusions are a simple way of combining logics. For normal modal logics,
fusions have been investigated in detail. In particular, it is known that,
under certain conditions, decidability transfers from the component logics to
their fusion. Though description logics are closely related to modal logics,
they are not necessarily normal. In addition, ABox reasoning in description
logics is not covered by the results from modal logics. In this paper, we
extend the decidability transfer results from normal modal logics to a large
class of description logics. To cover different description logics in a uniform
way, we introduce abstract description systems, which can be seen as a common
generalization of description and modal logics, and show the transfer results
in this general setting.",N/A
"Improving the Efficiency of Inductive Logic Programming Through the Use
  of Query Packs","Inductive logic programming, or relational learning, is a powerful paradigm
for machine learning or data mining. However, in order for ILP to become
practically useful, the efficiency of ILP systems must improve substantially.
To this end, the notion of a query pack is introduced: it structures sets of
similar queries. Furthermore, a mechanism is described for executing such query
packs. A complexity analysis shows that considerable efficiency improvements
can be achieved through the use of this query pack execution mechanism. This
claim is supported by empirical results obtained by incorporating support for
query pack execution in two existing learning systems.",N/A
A Critical Assessment of Benchmark Comparison in Planning,"Recent trends in planning research have led to empirical comparison becoming
commonplace. The field has started to settle into a methodology for such
comparisons, which for obvious practical reasons requires running a subset of
planners on a subset of problems. In this paper, we characterize the
methodology and examine eight implicit assumptions about the problems, planners
and metrics used in many of these comparisons. The problem assumptions are:
PR1) the performance of a general purpose planner should not be
penalized/biased if executed on a sampling of problems and domains, PR2) minor
syntactic differences in representation do not affect performance, and PR3)
problems should be solvable by STRIPS capable planners unless they require ADL.
The planner assumptions are: PL1) the latest version of a planner is the best
one to use, PL2) default parameter settings approximate good performance, and
PL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)
performance degrades similarly for each planner when run on degraded runtime
environments (e.g., machine platform) and M2) the number of plan steps
distinguishes performance. We find that most of these assumptions are not
supported empirically; in particular, that planners are affected differently by
these assumptions. We conclude with a call to the community to devote research
resources to improving the state of the practice and especially to enhancing
the available benchmark problems.",N/A
SMOTE: Synthetic Minority Over-sampling Technique,"An approach to the construction of classifiers from imbalanced datasets is
described. A dataset is imbalanced if the classification categories are not
approximately equally represented. Often real-world data sets are predominately
composed of ""normal"" examples with only a small percentage of ""abnormal"" or
""interesting"" examples. It is also the case that the cost of misclassifying an
abnormal (interesting) example as a normal example is often much higher than
the cost of the reverse error. Under-sampling of the majority (normal) class
has been proposed as a good means of increasing the sensitivity of a classifier
to the minority class. This paper shows that a combination of our method of
over-sampling the minority (abnormal) class and under-sampling the majority
(normal) class can achieve better classifier performance (in ROC space) than
only under-sampling the majority class. This paper also shows that a
combination of our method of over-sampling the minority class and
under-sampling the majority class can achieve better classifier performance (in
ROC space) than varying the loss ratios in Ripper or class priors in Naive
Bayes. Our method of over-sampling the minority class involves creating
synthetic minority class examples. Experiments are performed using C4.5, Ripper
and a Naive Bayes classifier. The method is evaluated using the area under the
Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.",N/A
When do Numbers Really Matter?,"Common wisdom has it that small distinctions in the probabilities
(parameters) quantifying a belief network do not matter much for the results of
probabilistic queries. Yet, one can develop realistic scenarios under which
small variations in network parameters can lead to significant changes in
computed queries. A pending theoretical question is then to analytically
characterize parameter changes that do or do not matter. In this paper, we
study the sensitivity of probabilistic queries to changes in network parameters
and prove some tight bounds on the impact that such parameters can have on
queries. Our analytic results pinpoint some interesting situations under which
parameter changes do or do not matter. These results are important for
knowledge engineers as they help them identify influential network parameters.
They also help explain some of the previous experimental results and
observations with regards to network robustness against parameter changes.",N/A
Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach,"Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.",N/A
"Automatically Training a Problematic Dialogue Predictor for a Spoken
  Dialogue System","Spoken dialogue systems promise efficient and natural access to a large
variety of information sources and services from any phone. However, current
spoken dialogue systems are deficient in their strategies for preventing,
identifying and repairing problems that arise in the conversation. This paper
reports results on automatically training a Problematic Dialogue Predictor to
predict problematic human-computer dialogues using a corpus of 4692 dialogues
collected with the 'How May I Help You' (SM) spoken dialogue system. The
Problematic Dialogue Predictor can be immediately applied to the system's
decision of whether to transfer the call to a human customer care agent, or be
used as a cue to the system's dialogue manager to modify its behavior to repair
problems, and even perhaps, to prevent them. We show that a Problematic
Dialogue Predictor using automatically-obtainable features from the first two
exchanges in the dialogue can predict problematic dialogues 13.2% more
accurately than the baseline.",N/A
"Inducing Interpretable Voting Classifiers without Trading Accuracy for
  Simplicity: Theoretical Results, Approximation Algorithms","Recent advances in the study of voting classification algorithms have brought
empirical and theoretical results clearly showing the discrimination power of
ensemble classifiers. It has been previously argued that the search of this
classification power in the design of the algorithms has marginalized the need
to obtain interpretable classifiers. Therefore, the question of whether one
might have to dispense with interpretability in order to keep classification
strength is being raised in a growing number of machine learning or data mining
papers. The purpose of this paper is to study both theoretically and
empirically the problem. First, we provide numerous results giving insight into
the hardness of the simplicity-accuracy tradeoff for voting classifiers. Then
we provide an efficient ""top-down and prune"" induction heuristic, WIDC, mainly
derived from recent results on the weak learning and boosting frameworks. It is
to our knowledge the first attempt to build a voting classifier as a base
formula using the weak learning framework (the one which was previously highly
successful for decision tree induction), and not the strong learning framework
(as usual for such classifiers with boosting-like approaches). While it uses a
well-known induction scheme previously successful in other classes of concept
representations, thus making it easy to implement and compare, WIDC also relies
on recent or new results we give about particular cases of boosting known as
partition boosting and ranking loss boosting. Experimental results on
thirty-one domains, most of which readily available, tend to display the
ability of WIDC to produce small, accurate, and interpretable decision
committees.",N/A
A Knowledge Compilation Map,"We propose a perspective on knowledge compilation which calls for analyzing
different compilation approaches according to two key dimensions: the
succinctness of the target compilation language, and the class of queries and
transformations that the language supports in polytime. We then provide a
knowledge compilation map, which analyzes a large number of existing target
compilation languages according to their succinctness and their polytime
transformations and queries. We argue that such analysis is necessary for
placing new compilation approaches within the context of existing ones. We also
go beyond classical, flat target compilation languages based on CNF and DNF,
and consider a richer, nested class based on directed acyclic graphs (such as
OBDDs), which we show to include a relatively large number of target
compilation languages.",N/A
"Inferring Strategies for Sentence Ordering in Multidocument News
  Summarization","The problem of organizing information for multidocument summarization so that
the generated summary is coherent has received relatively little attention.
While sentence ordering for single document summarization can be determined
from the ordering of sentences in the input article, this is not the case for
multidocument summarization where summary sentences may be drawn from different
input articles. In this paper, we propose a methodology for studying the
properties of ordering information in the news genre and describe experiments
done on a corpus of multiple acceptable orderings we developed for the task.
Based on these experiments, we implemented a strategy for ordering information
that combines constraints from chronological order of events and topical
relatedness. Evaluation of our augmented algorithm shows a significant
improvement of the ordering over two baseline strategies.",N/A
"Collective Intelligence, Data Routing and Braess' Paradox","We consider the problem of designing the the utility functions of the
utility-maximizing agents in a multi-agent system so that they work
synergistically to maximize a global utility. The particular problem domain we
explore is the control of network routing by placing agents on all the routers
in the network. Conventional approaches to this task have the agents all use
the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many
cases, due to the side-effects of one agent's actions on another agent's
performance, having agents use ISPA's is suboptimal as far as global aggregate
cost is concerned, even when they are only used to route infinitesimally small
amounts of traffic. The utility functions of the individual agents are not
""aligned"" with the global utility, intuitively speaking. As a particular
example of this we present an instance of Braess' paradox in which adding new
links to a network whose agents all use the ISPA results in a decrease in
overall throughput. We also demonstrate that load-balancing, in which the
agents' decisions are collectively made to optimize the global cost incurred by
all traffic currently being routed, is suboptimal as far as global cost
averaged across time is concerned. This is also due to 'side-effects', in this
case of current routing decision on future traffic. The mathematics of
Collective Intelligence (COIN) is concerned precisely with the issue of
avoiding such deleterious side-effects in multi-agent systems, both over time
and space. We present key concepts from that mathematics and use them to derive
an algorithm whose ideal version should have better performance than that of
having all agents use the ISPA, even in the infinitesimal limit. We present
experiments verifying this, and also showing that a machine-learning-based
version of this COIN algorithm in which costs are only imprecisely estimated
via empirical means (a version potentially applicable in the real world) also
outperforms the ISPA, despite having access to less information than does the
ISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.",N/A
Efficient Solution Algorithms for Factored MDPs,"This paper addresses the problem of planning under uncertainty in large
Markov Decision Processes (MDPs). Factored MDPs represent a complex state space
using state variables and the transition model using a dynamic Bayesian
network. This representation often allows an exponential reduction in the
representation size of structured MDPs, but the complexity of exact solution
algorithms for such MDPs can grow exponentially in the representation size. In
this paper, we present two approximate solution algorithms that exploit
structure in factored MDPs. Both use an approximate value function represented
as a linear combination of basis functions, where each basis function involves
only a small subset of the domain variables. A key contribution of this paper
is that it shows how the basic operations of both algorithms can be performed
efficiently in closed form, by exploiting both additive and context-specific
structure in a factored MDP. A central element of our algorithms is a novel
linear program decomposition technique, analogous to variable elimination in
Bayesian networks, which reduces an exponentially large LP to a provably
equivalent, polynomial-sized one. One algorithm uses approximate linear
programming, and the second approximate dynamic programming. Our dynamic
programming algorithm is novel in that it uses an approximation based on
max-norm, a technique that more directly minimizes the terms that appear in
error bounds for approximate MDP algorithms. We provide experimental results on
problems with over 10^40 states, demonstrating a promising indication of the
scalability of our approach, and compare our algorithm to an existing
state-of-the-art approach, showing, in some problems, exponential gains in
computation time.",N/A
Intelligent decision: towards interpreting the Pe Algorithm,"The human intelligence lies in the algorithm, the nature of algorithm lies in
the classification, and the classification is equal to outlier detection. A lot
of algorithms have been proposed to detect outliers, meanwhile a lot of
definitions. Unsatisfying point is that definitions seem vague, which makes the
solution an ad hoc one. We analyzed the nature of outliers, and give two clear
definitions. We then develop an efficient RDD algorithm, which converts outlier
problem to pattern and degree problem. Furthermore, a collapse mechanism was
introduced by IIR algorithm, which can be united seamlessly with the RDD
algorithm and serve for the final decision. Both algorithms are originated from
the study on general AI. The combined edition is named as Pe algorithm, which
is the basis of the intelligent decision. Here we introduce longest k-turn
subsequence problem and corresponding solution as an example to interpret the
function of Pe algorithm in detecting curve-type outliers. We also give a
comparison between IIR algorithm and Pe algorithm, where we can get a better
understanding at both algorithms. A short discussion about intelligence is
added to demonstrate the function of the Pe algorithm. Related experimental
results indicate its robustness.","23pages, 12 figures, 7 tables"
A Linear Time Natural Evolution Strategy for Non-Separable Functions,"We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES
(R1-NES), which uses a low rank approximation of the search distribution
covariance matrix. The algorithm allows computation of the natural gradient
with cost linear in the dimensionality of the parameter space, and excels in
solving high-dimensional non-separable problems, including the best result to
date on the Rosenbrock function (512 dimensions).",N/A
From Causal Models To Counterfactual Structures,"Galles and Pearl claimed that ""for recursive models, the causal model
framework does not add any restrictions to counterfactuals, beyond those
imposed by Lewis's [possible-worlds] framework."" This claim is examined
carefully, with the goal of clarifying the exact relationship between causal
models and Lewis's framework. Recursive models are shown to correspond
precisely to a subclass of (possible-world) counterfactual structures. On the
other hand, a slight generalization of recursive models, models where all
equations have unique solutions, is shown to be incomparable in expressive
power to counterfactual structures, despite the fact that the Galles and Pearl
arguments should apply to them as well. The problem with the Galles and Pearl
argument is identified: an axiom that they viewed as irrelevant, because it
involved disjunction (which was not in their language), is not irrelevant at
all.","A preliminary version of this paper appears in the Proceedings of the
  Twelfth International Conference on Principles of Knowledge Representation
  and Reasoning (KR 2010), 2010.}"
Actual causation and the art of modeling,"We look more carefully at the modeling of causality using structural
equations. It is clear that the structural equations can have a major impact on
the conclusions we draw about causality. In particular, the choice of variables
and their values can also have a significant impact on causality. These choices
are, to some extent, subjective. We consider what counts as an appropriate
choice. More generally, we consider what makes a model an appropriate model,
especially if we want to take defaults into account, as was argued is necessary
in recent work.","In <em>Heuristics, Probability and Causality: A Tribute to Judea
  Pearl</em> (editors, R. Dechter, H. Geffner, and J. Y. Halpern), College
  Publications, 2010, pp. 383-406"
Generating Schemata of Resolution Proofs,"Two distinct algorithms are presented to extract (schemata of) resolution
proofs from closed tableaux for propositional schemata. The first one handles
the most efficient version of the tableau calculus but generates very complex
derivations (denoted by rather elaborate rewrite systems). The second one has
the advantage that much simpler systems can be obtained, however the considered
proof procedure is less efficient.",N/A
"Random forest models of the retention constants in the thin layer
  chromatography","In the current study we examine an application of the machine learning
methods to model the retention constants in the thin layer chromatography
(TLC). This problem can be described with hundreds or even thousands of
descriptors relevant to various molecular properties, most of them redundant
and not relevant for the retention constant prediction. Hence we employed
feature selection to significantly reduce the number of attributes.
Additionally we have tested application of the bagging procedure to the feature
selection. The random forest regression models were built using selected
variables. The resulting models have better correlation with the experimental
data than the reference models obtained with linear regression. The
cross-validation confirms robustness of the models.",N/A
"Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion
  Applications","Nowadays ontologies present a growing interest in Data Fusion applications.
As a matter of fact, the ontologies are seen as a semantic tool for describing
and reasoning about sensor data, objects, relations and general domain
theories. In addition, uncertainty is perhaps one of the most important
characteristics of the data and information handled by Data Fusion. However,
the fundamental nature of ontologies implies that ontologies describe only
asserted and veracious facts of the world. Different probabilistic, fuzzy and
evidential approaches already exist to fill this gap; this paper recaps the
most popular tools. However none of the tools meets exactly our purposes.
Therefore, we constructed a Dempster-Shafer ontology that can be imported into
any specific domain ontology and that enables us to instantiate it in an
uncertain manner. We also developed a Java application that enables reasoning
about these uncertain ontological instances.","Workshop on Theory of Belief Functions, Brest: France (2010)"
Coincidences and the encounter problem: A formal account,"Individuals have an intuitive perception of what makes a good coincidence.
Though the sensitivity to coincidences has often been presented as resulting
from an erroneous assessment of probability, it appears to be a genuine
competence, based on non-trivial computations. The model presented here
suggests that coincidences occur when subjects perceive complexity drops.
Co-occurring events are, together, simpler than if considered separately. This
model leads to a possible redefinition of subjective probability.","30th Annual Conference of the Cognitive Science Society, Washington :
  United States (2008)"
"Rooting opinions in the minds: a cognitive model and a formal account of
  opinions and their dynamics","The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
like computer science and complexity, have tried to deal with this issue.
Despite the flourishing of different models and theories in both fields,
several key questions still remain unanswered. The understanding of how
opinions change and the way they are affected by social influence are
challenging issues requiring a thorough analysis of opinion per se but also of
the way in which they travel between agents' minds and are modulated by these
exchanges. To account for the two-faceted nature of opinions, which are mental
entities undergoing complex social processes, we outline a preliminary model in
which a cognitive theory of opinions is put forward and it is paired with a
formal description of them and of their spreading among minds. Furthermore,
investigating social influence also implies the necessity to account for the
way in which people change their minds, as a consequence of interacting with
other people, and the need to explain the higher or lower persistence of such
changes.",N/A
Understanding opinions. A cognitive and formal account,"The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
as computer science and complexity, have addressed this challenge. Despite the
flourishing of different models and theories in both fields, several key
questions still remain unanswered. The aim of this paper is to challenge the
current theories on opinion by putting forward a cognitively grounded model
where opinions are described as specific mental representations whose main
properties are put forward. A comparison with reputation will be also
presented.",N/A
"Learning When Training Data are Costly: The Effect of Class Distribution
  on Tree Induction","For large, real-world inductive learning problems, the number of training
examples often must be limited due to the costs associated with procuring,
preparing, and storing the training examples and/or the computational costs
associated with learning from them. In such circumstances, one question of
practical importance is: if only n training examples can be selected, in what
proportion should the classes be represented? In this article we help to answer
this question by analyzing, for a fixed training-set size, the relationship
between the class distribution of the training data and the performance of
classification trees induced from these data. We study twenty-six data sets
and, for each, determine the best class distribution for learning. The
naturally occurring class distribution is shown to generally perform well when
classifier performance is evaluated using undifferentiated error rate (0/1
loss). However, when the area under the ROC curve is used to evaluate
classifier performance, a balanced distribution is shown to perform well. Since
neither of these choices for class distribution always generates the
best-performing classifier, we introduce a budget-sensitive progressive
sampling algorithm for selecting training examples based on the class
associated with each example. An empirical analysis of this algorithm shows
that the class distribution of the resulting training set yields classifiers
with good (nearly-optimal) classification performance.",N/A
PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains,"In recent years research in the planning community has moved increasingly
toward s application of planners to realistic problems involving both time and
many typ es of resources. For example, interest in planning demonstrated by the
space res earch community has inspired work in observation scheduling,
planetary rover ex ploration and spacecraft control domains. Other temporal and
resource-intensive domains including logistics planning, plant control and
manufacturing have also helped to focus the community on the modelling and
reasoning issues that must be confronted to make planning technology meet the
challenges of application. The International Planning Competitions have acted
as an important motivating fo rce behind the progress that has been made in
planning since 1998. The third com petition (held in 2002) set the planning
community the challenge of handling tim e and numeric resources. This
necessitated the development of a modelling langua ge capable of expressing
temporal and numeric properties of planning domains. In this paper we describe
the language, PDDL2.1, that was used in the competition. We describe the syntax
of the language, its formal semantics and the validation of concurrent plans.
We observe that PDDL2.1 has considerable modelling power --- exceeding the
capabilities of current planning technology --- and presents a number of
important challenges to the research community.",N/A
"The Communicative Multiagent Team Decision Problem: Analyzing Teamwork
  Theories and Models","Despite the significant progress in multiagent teamwork, existing research
does not address the optimality of its prescriptions nor the complexity of the
teamwork problem. Without a characterization of the optimality-complexity
tradeoffs, it is impossible to determine whether the assumptions and
approximations made by a particular theory gain enough efficiency to justify
the losses in overall performance. To provide a tool for use by multiagent
researchers in evaluating this tradeoff, we present a unified framework, the
COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model
combines and extends existing multiagent theories, such as decentralized
partially observable Markov decision processes and economic team theory. In
addition to their generality of representation, COM-MTDPs also support the
analysis of both the optimality of team performance and the computational
complexity of the agents' decision problem. In analyzing complexity, we present
a breakdown of the computational complexity of constructing optimal teams under
various classes of problem domains, along the dimensions of observability and
communication cost. In analyzing optimality, we exploit the COM-MTDP's ability
to encode existing teamwork theories and models to encode two instantiations of
joint intentions theory taken from the literature. Furthermore, the COM-MTDP
model provides a basis for the development of novel team coordination
algorithms. We derive a domain-independent criterion for optimal communication
and provide a comparative analysis of the two joint intentions instantiations
with respect to this optimal policy. We have implemented a reusable,
domain-independent software package based on COM-MTDPs to analyze teamwork
coordination strategies, and we demonstrate its use by encoding and evaluating
the two joint intentions strategies within an example domain.",N/A
Towards Adjustable Autonomy for the Real World,"Adjustable autonomy refers to entities dynamically varying their own
autonomy, transferring decision-making control to other entities (typically
agents transferring control to human users) in key situations. Determining
whether and when such transfers-of-control should occur is arguably the
fundamental research problem in adjustable autonomy. Previous work has
investigated various approaches to addressing this problem but has often
focused on individual agent-human interactions. Unfortunately, domains
requiring collaboration between teams of agents and humans reveal two key
shortcomings of these previous approaches. First, these approaches use rigid
one-shot transfers of control that can result in unacceptable coordination
failures in multiagent settings. Second, they ignore costs (e.g., in terms of
time delays or effects on actions) to an agent's team due to such
transfers-of-control. To remedy these problems, this article presents a novel
approach to adjustable autonomy, based on the notion of a transfer-of-control
strategy. A transfer-of-control strategy consists of a conditional sequence of
two types of actions: (i) actions to transfer decision-making control (e.g.,
from an agent to a user or vice versa) and (ii) actions to change an agent's
pre-specified coordination constraints with team members, aimed at minimizing
miscoordination costs. The goal is for high-quality individual decisions to be
made with minimal disruption to the coordination of the team. We present a
mathematical model of transfer-of-control strategies. The model guides and
informs the operationalization of the strategies using Markov Decision
Processes, which select an optimal strategy, given an uncertain environment and
costs to the individuals and teams. The approach has been carefully evaluated,
including via its use in a real-world, deployed multi-agent system that assists
a research group in its daily activities.",N/A
An Analysis of Phase Transition in NK Landscapes,"In this paper, we analyze the decision version of the NK landscape model from
the perspective of threshold phenomena and phase transitions under two random
distributions, the uniform probability model and the fixed ratio model. For the
uniform probability model, we prove that the phase transition is easy in the
sense that there is a polynomial algorithm that can solve a random instance of
the problem with the probability asymptotic to 1 as the problem size tends to
infinity. For the fixed ratio model, we establish several upper bounds for the
solubility threshold, and prove that random instances with parameters above
these upper bounds can be solved polynomially. This, together with our
empirical study for random instances generated below and in the phase
transition region, suggests that the phase transition of the fixed ratio model
is also easy.",N/A
Expert-Guided Subgroup Discovery: Methodology and Application,"This paper presents an approach to expert-guided subgroup discovery. The main
step of the subgroup discovery process, the induction of subgroup descriptions,
is performed by a heuristic beam search algorithm, using a novel parametrized
definition of rule quality which is analyzed in detail. The other important
steps of the proposed subgroup discovery process are the detection of
statistically significant properties of selected subgroups and subgroup
visualization: statistically significant properties are used to enrich the
descriptions of induced subgroups, while the visualization shows subgroup
properties in the form of distributions of the numbers of examples in the
subgroups. The approach is illustrated by the results obtained for a medical
problem of early detection of patient risk groups.",N/A
"Propositional Independence - Formula-Variable Independence and
  Forgetting","Independence -- the study of what is relevant to a given problem of reasoning
-- has received an increasing attention from the AI community. In this paper,
we consider two basic forms of independence, namely, a syntactic one and a
semantic one. We show features and drawbacks of them. In particular, while the
syntactic form of independence is computationally easy to check, there are
cases in which things that intuitively are not relevant are not recognized as
such. We also consider the problem of forgetting, i.e., distilling from a
knowledge base only the part that is relevant to the set of queries constructed
from a subset of the alphabet. While such process is computationally hard, it
allows for a simplification of subsequent reasoning, and can thus be viewed as
a form of compilation: once the relevant part of a knowledge base has been
extracted, all reasoning tasks to be performed can be simplified.",N/A
Monte Carlo Methods for Tempo Tracking and Rhythm Quantization,"We present a probabilistic generative model for timing deviations in
expressive music performance. The structure of the proposed model is equivalent
to a switching state space model. The switch variables correspond to discrete
note locations as in a musical score. The continuous hidden variables denote
the tempo. We formulate two well known music recognition problems, namely tempo
tracking and automatic transcription (rhythm quantization) as filtering and
maximum a posteriori (MAP) state estimation tasks. Exact computation of
posterior features such as the MAP state is intractable in this model class, so
we introduce Monte Carlo methods for integration and optimization. We compare
Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated
annealing and iterative improvement) and sequential Monte Carlo methods
(particle filters). Our simulation results suggest better results with
sequential methods. The methods can be applied in both online and batch
scenarios such as tempo tracking and transcription and are thus potentially
useful in a number of music applications such as adaptive automatic
accompaniment, score typesetting and music information retrieval.",N/A
Exploiting Contextual Independence In Probabilistic Inference,"Bayesian belief networks have grown to prominence because they provide
compact representations for many problems for which probabilistic inference is
appropriate, and there are algorithms to exploit this compactness. The next
step is to allow compact representations of the conditional probabilities of a
variable given its parents. In this paper we present such a representation that
exploits contextual independence in terms of parent contexts; which variables
act as parents may depend on the value of other variables. The internal
representation is in terms of contextual factors (confactors) that is simply a
pair of a context and a table. The algorithm, contextual variable elimination,
is based on the standard variable elimination algorithm that eliminates the
non-query variables in turn, but when eliminating a variable, the tables that
need to be multiplied can depend on the context. This algorithm reduces to
standard variable elimination when there is no contextual independence
structure to exploit. We show how this can be much more efficient than variable
elimination when there is structure to exploit. We explain why this new method
can exploit more structure than previous methods for structured belief network
inference and an analogous algorithm that uses trees.",N/A
Bound Propagation,"In this article we present an algorithm to compute bounds on the marginals of
a graphical model. For several small clusters of nodes upper and lower bounds
on the marginal values are computed independently of the rest of the network.
The range of allowed probability distributions over the surrounding nodes is
restricted using earlier computed bounds. As we will show, this can be
considered as a set of constraints in a linear programming problem of which the
objective function is the marginal probability of the center nodes. In this way
knowledge about the maginals of neighbouring clusters is passed to other
clusters thereby tightening the bounds on their marginals. We show that sharp
bounds can be obtained for undirected and directed graphs that are used for
practical applications, but for which exact computations are infeasible.",N/A
On Polynomial Sized MDP Succinct Policies,"Policies of Markov Decision Processes (MDPs) determine the next action to
execute from the current state and, possibly, the history (the past states).
When the number of states is large, succinct representations are often used to
compactly represent both the MDPs and the policies in a reduced amount of
space. In this paper, some problems related to the size of succinctly
represented policies are analyzed. Namely, it is shown that some MDPs have
policies that can only be represented in space super-polynomial in the size of
the MDP, unless the polynomial hierarchy collapses. This fact motivates the
study of the problem of deciding whether a given MDP has a policy of a given
size and reward. Since some algorithms for MDPs work by finding a succinct
representation of the value function, the problem of deciding the existence of
a succinct representation of a value function of a given size and reward is
also considered.",N/A
"Compiling Causal Theories to Successor State Axioms and STRIPS-Like
  Systems","We describe a system for specifying the effects of actions. Unlike those
commonly used in AI planning, our system uses an action description language
that allows one to specify the effects of actions using domain rules, which are
state constraints that can entail new action effects from old ones.
Declaratively, an action domain in our language corresponds to a nonmonotonic
causal theory in the situation calculus. Procedurally, such an action domain is
compiled into a set of logical theories, one for each action in the domain,
from which fully instantiated successor state-like axioms and STRIPS-like
systems are then generated. We expect the system to be a useful tool for
knowledge engineers writing action specifications for classical AI planning
systems, GOLOG systems, and other systems where formal specifications of
actions are needed.",N/A
VHPOP: Versatile Heuristic Partial Order Planner,"VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.
It draws from the experience gained in the early to mid 1990's on flaw
selection strategies for POCL planning, and combines this with more recent
developments in the field of domain independent planning such as distance based
heuristics and reachability analysis. We present an adaptation of the additive
heuristic for plan space planning, and modify it to account for possible reuse
of existing actions in a plan. We also propose a large set of novel flaw
selection strategies, and show how these can help us solve more problems than
previously possible by POCL planners. VHPOP also supports planning with
durative actions by incorporating standard techniques for temporal constraint
reasoning. We demonstrate that the same heuristic techniques used to boost the
performance of classical POCL planning can be effective in domains with
durative actions as well. The result is a versatile heuristic POCL planner
competitive with established CSP-based and heuristic state space planners.",N/A
SHOP2: An HTN Planning System,"The SHOP2 planning system received one of the awards for distinguished
performance in the 2002 International Planning Competition. This paper
describes the features of SHOP2 which enabled it to excel in the competition,
especially those aspects of SHOP2 that deal with temporal and metric planning
domains.",N/A
"An Architectural Approach to Ensuring Consistency in Hierarchical
  Execution","Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis.",N/A
Wrapper Maintenance: A Machine Learning Approach,"The proliferation of online information sources has led to an increased use
of wrappers for extracting data from Web sources. While most of the previous
research has focused on quick and efficient generation of wrappers, the
development of tools for wrapper maintenance has received less attention. This
is an important research problem because Web sources often change in ways that
prevent the wrappers from extracting data correctly. We present an efficient
algorithm that learns structural information about data from positive examples
alone. We describe how this information can be used for two wrapper maintenance
applications: wrapper verification and reinduction. The wrapper verification
system detects when a wrapper is not extracting correct data, usually because
the Web source has changed its format. The reinduction algorithm automatically
recovers from changes in the Web source by identifying data on Web pages so
that a new wrapper may be generated for this source. To validate our approach,
we monitored 27 wrappers over a period of a year. The verification algorithm
correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes,
resulting in precision of 0.73 and recall of 0.95. We validated the reinduction
algorithm on ten Web sources. We were able to successfully reinduce the
wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data
extraction task.",N/A
Exploiting Reputation in Distributed Virtual Environments,"The cognitive research on reputation has shown several interesting properties
that can improve both the quality of services and the security in distributed
electronic environments. In this paper, the impact of reputation on
decision-making under scarcity of information will be shown. First, a cognitive
theory of reputation will be presented, then a selection of simulation
experimental results from different studies will be discussed. Such results
concern the benefits of reputation when agents need to find out good sellers in
a virtual market-place under uncertainty and informational cheating.",N/A
The All Relevant Feature Selection using Random Forest,"In this paper we examine the application of the random forest classifier for
the all relevant feature selection problem. To this end we first examine two
recently proposed all relevant feature selection algorithms, both being a
random forest wrappers, on a series of synthetic data sets with varying size.
We show that reasonable accuracy of predictions can be achieved and that
heuristic algorithms that were designed to handle the all relevant problem,
have performance that is close to that of the reference ideal algorithm. Then,
we apply one of the algorithms to four families of semi-synthetic data sets to
assess how the properties of particular data set influence results of feature
selection. Finally we test the procedure using a well-known gene expression
data set. The relevance of nearly all previously established important genes
was confirmed, moreover the relevance of several new ones is discovered.",N/A
Structure and Complexity in Planning with Unary Operators,"Unary operator domains -- i.e., domains in which operators have a single
effect -- arise naturally in many control problems. In its most general form,
the problem of STRIPS planning in unary operator domains is known to be as hard
as the general STRIPS planning problem -- both are PSPACE-complete. However,
unary operator domains induce a natural structure, called the domain's causal
graph. This graph relates between the preconditions and effect of each domain
operator. Causal graphs were exploited by Williams and Nayak in order to
analyze plan generation for one of the controllers in NASA's Deep-Space One
spacecraft. There, they utilized the fact that when this graph is acyclic, a
serialization ordering over any subgoal can be obtained quickly. In this paper
we conduct a comprehensive study of the relationship between the structure of a
domain's causal graph and the complexity of planning in this domain. On the
positive side, we show that a non-trivial polynomial time plan generation
algorithm exists for domains whose causal graph induces a polytree with a
constant bound on its node indegree. On the negative side, we show that even
plan existence is hard when the graph is a directed-path singly connected DAG.
More generally, we show that the number of paths in the causal graph is closely
related to the complexity of planning in the associated domain. Finally we
relate our results to the question of complexity of planning with serializable
subgoals.",N/A
Answer Set Planning Under Action Costs,"Recently, planning based on answer set programming has been proposed as an
approach towards realizing declarative planning systems. In this paper, we
present the language Kc, which extends the declarative planning language K by
action costs. Kc provides the notion of admissible and optimal plans, which are
plans whose overall action costs are within a given limit resp. minimum over
all plans (i.e., cheapest plans). As we demonstrate, this novel language allows
for expressing some nontrivial planning tasks in a declarative way.
Furthermore, it can be utilized for representing planning problems under other
optimality criteria, such as computing ``shortest'' plans (with the least
number of steps), and refinement combinations of cheapest and fastest plans. We
study complexity aspects of the language Kc and provide a transformation to
logic programs, such that planning problems are solved via answer set
programming. Furthermore, we report experimental results on selected problems.
Our experience is encouraging that answer set planning may be a valuable
approach to expressive planning systems in which intricate planning problems
can be naturally specified and solved.",N/A
Learning to Coordinate Efficiently: A Model-based Approach,"In common-interest stochastic games all players receive an identical payoff.
Players participating in such games must learn to coordinate with each other in
order to receive the highest-possible value. A number of reinforcement learning
algorithms have been proposed for this problem, and some have been shown to
converge to good solutions in the limit. In this paper we show that using very
simple model-based algorithms, much better (i.e., polynomial) convergence rates
can be attained. Moreover, our model-based algorithms are guaranteed to
converge to the optimal value, unlike many of the existing algorithms.",N/A
SAPA: A Multi-objective Metric Temporal Planner,"SAPA is a domain-independent heuristic forward chaining planner that can
handle durative actions, metric resource constraints, and deadline goals. It is
designed to be capable of handling the multi-objective nature of metric
temporal planning. Our technical contributions include (i) planning-graph based
methods for deriving heuristics that are sensitive to both cost and makespan
(ii) techniques for adjusting the heuristic estimates to take action
interactions and metric resource limitations into account and (iii) a linear
time greedy post-processing technique to improve execution flexibility of the
solution plans. An implementation of SAPA using many of the techniques
presented in this paper was one of the best domain independent planners for
domains with metric and temporal constraints in the third International
Planning Competition, held at AIPS-02. We describe the technical details of
extracting the heuristics and present an empirical evaluation of the current
implementation of SAPA.",N/A
"A New General Method to Generate Random Modal Formulae for Testing
  Decision Procedures","The recent emergence of heavily-optimized modal decision procedures has
highlighted the key role of empirical testing in this domain. Unfortunately,
the introduction of extensive empirical tests for modal logics is recent, and
so far none of the proposed test generators is very satisfactory. To cope with
this fact, we present a new random generation method that provides benefits
over previous methods for generating empirical tests. It fixes and much
generalizes one of the best-known methods, the random CNF_[]m test, allowing
for generating a much wider variety of problems, covering in principle the
whole input space. Our new method produces much more suitable test sets for the
current generation of modal decision procedures. We analyze the features of the
new method by means of an extensive collection of empirical tests.",N/A
AltAltp: Online Parallelization of Plans with Heuristic State Search,"Despite their near dominance, heuristic state search planners still lag
behind disjunctive planners in the generation of parallel plans in classical
planning. The reason is that directly searching for parallel solutions in state
space planners would require the planners to branch on all possible subsets of
parallel actions, thus increasing the branching factor exponentially. We
present a variant of our heuristic state search planner AltAlt, called AltAltp
which generates parallel plans by using greedy online parallelization of
partial plans. The greedy approach is significantly informed by the use of
novel distance heuristics that AltAltp derives from a graphplan-style planning
graph for the problem. While this approach is not guaranteed to provide optimal
parallel plans, empirical results show that AltAltp is capable of generating
good quality parallel plans at a fraction of the cost incurred by the
disjunctive planners.",N/A
New Polynomial Classes for Logic-Based Abduction,"We address the problem of propositional logic-based abduction, i.e., the
problem of searching for a best explanation for a given propositional
observation according to a given propositional knowledge base. We give a
general algorithm, based on the notion of projection; then we study
restrictions over the representations of the knowledge base and of the query,
and find new polynomial classes of abduction problems.",N/A
"Planning Through Stochastic Local Search and Temporal Action Graphs in
  LPG","We present some techniques for planning in domains specified with the recent
standard language PDDL2.1, supporting 'durative actions' and numerical
quantities. These techniques are implemented in LPG, a domain-independent
planner that took part in the 3rd International Planning Competition (IPC). LPG
is an incremental, any time system producing multi-criteria quality plans. The
core of the system is based on a stochastic local search method and on a
graph-based representation called 'Temporal Action Graphs' (TA-graphs). This
paper focuses on temporal planning, introducing TA-graphs and proposing some
techniques to guide the search in LPG using this representation. The
experimental results of the 3rd IPC, as well as further results presented in
this paper, show that our techniques can be very effective. Often LPG
outperforms all other fully-automated planners of the 3rd IPC in terms of speed
to derive a solution, or quality of the solutions that can be produced.",N/A
TALplanner in IPC-2002: Extensions and Control Rules,"TALplanner is a forward-chaining planner that relies on domain knowledge in
the shape of temporal logic formulas in order to prune irrelevant parts of the
search space. TALplanner recently participated in the third International
Planning Competition, which had a clear emphasis on increasing the complexity
of the problem domains being used as benchmark tests and the expressivity
required to represent these domains in a planning system. Like many other
planners, TALplanner had support for some but not all aspects of this increase
in expressivity, and a number of changes to the planner were required. After a
short introduction to TALplanner, this article describes some of the changes
that were made before and during the competition. We also describe the process
of introducing suitable domain knowledge for several of the competition
domains.",N/A
"Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems
  On-Board","The automatic generation of decision trees based on off-line reasoning on
models of a domain is a reasonable compromise between the advantages of using a
model-based approach in technical domains and the constraints imposed by
embedded applications. In this paper we extend the approach to deal with
temporal information. We introduce a notion of temporal decision tree, which is
designed to make use of relevant information as long as it is acquired, and we
present an algorithm for compiling such trees from a model-based reasoning
system.",N/A
"Optimal Schedules for Parallelizing Anytime Algorithms: The Case of
  Shared Resources","The performance of anytime algorithms can be improved by simultaneously
solving several instances of algorithm-problem pairs. These pairs may include
different instances of a problem (such as starting from a different initial
state), different algorithms (if several alternatives exist), or several runs
of the same algorithm (for non-deterministic algorithms). In this paper we
present a methodology for designing an optimal scheduling policy based on the
statistical characteristics of the algorithms involved. We formally analyze the
case where the processes share resources (a single-processor model), and
provide an algorithm for optimal scheduling. We analyze, theoretically and
empirically, the behavior of our scheduling algorithm for various distribution
types. Finally, we present empirical results of applying our scheduling
algorithm to the Latin Square problem.",N/A
"Decision-Theoretic Bidding Based on Learned Density Models in
  Simultaneous, Interacting Auctions","Auctions are becoming an increasingly popular method for transacting
business, especially over the Internet. This article presents a general
approach to building autonomous bidding agents to bid in multiple simultaneous
auctions for interacting goods. A core component of our approach learns a model
of the empirical price dynamics based on past data and uses the model to
analytically calculate, to the greatest extent possible, optimal bids. We
introduce a new and general boosting-based algorithm for conditional density
estimation problems of this kind, i.e., supervised learning problems in which
the goal is to estimate the entire conditional distribution of the real-valued
label. This approach is fully implemented as ATTac-2001, a top-scoring agent in
the second Trading Agent Competition (TAC-01). We present experiments
demonstrating the effectiveness of our boosting-based price predictor relative
to several reasonable alternatives.",N/A
"The Metric-FF Planning System: Translating ""Ignoring Delete Lists"" to
  Numeric State Variables","Planning with numeric state variables has been a challenge for many years,
and was a part of the 3rd International Planning Competition (IPC-3). Currently
one of the most popular and successful algorithmic techniques in STRIPS
planning is to guide search by a heuristic function, where the heuristic is
based on relaxing the planning task by ignoring the delete lists of the
available actions. We present a natural extension of ``ignoring delete lists''
to numeric state variables, preserving the relevant theoretical properties of
the STRIPS relaxation under the condition that the numeric task at hand is
``monotonic''. We then identify a subset of the numeric IPC-3 competition
language, ``linear tasks'', where monotonicity can be achieved by
pre-processing. Based on that, we extend the algorithms used in the heuristic
planning system FF to linear tasks. The resulting system Metric-FF is,
according to the IPC-3 results which we discuss, one of the two currently most
efficient numeric planners.",N/A
Manipulation of Nanson's and Baldwin's Rules,"Nanson's and Baldwin's voting rules select a winner by successively
eliminating candidates with low Borda scores. We show that these rules have a
number of desirable computational properties. In particular, with unweighted
votes, it is NP-hard to manipulate either rule with one manipulator, whilst
with weighted votes, it is NP-hard to manipulate either rule with a small
number of candidates and a coalition of manipulators. As only a couple of other
voting rules are known to be NP-hard to manipulate with a single manipulator,
Nanson's and Baldwin's rules appear to be particularly resistant to
manipulation from a theoretical perspective. We also propose a number of
approximation methods for manipulating these two rules. Experiments demonstrate
that both rules are often difficult to manipulate in practice. These results
suggest that elimination style voting rules deserve further study.",N/A
Theory and Algorithms for Partial Order Based Reduction in Planning,"Search is a major technique for planning. It amounts to exploring a state
space of planning domains typically modeled as a directed graph. However,
prohibitively large sizes of the search space make search expensive. Developing
better heuristic functions has been the main technique for improving search
efficiency. Nevertheless, recent studies have shown that improving heuristics
alone has certain fundamental limits on improving search efficiency. Recently,
a new direction of research called partial order based reduction (POR) has been
proposed as an alternative to improving heuristics. POR has shown promise in
speeding up searches.
  POR has been extensively studied in model checking research and is a key
enabling technique for scalability of model checking systems. Although the POR
theory has been extensively studied in model checking, it has never been
developed systematically for planning before. In addition, the conditions for
POR in the model checking theory are abstract and not directly applicable in
planning. Previous works on POR algorithms for planning did not establish the
connection between these algorithms and existing theory in model checking.
  In this paper, we develop a theory for POR in planning. The new theory we
develop connects the stubborn set theory in model checking and POR methods in
planning. We show that previous POR algorithms in planning can be explained by
the new theory. Based on the new theory, we propose a new, stronger POR
algorithm. Experimental results on various planning domains show further search
cost reduction using the new algorithm.",N/A
"A Comparison of Lex Bounds for Multiset Variables in Constraint
  Programming","Set and multiset variables in constraint programming have typically been
represented using subset bounds. However, this is a weak representation that
neglects potentially useful information about a set such as its cardinality.
For set variables, the length-lex (LL) representation successfully provides
information about the length (cardinality) and position in the lexicographic
ordering. For multiset variables, where elements can be repeated, we consider
richer representations that take into account additional information. We study
eight different representations in which we maintain bounds according to one of
the eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC),
length-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC)
orderings. These representations integrate together information about the
cardinality, variety (number of distinct elements in the multiset), and
position in some total ordering. Theoretical and empirical comparisons of
expressiveness and compactness of the eight representations suggest that
length-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually
give tighter bounds after constraint propagation. We implement the eight
representations and evaluate them against the subset bounds representation with
cardinality and variety reasoning. Results demonstrate that they offer
significantly better pruning and runtime.","7 pages, Proceedings of the Twenty-Fifth AAAI Conference on
  Artificial Intelligence (AAAI-11)"
The 3rd International Planning Competition: Results and Analysis,"This paper reports the outcome of the third in the series of biennial
international planning competitions, held in association with the International
Conference on AI Planning and Scheduling (AIPS) in 2002. In addition to
describing the domains, the planners and the objectives of the competition, the
paper includes analysis of the results. The results are analysed from several
perspectives, in order to address the questions of comparative performance
between planners, comparative difficulty of domains, the degree of agreement
between planners about the relative difficulty of individual problem instances
and the question of how well planners scale relative to one another over
increasingly difficult problems. The paper addresses these questions through
statistical analysis of the raw results of the competition, in order to
determine which results can be considered to be adequately supported by the
data. The paper concludes with a discussion of some challenges for the future
of the competition series.",N/A
"Use of Markov Chains to Design an Agent Bidding Strategy for Continuous
  Double Auctions","As computational agents are developed for increasingly complicated e-commerce
applications, the complexity of the decisions they face demands advances in
artificial intelligence techniques. For example, an agent representing a seller
in an auction should try to maximize the seller's profit by reasoning about a
variety of possibly uncertain pieces of information, such as the maximum prices
various buyers might be willing to pay, the possible prices being offered by
competing sellers, the rules by which the auction operates, the dynamic arrival
and matching of offers to buy and sell, and so on. A naive application of
multiagent reasoning techniques would require the seller's agent to explicitly
model all of the other agents through an extended time horizon, rendering the
problem intractable for many realistically-sized problems. We have instead
devised a new strategy that an agent can use to determine its bid price based
on a more tractable Markov chain model of the auction process. We have
experimentally identified the conditions under which our new strategy works
well, as well as how well it works in comparison to the optimal performance the
agent could have achieved had it known the future. Our results show that our
new strategy in general performs well, outperforming other tractable heuristic
strategies in a majority of experiments, and is particularly effective in a
'seller?s market', where many buy offers are available.",N/A
"Reasoning in the OWL 2 Full Ontology Language using First-Order
  Automated Theorem Proving","OWL 2 has been standardized by the World Wide Web Consortium (W3C) as a
family of ontology languages for the Semantic Web. The most expressive of these
languages is OWL 2 Full, but to date no reasoner has been implemented for this
language. Consistency and entailment checking are known to be undecidable for
OWL 2 Full. We have translated a large fragment of the OWL 2 Full semantics
into first-order logic, and used automated theorem proving systems to do
reasoning based on this theory. The results are promising, and indicate that
this approach can be applied in practice for effective OWL reasoning, beyond
the capabilities of current Semantic Web reasoners.
  This is an extended version of a paper with the same title that has been
published at CADE 2011, LNAI 6803, pp. 446-460. The extended version provides
appendices with additional resources that were used in the reported evaluation.",N/A
'Just Enough' Ontology Engineering,"This paper introduces 'just enough' principles and 'systems engineering'
approach to the practice of ontology development to provide a minimal yet
complete, lightweight, agile and integrated development process, supportive of
stakeholder management and implementation independence.",N/A
Conscious Machines and Consciousness Oriented Programming,"In this paper, we investigate the following question: how could you write
such computer programs that can work like conscious beings? The motivation
behind this question is that we want to create such applications that can see
the future. The aim of this paper is to provide an overall conceptual framework
for this new approach to machine consciousness. So we introduce a new
programming paradigm called Consciousness Oriented Programming (COP).","25 pages, 8 figures"
"A First Approach on Modelling Staff Proactiveness in Retail Simulation
  Models","There has been a noticeable shift in the relative composition of the industry
in the developed countries in recent years; manufacturing is decreasing while
the service sector is becoming more important. However, currently most
simulation models for investigating service systems are still built in the same
way as manufacturing simulation models, using a process-oriented world view,
i.e. they model the flow of passive entities through a system. These kinds of
models allow studying aspects of operational management but are not well suited
for studying the dynamics that appear in service systems due to human
behaviour. For these kinds of studies we require tools that allow modelling the
system and entities using an object-oriented world view, where intelligent
objects serve as abstract ""actors"" that are goal directed and can behave
proactively. In our work we combine process-oriented discrete event simulation
modelling and object-oriented agent based simulation modelling to investigate
the impact of people management practices on retail productivity. In this
paper, we reveal in a series of experiments what impact considering proactivity
can have on the output accuracy of simulation models of human centric systems.
The model and data we use for this investigation are based on a case study in a
UK department store. We show that considering proactivity positively influences
the validity of these kinds of models and therefore allows analysts to make
better recommendations regarding strategies to apply people management
practises.","25 pages, 3 figures, 10 tables"
"Reiter's Default Logic Is a Logic of Autoepistemic Reasoning And a Good
  One, Too","A fact apparently not observed earlier in the literature of nonmonotonic
reasoning is that Reiter, in his default logic paper, did not directly
formalize informal defaults. Instead, he translated a default into a certain
natural language proposition and provided a formalization of the latter. A few
years later, Moore noted that propositions like the one used by Reiter are
fundamentally different than defaults and exhibit a certain autoepistemic
nature. Thus, Reiter had developed his default logic as a formalization of
autoepistemic propositions rather than of defaults.
  The first goal of this paper is to show that some problems of Reiter's
default logic as a formal way to reason about informal defaults are directly
attributable to the autoepistemic nature of default logic and to the mismatch
between informal defaults and the Reiter's formal defaults, the latter being a
formal expression of the autoepistemic propositions Reiter used as a
representation of informal defaults.
  The second goal of our paper is to compare the work of Reiter and Moore.
While each of them attempted to formalize autoepistemic propositions, the modes
of reasoning in their respective logics were different. We revisit Moore's and
Reiter's intuitions and present them from the perspective of autotheoremhood,
where theories can include propositions referring to the theory's own theorems.
We then discuss the formalization of this perspective in the logics of Moore
and Reiter, respectively, using the unifying semantic framework for default and
autoepistemic logics that we developed earlier. We argue that Reiter's default
logic is a better formalization of Moore's intuitions about autoepistemic
propositions than Moore's own autoepistemic logic.","In G. Brewka, V.M. Marek, and M. Truszczynski, eds. Nonmonotonic
  Reasoning -- Essays Celebrating its 30th Anniversary, College Publications,
  2011 (a volume of papers presented at NonMOn at 30 meeting, Lexington, KY,
  USA, October 2010"
Revisiting Epistemic Specifications,"In 1991, Michael Gelfond introduced the language of epistemic specifications.
The goal was to develop tools for modeling problems that require some form of
meta-reasoning, that is, reasoning over multiple possible worlds. Despite their
relevance to knowledge representation, epistemic specifications have received
relatively little attention so far. In this paper, we revisit the formalism of
epistemic specification. We offer a new definition of the formalism, propose
several semantics (one of which, under syntactic restrictions we assume, turns
out to be equivalent to the original semantics by Gelfond), derive some
complexity results and, finally, show the effectiveness of the formalism for
modeling problems requiring meta-reasoning considered recently by Faber and
Woltran. All these results show that epistemic specifications deserve much more
attention that has been afforded to them so far.","In Marcello Balduccini and Tran Cao Son, Editors, Essays Dedicated to
  Michael Gelfond on the Occasion of His 65th Birthday, Lexington, KY, USA,
  October 2010, LNAI 6565, Springer"
"Origins of Answer-Set Programming - Some Background And Two Personal
  Accounts","We discuss the evolution of aspects of nonmonotonic reasoning towards the
computational paradigm of answer-set programming (ASP). We give a general
overview of the roots of ASP and follow up with the personal perspective on
research developments that helped verbalize the main principles of ASP and
differentiated it from the classical logic programming.","In G. Brewka, V.M. Marek, and M. Truszczynski, eds. Nonmonotonic
  Reasoning -- Essays Celebrating its 30th Anniversary, College Publications,
  2011 (a volume of papers presented at NonMon at 30 meeting, Lexington, KY,
  USA, October 2010)"
Doing Better Than UCT: Rational Monte Carlo Sampling in Trees,"UCT, a state-of-the art algorithm for Monte Carlo tree sampling (MCTS), is
based on UCB, a sampling policy for the Multi-armed Bandit Problem (MAB) that
minimizes the accumulated regret. However, MCTS differs from MAB in that only
the final choice, rather than all arm pulls, brings a reward, that is, the
simple regret, as opposite to the cumulative regret, must be minimized. This
ongoing work aims at applying meta-reasoning techniques to MCTS, which is
non-trivial. We begin by introducing policies for multi-armed bandits with
lower simple regret than UCB, and an algorithm for MCTS which combines
cumulative and simple regret minimization and outperforms UCT. We also develop
a sampling scheme loosely based on a myopic version of perfect value of
information. Finite-time and asymptotic analysis of the policies is provided,
and the algorithms are compared empirically.","Withdrawn: ""MCTS Based on Simple Regret"" (arXiv:1207.5589) is the
  final corrected version published in AAAI 2012 proceedings"
"Self-Organizing Mixture Networks for Representation of Grayscale Digital
  Images","Self-Organizing Maps are commonly used for unsupervised learning purposes.
This paper is dedicated to the certain modification of SOM called SOMN
(Self-Organizing Mixture Networks) used as a mechanism for representing
grayscale digital images. Any grayscale digital image regarded as a
distribution function can be approximated by the corresponding Gaussian
mixture. In this paper, the use of SOMN is proposed in order to obtain such
approximations for input grayscale images in unsupervised manner.",N/A
Detection and emergence,"Two different conceptions of emergence are reconciled as two instances of the
phenomenon of detection. In the process of comparing these two conceptions, we
find that the notions of complexity and detection allow us to form a unified
definition of emergence that clearly delineates the role of the observer.",jld-98072401
dynPARTIX - A Dynamic Programming Reasoner for Abstract Argumentation,"The aim of this paper is to announce the release of a novel system for
abstract argumentation which is based on decomposition and dynamic programming.
We provide first experimental evaluations to show the feasibility of this
approach.","The paper appears in the Proceedings of the 19th International
  Conference on Applications of Declarative Programming and Knowledge
  Management (INAP 2011)"
"Making Use of Advances in Answer-Set Programming for Abstract
  Argumentation Systems","Dung's famous abstract argumentation frameworks represent the core formalism
for many problems and applications in the field of argumentation which
significantly evolved within the last decade. Recent work in the field has thus
focused on implementations for these frameworks, whereby one of the main
approaches is to use Answer-Set Programming (ASP). While some of the
argumentation semantics can be nicely expressed within the ASP language, others
required rather cumbersome encoding techniques. Recent advances in ASP systems,
in particular, the metasp optimization frontend for the ASP-package
gringo/claspD provides direct commands to filter answer sets satisfying certain
subset-minimality (or -maximality) constraints. This allows for much simpler
encodings compared to the ones in standard ASP language. In this paper, we
experimentally compare the original encodings (for the argumentation semantics
based on preferred, semi-stable, and respectively, stage extensions) with new
metasp encodings. Moreover, we provide novel encodings for the recently
introduced resolution-based grounded semantics. Our experimental results
indicate that the metasp approach works well in those cases where the
complexity of the encoded problem is adequately mirrored within the metasp
approach.","Paper appears in the Proceedings of the 19th International Conference
  on Applications of Declarative Programming and Knowledge Management (INAP
  2011)"
"Verbal Characterization of Probabilistic Clusters using Minimal
  Discriminative Propositions","In a knowledge discovery process, interpretation and evaluation of the mined
results are indispensable in practice. In the case of data clustering, however,
it is often difficult to see in what aspect each cluster has been formed. This
paper proposes a method for automatic and objective characterization or
""verbalization"" of the clusters obtained by mixture models, in which we collect
conjunctions of propositions (attribute-value pairs) that help us interpret or
evaluate the clusters. The proposed method provides us with a new, in-depth and
consistent tool for cluster interpretation/evaluation, and works for various
types of datasets including continuous attributes and missing values.
Experimental results with a couple of standard datasets exhibit the utility of
the proposed method, and the importance of the feedbacks from the
interpretation/evaluation step.","13 pages including 3 figures. This is the full version of a paper at
  ICTAI-2011 (http://www.cse.fau.edu/ictai2011/)"
"Single-trial EEG Discrimination between Wrist and Finger Movement
  Imagery and Execution in a Sensorimotor BCI","A brain-computer interface (BCI) may be used to control a prosthetic or
orthotic hand using neural activity from the brain. The core of this
sensorimotor BCI lies in the interpretation of the neural information extracted
from electroencephalogram (EEG). It is desired to improve on the interpretation
of EEG to allow people with neuromuscular disorders to perform daily
activities. This paper investigates the possibility of discriminating between
the EEG associated with wrist and finger movements. The EEG was recorded from
test subjects as they executed and imagined five essential hand movements using
both hands. Independent component analysis (ICA) and time-frequency techniques
were used to extract spectral features based on event-related
(de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used
for feature reduction. Mahalanobis distance (MD) clustering and artificial
neural networks (ANN) were used as classifiers and obtained average accuracies
of 65 % and 71 % respectively. This shows that EEG discrimination between wrist
and finger movements is possible. The research introduces a new combination of
motor tasks to BCI research.",33rd Annual International IEEE EMBS Conference 2011
FdConfig: A Constraint-Based Interactive Product Configurator,"We present a constraint-based approach to interactive product configuration.
Our configurator tool FdConfig is based on feature models for the
representation of the product domain. Such models can be directly mapped into
constraint satisfaction problems and dealt with by appropriate constraint
solvers. During the interactive configuration process the user generates new
constraints as a result of his configuration decisions and even may retract
constraints posted earlier. We discuss the configuration process, explain the
underlying techniques and show optimizations.","19th International Conference on Applications of Declarative
  Programming and Knowledge Management (INAP 2011)"
Nested HEX-Programs,"Answer-Set Programming (ASP) is an established declarative programming
paradigm. However, classical ASP lacks subprogram calls as in procedural
programming, and access to external computations (like remote procedure calls)
in general. The feature is desired for increasing modularity and---assuming
proper access in place---(meta-)reasoning over subprogram results. While
HEX-programs extend classical ASP with external source access, they do not
support calls of (sub-)programs upfront. We present nested HEX-programs, which
extend HEX-programs to serve the desired feature, in a user-friendly manner.
Notably, the answer sets of called sub-programs can be individually accessed.
This is particularly useful for applications that need to reason over answer
sets like belief set merging, user-defined aggregate functions, or preferences
of answer sets.","Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011)"
Structure Selection from Streaming Relational Data,"Statistical relational learning techniques have been successfully applied in
a wide range of relational domains. In most of these applications, the human
designers capitalized on their background knowledge by following a
trial-and-error trajectory, where relational features are manually defined by a
human engineer, parameters are learned for those features on the training data,
the resulting model is validated, and the cycle repeats as the engineer adjusts
the set of features. This paper seeks to streamline application development in
large relational domains by introducing a light-weight approach that
efficiently evaluates relational features on pieces of the relational graph
that are streamed to it one at a time. We evaluate our approach on two social
media tasks and demonstrate that it leads to more accurate models that are
learned faster.",N/A
"A Constraint Logic Programming Approach for Computing Ordinal
  Conditional Functions","In order to give appropriate semantics to qualitative conditionals of the
form ""if A then normally B"", ordinal conditional functions (OCFs) ranking the
possible worlds according to their degree of plausibility can be used. An OCF
accepting all conditionals of a knowledge base R can be characterized as the
solution of a constraint satisfaction problem. We present a high-level,
declarative approach using constraint logic programming techniques for solving
this constraint satisfaction problem. In particular, the approach developed
here supports the generation of all minimal solutions; these minimal solutions
are of special interest as they provide a basis for model-based inference from
R.","To appear in the Proceedings of the 25th Workshop on Logic
  Programming (WLP 2011)"
"Confidentiality-Preserving Data Publishing for Credulous Users by
  Extended Abduction","Publishing private data on external servers incurs the problem of how to
avoid unwanted disclosure of confidential data. We study a problem of
confidentiality in extended disjunctive logic programs and show how it can be
solved by extended abduction. In particular, we analyze how credulous
non-monotonic reasoning affects confidentiality.","Paper appears in the Proceedings of the 19th International Conference
  on Applications of Declarative Programming and Knowledge Management (INAP
  2011)"
"Domain-specific Languages in a Finite Domain Constraint Programming
  System","In this paper, we present domain-specific languages (DSLs) that we devised
for their use in the implementation of a finite domain constraint programming
system, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs
are used in propagator selection and constraint reification. In these areas,
they lead to concise specifications that are easy to read and reason about. At
compilation time, these specifications are translated to Prolog code, reducing
interpretative run-time overheads. The devised languages can be used in the
implementation of other finite domain constraint solvers as well and may
contribute to their correctness, conciseness and efficiency.","Proceedings of the 19th International Conference on Applications of
  Declarative Programming and Knowledge Management (INAP 2011)"
Coprocessor - a Standalone SAT Preprocessor,"In this work a stand-alone preprocessor for SAT is presented that is able to
perform most of the known preprocessing techniques. Preprocessing a formula in
SAT is important for performance since redundancy can be removed. The
preprocessor is part of the SAT solver riss and is called Coprocessor. Not only
riss, but also MiniSat 2.2 benefit from it, because the SatELite preprocessor
of MiniSat does not implement recent techniques. By using more advanced
techniques, Coprocessor is able to reduce the redundancy in a formula further
and improves the overall solving performance.","system description, short paper, WLP 2011"
"A Combinatorial Optimisation Approach to Designing Dual-Parented
  Long-Reach Passive Optical Networks","We present an application focused on the design of resilient long-reach
passive optical networks. We specifically consider dual-parented networks
whereby each customer must be connected to two metro sites via local exchange
sites. An important property of such a placement is resilience to single metro
node failure. The objective of the application is to determine the optimal
position of a set of metro nodes such that the total optical fibre length is
minimized. We prove that this problem is NP-Complete. We present two
alternative combinatorial optimisation approaches to finding an optimal metro
node placement using: a mixed integer linear programming (MIP) formulation of
the problem; and, a hybrid approach that uses clustering as a preprocessing
step. We consider a detailed case-study based on a network for Ireland. The
hybrid approach scales well and finds solutions that are close to optimal, with
a runtime that is two orders-of-magnitude better than the MIP model.","University of Ulster, Intelligent System Research Centre, technical
  report series. ISSN 2041-6407"
Measuring Intelligence through Games,"Artificial general intelligence (AGI) refers to research aimed at tackling
the full problem of artificial intelligence, that is, create truly intelligent
agents. This sets it apart from most AI research which aims at solving
relatively narrow domains, such as character recognition, motion planning, or
increasing player satisfaction in games. But how do we know when an agent is
truly intelligent? A common point of reference in the AGI community is Legg and
Hutter's formal definition of universal intelligence, which has the appeal of
simplicity and generality but is unfortunately incomputable. Games of various
kinds are commonly used as benchmarks for ""narrow"" AI research, as they are
considered to have many important properties. We argue that many of these
properties carry over to the testing of general intelligence as well. We then
sketch how such testing could practically be carried out. The central part of
this sketch is an extension of universal intelligence to deal with finite time,
and the use of sampling of the space of games expressed in a suitably biased
game description language.",N/A
Structured Knowledge Representation for Image Retrieval,"We propose a structured approach to the problem of retrieval of images by
content and present a description logic that has been devised for the semantic
indexing and retrieval of images containing complex objects. As other
approaches do, we start from low-level features extracted with image analysis
to detect and characterize regions in an image. However, in contrast with
feature-based approaches, we provide a syntax to describe segmented regions as
basic objects and complex objects as compositions of basic ones. Then we
introduce a companion extensional semantics for defining reasoning services,
such as retrieval, classification, and subsumption. These services can be used
for both exact and approximate matching, using similarity measures. Using our
logical approach as a formal specification, we implemented a complete
client-server image retrieval system, which allows a user to pose both queries
by sketch and queries by example. A set of experiments has been carried out on
a testbed of images to assess the retrieval capabilities of the system in
comparison with expert users ranking. Results are presented adopting a
well-established measure of quality borrowed from textual information
retrieval.",N/A
"Predicting the Energy Output of Wind Farms Based on Weather Data:
  Important Variables and their Correlation","Wind energy plays an increasing role in the supply of energy world-wide. The
energy output of a wind farm is highly dependent on the weather condition
present at the wind farm. If the output can be predicted more accurately,
energy suppliers can coordinate the collaborative production of different
energy sources more efficiently to avoid costly overproductions.
  With this paper, we take a computer science perspective on energy prediction
based on weather data and analyze the important parameters as well as their
correlation on the energy output. To deal with the interaction of the different
parameters we use symbolic regression based on the genetic programming tool
DataModeler.
  Our studies are carried out on publicly available weather and energy data for
a wind farm in Australia. We reveal the correlation of the different variables
for the energy output. The model obtained for energy prediction gives a very
reliable prediction of the energy output for newly given weather data.","13 pages, 11 figures, 2 tables"
"The path inference filter: model-based low-latency map matching of probe
  vehicle data","We consider the problem of reconstructing vehicle trajectories from sparse
sequences of GPS points, for which the sampling interval is between 10 seconds
and 2 minutes. We introduce a new class of algorithms, called altogether path
inference filter (PIF), that maps GPS data in real time, for a variety of
trade-offs and scenarios, and with a high throughput. Numerous prior approaches
in map-matching can be shown to be special cases of the path inference filter
presented in this article. We present an efficient procedure for automatically
training the filter on new data, with or without ground truth observations. The
framework is evaluated on a large San Francisco taxi dataset and is shown to
improve upon the current state of the art. This filter also provides insights
about driving patterns of drivers. The path inference filter has been deployed
at an industrial scale inside the Mobile Millennium traffic information system,
and is used to map fleets of data in San Francisco, Sacramento, Stockholm and
Porto.","Preprint, 23 pages and 23 figures"
"An Expressive Language and Efficient Execution System for Software
  Agents","Software agents can be used to automate many of the tedious, time-consuming
information processing tasks that humans currently have to complete manually.
However, to do so, agent plans must be capable of representing the myriad of
actions and control flows required to perform those tasks. In addition, since
these tasks can require integrating multiple sources of remote information ?
typically, a slow, I/O-bound process ? it is desirable to make execution as
efficient as possible. To address both of these needs, we present a flexible
software agent plan language and a highly parallel execution system that enable
the efficient execution of expressive agent plans. The plan language allows
complex tasks to be more easily expressed by providing a variety of operators
for flexibly processing the data as well as supporting subplans (for
modularity) and recursion (for indeterminate looping). The executor is based on
a streaming dataflow model of execution to maximize the amount of operator and
data parallelism possible at runtime. We have implemented both the language and
executor in a system called THESEUS. Our results from testing THESEUS show that
streaming dataflow execution can yield significant speedups over both
traditional serial (von Neumann) as well as non-streaming dataflow-style
execution that existing software and robot agent execution systems currently
support. In addition, we show how plans written in the language we present can
represent certain types of subtasks that cannot be accomplished using the
languages supported by network query engines. Finally, we demonstrate that the
increased expressivity of our plan language does not hamper performance;
specifically, we show how data can be integrated from multiple remote sources
just as efficiently using our architecture as is possible with a
state-of-the-art streaming-dataflow network query engine.",N/A
"Structure-Based Local Search Heuristics for Circuit-Level Boolean
  Satisfiability","This work focuses on improving state-of-the-art in stochastic local search
(SLS) for solving Boolean satisfiability (SAT) instances arising from
real-world industrial SAT application domains. The recently introduced SLS
method CRSat has been shown to noticeably improve on previously suggested SLS
techniques in solving such real-world instances by combining
justification-based local search with limited Boolean constraint propagation on
the non-clausal formula representation form of Boolean circuits. In this work,
we study possibilities of further improving the performance of CRSat by
exploiting circuit-level structural knowledge for developing new search
heuristics for CRSat. To this end, we introduce and experimentally evaluate a
variety of search heuristics, many of which are motivated by circuit-level
heuristics originally developed in completely different contexts, e.g., for
electronic design automation applications. To the best of our knowledge, most
of the heuristics are novel in the context of SLS for SAT and, more generally,
SLS for constraint satisfaction problems.",15 pages
"Integrating Learning from Examples into the Search for Diagnostic
  Policies","This paper studies the problem of learning diagnostic policies from training
examples. A diagnostic policy is a complete description of the decision-making
actions of a diagnostician (i.e., tests followed by a diagnostic decision) for
all possible combinations of test results. An optimal diagnostic policy is one
that minimizes the expected total cost, which is the sum of measurement costs
and misdiagnosis costs. In most diagnostic settings, there is a tradeoff
between these two kinds of costs. This paper formalizes diagnostic decision
making as a Markov Decision Process (MDP). The paper introduces a new family of
systematic search algorithms based on the AO* algorithm to solve this MDP. To
make AO* efficient, the paper describes an admissible heuristic that enables
AO* to prune large parts of the search space. The paper also introduces several
greedy algorithms including some improvements over previously-published
methods. The paper then addresses the question of learning diagnostic policies
from examples. When the probabilities of diseases and test results are computed
from training data, there is a great danger of overfitting. To reduce
overfitting, regularizers are integrated into the search algorithms. Finally,
the paper compares the proposed methods on five benchmark diagnostic data sets.
The studies show that in most cases the systematic search methods produce
better diagnostic policies than the greedy methods. In addition, the studies
show that for training sets of realistic size, the systematic search algorithms
are practical on todays desktop computers.",N/A
"On the Practical use of Variable Elimination in Constraint Optimization
  Problems: 'Still-life' as a Case Study","Variable elimination is a general technique for constraint processing. It is
often discarded because of its high space complexity. However, it can be
extremely useful when combined with other techniques. In this paper we study
the applicability of variable elimination to the challenging problem of finding
still-lifes. We illustrate several alternatives: variable elimination as a
stand-alone algorithm, interleaved with search, and as a source of good quality
lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve
the n=20 instance, which is far beyond reach with alternative approaches.",N/A
Generalizing Boolean Satisfiability II: Theory,"This is the second of three planned papers describing ZAP, a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper presents
the theoretical basis for the ideas underlying ZAP, arguing that existing ideas
in this area exploit a single, recurring structure in that multiple database
axioms can be obtained by operating on a single axiom using a subgroup of the
group of permutations on the literals in the problem. We argue that the group
structure precisely captures the general structure at which earlier approaches
hinted, and give numerous examples of its use. We go on to extend the
Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and
show that earlier computational improvements are either subsumed or left intact
by the new method. The third paper in this series discusses ZAPs implementation
and presents experimental performance results.",N/A
Relational Dynamic Bayesian Networks,"Stochastic processes that involve the creation of objects and relations over
time are widespread, but relatively poorly studied. For example, accurate fault
diagnosis in factory assembly processes requires inferring the probabilities of
erroneous assembly operations, but doing this efficiently and accurately is
difficult. Modeled as dynamic Bayesian networks, these processes have discrete
variables with very large domains and extremely high dimensionality. In this
paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an
extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a
generalization of dynamic probabilistic relational models (DPRMs), which we had
proposed in our previous work to model dynamic uncertain domains. We first
extend the Rao-Blackwellised particle filtering described in our earlier work
to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in
RDBNs and propose two new forms of particle filtering. The first one uses
abstraction hierarchies over the predicates to smooth the particle filters
estimates. The second employs kernel density estimation with a kernel function
specifically designed for relational domains. Experiments show these two
methods greatly outperform standard particle filtering on the task of assembly
plan execution monitoring.",N/A
Reasoning about Action: An Argumentation - Theoretic Approach,"We present a uniform non-monotonic solution to the problems of reasoning
about action on the basis of an argumentation-theoretic approach. Our theory is
provably correct relative to a sensible minimisation policy introduced on top
of a temporal propositional logic. Sophisticated problem domains can be
formalised in our framework. As much attention of researchers in the field has
been paid to the traditional and basic problems in reasoning about actions such
as the frame, the qualification and the ramification problems, approaches to
these problems within our formalisation lie at heart of the expositions
presented in this paper.",N/A
Solving Set Constraint Satisfaction Problems using ROBDDs,"In this paper we present a new approach to modeling finite set domain
constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We
show that it is possible to construct an efficient set domain propagator which
compactly represents many set domains and set constraints using ROBDDs. We
demonstrate that the ROBDD-based approach provides unprecedented flexibility in
modeling constraint satisfaction problems, leading to performance improvements.
We also show that the ROBDD-based modeling approach can be extended to the
modeling of integer and multiset constraint problems in a straightforward
manner. Since domain propagation is not always practical, we also show how to
incorporate less strict consistency notions into the ROBDD framework, such as
set bounds, cardinality bounds and lexicographic bounds consistency. Finally,
we present experimental results that demonstrate the ROBDD-based solver
performs better than various more conventional constraint solvers on several
standard set constraint problems.",N/A
"Learning Concept Hierarchies from Text Corpora using Formal Concept
  Analysis","We present a novel approach to the automatic acquisition of taxonomies or
concept hierarchies from a text corpus. The approach is based on Formal Concept
Analysis (FCA), a method mainly used for the analysis of data, i.e. for
investigating and processing explicitly given information. We follow Harris
distributional hypothesis and model the context of a certain term as a vector
representing syntactic dependencies which are automatically acquired from the
text corpus with a linguistic parser. On the basis of this context information,
FCA produces a lattice that we convert into a special kind of partial order
constituting a concept hierarchy. The approach is evaluated by comparing the
resulting concept hierarchies with hand-crafted taxonomies for two domains:
tourism and finance. We also directly compare our approach with hierarchical
agglomerative clustering as well as with Bi-Section-KMeans as an instance of a
divisive clustering algorithm. Furthermore, we investigate the impact of using
different measures weighting the contribution of each attribute as well as of
applying a particular smoothing technique to cope with data sparseness.",N/A
Generalizing Boolean Satisfiability III: Implementation,"This is the third of three papers describing ZAP, a satisfiability engine
that substantially generalizes existing tools while retaining the performance
characteristics of modern high-performance solvers. The fundamental idea
underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal has been to define a representation in which this structure is apparent
and can be exploited to improve computational performance. The first paper
surveyed existing work that (knowingly or not) exploited problem structure to
improve the performance of satisfiability engines, and the second paper showed
that this structure could be understood in terms of groups of permutations
acting on individual clauses in any particular Boolean theory. We conclude the
series by discussing the techniques needed to implement our ideas, and by
reporting on their performance on a variety of problem instances.",N/A
Ignorability in Statistical and Probabilistic Inference,"When dealing with incomplete data in statistical learning, or incomplete
observations in probabilistic inference, one needs to distinguish the fact that
a certain event is observed from the fact that the observed event has happened.
Since the modeling and computational complexities entailed by maintaining this
proper distinction are often prohibitive, one asks for conditions under which
it can be safely ignored. Such conditions are given by the missing at random
(mar) and coarsened at random (car) assumptions. In this paper we provide an
in-depth analysis of several questions relating to mar/car assumptions. Main
purpose of our study is to provide criteria by which one may evaluate whether a
car assumption is reasonable for a particular data collecting or observational
process. This question is complicated by the fact that several distinct
versions of mar/car assumptions exist. We therefore first provide an overview
over these different versions, in which we highlight the distinction between
distributional and coarsening variable induced versions. We show that
distributional versions are less restrictive and sufficient for most
applications. We then address from two different perspectives the question of
when the mar/car assumption is warranted. First we provide a static analysis
that characterizes the admissibility of the car assumption in terms of the
support structure of the joint probability distribution of complete data and
incomplete observations. Here we obtain an equivalence characterization that
improves and extends a recent result by Grunwald and Halpern. We then turn to a
procedural analysis that characterizes the admissibility of the car assumption
in terms of procedural models for the actual data (or observation) generating
process. The main result of this analysis is that the stronger coarsened
completely at random (ccar) condition is arguably the most reasonable
assumption, as it alone corresponds to data coarsening procedures that satisfy
a natural robustness property.",N/A
Perseus: Randomized Point-based Value Iteration for POMDPs,"Partially observable Markov decision processes (POMDPs) form an attractive
and principled framework for agent planning under uncertainty. Point-based
approximate techniques for POMDPs compute a policy based on a finite set of
points collected in advance from the agents belief space. We present a
randomized point-based value iteration algorithm called Perseus. The algorithm
performs approximate value backup stages, ensuring that in each backup stage
the value of each point in the belief set is improved; the key observation is
that a single backup may improve the value of many belief points. Contrary to
other point-based methods, Perseus backs up only a (randomly selected) subset
of points in the belief set, sufficient for improving the value of each belief
point in the set. We show how the same idea can be extended to dealing with
continuous action spaces. Experimental results show the potential of Perseus in
large scale POMDP problems.",N/A
Logical Hidden Markov Models,"Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov
models to deal with sequences of structured symbols in the form of logical
atoms, rather than flat characters.
  This note formally introduces LOHMMs and presents solutions to the three
central inference problems for LOHMMs: evaluation, most likely hidden state
sequence and parameter estimation. The resulting representation and algorithms
are experimentally evaluated on problems from the domain of bioinformatics.",N/A
mGPT: A Probabilistic Planner Based on Heuristic Search,"We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version, called mGPT,
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different,
independent, deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy.",N/A
"Macro-FF: Improving AI Planning with Automatically Learned
  Macro-Operators","Despite recent progress in AI planning, many benchmarks remain challenging
for current planners. In many domains, the performance of a planner can greatly
be improved by discovering and exploiting information about the domain
structure that is not explicitly encoded in the initial PDDL formulation. In
this paper we present and compare two automated methods that learn relevant
information from previous experience in a domain and use it to solve new
problem instances. Our methods share a common four-step strategy. First, a
domain is analyzed and structural information is extracted, then
macro-operators are generated based on the previously discovered structure. A
filtering and ranking procedure selects the most useful macro-operators.
Finally, the selected macros are used to speed up future searches. We have
successfully used such an approach in the fourth international planning
competition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art
planner FF 2.3 with support for two kinds of macro-operators, and with
engineering enhancements. We demonstrate the effectiveness of our ideas on
benchmarks from international planning competitions. Our results indicate a
large reduction in search effort in those complex domains where structural
information can be inferred.",N/A
Optiplan: Unifying IP-based and Graph-based Planning,"The Optiplan planning system is the first integer programming-based planner
that successfully participated in the international planning competition. This
engineering note describes the architecture of Optiplan and provides the
integer programming formulation that enabled it to perform reasonably well in
the competition. We also touch upon some recent developments that make integer
programming encodings significantly more competitive.",N/A
"Approximate Policy Iteration with a Policy Language Bias: Solving
  Relational Markov Decision Processes","We study an approach to policy selection for large relational Markov Decision
Processes (MDPs). We consider a variant of approximate policy iteration (API)
that replaces the usual value-function learning step with a learning step in
policy space. This is advantageous in domains where good policies are easier to
represent and learn than the corresponding value functions, which is often the
case for the relational MDPs we are interested in. In order to apply API to
such problems, we introduce a relational policy language and corresponding
learner. In addition, we introduce a new bootstrapping routine for goal-based
planning domains, based on random walks. Such bootstrapping is necessary for
many large relational MDPs, where reward is extremely sparse, as API is
ineffective in such domains when initialized with an uninformed policy. Our
experiments show that the resulting system is able to find good policies for a
number of classical planning domains and their stochastic variants by solving
them as extremely large relational MDPs. The experiments also point to some
limitations of our approach, suggesting future work.",N/A
"Linking Search Space Structure, Run-Time Dynamics, and Problem
  Difficulty: A Step Toward Demystifying Tabu Search","Tabu search is one of the most effective heuristics for locating high-quality
solutions to a diverse array of NP-hard combinatorial optimization problems.
Despite the widespread success of tabu search, researchers have a poor
understanding of many key theoretical aspects of this algorithm, including
models of the high-level run-time dynamics and identification of those search
space features that influence problem difficulty. We consider these questions
in the context of the job-shop scheduling problem (JSP), a domain where tabu
search algorithms have been shown to be remarkably effective. Previously, we
demonstrated that the mean distance between random local optima and the nearest
optimal solution is highly correlated with problem difficulty for a well-known
tabu search algorithm for the JSP introduced by Taillard. In this paper, we
discuss various shortcomings of this measure and develop a new model of problem
difficulty that corrects these deficiencies. We show that Taillards algorithm
can be modeled with high fidelity as a simple variant of a straightforward
random walk. The random walk model accounts for nearly all of the variability
in the cost required to locate both optimal and sub-optimal solutions to random
JSPs, and provides an explanation for differences in the difficulty of random
versus structured JSPs. Finally, we discuss and empirically substantiate two
novel predictions regarding tabu search algorithm behavior. First, the method
for constructing the initial solution is highly unlikely to impact the
performance of tabu search. Second, tabu tenure should be selected to be as
small as possible while simultaneously avoiding search stagnation; values
larger than necessary lead to significant degradations in performance.",N/A
Breaking Instance-Independent Symmetries In Exact Graph Coloring,"Code optimization and high level synthesis can be posed as constraint
satisfaction and optimization problems, such as graph coloring used in register
allocation. Graph coloring is also used to model more traditional CSPs relevant
to AI, such as planning, time-tabling and scheduling. Provably optimal
solutions may be desirable for commercial and defense applications.
Additionally, for applications such as register allocation and code
optimization, naturally-occurring instances of graph coloring are often small
and can be solved optimally. A recent wave of improvements in algorithms for
Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests
generic problem-reduction methods, rather than problem-specific heuristics,
because (1) heuristics may be upset by new constraints, (2) heuristics tend to
ignore structure, and (3) many relevant problems are provably inapproximable.
  Problem reductions often lead to highly symmetric SAT instances, and
symmetries are known to slow down SAT solvers. In this work, we compare several
avenues for symmetry breaking, in particular when certain kinds of symmetry are
present in all generated instances. Our focus on reducing CSPs to SAT allows us
to leverage recent dramatic improvement in SAT solvers and automatically
benefit from future progress. We can use a variety of black-box SAT solvers
without modifying their source code because our symmetry-breaking techniques
are static, i.e., we detect symmetries and add symmetry breaking predicates
(SBPs) during pre-processing.
  An important result of our work is that among the types of
instance-independent SBPs we studied and their combinations, the simplest and
least complete constructions are the most effective. Our experiments also
clearly indicate that instance-independent symmetries should mostly be
processed together with instance-specific symmetries rather than at the
specification level, contrary to what has been suggested in the literature.",N/A
Decision-Theoretic Planning with non-Markovian Rewards,"A decision process in which rewards depend on history rather than merely on
the current state is called a decision process with non-Markovian rewards
(NMRDP). In decision-theoretic planning, where many desirable behaviours are
more naturally expressed as properties of execution sequences rather than as
properties of states, NMRDPs form a more natural model than the commonly
adopted fully Markovian decision process (MDP) model. While the more tractable
solution methods developed for MDPs do not directly apply in the presence of
non-Markovian rewards, a number of solution methods for NMRDPs have been
proposed in the literature. These all exploit a compact specification of the
non-Markovian reward function in temporal logic, to automatically translate the
NMRDP into an equivalent MDP which is solved using efficient MDP solution
methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process
Planner), a software platform for the development and experimentation of
methods for decision-theoretic planning with non-Markovian rewards. The current
version of NMRDPP implements, under a single interface, a family of methods
based on existing as well as new approaches which we describe in detail. These
include dynamic programming, heuristic search, and structured methods. Using
NMRDPP, we compare the methods and identify certain problem features that
affect their performance. NMRDPPs treatment of non-Markovian rewards is
inspired by the treatment of domain-specific search control knowledge in the
TLPlan planner, which it incorporates as a special case. In the First
International Probabilistic Planning Competition, NMRDPP was able to compete
and perform well in both the domain-independent and hand-coded tracks, using
search control knowledge in the latter.",N/A
On Validating Boolean Optimizers,"Boolean optimization finds a wide range of application domains, that
motivated a number of different organizations of Boolean optimizers since the
mid 90s. Some of the most successful approaches are based on iterative calls to
an NP oracle, using either linear search, binary search or the identification
of unsatisfiable sub-formulas. The increasing use of Boolean optimizers in
practical settings raises the question of confidence in computed results. For
example, the issue of confidence is paramount in safety critical settings. One
way of increasing the confidence of the results computed by Boolean optimizers
is to develop techniques for validating the results. Recent work studied the
validation of Boolean optimizers based on branch-and-bound search. This paper
complements existing work, and develops methods for validating Boolean
optimizers that are based on iterative calls to an NP oracle. This entails
implementing solutions for validating both satisfiable and unsatisfiable
answers from the NP oracle. The work described in this paper can be applied to
a wide range of Boolean optimizers, that find application in Pseudo-Boolean
Optimization and in Maximum Satisfiability. Preliminary experimental results
indicate that the impact of the proposed method in overall performance is
negligible.",N/A
"On the use of reference points for the biobjective Inventory Routing
  Problem","The article presents a study on the biobjective inventory routing problem.
Contrary to most previous research, the problem is treated as a true
multi-objective optimization problem, with the goal of identifying
Pareto-optimal solutions. Due to the hardness of the problem at hand, a
reference point based optimization approach is presented and implemented into
an optimization and decision support system, which allows for the computation
of a true subset of the optimal outcomes. Experimental investigation involving
local search metaheuristics are conducted on benchmark data, and numerical
results are reported and analyzed.",N/A
Neigborhood Selection in Variable Neighborhood Search,"Variable neighborhood search (VNS) is a metaheuristic for solving
optimization problems based on a simple principle: systematic changes of
neighborhoods within the search, both in the descent to local minima and in the
escape from the valleys which contain them. Designing these neighborhoods and
applying them in a meaningful fashion is not an easy task. Moreover, an
appropriate order in which they are applied must be determined. In this paper
we attempt to investigate this issue. Assume that we are given an optimization
problem that is intended to be solved by applying the VNS scheme, how many and
which types of neighborhoods should be investigated and what could be
appropriate selection criteria to apply these neighborhoods. More specifically,
does it pay to ""look ahead"" (see, e.g., in the context of VNS and GRASP) when
attempting to switch from one neighborhood to another?",ISBN 978-88-900984-3-7
"A Characterization of the Combined Effects of Overlap and Imbalance on
  the SVM Classifier","In this paper we demonstrate that two common problems in Machine
Learning---imbalanced and overlapping data distributions---do not have
independent effects on the performance of SVM classifiers. This result is
notable since it shows that a model of either of these factors must account for
the presence of the other. Our study of the relationship between these problems
has lead to the discovery of a previously unreported form of ""covert""
overfitting which is resilient to commonly used empirical regularization
techniques. We demonstrate the existance of this covert phenomenon through
several methods based around the parametric regularization of trained SVMs. Our
findings in this area suggest a possible approach to quantifying overlap in
real world data sets.",N/A
"Contradiction measures and specificity degrees of basic belief
  assignments","In the theory of belief functions, many measures of uncertainty have been
introduced. However, it is not always easy to understand what these measures
really try to represent. In this paper, we re-interpret some measures of
uncertainty in the theory of belief functions. We present some interests and
drawbacks of the existing measures. On these observations, we introduce a
measure of contradiction. Therefore, we present some degrees of non-specificity
and Bayesianity of a mass. We propose a degree of specificity based on the
distance between a mass and its most specific associated mass. We also show how
to use the degree of specificity to measure the specificity of a fusion rule.
Illustrations on simple examples are given.",N/A
Learning where to Attend with Deep Architectures for Image Tracking,"We discuss an attentional model for simultaneous object tracking and
recognition that is driven by gaze data. Motivated by theories of perception,
the model consists of two interacting pathways: identity and control, intended
to mirror the what and where pathways in neuroscience models. The identity
pathway models object appearance and performs classification using deep
(factored)-Restricted Boltzmann Machines. At each point in time the
observations consist of foveated images, with decaying resolution toward the
periphery of the gaze. The control pathway models the location, orientation,
scale and speed of the attended object. The posterior distribution of these
states is estimated with particle filtering. Deeper in the control pathway, we
encounter an attentional mechanism that learns to select gazes so as to
minimize tracking uncertainty. Unlike in our previous work, we introduce gaze
selection strategies which operate in the presence of partial information and
on a continuous action space. We show that a straightforward extension of the
existing approach to the partial information setting results in poor
performance, and we propose an alternative method based on modeling the reward
surface as a Gaussian Process. This approach gives good performance in the
presence of partial information and allows us to expand the action space from a
small, discrete set of fixation points to a continuous domain.",N/A
Social choice rules driven by propositional logic,"Several rules for social choice are examined from a unifying point of view
that looks at them as procedures for revising a system of degrees of belief in
accordance with certain specified logical constraints. Belief is here a social
attribute, its degrees being measured by the fraction of people who share a
given opinion. Different known rules and some new ones are obtained depending
on which particular constraints are assumed. These constraints allow to model
different notions of choiceness. In particular, we give a new method to deal
with approval-disapproval-preferential voting.",The title has been changed
Explicit Approximations of the Gaussian Kernel,"We investigate training and using Gaussian kernel SVMs by approximating the
kernel with an explicit finite- dimensional polynomial feature representation
based on the Taylor expansion of the exponential. Although not as efficient as
the recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms
of the number of features, we show how this polynomial representation can
provide a better approximation in terms of the computational cost involved.
This makes our ""Taylor features"" especially attractive for use on very large
data sets, in conjunction with online or stochastic training.","11 pages, 2 tables, 2 figures"
"Analysis of first prototype universal intelligence tests: evaluating and
  comparing AI algorithms and humans","Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test.","114 pages, master thesis"
The Deterministic Part of IPC-4: An Overview,"We provide an overview of the organization and results of the deterministic
part of the 4th International Planning Competition, i.e., of the part concerned
with evaluating systems doing deterministic planning. IPC-4 attracted even more
competing systems than its already large predecessors, and the competition
event was revised in several important respects. After giving an introduction
to the IPC, we briefly explain the main differences between the deterministic
part of IPC-4 and its predecessors. We then introduce formally the language
used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed
initial literals. We list the competing systems and overview the results of the
competition. The entire set of data is far too large to be presented in full.
We provide a detailed summary; the complete data is available in an online
appendix. We explain how we awarded the competition prizes.",N/A
PDDL2.1 - The Art of the Possible? Commentary on Fox and Long,"PDDL2.1 was designed to push the envelope of what planning algorithms can do,
and it has succeeded. It adds two important features: durative actions,which
take time (and may have continuous effects); and objective functions for
measuring the quality of plans. The concept of durative actions is flawed; and
the treatment of their semantics reveals too strong an attachment to the way
many contemporary planners work. Future PDDL innovators should focus on
producing a clean semantics for additions to the language, and let planner
implementers worry about coupling their algorithms to problems expressed in the
latest version of the language.",N/A
The Case for Durative Actions: A Commentary on PDDL2.1,"The addition of durative actions to PDDL2.1 sparked some controversy. Fox and
Long argued that actions should be considered as instantaneous, but can start
and stop processes. Ultimately, a limited notion of durative actions was
incorporated into the language. I argue that this notion is still impoverished,
and that the underlying philosophical position of regarding durative actions as
being a shorthand for a start action, process, and stop action ignores the
realities of modelling and execution for complex systems.",N/A
Engineering a Conformant Probabilistic Planner,"We present a partial-order, conformant, probabilistic planner, Probapop which
competed in the blind track of the Probabilistic Planning Competition in IPC-4.
We explain how we adapt distance based heuristics for use with probabilistic
domains. Probapop also incorporates heuristics based on probability of success.
We explain the successes and difficulties encountered during the design and
implementation of Probapop.",N/A
"Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning
  Benchmarks","Between 1998 and 2004, the planning community has seen vast progress in terms
of the sizes of benchmark examples that domain-independent planners can tackle
successfully. The key technique behind this progress is the use of heuristic
functions based on relaxing the planning task at hand, where the relaxation is
to assume that all delete lists are empty. The unprecedented success of such
methods, in many commonly used benchmark examples, calls for an understanding
of what classes of domains these methods are well suited for. In the
investigation at hand, we derive a formal background to such an understanding.
We perform a case study covering a range of 30 commonly used STRIPS and ADL
benchmark domains, including all examples used in the first four international
planning competitions. We *prove* connections between domain structure and
local search topology -- heuristic cost surface properties -- under an
idealized version of the heuristic functions used in modern planners. The
idealized heuristic function is called h^+, and differs from the practically
used functions in that it returns the length of an *optimal* relaxed plan,
which is NP-hard to compute. We identify several key characteristics of the
topology under h^+, concerning the existence/non-existence of unrecognized dead
ends, as well as the existence/non-existence of constant upper bounds on the
difficulty of escaping local minima and benches. These distinctions divide the
(set of all) planning domains into a taxonomy of classes of varying h^+
topology. As it turns out, many of the 30 investigated domains lie in classes
with a relatively easy topology. Most particularly, 12 of the domains lie in
classes where FFs search algorithm, provided with h^+, is a polynomial solving
mechanism. We also present results relating h^+ to its approximation as
implemented in FF. The behavior regarding dead ends is provably the same. We
summarize the results of an empirical investigation showing that, in many
domains, the topological qualities of h^+ are largely inherited by the
approximation. The overall investigation gives a rare example of a successful
analysis of the connections between typical-case problem structure, and search
performance. The theoretical investigation also gives hints on how the
topological phenomena might be automatically recognizable by domain analysis
techniques. We outline some preliminary steps we made into that direction.",N/A
"Binary Encodings of Non-binary Constraint Satisfaction Problems:
  Algorithms and Experimental Results","A non-binary Constraint Satisfaction Problem (CSP) can be solved directly
using extended versions of binary techniques. Alternatively, the non-binary
problem can be translated into an equivalent binary one. In this case, it is
generally accepted that the translated problem can be solved by applying
well-established techniques for binary CSPs. In this paper we evaluate the
applicability of the latter approach. We demonstrate that the use of standard
techniques for binary CSPs in the encodings of non-binary problems is
problematic and results in models that are very rarely competitive with the
non-binary representation. To overcome this, we propose specialized arc
consistency and search algorithms for binary encodings, and we evaluate them
theoretically and empirically. We consider three binary representations; the
hidden variable encoding, the dual encoding, and the double encoding.
Theoretical and empirical results show that, for certain classes of non-binary
constraints, binary encodings are a competitive option, and in many cases, a
better one than the non-binary representation.",N/A
"Distributed Reasoning in a Peer-to-Peer Setting: Application to the
  Semantic Web","In a peer-to-peer inference system, each peer can reason locally but can also
solicit some of its acquaintances, which are peers sharing part of its
vocabulary. In this paper, we consider peer-to-peer inference systems in which
the local theory of each peer is a set of propositional clauses defined upon a
local vocabulary. An important characteristic of peer-to-peer inference systems
is that the global theory (the union of all peer theories) is not known (as
opposed to partition-based reasoning systems). The main contribution of this
paper is to provide the first consequence finding algorithm in a peer-to-peer
setting: DeCA. It is anytime and computes consequences gradually from the
solicited peer to peers that are more and more distant. We exhibit a sufficient
condition on the acquaintance graph of the peer-to-peer inference system for
guaranteeing the completeness of this algorithm. Another important contribution
is to apply this general distributed reasoning setting to the setting of the
Semantic Web through the Somewhere semantic peer-to-peer data management
system. The last contribution of this paper is to provide an experimental
analysis of the scalability of the peer-to-peer infrastructure that we propose,
on large networks of 1000 peers.",N/A
Dynamic Local Search for the Maximum Clique Problem,"In this paper, we introduce DLS-MC, a new stochastic local search algorithm
for the maximum clique problem. DLS-MC alternates between phases of iterative
improvement, during which suitable vertices are added to the current clique,
and plateau search, during which vertices of the current clique are swapped
with vertices not contained in the current clique. The selection of vertices is
solely based on vertex penalties that are dynamically adjusted during the
search, and a perturbation mechanism is used to overcome search stagnation. The
behaviour of DLS-MC is controlled by a single parameter, penalty delay, which
controls the frequency at which vertex penalties are reduced. We show
empirically that DLS-MC achieves substantial performance improvements over
state-of-the-art algorithms for the maximum clique problem over a large range
of the commonly used DIMACS benchmark instances.",N/A
Representing Conversations for Scalable Overhearing,"Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings, agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis, validation, monitoring, etc) is an important aspect of
multi-agent applications. Recently, Petri nets have been shown to be an
interesting approach to such representation, and radically different approaches
using Petri nets have been proposed. However, their relative strengths and
weaknesses have not been examined. Moreover, their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First, we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing, an important task in
monitoring open multi-agent systems. Then, building on the insights gained, we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore, we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation), to
our Colored Petri net representation.",N/A
"Improving Heuristics Through Relaxed Search - An Analysis of TP4 and
  HSP*a in the 2004 Planning Competition","The hm admissible heuristics for (sequential and temporal) regression
planning are defined by a parameterized relaxation of the optimal cost function
in the regression search space, where the parameter m offers a trade-off
between the accuracy and computational cost of theheuristic. Existing methods
for computing the hm heuristic require time exponential in m, limiting them to
small values (m andlt= 2). The hm heuristic can also be viewed as the optimal
cost function in a relaxation of the search space: this paper presents relaxed
search, a method for computing this function partially by searching in the
relaxed space. The relaxed search method, because it computes hm only
partially, is computationally cheaper and therefore usable for higher values of
m. The (complete) hm heuristic is combined with partial hm heuristics, for m =
3,..., computed by relaxed search, resulting in a more accurate heuristic.
  This use of the relaxed search method to improve on the hm heuristic is
evaluated by comparing two optimal temporal planners: TP4, which does not use
it, and HSP*a, which uses it but is otherwise identical to TP4. The comparison
is made on the domains used in the 2004 International Planning Competition, in
which both planners participated. Relaxed search is found to be cost effective
in some of these domains, but not all. Analysis reveals a characterization of
the domains in which relaxed search can be expected to be cost effective, in
terms of two measures on the original and relaxed search spaces. In the domains
where relaxed search is cost effective, expanding small states is
computationally cheaper than expanding large states and small states tend to
have small successor states.",N/A
Models and Strategies for Variants of the Job Shop Scheduling Problem,"Recently, a variety of constraint programming and Boolean satisfiability
approaches to scheduling problems have been introduced. They have in common the
use of relatively simple propagation mechanisms and an adaptive way to focus on
the most constrained part of the problem. In some cases, these methods compare
favorably to more classical constraint programming methods relying on
propagation algorithms for global unary or cumulative resource constraints and
dedicated search heuristics. In particular, we described an approach that
combines restarting, with a generic adaptive heuristic and solution guided
branching on a simple model based on a decomposition of disjunctive
constraints. In this paper, we introduce an adaptation of this technique for an
important subclass of job shop scheduling problems (JSPs), where the objective
function involves minimization of earliness/tardiness costs. We further show
that our technique can be improved by adding domain specific information for
one variant of the JSP (involving time lag constraints). In particular we
introduce a dedicated greedy heuristic, and an improved model for the case
where the maximal time lag is 0 (also referred to as no-wait JSPs).","Principles and Practice of Constraint Programming - CP 2011, Perugia
  : Italy (2011)"
An Approximation of the Universal Intelligence Measure,"The Universal Intelligence Measure is a recently proposed formal definition
of intelligence. It is mathematically specified, extremely general, and
captures the essence of many informal definitions of intelligence. It is based
on Hutter's Universal Artificial Intelligence theory, an extension of Ray
Solomonoff's pioneering work on universal induction. Since the Universal
Intelligence Measure is only asymptotically computable, building a practical
intelligence test from it is not straightforward. This paper studies the
practical issues involved in developing a real-world UIM-based performance
metric. Based on our investigation, we develop a prototype implementation which
we use to evaluate a number of different artificial agents.",14 pages
An Improved Search Algorithm for Optimal Multiple-Sequence Alignment,"Multiple sequence alignment (MSA) is a ubiquitous problem in computational
biology. Although it is NP-hard to find an optimal solution for an arbitrary
number of sequences, due to the importance of this problem researchers are
trying to push the limits of exact algorithms further. Since MSA can be cast as
a classical path finding problem, it is attracting a growing number of AI
researchers interested in heuristic search algorithms as a challenge with
actual practical relevance. In this paper, we first review two previous,
complementary lines of research. Based on Hirschbergs algorithm, Dynamic
Programming needs O(kN^(k-1)) space to store both the search frontier and the
nodes needed to reconstruct the solution path, for k sequences of length N.
Best first search, on the other hand, has the advantage of bounding the search
space that has to be explored using a heuristic. However, it is necessary to
maintain all explored nodes up to the final solution in order to prevent the
search from re-expanding them at higher cost. Earlier approaches to reduce the
Closed list are either incompatible with pruning methods for the Open list, or
must retain at least the boundary of the Closed list. In this article, we
present an algorithm that attempts at combining the respective advantages; like
A* it uses a heuristic for pruning the search space, but reduces both the
maximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The
underlying idea is to conduct a series of searches with successively increasing
upper bounds, but using the DP ordering as the key for the Open priority queue.
With a suitable choice of thresholds, in practice, a running time below four
times that of A* can be expected. In our experiments we show that our algorithm
outperforms one of the currently most successful algorithms for optimal
multiple sequence alignments, Partial Expansion A*, both in time and memory.
Moreover, we apply a refined heuristic based on optimal alignments not only of
pairs of sequences, but of larger subsets. This idea is not new; however, to
make it practically relevant we show that it is equally important to bound the
heuristic computation appropriately, or the overhead can obliterate any
possible gain. Furthermore, we discuss a number of improvements in time and
space efficiency with regard to practical implementations. Our algorithm, used
in conjunction with higher-dimensional heuristics, is able to calculate for the
first time the optimal alignment for almost all of the problems in Reference 1
of the benchmark database BAliBASE.",N/A
"Probabilistic Hybrid Action Models for Predicting Concurrent
  Percept-driven Robot Behavior","This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic
causal model for predicting the behavior generated by modern percept-driven
robot plans. PHAMs represent aspects of robot behavior that cannot be
represented by most action models used in AI planning: the temporal structure
of continuous control processes, their non-deterministic effects, several modes
of their interferences, and the achievement of triggering conditions in
closed-loop robot plans.
  The main contributions of this article are: (1) PHAMs, a model of concurrent
percept-driven behavior, its formalization, and proofs that the model generates
probably, qualitatively accurate predictions; and (2) a resource-efficient
inference method for PHAMs based on sampling projections from probabilistic
action models and state descriptions. We show how PHAMs can be applied to
planning the course of action of an autonomous robot office courier based on
analytical and experimental results.",N/A
Generative Prior Knowledge for Discriminative Classification,"We present a novel framework for integrating prior knowledge into
discriminative classifiers. Our framework allows discriminative classifiers
such as Support Vector Machines (SVMs) to utilize prior knowledge specified in
the generative setting. The dual objective of fitting the data and respecting
prior knowledge is formulated as a bilevel program, which is solved
(approximately) via iterative application of second-order cone programming. To
test our approach, we consider the problem of using WordNet (a semantic
database of English language) to improve low-sample classification accuracy of
newsgroup categorization. WordNet is viewed as an approximate, but readily
available source of background knowledge, and our framework is capable of
utilizing it in a flexible way.",N/A
The Fast Downward Planning System,"Fast Downward is a classical planning system based on heuristic search. It
can deal with general deterministic planning problems encoded in the
propositional fragment of PDDL2.2, including advanced features like ADL
conditions and effects and derived predicates (axioms). Like other well-known
planners such as HSP and FF, Fast Downward is a progression planner, searching
the space of world states of a planning task in the forward direction. However,
unlike other PDDL planning systems, Fast Downward does not use the
propositional PDDL representation of a planning task directly. Instead, the
input is first translated into an alternative representation called
multi-valued planning tasks, which makes many of the implicit constraints of a
propositional planning task explicit. Exploiting this alternative
representation, Fast Downward uses hierarchical decompositions of planning
tasks for computing its heuristic function, called the causal graph heuristic,
which is very different from traditional HSP-like heuristics based on ignoring
negative interactions of operators.
  In this article, we give a full account of Fast Downwards approach to solving
multi-valued planning tasks. We extend our earlier discussion of the causal
graph heuristic to tasks involving axioms and conditional effects and present
some novel techniques for search control that are used within Fast Downwards
best-first search algorithm: preferred operators transfer the idea of helpful
actions from local search to global best-first search, deferred evaluation of
heuristic functions mitigates the negative effect of large branching factors on
search performance, and multi-heuristic best-first search combines several
heuristic evaluation functions within a single search algorithm in an
orthogonal way. We also describe efficient data structures for fast state
expansion (successor generators and axiom evaluators) and present a new
non-heuristic search algorithm called focused iterative-broadening search,
which utilizes the information encoded in causal graphs in a novel way.
  Fast Downward has proven remarkably successful: It won the ""classical (i.e.,
propositional, non-optimising) track of the 4th International Planning
Competition at ICAPS 2004, following in the footsteps of planners such as FF
and LPG. Our experiments show that it also performs very well on the benchmarks
of the earlier planning competitions and provide some insights about the
usefulness of the new search enhancements.",N/A
"Asynchronous Partial Overlay: A New Algorithm for Solving Distributed
  Constraint Satisfaction Problems","Distributed Constraint Satisfaction (DCSP) has long been considered an
important problem in multi-agent systems research. This is because many
real-world problems can be represented as constraint satisfaction and these
problems often present themselves in a distributed form. In this article, we
present a new complete, distributed algorithm called Asynchronous Partial
Overlay (APO) for solving DCSPs that is based on a cooperative mediation
process. The primary ideas behind this algorithm are that agents, when acting
as a mediator, centralize small, relevant portions of the DCSP, that these
centralized subproblems overlap, and that agents increase the size of their
subproblems along critical paths within the DCSP as the problem solving
unfolds. We present empirical evidence that shows that APO outperforms other
known, complete DCSP techniques.",N/A
Admissible and Restrained Revision,"As partial justification of their framework for iterated belief revision
Darwiche and Pearl convincingly argued against Boutiliers natural revision and
provided a prototypical revision operator that fits into their scheme. We show
that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller
class of operators which we refer to as admissible. Admissible revision ensures
that the penultimate input is not ignored completely, thereby eliminating
natural revision, but includes the Darwiche-Pearl operator, Nayaks
lexicographic revision operator, and a newly introduced operator called
restrained revision. We demonstrate that restrained revision is the most
conservative of admissible revision operators, effecting as few changes as
possible, while lexicographic revision is the least conservative, and point out
that restrained revision can also be viewed as a composite operator, consisting
of natural revision preceded by an application of a ""backwards revision""
operator previously studied by Papini. Finally, we propose the establishment of
a principled approach for choosing an appropriate revision operator in
different contexts and discuss future work.",N/A
On Graphical Modeling of Preference and Importance,"In recent years, CP-nets have emerged as a useful tool for supporting
preference elicitation, reasoning, and representation. CP-nets capture and
support reasoning with qualitative conditional preference statements,
statements that are relatively natural for users to express. In this paper, we
extend the CP-nets formalism to handle another class of very natural
qualitative statements one often uses in expressing preferences in daily life -
statements of relative importance of attributes. The resulting formalism,
TCP-nets, maintains the spirit of CP-nets, in that it remains focused on using
only simple and natural preference statements, uses the ceteris paribus
semantics, and utilizes a graphical representation of this information to
reason about its consistency and to perform, possibly constrained, optimization
using it. The extra expressiveness it provides allows us to better model
tradeoffs users would like to make, more faithfully representing their
preferences.",N/A
"The Planning Spectrum - One, Two, Three, Infinity","Linear Temporal Logic (LTL) is widely used for defining conditions on the
execution paths of dynamic systems. In the case of dynamic systems that allow
for nondeterministic evolutions, one has to specify, along with an LTL formula
f, which are the paths that are required to satisfy the formula. Two extreme
cases are the universal interpretation A.f, which requires that the formula be
satisfied for all execution paths, and the existential interpretation E.f,
which requires that the formula be satisfied for some execution path.
  When LTL is applied to the definition of goals in planning problems on
nondeterministic domains, these two extreme cases are too restrictive. It is
often impossible to develop plans that achieve the goal in all the
nondeterministic evolutions of a system, and it is too weak to require that the
goal is satisfied by some execution.
  In this paper we explore alternative interpretations of an LTL formula that
are between these extreme cases. We define a new language that permits an
arbitrary combination of the A and E quantifiers, thus allowing, for instance,
to require that each finite execution can be extended to an execution
satisfying an LTL formula (AE.f), or that there is some finite execution whose
extensions all satisfy an LTL formula (EA.f). We show that only eight of these
combinations of path quantifiers are relevant, corresponding to an alternation
of the quantifiers of length one (A and E), two (AE and EA), three (AEA and
EAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for
the new language that is based on an automata-theoretic approach, and study its
complexity.",N/A
Fault Tolerant Boolean Satisfiability,"A delta-model is a satisfying assignment of a Boolean formula for which any
small alteration, such as a single bit flip, can be repaired by flips to some
small number of other bits, yielding a new satisfying assignment. These
satisfying assignments represent robust solutions to optimization problems
(e.g., scheduling) where it is possible to recover from unforeseen events
(e.g., a resource becoming unavailable). The concept of delta-models was
introduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that
finding delta-models for general Boolean formulas is NP-complete. In this
paper, we extend that result by studying the complexity of finding delta-models
for classes of Boolean formulas which are known to have polynomial time
satisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT,
dual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the
complexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have
polynomial time tests for delta-models, testing whether a Horn-SAT formula has
one is NP-complete.",N/A
Cognitive Principles in Robust Multimodal Interpretation,"Multimodal conversational interfaces provide a natural means for users to
communicate with computer systems through multiple modalities such as speech
and gesture. To build effective multimodal interfaces, automated interpretation
of user multimodal inputs is important. Inspired by the previous investigation
on cognitive status in multimodal human machine interaction, we have developed
a greedy algorithm for interpreting user referring expressions (i.e.,
multimodal reference resolution). This algorithm incorporates the cognitive
principles of Conversational Implicature and Givenness Hierarchy and applies
constraints from various sources (e.g., temporal, semantic, and contextual) to
resolve references. Our empirical results have shown the advantage of this
algorithm in efficiently resolving a variety of user references. Because of its
simplicity and generality, this approach has the potential to improve the
robustness of multimodal input interpretation.",N/A
Multiple-Goal Heuristic Search,"This paper presents a new framework for anytime heuristic search where the
task is to achieve as many goals as possible within the allocated resources. We
show the inadequacy of traditional distance-estimation heuristics for tasks of
this type and present alternative heuristics that are more appropriate for
multiple-goal search. In particular, we introduce the marginal-utility
heuristic, which estimates the cost and the benefit of exploring a subtree
below a search node. We developed two methods for online learning of the
marginal-utility heuristic. One is based on local similarity of the partial
marginal utility of sibling nodes, and the other generalizes marginal-utility
over the state feature space. We apply our adaptive and non-adaptive
multiple-goal search algorithms to several problems, including focused
crawling, and show their superiority over existing methods.",N/A
FluCaP: A Heuristic Search Planner for First-Order MDPs,"We present a heuristic search algorithm for solving first-order Markov
Decision Processes (FOMDPs). Our approach combines first-order state
abstraction that avoids evaluating states individually, and heuristic search
that avoids evaluating all states. Firstly, in contrast to existing systems,
which start with propositionalizing the FOMDP and then perform state
abstraction on its propositionalized version we apply state abstraction
directly on the FOMDP avoiding propositionalization. This kind of abstraction
is referred to as first-order state abstraction. Secondly, guided by an
admissible heuristic, the search is restricted to those states that are
reachable from the initial state. We demonstrate the usefulness of the above
techniques for solving FOMDPs with a system, referred to as FluCaP (formerly,
FCPlanner), that entered the probabilistic track of the 2004 International
Planning Competition (IPC2004) and demonstrated an advantage over other
planners on the problems represented in first-order terms.",N/A
Learning Dependency-Based Compositional Semantics,"Suppose we want to build a system that answers a natural language question by
representing its semantics as a logical form and computing the answer given a
structured database of facts. The core part of such a system is the semantic
parser that maps questions to logical forms. Semantic parsers are typically
trained from examples of questions annotated with their target logical forms,
but this type of annotation is expensive.
  Our goal is to learn a semantic parser from question-answer pairs instead,
where the logical form is modeled as a latent variable. Motivated by this
challenging learning problem, we develop a new semantic formalism,
dependency-based compositional semantics (DCS), which has favorable linguistic,
statistical, and computational properties. We define a log-linear distribution
over DCS logical forms and estimate the parameters using a simple procedure
that alternates between beam search and numerical optimization. On two standard
semantic parsing benchmarks, our system outperforms all existing
state-of-the-art systems, despite using no annotated logical forms.",N/A
Causes of Ineradicable Spurious Predictions in Qualitative Simulation,"It was recently proved that a sound and complete qualitative simulator does
not exist, that is, as long as the input-output vocabulary of the
state-of-the-art QSIM algorithm is used, there will always be input models
which cause any simulator with a coverage guarantee to make spurious
predictions in its output. In this paper, we examine whether a meaningfully
expressive restriction of this vocabulary is possible so that one can build a
simulator with both the soundness and completeness properties. We prove several
negative results: All sound qualitative simulators, employing subsets of the
QSIM representation which retain the operating region transition feature, and
support at least the addition and constancy constraints, are shown to be
inherently incomplete. Even when the simulations are restricted to run in a
single operating region, a constraint vocabulary containing just the addition,
constancy, derivative, and multiplication relations makes the construction of
sound and complete qualitative simulators impossible.",N/A
"Properties and Applications of Programs with Monotone and Convex
  Constraints","We study properties of programs with monotone and convex constraints. We
extend to these formalisms concepts and results from normal logic programming.
They include the notions of strong and uniform equivalence with their
characterizations, tight programs and Fages Lemma, program completion and loop
formulas. Our results provide an abstract account of properties of some recent
extensions of logic programming with aggregates, especially the formalism of
lparse programs. They imply a method to compute stable models of lparse
programs by means of off-the-shelf solvers of pseudo-boolean constraints, which
is often much faster than the smodels system.",N/A
"How the Landscape of Random Job Shop Scheduling Instances Depends on the
  Ratio of Jobs to Machines","We characterize the search landscape of random instances of the job shop
scheduling problem (JSP). Specifically, we investigate how the expected values
of (1) backbone size, (2) distance between near-optimal schedules, and (3)
makespan of random schedules vary as a function of the job to machine ratio
(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we
provide analytical results, while for intermediate values of N/M we perform
experiments. We prove that as N/M approaches 0, backbone size approaches 100%,
while as N/M approaches infinity the backbone vanishes. In the process we show
that as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules
almost surely generate an optimal schedule, providing theoretical evidence of
an ""easy-hard-easy"" pattern of typical-case instance difficulty in job shop
scheduling. We also draw connections between our theoretical results and the
""big valley"" picture of JSP landscapes.",N/A
Preference-based Search using Example-Critiquing with Suggestions,"We consider interactive tools that help users search for their most preferred
item in a large collection of options. In particular, we examine
example-critiquing, a technique for enabling users to incrementally construct
preference models by critiquing example options that are presented to them. We
present novel techniques for improving the example-critiquing technology by
adding suggestions to its displayed options. Such suggestions are calculated
based on an analysis of users current preference model and their potential
hidden preferences. We evaluate the performance of our model-based suggestion
techniques with both synthetic and real users. Results show that such
suggestions are highly attractive to users and can stimulate them to express
more preferences to improve the chance of identifying their most preferred item
by up to 78%.",N/A
Anytime Point-Based Approximations for Large POMDPs,"The Partially Observable Markov Decision Process has long been recognized as
a rich framework for real-world planning and control problems, especially in
robotics. However exact solutions in this framework are typically
computationally intractable for all but the smallest problems. A well-known
technique for speeding up POMDP solving involves performing value backups at
specific belief points, rather than over the entire belief simplex. The
efficiency of this approach, however, depends greatly on the selection of
points. This paper presents a set of novel techniques for selecting informative
belief points which work well in practice. The point selection procedure is
combined with point-based value backups to form an effective anytime POMDP
algorithm called Point-Based Value Iteration (PBVI). The first aim of this
paper is to introduce this algorithm and present a theoretical analysis
justifying the choice of belief selection technique. The second aim of this
paper is to provide a thorough empirical comparison between PBVI and other
state-of-the-art POMDP methods, in particular the Perseus algorithm, in an
effort to highlight their similarities and differences. Evaluation is performed
using both standard POMDP domains and realistic robotic tasks.",N/A
Solving Factored MDPs with Hybrid State and Action Variables,"Efficient representations and solutions for large decision problems with
continuous and discrete variables are among the most important challenges faced
by the designers of automated decision support systems. In this paper, we
describe a novel hybrid factored Markov decision process (MDP) model that
allows for a compact representation of these problems, and a new hybrid
approximate linear programming (HALP) framework that permits their efficient
solutions. The central idea of HALP is to approximate the optimal value
function by a linear combination of basis functions and optimize its weights by
linear programming. We analyze both theoretical and computational aspects of
this approach, and demonstrate its scale-up potential on several hybrid
optimization problems.",N/A
Combination Strategies for Semantic Role Labeling,"This paper introduces and analyzes a battery of inference models for the
problem of semantic role labeling: one based on constraint satisfaction, and
several strategies that model the inference as a meta-learning problem using
discriminative classifiers. These classifiers are developed with a rich set of
novel features that encode proposition and sentence-level information. To our
knowledge, this is the first work that: (a) performs a thorough analysis of
learning-based inference models for semantic role labeling, and (b) compares
several inference strategies in this context. We evaluate the proposed
inference strategies in the framework of the CoNLL-2005 shared task using only
automatically-generated syntactic information. The extensive experimental
evaluation and analysis indicates that all the proposed inference strategies
are successful -they all outperform the current best results reported in the
CoNLL-2005 evaluation exercise- but each of the proposed approaches has its
advantages and disadvantages. Several important traits of a state-of-the-art
SRL combination strategy emerge from this analysis: (i) individual models
should be combined at the granularity of candidate arguments rather than at the
granularity of complete solutions; (ii) the best combination strategy uses an
inference model based in learning; and (iii) the learning-based inference
benefits from max-margin classifiers and global feedback.",N/A
A Behavioral Distance for Fuzzy-Transition Systems,"In contrast to the existing approaches to bisimulation for fuzzy systems, we
introduce a behavioral distance to measure the behavioral similarity of states
in a nondeterministic fuzzy-transition system. This behavioral distance is
defined as the greatest fixed point of a suitable monotonic function and
provides a quantitative analogue of bisimilarity. The behavioral distance has
the important property that two states are at zero distance if and only if they
are bisimilar. Moreover, for any given threshold, we find that states with
behavioral distances bounded by the threshold are equivalent. In addition, we
show that two system combinators---parallel composition and product---are
non-expansive with respect to our behavioral distance, which makes
compositional verification possible.",12 double column pages
"Engineering Benchmarks for Planning: the Domains Used in the
  Deterministic Part of IPC-4","In a field of research about general reasoning mechanisms, it is essential to
have appropriate benchmarks. Ideally, the benchmarks should reflect possible
applications of the developed technology. In AI Planning, researchers more and
more tend to draw their testing examples from the benchmark collections used in
the International Planning Competition (IPC). In the organization of (the
deterministic part of) the fourth IPC, IPC-4, the authors therefore invested
significant effort to create a useful set of benchmarks. They come from five
different (potential) real-world applications of planning: airport ground
traffic control, oil derivative transportation in pipeline networks,
model-checking safety properties, power supply restoration, and UMTS call
setup. Adapting and preparing such an application for use as a benchmark in the
IPC involves, at the time, inevitable (often drastic) simplifications, as well
as careful choice between, and engineering of, domain encodings. For the first
time in the IPC, we used compilations to formulate complex domain features in
simple languages such as STRIPS, rather than just dropping the more interesting
problem constraints in the simpler language subsets. The article explains and
discusses the five application domains and their adaptation to form the PDDL
test suites used in IPC-4. We summarize known theoretical results on structural
properties of the domains, regarding their computational complexity and
provable properties of their topology under the h+ function (an idealized
version of the relaxed plan heuristic). We present new (empirical) results
illuminating properties such as the quality of the most wide-spread heuristic
functions (planning graph, serial planning graph, and relaxed plan), the growth
of propositional representations over instance size, and the number of actions
available to achieve each fact; we discuss these data in conjunction with the
best results achieved by the different kinds of planners participating in
IPC-4.",N/A
Modelling Mixed Discrete-Continuous Domains for Planning,"In this paper we present pddl+, a planning domain description language for
modelling mixed discrete-continuous planning domains. We describe the syntax
and modelling style of pddl+, showing that the language makes convenient the
modelling of complex time-dependent effects. We provide a formal semantics for
pddl+ by mapping planning instances into constructs of hybrid automata. Using
the syntax of HAs as our semantic model we construct a semantic mapping to
labelled transition systems to complete the formal interpretation of pddl+
planning instances. An advantage of building a mapping from pddl+ to HA theory
is that it forms a bridge between the Planning and Real Time Systems research
communities. One consequence is that we can expect to make use of some of the
theoretical properties of HAs. For example, for a restricted class of HAs the
Reachability problem (which is equivalent to Plan Existence) is decidable.
pddl+ provides an alternative to the continuous durative action model of
pddl2.1, adding a more flexible and robust model of time-dependent behaviour.",N/A
Set Intersection and Consistency in Constraint Networks,"In this paper, we show that there is a close relation between consistency in
a constraint network and set intersection. A proof schema is provided as a
generic way to obtain consistency properties from properties on set
intersection. This approach not only simplifies the understanding of and
unifies many existing consistency results, but also directs the study of
consistency to that of set intersection properties in many situations, as
demonstrated by the results on the convexity and tightness of constraints in
this paper. Specifically, we identify a new class of tree convex constraints
where local consistency ensures global consistency. This generalizes row convex
constraints. Various consistency results are also obtained on constraint
networks where only some, in contrast to all in the existing work,constraints
are tight.",N/A
Consistency and Random Constraint Satisfaction Models,"In this paper, we study the possibility of designing non-trivial random CSP
models by exploiting the intrinsic connection between structures and
typical-case hardness. We show that constraint consistency, a notion that has
been developed to improve the efficiency of CSP algorithms, is in fact the key
to the design of random CSP models that have interesting phase transition
behavior and guaranteed exponential resolution complexity without putting much
restriction on the parameter of constraint tightness or the domain size of the
problem. We propose a very flexible framework for constructing problem
instances withinteresting behavior and develop a variety of concrete methods to
construct specific random CSP models that enforce different levels of
constraint consistency. A series of experimental studies with interesting
observations are carried out to illustrate the effectiveness of introducing
structural elements in random instances, to verify the robustness of our
proposal, and to investigate features of some specific models based on our
framework that are highly related to the behavior of backtracking search
algorithms.",N/A
Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms,"In this paper, we present two alternative approaches to defining answer sets
for logic programs with arbitrary types of abstract constraint atoms (c-atoms).
These approaches generalize the fixpoint-based and the level mapping based
answer set semantics of normal logic programs to the case of logic programs
with arbitrary types of c-atoms. The results are four different answer set
definitions which are equivalent when applied to normal logic programs. The
standard fixpoint-based semantics of logic programs is generalized in two
directions, called answer set by reduct and answer set by complement. These
definitions, which differ from each other in the treatment of
negation-as-failure (naf) atoms, make use of an immediate consequence operator
to perform answer set checking, whose definition relies on the notion of
conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other
two definitions, called strongly and weakly well-supported models, are
generalizations of the notion of well-supported models of normal logic programs
to the case of programs with c-atoms. As for the case of fixpoint-based
semantics, the difference between these two definitions is rooted in the
treatment of naf atoms. We prove that answer sets by reduct (resp. by
complement) are equivalent to weakly (resp. strongly) well-supported models of
a program, thus generalizing the theorem on the correspondence between stable
models and well-supported models of a normal logic program to the class of
programs with c-atoms. We show that the newly defined semantics coincide with
previously introduced semantics for logic programs with monotone c-atoms, and
they extend the original answer set semantics of normal logic programs. We also
study some properties of answer sets of programs with c-atoms, and relate our
definitions to several semantics for logic programs with aggregates presented
in the literature.",N/A
"Bin Completion Algorithms for Multicontainer Packing, Knapsack, and
  Covering Problems","Many combinatorial optimization problems such as the bin packing and multiple
knapsack problems involve assigning a set of discrete objects to multiple
containers. These problems can be used to model task and resource allocation
problems in multi-agent systems and distributed systms, and can also be found
as subproblems of scheduling problems. We propose bin completion, a
branch-and-bound strategy for one-dimensional, multicontainer packing problems.
Bin completion combines a bin-oriented search space with a powerful dominance
criterion that enables us to prune much of the space. The performance of the
basic bin completion framework can be enhanced by using a number of extensions,
including nogood-based pruning techniques that allow further exploitation of
the dominance criterion. Bin completion is applied to four problems: multiple
knapsack, bin covering, min-cost covering, and bin packing. We show that our
bin completion algorithms yield new, state-of-the-art results for the multiple
knapsack, bin covering, and min-cost covering problems, outperforming previous
algorithms by several orders of magnitude with respect to runtime on some
classes of hard, random problem instances. For the bin packing problem, we
demonstrate significant improvements compared to most previous results, but
show that bin completion is not competitive with current state-of-the-art
cutting-stock based approaches.",N/A
"Uncertainty in Soft Temporal Constraint Problems:A General Framework and
  Controllability Algorithms for the Fuzzy Case","In real-life temporal scenarios, uncertainty and preferences are often
essential and coexisting aspects. We present a formalism where quantitative
temporal constraints with both preferences and uncertainty can be defined. We
show how three classical notions of controllability (that is, strong, weak, and
dynamic), which have been developed for uncertain temporal problems, can be
generalized to handle preferences as well. After defining this general
framework, we focus on problems where preferences follow the fuzzy approach,
and with properties that assure tractability. For such problems, we propose
algorithms to check the presence of the controllability properties. In
particular, we show that in such a setting dealing simultaneously with
preferences and uncertainty does not increase the complexity of controllability
testing. We also develop a dynamic execution algorithm, of polynomial
complexity, that produces temporal plans under uncertainty that are optimal
with respect to fuzzy preferences.",N/A
"Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal
  Periodic Sets","In the recent years several research efforts have focused on the concept of
time granularity and its applications. A first stream of research investigated
the mathematical models behind the notion of granularity and the algorithms to
manage temporal data based on those models. A second stream of research
investigated symbolic formalisms providing a set of algebraic operators to
define granularities in a compact and compositional way. However, only very
limited manipulation algorithms have been proposed to operate directly on the
algebraic representation making it unsuitable to use the symbolic formalisms in
applications that need manipulation of granularities.
  This paper aims at filling the gap between the results from these two streams
of research, by providing an efficient conversion from the algebraic
representation to the equivalent low-level representation based on the
mathematical models. In addition, the conversion returns a minimal
representation in terms of period length. Our results have a major practical
impact: users can more easily define arbitrary granularities in terms of
algebraic operators, and then access granularity reasoning and other services
operating efficiently on the equivalent, minimal low-level representation. As
an example, we illustrate the application to temporal constraint reasoning with
multiple granularities.
  From a technical point of view, we propose an hybrid algorithm that
interleaves the conversion of calendar subexpressions into periodical sets with
the minimization of the period length. The algorithm returns set-based
granularity representations having minimal period length, which is the most
relevant parameter for the performance of the considered reasoning services.
Extensive experimental work supports the techniques used in the algorithm, and
shows the efficiency and effectiveness of the algorithm.",N/A
The Generalized A* Architecture,"We consider the problem of computing a lightest derivation of a global
structure using a set of weighted rules. A large variety of inference problems
in AI can be formulated in this framework. We generalize A* search and
heuristics derived from abstractions to a broad class of lightest derivation
problems. We also describe a new algorithm that searches for lightest
derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss
how the algorithms described here provide a general architecture for addressing
the pipeline problem --- the problem of passing information back and forth
between various stages of processing in a perceptual system. We consider
examples in computer vision and natural language processing. We apply the
hierarchical search algorithm to the problem of estimating the boundaries of
convex objects in grayscale images and compare it to other search methods. A
second set of experiments demonstrate the use of a new compositional model for
finding salient curves in images.",N/A
Combining Spatial and Temporal Logics: Expressiveness vs. Complexity,"In this paper, we construct and investigate a hierarchy of spatio-temporal
formalisms that result from various combinations of propositional spatial and
temporal logics such as the propositional temporal logic PTL, the spatial
logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a
clear picture of the trade-off between expressiveness and computational
realisability within the hierarchy. We demonstrate how different combining
principles as well as spatial and temporal primitives can produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out
of components that are at most NP- or PSPACE-complete.",N/A
"An Approach to Temporal Planning and Scheduling in Domains with
  Predictable Exogenous Events","The treatment of exogenous events in planning is practically important in
many real-world domains where the preconditions of certain plan actions are
affected by such events. In this paper we focus on planning in temporal domains
with exogenous events that happen at known times, imposing the constraint that
certain actions in the plan must be executed during some predefined time
windows. When actions have durations, handling such temporal constraints adds
an extra difficulty to planning. We propose an approach to planning in these
domains which integrates constraint-based temporal reasoning into a graph-based
planning framework using local search. Our techniques are implemented in a
planner that took part in the 4th International Planning Competition (IPC-4). A
statistical analysis of the results of IPC-4 demonstrates the effectiveness of
our approach in terms of both CPU-time and plan quality. Additional experiments
show the good performance of the temporal reasoning techniques integrated into
our planner.",N/A
The Power of Modeling - a Response to PDDL2.1,"In this commentary I argue that although PDDL is a very useful standard for
the planning competition, its design does not properly consider the issue of
domain modeling. Hence, I would not advocate its use in specifying planning
domains outside of the context of the planning competition. Rather, the field
needs to explore different approaches and grapple more directly with the
problem of effectively modeling and utilizing all of the diverse pieces of
knowledge we typically have about planning domains.",N/A
Imperfect Match: PDDL 2.1 and Real Applications,"PDDL was originally conceived and constructed as a lingua franca for the
International Planning Competition. PDDL2.1 embodies a set of extensions
intended to support the expression of something closer to real planning
problems. This objective has only been partially achieved, due in large part to
a deliberate focus on not moving too far from classical planning models and
solution methods.",N/A
PDDL 2.1: Representation vs. Computation,"I comment on the PDDL 2.1 language and its use in the planning competition,
focusing on the choices made for accommodating time and concurrency. I also
discuss some methodological issues that have to do with the move toward more
expressive planning languages and the balance needed in planning research
between semantics and computation.",N/A
"Proactive Algorithms for Job Shop Scheduling with Probabilistic
  Durations","Most classical scheduling formulations assume a fixed and known duration for
each activity. In this paper, we weaken this assumption, requiring instead that
each duration can be represented by an independent random variable with a known
mean and variance. The best solutions are ones which have a high probability of
achieving a good makespan. We first create a theoretical framework, formally
showing how Monte Carlo simulation can be combined with deterministic
scheduling algorithms to solve this problem. We propose an associated
deterministic scheduling problem whose solution is proved, under certain
conditions, to be a lower bound for the probabilistic problem. We then propose
and investigate a number of techniques for solving such problems based on
combinations of Monte Carlo simulation, solutions to the associated
deterministic problem, and either constraint programming or tabu search. Our
empirical results demonstrate that a combination of the use of the associated
deterministic problem and Monte Carlo simulation results in algorithms that
scale best both in terms of problem size and uncertainty. Further experiments
point to the correlation between the quality of the deterministic solution and
the quality of the probabilistic solution as a major factor responsible for
this success.",N/A
The Language of Search,"This paper is concerned with a class of algorithms that perform exhaustive
search on propositional knowledge bases. We show that each of these algorithms
defines and generates a propositional language. Specifically, we show that the
trace of a search can be interpreted as a combinational circuit, and a search
algorithm then defines a propositional language consisting of circuits that are
generated across all possible executions of the algorithm. In particular, we
show that several versions of exhaustive DPLL search correspond to such
well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF.
By thus mapping search algorithms to propositional languages, we provide a
uniform and practical framework in which successful search techniques can be
harnessed for compilation of knowledge into various languages of interest, and
a new methodology whereby the power and limitations of search algorithms can be
understood by looking up the tractability and succinctness of the corresponding
propositional languages.",N/A
"Understanding Algorithm Performance on an Oversubscribed Scheduling
  Application","The best performing algorithms for a particular oversubscribed scheduling
application, Air Force Satellite Control Network (AFSCN) scheduling, appear to
have little in common. Yet, through careful experimentation and modeling of
performance in real problem instances, we can relate characteristics of the
best algorithms to characteristics of the application. In particular, we find
that plateaus dominate the search spaces (thus favoring algorithms that make
larger changes to solutions) and that some randomization in exploration is
critical to good performance (due to the lack of gradient information on the
plateaus). Based on our explanations of algorithm performance, we develop a new
algorithm that combines characteristics of the best performers; the new
algorithms performance is better than the previous best. We show how hypothesis
driven experimentation and search modeling can both explain algorithm
performance and motivate the design of a new algorithm.",N/A
Marvin: A Heuristic Search Planner with Online Macro-Action Learning,"This paper describes Marvin, a planner that competed in the Fourth
International Planning Competition (IPC 4). Marvin uses
action-sequence-memoisation techniques to generate macro-actions, which are
then used during search for a solution plan. We provide an overview of its
architecture and search behaviour, detailing the algorithms used. We also
empirically demonstrate the effectiveness of its features in various planning
domains; in particular, the effects on performance due to the use of
macro-actions, the novel features of its search behaviour, and the native
support of ADL and Derived Predicates.",N/A
Anytime Heuristic Search,"We describe how to convert the heuristic search algorithm A* into an anytime
algorithm that finds a sequence of improved solutions and eventually converges
to an optimal solution. The approach we adopt uses weighted heuristic search to
find an approximate solution quickly, and then continues the weighted search to
find improved solutions as well as to improve a bound on the suboptimality of
the current solution. When the time available to solve a search problem is
limited or uncertain, this creates an anytime heuristic search algorithm that
allows a flexible tradeoff between search time and solution quality. We analyze
the properties of the resulting Anytime A* algorithm, and consider its
performance in three domains; sliding-tile puzzles, STRIPS planning, and
multiple sequence alignment. To illustrate the generality of this approach, we
also describe how to transform the memory-efficient search algorithm Recursive
Best-First Search (RBFS) into an anytime algorithm.",N/A
Discovering Classes of Strongly Equivalent Logic Programs,"In this paper we apply computer-aided theorem discovery technique to discover
theorems about strongly equivalent logic programs under the answer set
semantics. Our discovered theorems capture new classes of strongly equivalent
logic programs that can lead to new program simplification rules that preserve
strong equivalence. Specifically, with the help of computers, we discovered
exact conditions that capture the strong equivalence between a rule and the
empty set, between two rules, between two rules and one of the two rules,
between two rules and another rule, and between three rules and two of the
three rules.",N/A
Phase Transition for Random Quantified XOR-Formulas,"The QXORSAT problem is the quantified version of the satisfiability problem
XORSAT in which the connective exclusive-or is used instead of the usual or. We
study the phase transition associated with random QXORSAT instances. We give a
description of this phase transition in the case of one alternation of
quantifiers, thus performing an advanced practical and theoretical study on the
phase transition of a quantified roblem.",N/A
Cutset Sampling for Bayesian Networks,"The paper presents a new sampling methodology for Bayesian networks that
samples only a subset of variables and applies exact inference to the rest.
Cutset sampling is a network structure-exploiting application of the
Rao-Blackwellisation principle to sampling in Bayesian networks. It improves
convergence by exploiting memory-based inference algorithms. It can also be
viewed as an anytime approximation of the exact cutset-conditioning algorithm
developed by Pearl. Cutset sampling can be implemented efficiently when the
sampled variables constitute a loop-cutset of the Bayesian network and, more
generally, when the induced width of the networks graph conditioned on the
observed sampled variables is bounded by a constant w. We demonstrate
empirically the benefit of this scheme on a range of benchmarks.",N/A
"An Algebraic Graphical Model for Decision with Uncertainties,
  Feasibilities, and Utilities","Numerous formalisms and dedicated algorithms have been designed in the last
decades to model and solve decision making problems. Some formalisms, such as
constraint networks, can express ""simple"" decision problems, while others are
designed to take into account uncertainties, unfeasible decisions, and
utilities. Even in a single formalism, several variants are often proposed to
model different types of uncertainty (probability, possibility...) or utility
(additive or not). In this article, we introduce an algebraic graphical model
that encompasses a large number of such formalisms: (1) we first adapt previous
structures from Friedman, Chu and Halpern for representing uncertainty,
utility, and expected utility in order to deal with generic forms of sequential
decision making; (2) on these structures, we then introduce composite graphical
models that express information via variables linked by ""local"" functions,
thanks to conditional independence; (3) on these graphical models, we finally
define a simple class of queries which can represent various scenarios in terms
of observabilities and controllabilities. A natural decision-tree semantics for
such queries is completed by an equivalent operational semantics, which induces
generic algorithms. The proposed framework, called the
Plausibility-Feasibility-Utility (PFU) framework, not only provides a better
understanding of the links between existing formalisms, but it also covers yet
unpublished frameworks (such as possibilistic influence diagrams) and unifies
formalisms such as quantified boolean formulas and influence diagrams. Our
backtrack and variable elimination generic algorithms are a first step towards
unified algorithms.",N/A
"Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic
  Approach","Matchmaking arises when supply and demand meet in an electronic marketplace,
or when agents search for a web service to perform some task, or even when
recruiting agencies match curricula and job profiles. In such open
environments, the objective of a matchmaking process is to discover best
available offers to a given request. We address the problem of matchmaking from
a knowledge representation perspective, with a formalization based on
Description Logics. We devise Concept Abduction and Concept Contraction as
non-monotonic inferences in Description Logics suitable for modeling
matchmaking in a logical framework, and prove some related complexity results.
We also present reasonable algorithms for semantic matchmaking based on the
devised inferences, and prove that they obey to some commonsense properties.
Finally, we report on the implementation of the proposed matchmaking framework,
which has been used both as a mediator in e-marketplaces and for semantic web
services discovery.",N/A
Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling,"Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel
constructive search technique that performs a series of resource-limited tree
searches where each search begins either from an empty solution (as in
randomized restart) or from a solution that has been encountered during the
search. A small number of these ""elite solutions is maintained during the
search. We introduce the technique and perform three sets of experiments on the
job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS
is carried out to evaluate the performance impact of various parameter
settings. Second, we inquire into the diversity of the elite solution set,
showing, contrary to expectations, that a less diverse set leads to stronger
performance. Finally, we compare the best parameter setting of SGMPCS from the
first two experiments to chronological backtracking, limited discrepancy
search, randomized restart, and a sophisticated tabu search algorithm on a set
of well-known benchmark problems. Results demonstrate that SGMPCS is
significantly better than the other constructive techniques tested, though lags
behind the tabu search.",N/A
Are Minds Computable?,"This essay explores the limits of Turing machines concerning the modeling of
minds and suggests alternatives to go beyond those limits.","7 pages, comments welcome"
Fuzzy Inference Systems Optimization,"This paper compares various optimization methods for fuzzy inference system
optimization. The optimization methods compared are genetic algorithm, particle
swarm optimization and simulated annealing. When these techniques were
implemented it was observed that the performance of each technique within the
fuzzy inference system classification was context dependent.",Paper Submitted to INTECH
Handling controversial arguments by matrix,"We introduce matrix and its block to the Dung's theory of argumentation
framework. It is showed that each argumentation framework has a matrix
representation, and the indirect attack relation and indirect defence relation
can be characterized by computing the matrix. This provide a powerful
mathematics way to determine the ""controversial arguments"" in an argumentation
framework. Also, we introduce several kinds of blocks based on the matrix, and
various prudent semantics of argumentation frameworks can all be determined by
computing and comparing the matrices and their blocks which we have defined. In
contrast with traditional method of directed graph, the matrix method has an
excellent advantage: computability(even can be realized on computer easily).
So, there is an intensive perspective to import the theory of matrix to the
research of argumentation frameworks and its related areas.","21 pages, 2 figures"
Learning in Real-Time Search: A Unifying Framework,"Real-time search methods are suited for tasks in which the agent is
interacting with an initially unknown environment in real time. In such
simultaneous planning and learning problems, the agent has to select its
actions in a limited amount of time, while sensing only a local part of the
environment centered at the agents current location. Real-time heuristic search
agents select actions using a limited lookahead search and evaluating the
frontier states with a heuristic function. Over repeated experiences, they
refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and
hardware agents has led to an explosion of real-time search algorithms over the
last two decades. Not only is a potential user confronted with a hodgepodge of
algorithms, but he also faces the choice of control parameters they use. In
this paper we address both problems. The first contribution is an introduction
of a simple three-parameter framework (named LRTS) which extracts the core
ideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,
SLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they
are unified and extended with additional features. Second, we prove
completeness and convergence of any algorithm covered by the LRTS framework.
Third, we prove several upper-bounds relating the control parameters and
solution quality. Finally, we analyze the influence of the three control
parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as
well as routing in ad hoc sensor networks.",N/A
"A Generalized Arc-Consistency Algorithm for a Class of Counting
  Constraints: Revised Edition that Incorporates One Correction","This paper introduces the SEQ BIN meta-constraint with a polytime algorithm
achieving general- ized arc-consistency according to some properties. SEQ BIN
can be used for encoding counting con- straints such as CHANGE, SMOOTH or
INCREAS- ING NVALUE. For some of these constraints and some of their variants
GAC can be enforced with a time and space complexity linear in the sum of
domain sizes, which improves or equals the best known results of the
literature.",N/A
"Quels formalismes temporels pour représenter des connaissances
  extraites de textes de recettes de cuisine ?","The Taaable projet goal is to create a case-based reasoning system for
retrieval and adaptation of cooking recipes. Within this framework, we are
discussing the temporal aspects of recipes and the means of representing those
in order to adapt their text.",Repr\'esentation et raisonnement sur le temps et l'espace (2011)
Modelling Constraint Solver Architecture Design as a Constraint Problem,"Designing component-based constraint solvers is a complex problem. Some
components are required, some are optional and there are interdependencies
between the components. Because of this, previous approaches to solver design
and modification have been ad-hoc and limited. We present a system that
transforms a description of the components and the characteristics of the
target constraint solver into a constraint problem. Solving this problem yields
the description of a valid solver. Our approach represents a significant step
towards the automated design and synthesis of constraint solvers that are
specialised for individual constraint problem classes or instances.",N/A
A cognitive diversity framework for radar target classification,"Classification of targets by radar has proved to be notoriously difficult
with the best systems still yet to attain sufficiently high levels of
performance and reliability. In the current contribution we explore a new
design of radar based target recognition, where angular diversity is used in a
cognitive manner to attain better performance. Performance is bench- marked
against conventional classification schemes. The proposed scheme can easily be
extended to cognitive target recognition based on multiple diversity
strategies.",N/A
Reasoning with Very Expressive Fuzzy Description Logics,"It is widely recognized today that the management of imprecision and
vagueness will yield more intelligent and realistic knowledge-based
applications. Description Logics (DLs) are a family of knowledge representation
languages that have gained considerable attention the last decade, mainly due
to their decidability and the existence of empirically high performance of
reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to
the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms
(S), inverse roles (I), role hierarchies (H) and number restrictions (N). We
illustrate why transitive role axioms are difficult to handle in the presence
of fuzzy interpretations and how to handle them properly. Then we extend these
results by adding role hierarchies and finally number restrictions. The main
contributions of the paper are the decidability proof of the fuzzy DL languages
fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base
satisfiability problem of the fuzzy-SI and fuzzy-SHIN.",N/A
New Inference Rules for Max-SAT,"Exact Max-SAT solvers, compared with SAT solvers, apply little inference at
each node of the proof tree. Commonly used SAT inference rules like unit
propagation produce a simplified formula that preserves satisfiability but,
unfortunately, solving the Max-SAT problem for the simplified formula is not
equivalent to solving it for the original formula. In this paper, we define a
number of original inference rules that, besides being applied efficiently,
transform Max-SAT instances into equivalent Max-SAT instances which are easier
to solve. The soundness of the rules, that can be seen as refinements of unit
resolution adapted to Max-SAT, are proved in a novel and simple way via an
integer programming transformation. With the aim of finding out how powerful
the inference rules are in practice, we have developed a new Max-SAT solver,
called MaxSatz, which incorporates those rules, and performed an experimental
investigation. The results provide empirical evidence that MaxSatz is very
competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph
3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation
2006.",N/A
Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms,"Reputation mechanisms offer an effective alternative to verification
authorities for building trust in electronic markets with moral hazard. Future
clients guide their business decisions by considering the feedback from past
transactions; if truthfully exposed, cheating behavior is sanctioned and thus
becomes irrational.
  It therefore becomes important to ensure that rational clients have the right
incentives to report honestly. As an alternative to side-payment schemes that
explicitly reward truthful reports, we show that honesty can emerge as a
rational behavior when clients have a repeated presence in the market. To this
end we describe a mechanism that supports an equilibrium where truthful
feedback is obtained. Then we characterize the set of pareto-optimal equilibria
of the mechanism, and derive an upper bound on the percentage of false reports
that can be recorded by the mechanism. An important role in the existence of
this bound is played by the fact that rational clients can establish a
reputation for reporting honestly.",N/A
"Probabilistic Planning via Heuristic Forward Search and Weighted Model
  Counting","We present a new algorithm for probabilistic planning with no observability.
Our algorithm, called Probabilistic-FF, extends the heuristic forward-search
machinery of Conformant-FF to problems with probabilistic uncertainty about
both the initial state and action effects. Specifically, Probabilistic-FF
combines Conformant-FFs techniques with a powerful machinery for weighted model
counting in (weighted) CNFs, serving to elegantly define both the search space
and the heuristic function. Our evaluation of Probabilistic-FF shows its fine
scalability in a range of probabilistic domains, constituting a several orders
of magnitude improvement over previous results in this area. We use a
problematic case to point out the main open issue to be addressed by further
research.",N/A
Conjunctive Query Answering for the Description Logic SHIQ,"Conjunctive queries play an important role as an expressive query language
for Description Logics (DLs). Although modern DLs usually provide for
transitive roles, conjunctive query answering over DL knowledge bases is only
poorly understood if transitive roles are admitted in the query. In this paper,
we consider unions of conjunctive queries over knowledge bases formulated in
the prominent DL SHIQ and allow transitive roles in both the query and the
knowledge base. We show decidability of query answering in this setting and
establish two tight complexity bounds: regarding combined complexity, we prove
that there is a deterministic algorithm for query answering that needs time
single exponential in the size of the KB and double exponential in the size of
the query, which is optimal. Regarding data complexity, we prove containment in
co-NP.",N/A
Qualitative System Identification from Imperfect Data,"Experience in the physical sciences suggests that the only realistic means of
understanding complex systems is through the use of mathematical models.
Typically, this has come to mean the identification of quantitative models
expressed as differential equations. Quantitative modelling works best when the
structure of the model (i.e., the form of the equations) is known; and the
primary concern is one of estimating the values of the parameters in the model.
For complex biological systems, the model-structure is rarely known and the
modeler has to deal with both model-identification and parameter-estimation. In
this paper we are concerned with providing automated assistance to the first of
these problems. Specifically, we examine the identification by machine of the
structural relationships between experimentally observed variables. These
relationship will be expressed in the form of qualitative abstractions of a
quantitative model. Such qualitative models may not only provide clues to the
precise quantitative model, but also assist in understanding the essence of
that model. Our position in this paper is that background knowledge
incorporating system modelling principles can be used to constrain effectively
the set of good qualitative models. Utilising the model-identification
framework provided by Inductive Logic Programming (ILP) we present empirical
support for this position using a series of increasingly complex artificial
datasets. The results are obtained with qualitative and quantitative data
subject to varying amounts of noise and different degrees of sparsity. The
results also point to the presence of a set of qualitative states, which we
term kernel subsets, that may be necessary for a qualitative model-learner to
learn correct models. We demonstrate scalability of the method to biological
system modelling by identification of the glycolysis metabolic pathway from
data.",N/A
Exploiting Subgraph Structure in Multi-Robot Path Planning,"Multi-robot path planning is difficult due to the combinatorial explosion of
the search space with every new robot added. Complete search of the combined
state-space soon becomes intractable. In this paper we present a novel form of
abstraction that allows us to plan much more efficiently. The key to this
abstraction is the partitioning of the map into subgraphs of known structure
with entry and exit restrictions which we can represent compactly. Planning
then becomes a search in the much smaller space of subgraph configurations.
Once an abstract plan is found, it can be quickly resolved into a correct (but
possibly sub-optimal) concrete plan without the need for further search. We
prove that this technique is sound and complete and demonstrate its practical
effectiveness on a real map.
  A contending solution, prioritised planning, is also evaluated and shown to
have similar performance albeit at the cost of completeness. The two approaches
are not necessarily conflicting; we demonstrate how they can be combined into a
single algorithm which outperforms either approach alone.",N/A
Extended RDF as a Semantic Foundation of Rule Markup Languages,"Ontologies and automated reasoning are the building blocks of the Semantic
Web initiative. Derivation rules can be included in an ontology to define
derived concepts, based on base concepts. For example, rules allow to define
the extension of a class or property, based on a complex relation between the
extensions of the same or other classes and properties. On the other hand, the
inclusion of negative information both in the form of negation-as-failure and
explicit negative information is also needed to enable various forms of
reasoning. In this paper, we extend RDF graphs with weak and strong negation,
as well as derivation rules. The ERDF stable model semantics of the extended
framework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive
feature of our theory, which is based on Partial Logic, is that both truth and
falsity extensions of properties and classes are considered, allowing for truth
value gaps. Our framework supports both closed-world and open-world reasoning
through the explicit representation of the particular closed-world assumptions
and the ERDF ontological categories of total properties and total classes.",N/A
The Complexity of Planning Problems With Simple Causal Graphs,"We present three new complexity results for classes of planning problems with
simple causal graphs. First, we describe a polynomial-time algorithm that uses
macros to generate plans for the class 3S of planning problems with binary
state variables and acyclic causal graphs. This implies that plan generation
may be tractable even when a planning problem has an exponentially long minimal
solution. We also prove that the problem of plan existence for planning
problems with multi-valued variables and chain causal graphs is NP-hard.
Finally, we show that plan existence for planning problems with binary state
variables and polytree causal graphs is NP-complete.",N/A
"Loosely Coupled Formulations for Automated Planning: An Integer
  Programming Perspective","We represent planning as a set of loosely coupled network flow problems,
where each network corresponds to one of the state variables in the planning
domain. The network nodes correspond to the state variable values and the
network arcs correspond to the value transitions. The planning problem is to
find a path (a sequence of actions) in each network such that, when merged,
they constitute a feasible plan. In this paper we present a number of integer
programming formulations that model these loosely coupled networks with varying
degrees of flexibility. Since merging may introduce exponentially many ordering
constraints we implement a so-called branch-and-cut algorithm, in which these
constraints are dynamically generated and added to the formulation when needed.
Our results are very promising, they improve upon previous planning as integer
programming approaches and lay the foundation for integer programming
approaches for cost optimal planning.",N/A
A Constraint Programming Approach for Solving a Queueing Control Problem,"In a facility with front room and back room operations, it is useful to
switch workers between the rooms in order to cope with changing customer
demand. Assuming stochastic customer arrival and service times, we seek a
policy for switching workers such that the expected customer waiting time is
minimized while the expected back room staffing is sufficient to perform all
work. Three novel constraint programming models and several shaving procedures
for these models are presented. Experimental results show that a model based on
closed-form expressions together with a combination of shaving procedures is
the most efficient. This model is able to find and prove optimal solutions for
many problem instances within a reasonable run-time. Previously, the only
available approach was a heuristic algorithm. Furthermore, a hybrid method
combining the heuristic and the best constraint programming method is shown to
perform as well as the heuristic in terms of solution quality over time, while
achieving the same performance in terms of proving optimality as the pure
constraint programming model. This is the first work of which we are aware that
solves such queueing-based problems with constraint programming.",N/A
Optimal and Approximate Q-value Functions for Decentralized POMDPs,"Decision-theoretic planning is a popular approach to sequential decision
making problems, because it treats uncertainty in sensing and acting in a
principled way. In single-agent frameworks like MDPs and POMDPs, planning can
be carried out by resorting to Q-value functions: an optimal Q-value function
Q* is computed in a recursive manner by dynamic programming, and then an
optimal policy is extracted from Q*. In this paper we study whether similar
Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs),
and how policies can be extracted from such value functions. We define two
forms of the optimal Q-value function for Dec-POMDPs: one that gives a
normative description as the Q-value function of an optimal pure joint policy
and another one that is sequentially rational and thus gives a recipe for
computation. This computation, however, is infeasible for all but the smallest
problems. Therefore, we analyze various approximate Q-value functions that
allow for efficient computation. We describe how they relate, and we prove that
they all provide an upper bound to the optimal Q-value function Q*. Finally,
unifying some previous approaches for solving Dec-POMDPs, we describe a family
of algorithms for extracting policies from such Q-value functions, and perform
an experimental evaluation on existing test problems, including a new
firefighting benchmark problem.",N/A
Communication-Based Decomposition Mechanisms for Decentralized MDPs,"Multi-agent planning in stochastic environments can be framed formally as a
decentralized Markov decision problem. Many real-life distributed problems that
arise in manufacturing, multi-robot coordination and information gathering
scenarios can be formalized using this framework. However, finding the optimal
solution in the general case is hard, limiting the applicability of recently
developed algorithms. This paper provides a practical approach for solving
decentralized control problems when communication among the decision makers is
possible, but costly. We develop the notion of communication-based mechanism
that allows us to decompose a decentralized MDP into multiple single-agent
problems. In this framework, referred to as decentralized semi-Markov decision
process with direct communication (Dec-SMDP-Com), agents operate separately
between communications. We show that finding an optimal mechanism is equivalent
to solving optimally a Dec-SMDP-Com. We also provide a heuristic search
algorithm that converges on the optimal decomposition. Restricting the
decomposition to some specific types of local behaviors reduces significantly
the complexity of planning. In particular, we present a polynomial-time
algorithm for the case in which individual agents perform goal-oriented
behaviors between communications. The paper concludes with an additional
tractable algorithm that enables the introduction of human knowledge, thereby
reducing the overall problem to finding the best time to communicate. Empirical
results show that these approaches provide good approximate solutions.",N/A
A General Theory of Additive State Space Abstractions,"Informally, a set of abstractions of a state space S is additive if the
distance between any two states in S is always greater than or equal to the sum
of the corresponding distances in the abstract spaces. The first known additive
abstractions, called disjoint pattern databases, were experimentally
demonstrated to produce state of the art performance on certain state spaces.
However, previous applications were restricted to state spaces with special
properties, which precludes disjoint pattern databases from being defined for
several commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake
puzzle. In this paper we give a general definition of additive abstractions
that can be applied to any state space and prove that heuristics based on
additive abstractions are consistent as well as admissible. We use this new
definition to create additive abstractions for these testbeds and show
experimentally that well chosen additive abstractions can reduce search time
substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude
over state of the art methods for the 17-Pancake puzzle. We also derive a way
of testing if the heuristic value returned by additive abstractions is provably
too low and show that the use of this test can reduce search time for the
15-puzzle and TopSpin by roughly a factor of two.",N/A
First Order Decision Diagrams for Relational MDPs,"Markov decision processes capture sequential decision making under
uncertainty, where an agent must choose actions so as to optimize long term
reward. The paper studies efficient reasoning mechanisms for Relational Markov
Decision Processes (RMDP) where world states have an internal relational
structure that can be naturally described in terms of objects and relations
among them. Two contributions are presented. First, the paper develops First
Order Decision Diagrams (FODD), a new compact representation for functions over
relational structures, together with a set of operators to combine FODDs, and
novel reduction techniques to keep the representation small. Second, the paper
shows how FODDs can be used to develop solutions for RMDPs, where reasoning is
performed at the abstract level and the resulting optimal policy is independent
of domain size (number of objects) or instantiation. In particular, a variant
of the value iteration algorithm is developed by using special operations over
FODDs, and the algorithm is shown to converge to the optimal policy.",N/A
"Clause/Term Resolution and Learning in the Evaluation of Quantified
  Boolean Formulas","Resolution is the rule of inference at the basis of most procedures for
automated reasoning. In these procedures, the input formula is first translated
into an equisatisfiable formula in conjunctive normal form (CNF) and then
represented as a set of clauses. Deduction starts by inferring new clauses by
resolution, and goes on until the empty clause is generated or satisfiability
of the set of clauses is proven, e.g., because no new clauses can be generated.
  In this paper, we restrict our attention to the problem of evaluating
Quantified Boolean Formulas (QBFs). In this setting, the above outlined
deduction process is known to be sound and complete if given a formula in CNF
and if a form of resolution, called Q-resolution, is used. We introduce
Q-resolution on terms, to be used for formulas in disjunctive normal form. We
show that the computation performed by most of the available procedures for
QBFs --based on the Davis-Logemann-Loveland procedure (DLL) for propositional
satisfiability-- corresponds to a tree in which Q-resolution on terms and
clauses alternate. This poses the theoretical bases for the introduction of
learning, corresponding to recording Q-resolution formulas associated with the
nodes of the tree. We discuss the problems related to the introduction of
learning in DLL based procedures, and present solutions extending
state-of-the-art proposals coming from the literature on propositional
satisfiability. Finally, we show that our DLL based solver extended with
learning, performs significantly better on benchmarks used in the 2003 QBF
solvers comparative evaluation.",N/A
"MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and
  Practical Realization of Automated Constructor of Algorithms Handling More
  than Three Million Production Rules","The theoretical transition from the graphs of production systems to the
bipartite graphs of the MIVAR nets is shown. Examples of the implementation of
the MIVAR nets in the formalisms of matrixes and graphs are given. The linear
computational complexity of algorithms for automated building of objects and
rules of the MIVAR nets is theoretically proved. On the basis of the MIVAR nets
the UDAV software complex is developed, handling more than 1.17 million objects
and more than 3.5 million rules on ordinary computers. The results of
experiments that confirm a linear computational complexity of the MIVAR method
of information processing are given.
  Keywords: MIVAR, MIVAR net, logical inference, computational complexity,
artificial intelligence, intelligent systems, expert systems, General Problem
Solver.","23 pages, 21 figures"
Embedding Description Logic Programs into Default Logic,"Description logic programs (dl-programs) under the answer set semantics
formulated by Eiter {\em et al.} have been considered as a prominent formalism
for integrating rules and ontology knowledge bases. A question of interest has
been whether dl-programs can be captured in a general formalism of nonmonotonic
logic. In this paper, we study the possibility of embedding dl-programs into
default logic. We show that dl-programs under the strong and weak answer set
semantics can be embedded in default logic by combining two translations, one
of which eliminates the constraint operator from nonmonotonic dl-atoms and the
other translates a dl-program into a default theory. For dl-programs without
nonmonotonic dl-atoms but with the negation-as-failure operator, our embedding
is polynomial, faithful, and modular. In addition, our default logic encoding
can be extended in a simple way to capture recently proposed weakly
well-supported answer set semantics, for arbitrary dl-programs. These results
reinforce the argument that default logic can serve as a fruitful foundation
for query-based approaches to integrating ontology and rules. With its simple
syntax and intuitive semantics, plus available computational results, default
logic can be considered an attractive approach to integration of ontology and
rules.",53 pages
"Semantic-Driven e-Government: Application of Uschold and King Ontology
  Building Methodology for Semantic Ontology Models Development","Electronic government (e-government) has been one of the most active areas of
ontology development during the past six years. In e-government, ontologies are
being used to describe and specify e-government services (e-services) because
they enable easy composition, matching, mapping and merging of various
e-government services. More importantly, they also facilitate the semantic
integration and interoperability of e-government services. However, it is still
unclear in the current literature how an existing ontology building methodology
can be applied to develop semantic ontology models in a government service
domain. In this paper the Uschold and King ontology building methodology is
applied to develop semantic ontology models in a government service domain.
Firstly, the Uschold and King methodology is presented, discussed and applied
to build a government domain ontology. Secondly, the domain ontology is
evaluated for semantic consistency using its semi-formal representation in
Description Logic. Thirdly, an alignment of the domain ontology with the
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper
level ontology is drawn to allow its wider visibility and facilitate its
integration with existing metadata standard. Finally, the domain ontology is
formally written in Web Ontology Language (OWL) to enable its automatic
processing by computers. The study aims to provide direction for the
application of existing ontology building methodologies in the Semantic Web
development processes of e-government domain specific ontology models; which
would enable their repeatability in other e-government projects and strengthen
the adoption of semantic technologies in e-government.","20 pages, 6 figures"
SATzilla: Portfolio-based Algorithm Selection for SAT,"It has been widely observed that there is no single ""dominant"" SAT solver;
instead, different solvers perform best on different instances. Rather than
following the traditional approach of choosing the best solver for a given
class of instances, we advocate making this decision online on a per-instance
basis. Building on previous work, we describe SATzilla, an automated approach
for constructing per-instance algorithm portfolios for SAT that use so-called
empirical hardness models to choose among their constituent solvers. This
approach takes as input a distribution of problem instances and a set of
component solvers, and constructs a portfolio optimizing a given objective
function (such as mean runtime, percent of instances solved, or score in a
competition). The excellent performance of SATzilla was independently verified
in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one
silver and one bronze medal. In this article, we go well beyond SATzilla07 by
making the portfolio construction scalable and completely automated, and
improving it by integrating local search solvers as candidate solvers, by
predicting performance score instead of runtime, and by using hierarchical
hardness models that take into account different types of SAT instances. We
demonstrate the effectiveness of these new techniques in extensive experimental
results on data sets including instances from the most recent SAT competition.",N/A
8-Valent Fuzzy Logic for Iris Recognition and Biometry,"This paper shows that maintaining logical consistency of an iris recognition
system is a matter of finding a suitable partitioning of the input space in
enrollable and unenrollable pairs by negotiating the user comfort and the
safety of the biometric system. In other words, consistent enrollment is
mandatory in order to preserve system consistency. A fuzzy 3-valued
disambiguated model of iris recognition is proposed and analyzed in terms of
completeness, consistency, user comfort and biometric safety. It is also shown
here that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued
Boolean algebra of modulo 8 integers that represents the computational
formalization in which a biometric system (a software agent) can achieve the
artificial understanding of iris recognition in a logically consistent manner.","6 pages, 2 figures, 5th IEEE Int. Symp. on Computational Intelligence
  and Intelligent Informatics (Floriana, Malta, September 15-17), ISBN:
  978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print), 2011"
"New Candidates Welcome! Possible Winners with respect to the Addition of
  New Candidates","In voting contexts, some new candidates may show up in the course of the
process. In this case, we may want to determine which of the initial candidates
are possible winners, given that a fixed number $k$ of new candidates will be
added. We give a computational study of this problem, focusing on scoring
rules, and we provide a formal comparison with related problems such as control
via adding candidates or cloning.",34 pages
Model-based Utility Functions,"Orseau and Ring, as well as Dewey, have recently described problems,
including self-delusion, with the behavior of agents using various definitions
of utility functions. An agent's utility function is defined in terms of the
agent's history of interactions with its environment. This paper argues, via
two examples, that the behavior problems can be avoided by formulating the
utility function in two steps: 1) inferring a model of the environment from
interactions, and 2) computing utility as a function of the environment model.
Basing a utility function on a model that the agent must learn implies that the
utility function must initially be expressed in terms of specifications to be
matched to structures in the learned model. These specifications constitute
prior assumptions about the environment so this approach will not work with
arbitrary environments. But the approach should work for agents designed by
humans to act in the physical world. The paper also addresses the issue of
self-modifying agents and shows that if provided with the possibility to modify
their utility functions agents will not choose to do so, under some usual
assumptions.","24 pages, extensive revisions"
Unbiased Statistics of a CSP - A Controlled-Bias Generator,"We show that estimating the complexity (mean and distribution) of the
instances of a fixed size Constraint Satisfaction Problem (CSP) can be very
hard. We deal with the main two aspects of the problem: defining a measure of
complexity and generating random unbiased instances. For the first problem, we
rely on a general framework and a measure of complexity we presented at
CISSE08. For the generation problem, we restrict our analysis to the Sudoku
example and we provide a solution that also explains why it is so difficult.",N/A
A Model of Spatial Thinking for Computational Intelligence,"Trying to be effective (no matter who exactly and in what field) a person
face the problem which inevitably destroys all our attempts to easily get to a
desired goal. The problem is the existence of some insuperable barriers for our
mind, anotherwords barriers for principles of thinking. They are our clue and
main reason for research. Here we investigate these barriers and their features
exposing the nature of mental process. We start from special structures which
reflect the ways to define relations between objects. Then we came to realizing
about what is the material our mind uses to build thoughts, to make
conclusions, to understand, to form reasoning, etc. This can be called a mental
dynamics. After this the nature of mental barriers on the required level of
abstraction as well as the ways to pass through them became clear. We begin to
understand why thinking flows in such a way, with such specifics and with such
limitations we can observe in reality. This can help us to be more optimal. At
the final step we start to understand, what ma-thematical models can be applied
to such a picture. We start to express our thoughts in a language of
mathematics, developing an apparatus for our Spatial Theory of Mind, suitable
to represent processes and infrastructure of thinking. We use abstract algebra
and stay invariant in relation to the nature of objects.","8 pages, 5 figures; IEEE East-West Design & Test Symposium, 2011"
Revisiting Numerical Pattern Mining with Formal Concept Analysis,"In this paper, we investigate the problem of mining numerical data in the
framework of Formal Concept Analysis. The usual way is to use a scaling
procedure --transforming numerical attributes into binary ones-- leading either
to a loss of information or of efficiency, in particular w.r.t. the volume of
extracted patterns. By contrast, we propose to directly work on numerical data
in a more precise and efficient way, and we prove it. For that, the notions of
closed patterns, generators and equivalent classes are revisited in the
numerical context. Moreover, two original algorithms are proposed and used in
an evaluation involving real-world data, showing the predominance of the
present approach.",N/A
Principles of Solomonoff Induction and AIXI,"We identify principles characterizing Solomonoff Induction by demands on an
agent's external behaviour. Key concepts are rationality, computability,
indifference and time consistency. Furthermore, we discuss extensions to the
full AI case to derive AIXI.",14 LaTeX pages
Pattern-Based Classification: A Unifying Perspective,"The use of patterns in predictive models is a topic that has received a lot
of attention in recent years. Pattern mining can help to obtain models for
structured domains, such as graphs and sequences, and has been proposed as a
means to obtain more accurate and more interpretable models. Despite the large
amount of publications devoted to this topic, we believe however that an
overview of what has been accomplished in this area is missing. This paper
presents our perspective on this evolving area. We identify the principles of
pattern mining that are important when mining patterns for models and provide
an overview of pattern-based classification methods. We categorize these
methods along the following dimensions: (1) whether they post-process a
pre-computed set of patterns or iteratively execute pattern mining algorithms;
(2) whether they select patterns model-independently or whether the pattern
selection is guided by a model. We summarize the results that have been
obtained for each of these methods.",N/A
Graph based E-Government web service composition,"Nowadays, e-government has emerged as a government policy to improve the
quality and efficiency of public administrations. By exploiting the potential
of new information and communication technologies, government agencies are
providing a wide spectrum of online services. These services are composed of
several web services that comply with well defined processes. One of the big
challenges is the need to optimize the composition of the elementary web
services. In this paper, we present a solution for optimizing the computation
effort in web service composition. Our method is based on Graph Theory. We
model the semantic relationship between the involved web services through a
directed graph. Then, we compute all shortest paths using for the first time,
an extended version of the Floyd-Warshall algorithm.",N/A
An Enhanced Indexing And Ranking Technique On The Semantic Web,"With the fast growth of the Internet, more and more information is available
on the Web. The Semantic Web has many features which cannot be handled by using
the traditional search engines. It extracts metadata for each discovered Web
documents in RDF or OWL formats, and computes relations between documents. We
proposed a hybrid indexing and ranking technique for the Semantic Web which
finds relevant documents and computes the similarity among a set of documents.
First, it returns with the most related document from the repository of
Semantic Web Documents (SWDs) by using a modified version of the ObjectRank
technique. Then, it creates a sub-graph for the most related SWDs. Finally, It
returns the hubs and authorities of these document by using the HITS algorithm.
Our technique increases the quality of the results and decreases the execution
time of processing the user's query.","8 pages, 7 figures"
"Constraining the Size Growth of the Task Space with Socially Guided
  Intrinsic Motivation using Demonstrations","This paper presents an algorithm for learning a highly redundant inverse
model in continuous and non-preset environments. Our Socially Guided Intrinsic
Motivation by Demonstrations (SGIM-D) algorithm combines the advantages of both
social learning and intrinsic motivation, to specialise in a wide range of
skills, while lessening its dependence on the teacher. SGIM-D is evaluated on a
fishing skill learning experiment.","JCAI Workshop on Agents Learning Interactively from Human Teachers
  (ALIHT), Barcelona : Spain (2011)"
"Label Ranking with Abstention: Predicting Partial Orders by Thresholding
  Probability Distributions (Extended Abstract)","We consider an extension of the setting of label ranking, in which the
learner is allowed to make predictions in the form of partial instead of total
orders. Predictions of that kind are interpreted as a partial abstention: If
the learner is not sufficiently certain regarding the relative order of two
alternatives, it may abstain from this decision and instead declare these
alternatives as being incomparable. We propose a new method for learning to
predict partial orders that improves on an existing approach, both
theoretically and empirically. Our method is based on the idea of thresholding
the probabilities of pairwise preferences between labels as induced by a
predicted (parameterized) probability distribution on the set of all rankings.","4 pages, 1 figure, appeared at NIPS 2011 Choice Models and Preference
  Learning workshop"
Multi-granular Perspectives on Covering,"Covering model provides a general framework for granular computing in that
overlapping among granules are almost indispensable. For any given covering,
both intersection and union of covering blocks containing an element are
exploited as granules to form granular worlds at different abstraction levels,
respectively, and transformations among these different granular worlds are
also discussed. As an application of the presented multi-granular perspective
on covering, relational interpretation and axiomization of four types of
covering based rough upper approximation operators are investigated, which can
be dually applied to lower ones.",12 pages
"Incremental Slow Feature Analysis: Adaptive and Episodic Learning from
  High-Dimensional Input Streams","Slow Feature Analysis (SFA) extracts features representing the underlying
causes of changes within a temporally coherent high-dimensional raw sensory
input signal. Our novel incremental version of SFA (IncSFA) combines
incremental Principal Components Analysis and Minor Components Analysis. Unlike
standard batch-based SFA, IncSFA adapts along with non-stationary environments,
is amenable to episodic training, is not corrupted by outliers, and is
covariance-free. These properties make IncSFA a generally useful unsupervised
preprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA
and MCA updates take the form of Hebbian and anti-Hebbian updating, extending
the biological plausibility of SFA. In both single node and deep network
versions, IncSFA learns to encode its input streams (such as high-dimensional
video) by informative slow features representing meaningful abstract
environmental properties. It can handle cases where batch SFA fails.",N/A
Threshold Choice Methods: the Missing Link,"Many performance metrics have been introduced for the evaluation of
classification performance, with different origins and niches of application:
accuracy, macro-accuracy, area under the ROC curve, the ROC convex hull, the
absolute error, and the Brier score (with its decomposition into refinement and
calibration). One way of understanding the relation among some of these metrics
is the use of variable operating conditions (either in the form of
misclassification costs or class proportions). Thus, a metric may correspond to
some expected loss over a range of operating conditions. One dimension for the
analysis has been precisely the distribution we take for this range of
operating conditions, leading to some important connections in the area of
proper scoring rules. However, we show that there is another dimension which
has not received attention in the analysis of performance metrics. This new
dimension is given by the decision rule, which is typically implemented as a
threshold choice method when using scoring models. In this paper, we explore
many old and new threshold choice methods: fixed, score-uniform, score-driven,
rate-driven and optimal, among others. By calculating the loss of these methods
for a uniform range of operating conditions we get the 0-1 loss, the absolute
error, the Brier score (mean squared error), the AUC and the refinement loss
respectively. This provides a comprehensive view of performance metrics as well
as a systematic approach to loss minimisation, namely: take a model, apply
several threshold choice methods consistent with the information which is (and
will be) available about the operating condition, and compare their expected
losses. In order to assist in this procedure we also derive several connections
between the aforementioned performance metrics, and we highlight the role of
calibration in choosing the threshold choice method.",N/A
"Inference in Probabilistic Logic Programs with Continuous Random
  Variables","Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's
PRISM, Poole's ICL, Raedt et al's ProbLog and Vennekens et al's LPAD, is aimed
at combining statistical and logical knowledge representation and inference. A
key characteristic of PLP frameworks is that they are conservative extensions
to non-probabilistic logic programs which have been widely used for knowledge
representation. PLP frameworks extend traditional logic programming semantics
to a distribution semantics, where the semantics of a probabilistic logic
program is given in terms of a distribution over possible models of the
program. However, the inference techniques used in these works rely on
enumerating sets of explanations for a query answer. Consequently, these
languages permit very limited use of random variables with continuous
distributions. In this paper, we present a symbolic inference procedure that
uses constraints and represents sets of explanations without enumeration. This
permits us to reason over PLPs with Gaussian or Gamma-distributed random
variables (in addition to discrete-valued random variables) and linear equality
constraints over reals. We develop the inference procedure in the context of
PRISM; however the procedure's core ideas can be easily applied to other PLP
languages as well. An interesting aspect of our inference procedure is that
PRISM's query evaluation process becomes a special case in the absence of any
continuous random variables in the program. The symbolic inference procedure
enables us to reason over complex probabilistic models such as Kalman filters
and a large subclass of Hybrid Bayesian networks that were hitherto not
possible in PLP frameworks. (To appear in Theory and Practice of Logic
Programming).","12 pages. arXiv admin note: substantial text overlap with
  arXiv:1203.4287"
"Improving the Efficiency of Approximate Inference for Probabilistic
  Logical Models by means of Program Specialization","We consider the task of performing probabilistic inference with probabilistic
logical models. Many algorithms for approximate inference with such models are
based on sampling. From a logic programming perspective, sampling boils down to
repeatedly calling the same queries on a knowledge base composed of a static
part and a dynamic part. The larger the static part, the more redundancy there
is in these repeated calls. This is problematic since inefficient sampling
yields poor approximations.
  We show how to apply logic program specialization to make sampling-based
inference more efficient. We develop an algorithm that specializes the
definitions of the query predicates with respect to the static part of the
knowledge base. In experiments on real-world data we obtain speedups of up to
an order of magnitude, and these speedups grow with the data-size.",17 pages
Continuity in Information Algebras,"In this paper, the continuity and strong continuity in domain-free
information algebras and labeled information algebras are introduced
respectively. A more general concept of continuous function which is defined
between two domain-free continuous information algebras is presented. It is
shown that, with the operations combination and focusing, the set of all
continuous functions between two domain-free s-continuous information algebras
forms a new s-continuous information algebra. By studying the relationship
between domain-free information algebras and labeled information algebras, it
is demonstrated that they do correspond to each other on s-compactness.",N/A
The RegularGcc Matrix Constraint,"We study propagation of the RegularGcc global constraint. This ensures that
each row of a matrix of decision variables satisfies a Regular constraint, and
each column satisfies a Gcc constraint. On the negative side, we prove that
propagation is NP-hard even under some strong restrictions (e.g. just 3 values,
just 4 states in the automaton, or just 5 columns to the matrix). On the
positive side, we identify two cases where propagation is fixed parameter
tractable. In addition, we show how to improve propagation over a simple
decomposition into separate Regular and Gcc constraints by identifying some
necessary but insufficient conditions for a solution. We enforce these
conditions with some additional weighted row automata. Experimental results
demonstrate the potential of these methods on some standard benchmark problems.",Submitted to CPAIOR 2012
"Optimal Fuzzy Model Construction with Statistical Information using
  Genetic Algorithm","Fuzzy rule based models have a capability to approximate any continuous
function to any degree of accuracy on a compact domain. The majority of FLC
design process relies on heuristic knowledge of experience operators. In order
to make the design process automatic we present a genetic approach to learn
fuzzy rules as well as membership function parameters. Moreover, several
statistical information criteria such as the Akaike information criterion
(AIC), the Bhansali-Downham information criterion (BDIC), and the
Schwarz-Rissanen information criterion (SRIC) are used to construct optimal
fuzzy models by reducing fuzzy rules. A genetic scheme is used to design
Takagi-Sugeno-Kang (TSK) model for identification of the antecedent rule
parameters and the identification of the consequent parameters. Computer
simulations are presented confirming the performance of the constructed fuzzy
logic controller.",N/A
"Ultrametric Model of Mind, I: Review","We mathematically model Ignacio Matte Blanco's principles of symmetric and
asymmetric being through use of an ultrametric topology. We use for this the
highly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born
1908, died 1995). Such an ultrametric model corresponds to hierarchical
clustering in the empirical data, e.g. text. We show how an ultrametric
topology can be used as a mathematical model for the structure of the logic
that reflects or expresses Matte Blanco's symmetric being, and hence of the
reasoning and thought processes involved in conscious reasoning or in reasoning
that is lacking, perhaps entirely, in consciousness or awareness of itself. In
a companion paper we study how symmetric (in the sense of Matte Blanco's)
reasoning can be demarcated in a context of symmetric and asymmetric reasoning
provided by narrative text.","20 pages, 2 figures, 46 references. arXiv admin note: substantial
  text overlap with arXiv:0709.0116, arXiv:0805.2744, and arXiv:1105.0121 (V3:
  2 typos corrected)"
"Tacit knowledge mining algorithm based on linguistic truth-valued
  concept lattice","This paper is the continuation of our research work about linguistic
truth-valued concept lattice. In order to provide a mathematical tool for
mining tacit knowledge, we establish a concrete model of 6-ary linguistic
truth-valued concept lattice and introduce a mining algorithm through the
structure consistency. Specifically, we utilize the attributes to depict
knowledge, propose the 6-ary linguistic truth-valued attribute extended context
and congener context to characterize tacit knowledge, and research the
necessary and sufficient conditions of forming tacit knowledge. We respectively
give the algorithms of generating the linguistic truth-valued congener context
and constructing the linguistic truth-valued concept lattice.",N/A
"Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy","Large-scale, parallel clusters composed of commodity processors are
increasingly available, enabling the use of vast processing capabilities and
distributed RAM to solve hard search problems. We investigate Hash-Distributed
A* (HDA*), a simple approach to parallel best-first search that asynchronously
distributes and schedules work among processors based on a hash function of the
search state. We use this approach to parallelize the A* algorithm in an
optimal sequential version of the Fast Downward planner, as well as a 24-puzzle
solver. The scaling behavior of HDA* is evaluated experimentally on a shared
memory, multicore machine with 8 cores, a cluster of commodity machines using
up to 64 cores, and large-scale high-performance clusters, using up to 2400
processors. We show that this approach scales well, allowing the effective
utilization of large amounts of distributed memory to optimally solve problems
which require terabytes of RAM. We also compare HDA* to Transposition-table
Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that,
in planning, HDA* significantly outperforms TDS. A simple hybrid which combines
HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.","in press, to appear in Artificial Intelligence"
The computation of first order moments on junction trees,"We review some existing methods for the computation of first order moments on
junction trees using Shafer-Shenoy algorithm. First, we consider the problem of
first order moments computation as vertices problem in junction trees. In this
way, the problem is solved using the memory space of an order of the junction
tree edge-set cardinality. After that, we consider two algorithms,
Lauritzen-Nilsson algorithm, and Mau\'a et al. algorithm, which computes the
first order moments as the normalization problem in junction tree, using the
memory space of an order of the junction tree leaf-set cardinality.","9 pages, 1 figure"
"Progress in animation of an EMA-controlled tongue model for
  acoustic-visual speech synthesis","We present a technique for the animation of a 3D kinematic tongue model, one
component of the talking head of an acoustic-visual (AV) speech synthesizer.
The skeletal animation approach is adapted to make use of a deformable rig
controlled by tongue motion capture data obtained with electromagnetic
articulography (EMA), while the tongue surface is extracted from volumetric
magnetic resonance imaging (MRI) data. Initial results are shown and future
work outlined.",N/A
Constraint Propagation as Information Maximization,"This paper draws on diverse areas of computer science to develop a unified
view of computation:
  (1) Optimization in operations research, where a numerical objective function
is maximized under constraints, is generalized from the numerical total order
to a non-numerical partial order that can be interpreted in terms of
information. (2) Relations are generalized so that there are relations of which
the constituent tuples have numerical indexes, whereas in other relations these
indexes are variables. The distinction is essential in our definition of
constraint satisfaction problems. (3) Constraint satisfaction problems are
formulated in terms of semantics of conjunctions of atomic formulas of
predicate logic. (4) Approximation structures, which are available for several
important domains, are applied to solutions of constraint satisfaction
problems.
  As application we treat constraint satisfaction problems over reals. These
cover a large part of numerical analysis, most significantly nonlinear
equations and inequalities. The chaotic algorithm analyzed in the paper
combines the efficiency of floating-point computation with the correctness
guarantees of arising from our logico-mathematical model of
constraint-satisfaction problems.",21 pages
A multiagent urban traffic simulation,"We built a multiagent simulation of urban traffic to model both ordinary
traffic and emergency or crisis mode traffic. This simulation first builds a
modeled road network based on detailed geographical information. On this
network, the simulation creates two populations of agents: the Transporters and
the Mobiles. Transporters embody the roads themselves; they are utilitarian and
meant to handle the low level realism of the simulation. Mobile agents embody
the vehicles that circulate on the network. They have one or several
destinations they try to reach using initially their beliefs of the structure
of the network (length of the edges, speed limits, number of lanes etc.).
Nonetheless, when confronted to a dynamic, emergent prone environment (other
vehicles, unexpectedly closed ways or lanes, traffic jams etc.), the rather
reactive agent will activate more cognitive modules to adapt its beliefs,
desires and intentions. It may change its destination(s), change the tactics
used to reach the destination (favoring less used roads, following other
agents, using general headings), etc. We describe our current validation of our
model and the next planned improvements, both in validation and in
functionalities.","arXiv admin note: significant text overlap with arXiv:0909.1021 and
  arXiv:0910.1026"
The thermodynamic cost of fast thought,"After more than sixty years, Shannon's research [1-3] continues to raise
fundamental questions, such as the one formulated by Luce [4,5], which is still
unanswered: ""Why is information theory not very applicable to psychological
problems, despite apparent similarities of concepts?"" On this topic, Pinker
[6], one of the foremost defenders of the computational theory of mind [6], has
argued that thought is simply a type of computation, and that the gap between
human cognition and computational models may be illusory. In this context, in
his latest book, titled Thinking Fast and Slow [8], Kahneman [7,8] provides
further theoretical interpretation by differentiating the two assumed systems
of the cognitive functioning of the human mind. He calls them intuition (system
1) determined to be an associative (automatic, fast and perceptual) machine,
and reasoning (system 2) required to be voluntary and to operate logical-
deductively. In this paper, we propose an ansatz inspired by Ausubel's learning
theory for investigating, from the constructivist perspective [9-12],
information processing in the working memory of cognizers. Specifically, a
thought experiment is performed utilizing the mind of a dual-natured creature
known as Maxwell's demon: a tiny ""man-machine"" solely equipped with the
characteristics of system 1, which prevents it from reasoning. The calculation
presented here shows that [...]. This result indicates that when the system 2
is shut down, both an intelligent being, as well as a binary machine, incur the
same energy cost per unit of information processed, which mathematically proves
the computational attribute of the system 1, as Kahneman [7,8] theorized. This
finding links information theory to human psychological features and opens a
new path toward the conception of a multi-bit reasoning machine.",N/A
Ontologies for the Integration of Air Quality Models and 3D City Models,"The holistic approach to sustainable urban planning implies using different
models in an integrated way that is capable of simulating the urban system. As
the interconnection of such models is not a trivial task, one of the key
elements that may be applied is the description of the urban geometric
properties in an ""interoperable"" way. Focusing on air quality as one of the
most pronounced urban problems, the geometric aspects of a city may be
described by objects such as those defined in CityGML, so that an appropriate
air quality model can be applied for estimating the quality of the urban air on
the basis of atmospheric flow and chemistry equations.
  In this paper we first present theoretical background and motivations for the
interconnection of 3D city models and other models related to sustainable
development and urban planning. Then we present a practical experiment based on
the interconnection of CityGML with an air quality model. Our approach is based
on the creation of an ontology of air quality models and on the extension of an
ontology of urban planning process (OUPP) that acts as an ontology mediator.",N/A
"The implications of embodiment for behavior and cognition: animal and
  robotic case studies","In this paper, we will argue that if we want to understand the function of
the brain (or the control in the case of robots), we must understand how the
brain is embedded into the physical system, and how the organism interacts with
the real world. While embodiment has often been used in its trivial meaning,
i.e. 'intelligence requires a body', the concept has deeper and more important
implications, concerned with the relation between physical and information
(neural, control) processes. A number of case studies are presented to
illustrate the concept. These involve animals and robots and are concentrated
around locomotion, grasping, and visual perception. A theoretical scheme that
can be used to embed the diverse case studies will be presented. Finally, we
will establish a link between the low-level sensory-motor processes and
cognition. We will present an embodied view on categorization, and propose the
concepts of 'body schema' and 'forward models' as a natural extension of the
embodied approach toward first representations.","Book chapter in W. Tschacher & C. Bergomi, ed., 'The Implications of
  Embodiment: Cognition and Communication', Exeter: Imprint Academic, pp. 31-58"
"On the influence of intelligence in (social) intelligence testing
  environments","This paper analyses the influence of including agents of different degrees of
intelligence in a multiagent system. The goal is to better understand how we
can develop intelligence tests that can evaluate social intelligence. We
analyse several reinforcement algorithms in several contexts of cooperation and
competition. Our experimental setting is inspired by the recently developed
Darwin-Wallace distribution.",N/A
Classification of artificial intelligence ids for smurf attack,"Many methods have been developed to secure the network infrastructure and
communication over the Internet. Intrusion detection is a relatively new
addition to such techniques. Intrusion detection systems (IDS) are used to find
out if someone has intrusion into or is trying to get it the network. One big
problem is amount of Intrusion which is increasing day by day. We need to know
about network attack information using IDS, then analysing the effect. Due to
the nature of IDSs which are solely signature based, every new intrusion cannot
be detected; so it is important to introduce artificial intelligence (AI)
methods / techniques in IDS. Introduction of AI necessitates the importance of
normalization in intrusions. This work is focused on classification of AI based
IDS techniques which will help better design intrusion detection systems in the
future. We have also proposed a support vector machine for IDS to detect Smurf
attack with much reliable accuracy.","6 pages, 5 figures, 1 table"
"Hyper heuristic based on great deluge and its variants for exam
  timetabling problem","Today, University Timetabling problems are occurred annually and they are
often hard and time consuming to solve. This paper describes Hyper Heuristics
(HH) method based on Great Deluge (GD) and its variants for solving large,
highly constrained timetabling problems from different domains. Generally, in
hyper heuristic framework, there are two main stages: heuristic selection and
move acceptance. This paper emphasizes on the latter stage to develop Hyper
Heuristic (HH) framework. The main contribution of this paper is that Great
Deluge (GD) and its variants: Flex Deluge(FD), Non-linear(NLGD), Extended Great
Deluge(EGD) are used as move acceptance method in HH by combining Reinforcement
learning (RL).These HH methods are tested on exam benchmark timetabling problem
and best results and comparison analysis are reported.",N/A
"A framework: Cluster detection and multidimensional visualization of
  automated data mining using intelligent agents","Data Mining techniques plays a vital role like extraction of required
knowledge, finding unsuspected information to make strategic decision in a
novel way which in term understandable by domain experts. A generalized frame
work is proposed by considering non - domain experts during mining process for
better understanding, making better decision and better finding new patters in
case of selecting suitable data mining techniques based on the user profile by
means of intelligent agents. KEYWORDS: Data Mining Techniques, Intelligent
Agents, User Profile, Multidimensional Visualization, Knowledge Discovery.",15 pages
Extended Lifted Inference with Joint Formulas,"The First-Order Variable Elimination (FOVE) algorithm allows exact inference
to be applied directly to probabilistic relational models, and has proven to be
vastly superior to the application of standard inference methods on a grounded
propositional model. Still, FOVE operators can be applied under restricted
conditions, often forcing one to resort to propositional inference. This paper
aims to extend the applicability of FOVE by providing two new model conversion
operators: the first and the primary is joint formula conversion and the second
is just-different counting conversion. These new operations allow efficient
inference methods to be applied directly on relational models, where no
existing efficient method could be applied hitherto. In addition, aided by
these capabilities, we show how to adapt FOVE to provide exact solutions to
Maximum Expected Utility (MEU) queries over relational models for decision
under uncertainty. Experimental evaluations show our algorithms to provide
significant speedup over the alternatives.",N/A
"Learning is planning: near Bayes-optimal reinforcement learning via
  Monte-Carlo tree search","Bayes-optimal behavior, while well-defined, is often difficult to achieve.
Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it
is possible to act near-optimally in Markov Decision Processes (MDPs) with very
large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is
equivalent to optimal behavior in the known belief-space MDP, although the size
of this belief-space MDP grows exponentially with the amount of history
retained, and is potentially infinite. We show how an agent can use one
particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an
efficient way to act nearly Bayes-optimally for all but a polynomial number of
steps, assuming that FSSS can be used to act efficiently in any possible
underlying MDP.",N/A
A temporally abstracted Viterbi algorithm,"Hierarchical problem abstraction, when applicable, may offer exponential
reductions in computational complexity. Previous work on coarse-to-fine dynamic
programming (CFDP) has demonstrated this possibility using state abstraction to
speed up the Viterbi algorithm. In this paper, we show how to apply temporal
abstraction to the Viterbi problem. Our algorithm uses bounds derived from
analysis of coarse timescales to prune large parts of the state trellis at
finer timescales. We demonstrate improvements of several orders of magnitude
over the standard Viterbi algorithm, as well as significant speedups over CFDP,
for problems whose state variables evolve at widely differing rates.",N/A
EDML: A Method for Learning Parameters in Bayesian Networks,"We propose a method called EDML for learning MAP parameters in binary
Bayesian networks under incomplete data. The method assumes Beta priors and can
be used to learn maximum likelihood parameters when the priors are
uninformative. EDML exhibits interesting behaviors, especially when compared to
EM. We introduce EDML, explain its origin, and study some of its properties
both analytically and empirically.",N/A
A Logical Characterization of Constraint-Based Causal Discovery,"We present a novel approach to constraint-based causal discovery, that takes
the form of straightforward logical inference, applied to a list of simple,
logical statements about causal relations that are derived directly from
observed (in)dependencies. It is both sound and complete, in the sense that all
invariant features of the corresponding partial ancestral graph (PAG) are
identified, even in the presence of latent variables and selection bias. The
approach shows that every identifiable causal relation corresponds to one of
just two fundamental forms. More importantly, as the basic building blocks of
the method do not rely on the detailed (graphical) structure of the
corresponding PAG, it opens up a range of new opportunities, including more
robust inference, detailed accountability, and application to large models.",N/A
Bayesian network learning with cutting planes,"The problem of learning the structure of Bayesian networks from complete
discrete data with a limit on parent set size is considered. Learning is cast
explicitly as an optimisation problem where the goal is to find a BN structure
which maximises log marginal likelihood (BDe score). Integer programming,
specifically the SCIP framework, is used to solve this optimisation problem.
Acyclicity constraints are added to the integer program (IP) during solving in
the form of cutting planes. Finding good cutting planes is the key to the
success of the approach -the search for such cutting planes is effected using a
sub-IP. Results show that this is a particularly fast method for exact BN
learning.",N/A
On the Complexity of Decision Making in Possibilistic Decision Trees,"When the information about uncertainty cannot be quantified in a simple,
probabilistic way, the topic of possibilistic decision theory is often a
natural one to consider. The development of possibilistic decision theory has
lead to a series of possibilistic criteria, e.g pessimistic possibilistic
qualitative utility, possibilistic likely dominance, binary possibilistic
utility and possibilistic Choquet integrals. This paper focuses on sequential
decision making in possibilistic decision trees. It proposes a complexity study
of the problem of finding an optimal strategy depending on the monotonicity
property of the optimization criteria which allows the application of dynamic
programming that offers a polytime reduction of the decision problem. It also
shows that possibilistic Choquet integrals do not satisfy this property, and
that in this case the optimization problem is NP - hard.",N/A
Inference in Probabilistic Logic Programs using Weighted CNF's,"Probabilistic logic programs are logic programs in which some of the facts
are annotated with probabilities. Several classical probabilistic inference
tasks (such as MAP and computing marginals) have not yet received a lot of
attention for this formalism. The contribution of this paper is that we develop
efficient inference algorithms for these tasks. This is based on a conversion
of the probabilistic logic program and the query and evidence to a weighted CNF
formula. This allows us to reduce the inference tasks to well-studied tasks
such as weighted model counting. To solve such tasks, we employ
state-of-the-art methods. We consider multiple methods for the conversion of
the programs as well as for inference on the weighted CNF. The resulting
approach is evaluated experimentally and shown to improve upon the
state-of-the-art in probabilistic logic programming.",N/A
Dynamic consistency and decision making under vacuous belief,"The ideas about decision making under ignorance in economics are combined
with the ideas about uncertainty representation in computer science. The
combination sheds new light on the question of how artificial agents can act in
a dynamically consistent manner. The notion of sequential consistency is
formalized by adapting the law of iterated expectation for plausibility
measures. The necessary and sufficient condition for a certainty equivalence
operator for Nehring-Puppe's preference to be sequentially consistent is given.
This result sheds light on the models of decision making under uncertainty.",N/A
Approximation by Quantization,"Inference in graphical models consists of repeatedly multiplying and summing
out potentials. It is generally intractable because the derived potentials
obtained in this way can be exponentially large. Approximate inference
techniques such as belief propagation and variational methods combat this by
simplifying the derived potentials, typically by dropping variables from them.
We propose an alternate method for simplifying potentials: quantizing their
values. Quantization causes different states of a potential to have the same
value, and therefore introduces context-specific independencies that can be
exploited to represent the potential more compactly. We use algebraic decision
diagrams (ADDs) to do this efficiently. We apply quantization and ADD reduction
to variable elimination and junction tree propagation, yielding a family of
bounded approximate inference schemes. Our experimental tests show that our new
schemes significantly outperform state-of-the-art approaches on many benchmark
instances.",N/A
Probabilistic Theorem Proving,"Many representation schemes combining first-order logic and probability have
been proposed in recent years. Progress in unifying logical and probabilistic
inference has been slower. Existing methods are mainly variants of lifted
variable elimination and belief propagation, neither of which take logical
structure into account. We propose the first method that has the full power of
both graphical model inference and first-order theorem proving (in finite
domains with Herbrand interpretations). We first define probabilistic theorem
proving, their generalization, as the problem of computing the probability of a
logical formula given the probabilities or weights of a set of formulas. We
then show how this can be reduced to the problem of lifted weighted model
counting, and develop an efficient algorithm for the latter. We prove the
correctness of this algorithm, investigate its properties, and show how it
generalizes previous approaches. Experiments show that it greatly outperforms
lifted variable elimination when logical structure is present. Finally, we
propose an algorithm for approximate probabilistic theorem proving, and show
that it can greatly outperform lifted belief propagation.",N/A
Reasoning about RoboCup Soccer Narratives,"This paper presents an approach for learning to translate simple narratives,
i.e., texts (sequences of sentences) describing dynamic systems, into coherent
sequences of events without the need for labeled training data. Our approach
incorporates domain knowledge in the form of preconditions and effects of
events, and we show that it outperforms state-of-the-art supervised learning
systems on the task of reconstructing RoboCup soccer games from their
commentaries.",N/A
Suboptimality Bounds for Stochastic Shortest Path Problems,"We consider how to use the Bellman residual of the dynamic programming
operator to compute suboptimality bounds for solutions to stochastic shortest
path problems. Such bounds have been previously established only in the special
case that ""all policies are proper,"" in which case the dynamic programming
operator is known to be a contraction, and have been shown to be easily
computable only in the more limited special case of discounting. Under the
condition that transition costs are positive, we show that suboptimality bounds
can be easily computed even when not all policies are proper. In the general
case when there are no restrictions on transition costs, the analysis is more
complex. But we present preliminary results that show such bounds are possible.",N/A
"An Efficient Protocol for Negotiation over Combinatorial Domains with
  Incomplete Information","We study the problem of agent-based negotiation in combinatorial domains. It
is difficult to reach optimal agreements in bilateral or multi-lateral
negotiations when the agents' preferences for the possible alternatives are not
common knowledge. Self-interested agents often end up negotiating inefficient
agreements in such situations. In this paper, we present a protocol for
negotiation in combinatorial domains which can lead rational agents to reach
optimal agreements under incomplete information setting. Our proposed protocol
enables the negotiating agents to identify efficient solutions using
distributed search that visits only a small subspace of the whole outcome
space. Moreover, the proposed protocol is sufficiently general that it is
applicable to most preference representation models in combinatorial domains.
We also present results of experiments that demonstrate the feasibility and
computational efficiency of our approach.",N/A
Noisy Search with Comparative Feedback,"We present theoretical results in terms of lower and upper bounds on the
query complexity of noisy search with comparative feedback. In this search
model, the noise in the feedback depends on the distance between query points
and the search target. Consequently, the error probability in the feedback is
not fixed but varies for the queries posed by the search algorithm. Our results
show that a target out of n items can be found in O(log n) queries. We also
show the surprising result that for k possible answers per query, the speedup
is not log k (as for k-ary search) but only log log k in some cases.",N/A
Belief change with noisy sensing in the situation calculus,"Situation calculus has been applied widely in artificial intelligence to
model and reason about actions and changes in dynamic systems. Since actions
carried out by agents will cause constant changes of the agents' beliefs, how
to manage these changes is a very important issue. Shapiro et al. [22] is one
of the studies that considered this issue. However, in this framework, the
problem of noisy sensing, which often presents in real-world applications, is
not considered. As a consequence, noisy sensing actions in this framework will
lead to an agent facing inconsistent situation and subsequently the agent
cannot proceed further. In this paper, we investigate how noisy sensing actions
can be handled in iterated belief change within the situation calculus
formalism. We extend the framework proposed in [22] with the capability of
managing noisy sensings. We demonstrate that an agent can still detect the
actual situation when the ratio of noisy sensing actions vs. accurate sensing
actions is limited. We prove that our framework subsumes the iterated belief
change strategy in [22] when all sensing actions are accurate. Furthermore, we
prove that our framework can adequately handle belief introspection, mistaken
beliefs, belief revision and belief update even with noisy sensing, as done in
[22] with accurate sensing actions only.",N/A
"Improving the Scalability of Optimal Bayesian Network Learning with
  External-Memory Frontier Breadth-First Branch and Bound Search","Previous work has shown that the problem of learning the optimal structure of
a Bayesian network can be formulated as a shortest path finding problem in a
graph and solved using A* search. In this paper, we improve the scalability of
this approach by developing a memory-efficient heuristic search algorithm for
learning the structure of a Bayesian network. Instead of using A*, we propose a
frontier breadth-first branch and bound search that leverages the layered
structure of the search graph of this problem so that no more than two layers
of the graph, plus solution reconstruction information, need to be stored in
memory at a time. To further improve scalability, the algorithm stores most of
the graph in external memory, such as hard disk, when it does not fit in RAM.
Experimental results show that the resulting algorithm solves significantly
larger problems than the current state of the art.",N/A
Order-of-Magnitude Influence Diagrams,"In this paper, we develop a qualitative theory of influence diagrams that can
be used to model and solve sequential decision making tasks when only
qualitative (or imprecise) information is available. Our approach is based on
an order-of-magnitude approximation of both probabilities and utilities and
allows for specifying partially ordered preferences via sets of utility values.
We also propose a dedicated variable elimination algorithm that can be applied
for solving order-of-magnitude influence diagrams.",N/A
"Compact Mathematical Programs For DEC-MDPs With Structured Agent
  Interactions","To deal with the prohibitive complexity of calculating policies in
Decentralized MDPs, researchers have proposed models that exploit structured
agent interactions. Settings where most agent actions are independent except
for few actions that affect the transitions and/or rewards of other agents can
be modeled using Event-Driven Interactions with Complex Rewards (EDI-CR).
Finding the optimal joint policy can be formulated as an optimization problem.
However, existing formulations are too verbose and/or lack optimality
guarantees. We propose a compact Mixed Integer Linear Program formulation of
EDI-CR instances. The key insight is that most action sequences of a group of
agents have the same effect on a given agent. This allows us to treat these
sequences similarly and use fewer variables. Experiments show that our
formulation is more compact and leads to faster solution times and better
solutions than existing formulations.",N/A
A Geometric Traversal Algorithm for Reward-Uncertain MDPs,"Markov decision processes (MDPs) are widely used in modeling decision making
problems in stochastic environments. However, precise specification of the
reward functions in MDPs is often very difficult. Recent approaches have
focused on computing an optimal policy based on the minimax regret criterion
for obtaining a robust policy under uncertainty in the reward function. One of
the core tasks in computing the minimax regret policy is to obtain the set of
all policies that can be optimal for some candidate reward function. In this
paper, we propose an efficient algorithm that exploits the geometric properties
of the reward function associated with the policies. We also present an
approximate version of the method for further speed up. We experimentally
demonstrate that our algorithm improves the performance by orders of magnitude.",N/A
Compressed Inference for Probabilistic Sequential Models,"Hidden Markov models (HMMs) and conditional random fields (CRFs) are two
popular techniques for modeling sequential data. Inference algorithms designed
over CRFs and HMMs allow estimation of the state sequence given the
observations. In several applications, estimation of the state sequence is not
the end goal; instead the goal is to compute some function of it. In such
scenarios, estimating the state sequence by conventional inference techniques,
followed by computing the functional mapping from the estimate is not
necessarily optimal. A more formal approach is to directly infer the final
outcome from the observations. In particular, we consider the specific
instantiation of the problem where the goal is to find the state trajectories
without exact transition points and derive a novel polynomial time inference
algorithm that outperforms vanilla inference techniques. We show that this
particular problem arises commonly in many disparate applications and present
experiments on three of them: (1) Toy robot tracking; (2) Single stroke
character recognition; (3) Handwritten word recognition.",N/A
Symbolic Dynamic Programming for Discrete and Continuous State MDPs,"Many real-world decision-theoretic planning problems can be naturally modeled
with discrete and continuous state Markov decision processes (DC-MDPs). While
previous work has addressed automated decision-theoretic planning for DCMDPs,
optimal solutions have only been defined so far for limited settings, e.g.,
DC-MDPs having hyper-rectangular piecewise linear value functions. In this
work, we extend symbolic dynamic programming (SDP) techniques to provide
optimal solutions for a vastly expanded class of DCMDPs. To address the
inherent combinatorial aspects of SDP, we introduce the XADD - a continuous
variable extension of the algebraic decision diagram (ADD) - that maintains
compact representations of the exact value function. Empirically, we
demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the
first optimal automated solutions to DCMDPs with linear and nonlinear piecewise
partitioned value functions and showing the advantages of constraint-based
pruning for XADDs.",N/A
Adjustment Criteria in Causal Diagrams: An Algorithmic Perspective,"Identifying and controlling bias is a key problem in empirical sciences.
Causal diagram theory provides graphical criteria for deciding whether and how
causal effects can be identified from observed (nonexperimental) data by
covariate adjustment. Here we prove equivalences between existing as well as
new criteria for adjustment and we provide a new simplified but still
equivalent notion of d-separation. These lead to efficient algorithms for two
important tasks in causal diagram analysis: (1) listing minimal covariate
adjustments (with polynomial delay); and (2) identifying the subdiagram
involved in biasing paths (in linear time). Our results improve upon existing
exponential-time solutions for these problems, enabling users to assess the
effects of covariate adjustment on diagrams with tens to hundreds of variables
interactively in real time.",N/A
Distributed Anytime MAP Inference,"We present a distributed anytime algorithm for performing MAP inference in
graphical models. The problem is formulated as a linear programming relaxation
over the edges of a graph. The resulting program has a constraint structure
that allows application of the Dantzig-Wolfe decomposition principle.
Subprograms are defined over individual edges and can be computed in a
distributed manner. This accommodates solutions to graphs whose state space
does not fit in memory. The decomposition master program is guaranteed to
compute the optimal solution in a finite number of iterations, while the
solution converges monotonically with each iteration. Formulating the MAP
inference problem as a linear program allows additional (global) constraints to
be defined; something not possible with message passing algorithms.
Experimental results show that our algorithm's solution quality outperforms
most current algorithms and it scales well to large problems.",N/A
"Measuring the Hardness of Stochastic Sampling on Bayesian Networks with
  Deterministic Causalities: the k-Test","Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local
Variance Bound (LVB) to measure the approximation hardness of Bayesian
inference on Bayesian networks, assuming the networks model strictly positive
joint probability distributions, i.e. zero probabilities are not permitted.
This paper introduces the k-test to measure the approximation hardness of
inference on Bayesian networks with deterministic causalities in the
probability distribution, i.e. when zero conditional probabilities are
permitted. Approximation by stochastic sampling is a widely-used inference
method that is known to suffer from inefficiencies due to sample rejection. The
k-test predicts when rejection rates of stochastic sampling a Bayesian network
will be low, modest, high, or when sampling is intractable.",N/A
"Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method
  and Modified Cuckoo Search","This paper investigates a new method for improving the learning algorithm of
Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS)
and Conjugate Gradient (CG) as a second order optimization technique. The CG
technique is combined with Back-Propagation (BP) algorithm to yield a much more
efficient learning algorithm for ME structure. In addition, the experts and
gating networks in enhanced model are replaced by CG based Multi-Layer
Perceptrons (MLPs) to provide faster and more accurate learning. The CG is
considerably depends on initial weights of connections of Artificial Neural
Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo
Search is applied in order to select the optimal weights. The performance of
proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate
Gradient Based ME (CGME) in classification and regression problems. The
experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster
convergence and better performance in utilized benchmark data sets.","13 pages, 2 figures"
"Generalized FMD Detection for Spectrum Sensing Under Low Signal-to-Noise
  Ratio","Spectrum sensing is a fundamental problem in cognitive radio. We propose a
function of covariance matrix based detection algorithm for spectrum sensing in
cognitive radio network. Monotonically increasing property of function of
matrix involving trace operation is utilized as the cornerstone for this
algorithm. The advantage of proposed algorithm is it works under extremely low
signal-to-noise ratio, like lower than -30 dB with limited sample data.
Theoretical analysis of threshold setting for the algorithm is discussed. A
performance comparison between the proposed algorithm and other
state-of-the-art methods is provided, by the simulation on captured digital
television (DTV) signal.","4 pages, 1 figure, 1 table"
"Marginality: a numerical mapping for enhanced treatment of nominal and
  hierarchical attributes","The purpose of statistical disclosure control (SDC) of microdata, a.k.a. data
anonymization or privacy-preserving data mining, is to publish data sets
containing the answers of individual respondents in such a way that the
respondents corresponding to the released records cannot be re-identified and
the released data are analytically useful. SDC methods are either based on
masking the original data, generating synthetic versions of them or creating
hybrid versions by combining original and synthetic data. The choice of SDC
methods for categorical data, especially nominal data, is much smaller than the
choice of methods for numerical data. We mitigate this problem by introducing a
numerical mapping for hierarchical nominal data which allows computing means,
variances and covariances on them.",12 pages
One Decade of Universal Artificial Intelligence,"The first decade of this century has seen the nascency of the first
mathematical theory of general artificial intelligence. This theory of
Universal Artificial Intelligence (UAI) has made significant contributions to
many theoretical, philosophical, and practical AI questions. In a series of
papers culminating in book (Hutter, 2005), an exciting sound and complete
mathematical model for a super intelligent agent (AIXI) has been developed and
rigorously analyzed. While nowadays most AI researchers avoid discussing
intelligence, the award-winning PhD thesis (Legg, 2008) provided the
philosophical embedding and investigated the UAI-based universal measure of
rational intelligence, which is formal, objective and non-anthropocentric.
Recently, effective approximations of AIXI have been derived and experimentally
investigated in JAIR paper (Veness et al. 2011). This practical breakthrough
has resulted in some impressive applications, finally muting earlier critique
that UAI is only a theory. For the first time, without providing any domain
knowledge, the same agent is able to self-adapt to a diverse range of
interactive environments. For instance, AIXI is able to learn from scratch to
play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without
even providing the rules of the games.
  These achievements give new hope that the grand goal of Artificial General
Intelligence is not elusive.
  This article provides an informal overview of UAI in context. It attempts to
gently introduce a very theoretical, formal, and mathematical subject, and
discusses philosophical and technical ingredients, traits of intelligence, some
social questions, and the past and future of UAI.",20 LaTeX pages
Relational Reinforcement Learning in Infinite Mario,"Relational representations in reinforcement learning allow for the use of
structural information like the presence of objects and relationships between
them in the description of value functions. Through this paper, we show that
such representations allow for the inclusion of background knowledge that
qualitatively describes a state and can be used to design agents that
demonstrate learning behavior in domains with large state and actions spaces
such as computer games.",N/A
"Development of an Ontology to Assist the Modeling of Accident Scenarii
  ""Application on Railroad Transport ""","In a world where communication and information sharing are at the heart of
our business, the terminology needs are most pressing. It has become imperative
to identify the terms used and defined in a consensual and coherent way while
preserving linguistic diversity. To streamline and strengthen the process of
acquisition, representation and exploitation of scenarii of train accidents, it
is necessary to harmonize and standardize the terminology used by players in
the security field. The research aims to significantly improve analytical
activities and operations of the various safety studies, by tracking the error
in system, hardware, software and human. This paper presents the contribution
of ontology to modeling scenarii for rail accidents through a knowledge model
based on a generic ontology and domain ontology. After a detailed presentation
of the state of the art material, this article presents the first results of
the developed model.","7 pages, 9 figures, Journal of Computing (ISSN 2151-9617); Journal of
  Computing, Volume 3, Issue 7, July 2011"
Search Combinators,"The ability to model search in a constraint solver can be an essential asset
for solving combinatorial problems. However, existing infrastructure for
defining search heuristics is often inadequate. Either modeling capabilities
are extremely limited or users are faced with a general-purpose programming
language whose features are not tailored towards writing search heuristics. As
a result, major improvements in performance may remain unexplored.
  This article introduces search combinators, a lightweight and
solver-independent method that bridges the gap between a conceptually simple
modeling language for search (high-level, functional and naturally
compositional) and an efficient implementation (low-level, imperative and
highly non-modular). By allowing the user to define application-tailored search
strategies from a small set of primitives, search combinators effectively
provide a rich domain-specific language (DSL) for modeling search to the user.
Remarkably, this DSL comes at a low implementation cost to the developer of a
constraint solver.
  The article discusses two modular implementation approaches and shows, by
empirical evaluation, that search combinators can be implemented without
overhead compared to a native, direct implementation in a constraint solver.",N/A
"Multi source feedback based performance appraisal system using Fuzzy
  logic decision support system","In Multi-Source Feedback or 360 Degree Feedback, data on the performance of
an individual are collected systematically from a number of stakeholders and
are used for improving performance. The 360-Degree Feedback approach provides a
consistent management philosophy meeting the criterion outlined previously. The
360-degree feedback appraisal process describes a human resource methodology
that is frequently used for both employee appraisal and employee development.
Used in employee performance appraisals, the 360-degree feedback methodology is
differentiated from traditional, top-down appraisal methods in which the
supervisor responsible for the appraisal provides the majority of the data.
Instead it seeks to use information gained from other sources to provide a
fuller picture of employees' performances. Similarly, when this technique used
in employee development it augments employees' perceptions of training needs
with those of the people with whom they interact. The 360-degree feedback based
appraisal is a comprehensive method where in the feedback about the employee
comes from all the sources that come into contact with the employee on his/her
job. The respondents for an employee can be her/his peers, managers,
subordinates team members, customers, suppliers and vendors. Hence anyone who
comes into contact with the employee, the 360 degree appraisal has four
components that include self-appraisal, superior's appraisal, subordinate's
appraisal student's appraisal and peer's appraisal .The proposed system is an
attempt to implement the 360 degree feedback based appraisal system in
academics especially engineering colleges.",16 pages
Combining Voting Rules Together,"We propose a simple method for combining together voting rules that performs
a run-off between the different winners of each voting rule. We prove that this
combinator has several good properties. For instance, even if just one of the
base voting rules has a desirable property like Condorcet consistency, the
combination inherits this property. In addition, we prove that combining voting
rules together in this way can make finding a manipulation more computationally
difficult. Finally, we study the impact of this combinator on approximation
methods that find close to optimal manipulations.",N/A
Gibbs Sampling in Open-Universe Stochastic Languages,"Languages for open-universe probabilistic models (OUPMs) can represent
situations with an unknown number of objects and iden- tity uncertainty. While
such cases arise in a wide range of important real-world appli- cations,
existing general purpose inference methods for OUPMs are far less efficient
than those available for more restricted lan- guages and model classes. This
paper goes some way to remedying this deficit by in- troducing, and proving
correct, a generaliza- tion of Gibbs sampling to partial worlds with possibly
varying model structure. Our ap- proach draws on and extends previous generic
OUPM inference methods, as well as aux- iliary variable samplers for
nonparametric mixture models. It has been implemented for BLOG, a well-known
OUPM language. Combined with compile-time optimizations, the resulting
algorithm yields very substan- tial speedups over existing methods on sev- eral
test cases, and substantially improves the practicality of OUPM languages
generally.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"Compiling Possibilistic Networks: Alternative Approaches to
  Possibilistic Inference","Qualitative possibilistic networks, also known as min-based possibilistic
networks, are important tools for handling uncertain information in the
possibility theory frame- work. Despite their importance, only the junction
tree adaptation has been proposed for exact reasoning with such networks. This
paper explores alternative algorithms using compilation techniques. We first
propose possibilistic adaptations of standard compilation-based probabilistic
methods. Then, we develop a new, purely possibilistic, method based on the
transformation of the initial network into a possibilistic base. A comparative
study shows that this latter performs better than the possibilistic adap-
tations of probabilistic methods. This result is also confirmed by experimental
results.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Possibilistic Answer Set Programming Revisited,"Possibilistic answer set programming (PASP) extends answer set programming
(ASP) by attaching to each rule a degree of certainty. While such an extension
is important from an application point of view, existing semantics are not
well-motivated, and do not always yield intuitive results. To develop a more
suitable semantics, we first introduce a characterization of answer sets of
classical ASP programs in terms of possibilistic logic where an ASP program
specifies a set of constraints on possibility distributions. This
characterization is then naturally generalized to define answer sets of PASP
programs. We furthermore provide a syntactic counterpart, leading to a
possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we
show how our framework can readily be implemented using standard ASP solvers.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Three new sensitivity analysis methods for influence diagrams,"Performing sensitivity analysis for influence diagrams using the decision
circuit framework is particularly convenient, since the partial derivatives
with respect to every parameter are readily available [Bhattacharjya and
Shachter, 2007; 2008]. In this paper we present three non-linear sensitivity
analysis methods that utilize this partial derivative information and therefore
do not require re-evaluating the decision situation multiple times.
Specifically, we show how to efficiently compare strategies in decision
situations, perform sensitivity to risk aversion and compute the value of
perfect hedging [Seyller, 2008].","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Probabilistic Similarity Logic,"Many machine learning applications require the ability to learn from and
reason about noisy multi-relational data. To address this, several effective
representations have been developed that provide both a language for expressing
the structural regularities of a domain, and principled support for
probabilistic inference. In addition to these two aspects, however, many
applications also involve a third aspect-the need to reason about
similarities-which has not been directly supported in existing frameworks. This
paper introduces probabilistic similarity logic (PSL), a general-purpose
framework for joint reasoning about similarity in relational domains that
incorporates probabilistic reasoning about similarities and relational
structure in a principled way. PSL can integrate any existing domain-specific
similarity measures and also supports reasoning about similarities between sets
of entities. We provide efficient inference and learning techniques for PSL and
demonstrate its effectiveness both in common relational tasks and in settings
that require reasoning about similarity.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"ALARMS: Alerting and Reasoning Management System for Next Generation
  Aircraft Hazards","The Next Generation Air Transportation System will introduce new, advanced
sensor technologies into the cockpit. With the introduction of such systems,
the responsibilities of the pilot are expected to dramatically increase. In the
ALARMS (ALerting And Reasoning Management System) project for NASA, we focus on
a key challenge of this environment, the quick and efficient handling of
aircraft sensor alerts. It is infeasible to alert the pilot on the state of all
subsystems at all times. Furthermore, there is uncertainty as to the true
hazard state despite the evidence of the alerts, and there is uncertainty as to
the effect and duration of actions taken to address these alerts. This paper
reports on the first steps in the construction of an application designed to
handle Next Generation alerts. In ALARMS, we have identified 60 different
aircraft subsystems and 20 different underlying hazards. In this paper, we show
how a Bayesian network can be used to derive the state of the underlying
hazards, based on the sensor input. Then, we propose a framework whereby an
automated system can plan to address these hazards in cooperation with the
pilot, using a Time-Dependent Markov Process (TMDP). Different hazards and
pilot states will call for different alerting automation plans. We demonstrate
this emerging application of Bayesian networks and TMDPs to cockpit automation,
for a use case where a small number of hazards are present, and analyze the
resulting alerting automation policies.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Lifted Inference for Relational Continuous Models,"Relational Continuous Models (RCMs) represent joint probability densities
over attributes of objects, when the attributes have continuous domains. With
relational representations, they can model joint probability distributions over
large numbers of variables compactly in a natural way. This paper presents a
new exact lifted inference algorithm for RCMs, thus it scales up to large
models of real world applications. The algorithm applies to Relational Pairwise
Models which are (relational) products of potentials of arity 2. Our algorithm
is unique in two ways. First, it substantially improves the efficiency of
lifted inference with variables of continuous domains. When a relational model
has Gaussian potentials, it takes only linear-time compared to cubic time of
previous methods. Second, it is the first exact inference algorithm which
handles RCMs in a lifted way. The algorithm is illustrated over an example from
econometrics. Experimental results show that our algorithm outperforms both a
groundlevel inference algorithm and an algorithm built with previously-known
lifted methods.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Distribution over Beliefs for Memory Bounded Dec-POMDP Planning,"We propose a new point-based method for approximate planning in Dec-POMDP
which outperforms the state-of-the-art approaches in terms of solution quality.
It uses a heuristic estimation of the prior probability of beliefs to choose a
bounded number of policy trees: this choice is formulated as a combinatorial
optimisation problem minimising the error induced by pruning.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"A Scalable Method for Solving High-Dimensional Continuous POMDPs Using
  Local Approximation","Partially-Observable Markov Decision Processes (POMDPs) are typically solved
by finding an approximate global solution to a corresponding belief-MDP. In
this paper, we offer a new planning algorithm for POMDPs with continuous state,
action and observation spaces. Since such domains have an inherent notion of
locality, we can find an approximate solution using local optimization methods.
We parameterize the belief distribution as a Gaussian mixture, and use the
Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is
a first-order filter, we can marginalize over the observations analytically. By
using feedback control and state estimation during policy execution, we recover
a behavior that is effectively conditioned on incoming observations despite the
unconditioned planning. Local optimization provides no guarantees of global
optimality, but it allows us to tackle domains that are at least an order of
magnitude larger than the current state-of-the-art. We demonstrate the
scalability of our algorithm by considering a simulated hand-eye coordination
domain with 16 continuous state dimensions and 6 continuous action dimensions.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Formula-Based Probabilistic Inference,"Computing the probability of a formula given the probabilities or weights
associated with other formulas is a natural extension of logical inference to
the probabilistic setting. Surprisingly, this problem has received little
attention in the literature to date, particularly considering that it includes
many standard inference problems as special cases. In this paper, we propose
two algorithms for this problem: formula decomposition and conditioning, which
is an exact method, and formula importance sampling, which is an approximate
method. The latter is, to our knowledge, the first application of model
counting to approximate probabilistic inference. Unlike conventional
variable-based algorithms, our algorithms work in the dual realm of logical
formulas. Theoretically, we show that our algorithms can greatly improve
efficiency by exploiting the structural information in the formulas.
Empirically, we show that they are indeed quite powerful, often achieving
substantial performance gains over state-of-the-art schemes.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Anytime Planning for Decentralized POMDPs using Expectation Maximization,"Decentralized POMDPs provide an expressive framework for multi-agent
sequential decision making. While fnite-horizon DECPOMDPs have enjoyed
signifcant success, progress remains slow for the infnite-horizon case mainly
due to the inherent complexity of optimizing stochastic controllers
representing agent policies. We present a promising new class of algorithms for
the infnite-horizon case, which recasts the optimization problem as inference
in a mixture of DBNs. An attractive feature of this approach is the
straightforward adoption of existing inference techniques in DBNs for solving
DEC-POMDPs and supporting richer representations such as factored or continuous
states and actions. We also derive the Expectation Maximization (EM) algorithm
to optimize the joint policy represented as DBNs. Experiments on benchmark
domains show that EM compares favorably against the state-of-the-art solvers.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Solving Hybrid Influence Diagrams with Deterministic Variables,"We describe a framework and an algorithm for solving hybrid influence
diagrams with discrete, continuous, and deterministic chance variables, and
discrete and continuous decision variables. A continuous chance variable in an
influence diagram is said to be deterministic if its conditional distributions
have zero variances. The solution algorithm is an extension of Shenoy's fusion
algorithm for discrete influence diagrams. We describe an extended
Shenoy-Shafer architecture for propagation of discrete, continuous, and utility
potentials in hybrid influence diagrams that include deterministic chance
variables. The algorithm and framework are illustrated by solving two small
examples.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference
  in Markov Logic Networks","The paper introduces k-bounded MAP inference, a parameterization of MAP
inference in Markov logic networks. k-Bounded MAP states are MAP states with at
most k active ground atoms of hidden (non-evidence) predicates. We present a
novel delayed column generation algorithm and provide empirical evidence that
the algorithm efficiently computes k-bounded MAP states for meaningful
real-world graph matching problems. The underlying idea is that, instead of
solving one large optimization problem, it is often more efficient to tackle
several small ones.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"Comparative Analysis of Probabilistic Models for Activity Recognition
  with an Instrumented Walker","Rollating walkers are popular mobility aids used by older adults to improve
balance control. There is a need to automatically recognize the activities
performed by walker users to better understand activity patterns, mobility
issues and the context in which falls are more likely to happen. We design and
compare several techniques to recognize walker related activities. A
comprehensive evaluation with control subjects and walker users from a
retirement community is presented.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"Merging Knowledge Bases in Possibilistic Logic by Lexicographic
  Aggregation","Belief merging is an important but difficult problem in Artificial
Intelligence, especially when sources of information are pervaded with
uncertainty. Many merging operators have been proposed to deal with this
problem in possibilistic logic, a weighted logic which is powerful for handling
inconsistency and deal- ing with uncertainty. They often result in a
possibilistic knowledge base which is a set of weighted formulas. Although
possibilistic logic is inconsistency tolerant, it suers from the well-known
""drowning effect"". Therefore, we may still want to obtain a consistent possi-
bilistic knowledge base as the result of merg- ing. In such a case, we argue
that it is not always necessary to keep weighted informa- tion after merging.
In this paper, we define a merging operator that maps a set of pos- sibilistic
knowledge bases and a formula rep- resenting the integrity constraints to a
clas- sical knowledge base by using lexicographic ordering. We show that it
satisfies nine pos- tulates that generalize basic postulates for propositional
merging given in [11]. These postulates capture the principle of minimal change
in some sense. We then provide an algorithm for generating the resulting knowl-
edge base of our merging operator. Finally, we discuss the compatibility of our
merging operator with propositional merging and es- tablish the advantage of
our merging opera- tor over existing semantic merging operators in the
propositional case.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
"Characterizing the Set of Coherent Lower Previsions with a Finite Number
  of Constraints or Vertices","The standard coherence criterion for lower previsions is expressed using an
infinite number of linear constraints. For lower previsions that are
essentially defined on some finite set of gambles on a finite possibility
space, we present a reformulation of this criterion that only uses a finite
number of constraints. Any such lower prevision is coherent if it lies within
the convex polytope defined by these constraints. The vertices of this polytope
are the extreme coherent lower previsions for the given set of gambles. Our
reformulation makes it possible to compute them. We show how this is done and
illustrate the procedure and its results.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Dynamic programming in in uence diagrams with decision circuits,"Decision circuits perform efficient evaluation of influence diagrams,
building on the ad- vances in arithmetic circuits for belief net- work
inference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how
even more compact decision circuits can be con- structed for dynamic
programming in influ- ence diagrams with separable value functions and
conditionally independent subproblems. Once a decision circuit has been
constructed based on the diagram's ""global"" graphical structure, it can be
compiled to exploit ""lo- cal"" structure for efficient evaluation and sen-
sitivity analysis.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Learning Why Things Change: The Difference-Based Causality Learner,"In this paper, we present the Difference- Based Causality Learner (DBCL), an
algorithm for learning a class of discrete-time dynamic models that represents
all causation across time by means of difference equations driving change in a
system. We motivate this representation with real-world mechanical systems and
prove DBCL's correctness for learning structure from time series data, an
endeavour that is complicated by the existence of latent derivatives that have
to be detected. We also prove that, under common assumptions for causal
discovery, DBCL will identify the presence or absence of feedback loops, making
the model more useful for predicting the effects of manipulating variables when
the system is in equilibrium. We argue analytically and show empirically the
advantages of DBCL over vector autoregression (VAR) and Granger causality
models as well as modified forms of Bayesian and constraintbased structure
discovery algorithms. Finally, we show that our algorithm can discover causal
directions of alpha rhythms in human brains from EEG data.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Rollout Sampling Policy Iteration for Decentralized POMDPs,"We present decentralized rollout sampling policy iteration (DecRSPI) - a new
algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI
is designed to improve scalability and tackle problems that lack an explicit
model. The algorithm uses Monte- Carlo methods to generate a sample of
reachable belief states. Then it computes a joint policy for each belief state
based on the rollout estimations. A new policy representation allows us to
represent solutions compactly. The key benefits of the algorithm are its linear
time complexity over the number of agents, its bounded memory usage and good
solution quality. It can solve larger problems that are intractable for
existing planning algorithms. Experimental results confirm the effectiveness
and scalability of the approach.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Solving Multistage Influence Diagrams using Branch-and-Bound Search,"A branch-and-bound approach to solving influ- ence diagrams has been
previously proposed in the literature, but appears to have never been
implemented and evaluated - apparently due to the difficulties of computing
effective bounds for the branch-and-bound search. In this paper, we describe
how to efficiently compute effective bounds, and we develop a practical
implementa- tion of depth-first branch-and-bound search for influence diagram
evaluation that outperforms existing methods for solving influence diagrams
with multiple stages.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains,"Despite the intractability of generic optimal partially observable Markov
decision process planning, there exist important problems that have highly
structured models. Previous researchers have used this insight to construct
more efficient algorithms for factored domains, and for domains with
topological structure in the flat state dynamics model. In our work, motivated
by findings from the education community relevant to automated tutoring, we
consider problems that exhibit a form of topological structure in the factored
dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains
(RAPID) leverages this structure to efficiently compute a good initial envelope
of reachable states under the optimal MDP policy in time linear in the number
of state variables. RAPID performs partially-observable planning over the
limited envelope of states, and slowly expands the state space considered as
time allows. RAPID performs well on a large tutoring-inspired problem
simulation with 122 state variables, corresponding to a flat state space of
over 10^30 states.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Understanding Sampling Style Adversarial Search Methods,"UCT has recently emerged as an exciting new adversarial reasoning technique
based on cleverly balancing exploration and exploitation in a Monte-Carlo
sampling setting. It has been particularly successful in the game of Go but the
reasons for its success are not well understood and attempts to replicate its
success in other domains such as Chess have failed. We provide an in-depth
analysis of the potential of UCT in domain-independent settings, in cases where
heuristic values are available, and the effect of enhancing random playouts to
more informed playouts between two weak minimax players. To provide further
insights, we develop synthetic game tree instances and discuss interesting
properties of UCT, both empirically and analytically.","Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)"
Parameter Learning in PRISM Programs with Continuous Random Variables,"Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's
PRISM, Poole's ICL, De Raedt et al's ProbLog and Vennekens et al's LPAD,
combines statistical and logical knowledge representation and inference.
Inference in these languages is based on enumerative construction of proofs
over logic programs. Consequently, these languages permit very limited use of
random variables with continuous distributions. In this paper, we extend PRISM
with Gaussian random variables and linear equality constraints, and consider
the problem of parameter learning in the extended language. Many statistical
models such as finite mixture models and Kalman filter can be encoded in
extended PRISM. Our EM-based learning algorithm uses a symbolic inference
procedure that represents sets of derivations without enumeration. This permits
us to learn the distribution parameters of extended PRISM programs with
discrete as well as Gaussian variables. The learning algorithm naturally
generalizes the ones used for PRISM and Hybrid Bayesian Networks.","7 pages. Main contribution: Learning algorithm. Inference appears in
  http://arxiv.org/abs/1112.2681"
Modeling of Mixed Decision Making Process,"Decision making whenever and wherever it is happened is key to organizations
success. In order to make correct decision, individuals, teams and
organizations need both knowledge management (to manage content) and
collaboration (to manage group processes) to make that more effective and
efficient. In this paper, we explain the knowledge management and collaboration
convergence. Then, we propose a formal description of mixed and multimodal
decision making (MDM) process where decision may be made by three possible
modes: individual, collective or hybrid. Finally, we explicit the MDM process
based on UML-G profile.","Keywords-collaborative knowledge management; mixed decision making;
  dynamicity of actors; UML-G"
"On the Use of Non-Stationary Policies for Infinite-Horizon Discounted
  Markov Decision Processes","We consider infinite-horizon $\gamma$-discounted Markov Decision Processes,
for which it is known that there exists a stationary optimal policy. We
consider the algorithm Value Iteration and the sequence of policies
$\pi_1,...,\pi_k$ it implicitely generates until some iteration $k$. We provide
performance bounds for non-stationary policies involving the last $m$ generated
policies that reduce the state-of-the-art bound for the last stationary policy
$\pi_k$ by a factor $\frac{1-\gamma}{1-\gamma^m}$. In particular, the use of
non-stationary policies allows to reduce the usual asymptotic performance
bounds of Value Iteration with errors bounded by $\epsilon$ at each iteration
from $\frac{\gamma}{(1-\gamma)^2}\epsilon$ to
$\frac{\gamma}{1-\gamma}\epsilon$, which is significant in the usual situation
when $\gamma$ is close to 1. Given Bellman operators that can only be computed
with some error $\epsilon$, a surprising consequence of this result is that the
problem of ""computing an approximately optimal non-stationary policy"" is much
simpler than that of ""computing an approximately optimal stationary policy"",
and even slightly simpler than that of ""approximately computing the value of
some fixed policy"", since this last problem only has a guarantee of
$\frac{1}{1-\gamma}\epsilon$.",N/A
"Creating Intelligent Linking for Information Threading in Knowledge
  Networks","Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper, we aim to put forward the link dynamics involved in intelligent
processing of information in ILS. There has been advancement in knowledge
management field which involve managing information in databases from a single
domain. ILS works with information from multiple domains stored in distributed
way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with
the concept under consideration, KNNs store the processed information linking
concepts and processors leading to the appropriate processing of information.","5 Pages, 6 Figures, 2 Tables, India Conference (INDICON), 2011"
Expert PC Troubleshooter With Fuzzy-Logic And Self-Learning Support,"Expert systems use human knowledge often stored as rules within the computer
to solve problems that generally would entail human intelligence. Today, with
information systems turning out to be more pervasive and with the myriad
advances in information technologies, automating computer fault diagnosis is
becoming so fundamental that soon every enterprise has to endorse it. This
paper proposes an expert system called Expert PC Troubleshooter for diagnosing
computer problems. The system is composed of a user interface, a rule-base, an
inference engine, and an expert interface. Additionally, the system features a
fuzzy-logic module to troubleshoot POST beep errors, and an intelligent agent
that assists in the knowledge acquisition process. The proposed system is meant
to automate the maintenance, repair, and operations (MRO) process, and free-up
human technicians from manually performing routine, laborious, and
timeconsuming maintenance tasks. As future work, the proposed system is to be
parallelized so as to boost its performance and speed-up its various
operations.","LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Artificial Intelligence &
  Applications (IJAIA), Vol.3, No.2, March 2012"
Unit contradiction versus unit propagation,"Some aspects of the result of applying unit resolution on a CNF formula can
be formalized as functions with domain a set of partial truth assignments. We
are interested in two ways for computing such functions, depending on whether
the result is the production of the empty clause or the assignment of a
variable with a given truth value. We show that these two models can compute
the same functions with formulae of polynomially related sizes, and we explain
how this result is related to the CNF encoding of Boolean constraints.",N/A
"Development of knowledge Base Expert System for Natural treatment of
  Diabetes disease","The development of expert system for treatment of Diabetes disease by using
natural methods is new information technology derived from Artificial
Intelligent research using ESTA (Expert System Text Animation) System. The
proposed expert system contains knowledge about various methods of natural
treatment methods (Massage, Herbal/Proper Nutrition, Acupuncture, Gems) for
Diabetes diseases of Human Beings. The system is developed in the ESTA (Expert
System shell for Text Animation) which is Visual Prolog 7.3 Application. The
knowledge for the said system will be acquired from domain experts, texts and
other related sources.",N/A
Characterization of Dynamic Bayesian Network,"In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a
model that tries to incorporate temporal dimension with uncertainty. We start
with basics of DBN where we especially focus in Inference and Learning concepts
and algorithms. Then we will present different levels and methods of creating
DBNs as well as approaches of incorporating temporal dimension in static
Bayesian network.","9 pages, (IJACSA) International Journal of Advanced Computer Science
  and Applications, Vol. 2, No. 7, 2011"
Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.","EPAM, General Problem solver"
A Probabilistic Logic Programming Event Calculus,"We present a system for recognising human activity given a symbolic
representation of video content. The input of our system is a set of
time-stamped short-term activities (STA) detected on video frames. The output
is a set of recognised long-term activities (LTA), which are pre-defined
temporal combinations of STA. The constraints on the STA that, if satisfied,
lead to the recognition of a LTA, have been expressed using a dialect of the
Event Calculus. In order to handle the uncertainty that naturally occurs in
human activity recognition, we adapted this dialect to a state-of-the-art
probabilistic logic programming framework. We present a detailed evaluation and
comparison of the crisp and probabilistic approaches through experimentation on
a benchmark dataset of human surveillance videos.","Accepted for publication in the Theory and Practice of Logic
  Programming (TPLP) journal"
Applications of fuzzy logic to Case-Based Reasoning,"The article discusses some applications of fuzzy logic ideas to formalizing
of the Case-Based Reasoning (CBR) process and to measuring the effectiveness of
CBR systems",N/A
Lower Complexity Bounds for Lifted Inference,"One of the big challenges in the development of probabilistic relational (or
probabilistic logical) modeling and learning frameworks is the design of
inference techniques that operate on the level of the abstract model
representation language, rather than on the level of ground, propositional
instances of the model. Numerous approaches for such ""lifted inference""
techniques have been proposed. While it has been demonstrated that these
techniques will lead to significantly more efficient inference on some specific
models, there are only very recent and still quite restricted results that show
the feasibility of lifted inference on certain syntactically defined classes of
models. Lower complexity bounds that imply some limitations for the feasibility
of lifted inference on more expressive model classes were established early on
in (Jaeger 2000). However, it is not immediate that these results also apply to
the type of modeling languages that currently receive the most attention, i.e.,
weighted, quantifier-free formulas. In this paper we extend these earlier
results, and show that under the assumption that NETIME =/= ETIME, there is no
polynomial lifted inference algorithm for knowledge bases of weighted,
quantifier- and function-free formulas. Further strengthening earlier results,
this is also shown to hold for approximate inference, and for knowledge bases
not containing the equality predicate.",To appear in Theory and Practice of Logic Programming (TPLP)
On how percolation threshold affects PSO performance,"Statistical evidence of the influence of neighborhood topology on the
performance of particle swarm optimization (PSO) algorithms has been shown in
many works. However, little has been done about the implications could have the
percolation threshold in determining the topology of this neighborhood. This
work addresses this problem for individuals that, like robots, are able to
sense in a limited neighborhood around them. Based on the concept of
percolation threshold, and more precisely, the disk percolation model in 2D, we
show that better results are obtained for low values of radius, when
individuals occasionally ask others their best visited positions, with the
consequent decrease of computational complexity. On the other hand, since
percolation threshold is a universal measure, it could have a great interest to
compare the performance of different hybrid PSO algorithms.",N/A
"Solution Representations and Local Search for the bi-objective Inventory
  Routing Problem","The solution of the biobjective IRP is rather challenging, even for
metaheuristics. We are still lacking a profound understanding of appropriate
solution representations and effective neighborhood structures. Clearly, both
the delivery volumes and the routing aspects of the alternatives need to be
reflected in an encoding, and must be modified when searching by means of local
search. Our work contributes to the better understanding of such solution
representations. On the basis of an experimental investigation, the advantages
and drawbacks of two encodings are studied and compared.","Proceedings of EU/ME 2012, Workshop on Metaheuristics for Global
  Challenges, May 10-11, 2012, Copenhagen, Denmark"
Automatic Sampling of Geographic objects,"Today, one's disposes of large datasets composed of thousands of geographic
objects. However, for many processes, which require the appraisal of an expert
or much computational time, only a small part of these objects can be taken
into account. In this context, robust sampling methods become necessary. In
this paper, we propose a sampling method based on clustering techniques. Our
method consists in dividing the objects in clusters, then in selecting in each
cluster, the most representative objects. A case-study in the context of a
process dedicated to knowledge revision for geographic data generalisation is
presented. This case-study shows that our method allows to select relevant
samples of objects.",N/A
"Using Belief Theory to Diagnose Control Knowledge Quality. Application
  to cartographic generalisation","Both humans and artificial systems frequently use trial and error methods to
problem solving. In order to be effective, this type of strategy implies having
high quality control knowledge to guide the quest for the optimal solution.
Unfortunately, this control knowledge is rarely perfect. Moreover, in
artificial systems-as in humans-self-evaluation of one's own knowledge is often
difficult. Yet, this self-evaluation can be very useful to manage knowledge and
to determine when to revise it. The objective of our work is to propose an
automated approach to evaluate the quality of control knowledge in artificial
systems based on a specific trial and error strategy, namely the informed tree
search strategy. Our revision approach consists in analysing the system's
execution logs, and in using the belief theory to evaluate the global quality
of the knowledge. We present a real-world industrial application in the form of
an experiment using this approach in the domain of cartographic generalisation.
Thus far, the results of using our approach have been encouraging.","Best paper award, International Conference on Computing and
  Communication Technologies (IEEE-RIVF), Danang : Viet Nam (2009)"
A Fuzzy Model for Analogical Problem Solving,"In this paper we develop a fuzzy model for the description of the process of
Analogical Reasoning by representing its main steps as fuzzy subsets of a set
of linguistic labels characterizing the individuals' performance in each step
and we use the Shannon- Wiener diversity index as a measure of the individuals'
abilities in analogical problem solving. This model is compared with a
stochastic model presented in author's earlier papers by introducing a finite
Markov chain on the steps of the process of Analogical Reasoning. A classroom
experiment is also presented to illustrate the use of our results in practice.","10 pages, 1 Table"
Publishing and linking transport data on the Web,"Without Linked Data, transport data is limited to applications exclusively
around transport. In this paper, we present a workflow for publishing and
linking transport data on the Web. So we will be able to develop transport
applications and to add other features which will be created from other
datasets. This will be possible because transport data will be linked to these
datasets. We apply this workflow to two datasets: NEPTUNE, a French standard
describing a transport line, and Passim, a directory containing relevant
information on transport services, in every French city.","Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)"
An improved approach to attribute reduction with covering rough sets,"Attribute reduction is viewed as an important preprocessing step for pattern
recognition and data mining. Most of researches are focused on attribute
reduction by using rough sets. Recently, Tsang et al. discussed attribute
reduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel
S. Yeung, Approximations and reducts with covering generalized rough sets,
Computers and Mathematics with Applications 56 (2008) 279-289], where an
approach based on discernibility matrix was presented to compute all attribute
reducts. In this paper, we provide an improved approach by constructing simpler
discernibility matrix with covering rough sets, and then proceed to improve
some characterizations of attribute reduction provided by Tsang et al. It is
proved that the improved discernible matrix is equivalent to the old one, but
the computational complexity of discernible matrix is greatly reduced.",N/A
"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.",N/A
"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.",N/A
"Most Relevant Explanation: Properties, Algorithms, and Evaluations","Most Relevant Explanation (MRE) is a method for finding multivariate
explanations for given evidence in Bayesian networks [12]. This paper studies
the theoretical properties of MRE and develops an algorithm for finding
multiple top MRE solutions. Our study shows that MRE relies on an implicit soft
relevance measure in automatically identifying the most relevant target
variables and pruning less relevant variables from an explanation. The soft
measure also enables MRE to capture the intuitive phenomenon of explaining away
encoded in Bayesian networks. Furthermore, our study shows that the solution
space of MRE has a special lattice structure which yields interesting dominance
relations among the solutions. A K-MRE algorithm based on these dominance
relations is developed for generating a set of top solutions that are more
representative. Our empirical results show that MRE methods are promising
approaches for explanation in Bayesian networks.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Measuring Inconsistency in Probabilistic Knowledge Bases,"This paper develops an inconsistency measure on conditional probabilistic
knowledge bases. The measure is based on fundamental principles for
inconsistency measures and thus provides a solid theoretical framework for the
treatment of inconsistencies in probabilistic expert systems. We illustrate its
usefulness and immediate application on several examples and present some
formal results. Building on this measure we use the Shapley value-a well-known
solution for coalition games-to define a sophisticated indicator that is not
only able to measure inconsistencies but to reveal the causes of
inconsistencies in the knowledge base. Altogether these tools guide the
knowledge engineer in his aim to restore consistency and therefore enable him
to build a consistent and usable knowledge base that can be employed in
probabilistic expert systems.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Bisimulation-based Approximate Lifted Inference,"There has been a great deal of recent interest in methods for performing
lifted inference; however, most of this work assumes that the first-order model
is given as input to the system. Here, we describe lifted inference algorithms
that determine symmetries and automatically lift the probabilistic model to
speedup inference. In particular, we describe approximate lifted inference
techniques that allow the user to trade off inference accuracy for
computational efficiency by using a handful of tunable parameters, while
keeping the error bounded. Our algorithms are closely related to the
graph-theoretic concept of bisimulation. We report experiments on both
synthetic and real data to show that in the presence of symmetries, run-times
for inference can be improved significantly, with approximate lifted inference
providing orders of magnitude speedup over ground inference.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Regret-based Reward Elicitation for Markov Decision Processes,"The specification of aMarkov decision process (MDP) can be difficult. Reward
function specification is especially problematic; in practice, it is often
cognitively complex and time-consuming for users to precisely specify rewards.
This work casts the problem of specifying rewards as one of preference
elicitation and aims to minimize the degree of precision with which a reward
function must be specified while still allowing optimal or near-optimal
policies to be produced. We first discuss how robust policies can be computed
for MDPs given only partial reward information using the minimax regret
criterion. We then demonstrate how regret can be reduced by efficiently
eliciting reward information using bound queries, using regret-reduction as a
means for choosing suitable queries. Empirical results demonstrate that
regret-based reward elicitation offers an effective way to produce near-optimal
policies without resorting to the precise specification of the entire reward
function.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Logical Inference Algorithms and Matrix Representations for
  Probabilistic Conditional Independence","Logical inference algorithms for conditional independence (CI) statements
have important applications from testing consistency during knowledge
elicitation to constraintbased structure learning of graphical models. We prove
that the implication problem for CI statements is decidable, given that the
size of the domains of the random variables is known and fixed. We will present
an approximate logical inference algorithm which combines a falsification and a
novel validation algorithm. The validation algorithm represents each set of CI
statements as a sparse 0-1 matrix A and validates instances of the implication
problem by solving specific linear programs with constraint matrix A. We will
show experimentally that the algorithm is both effective and efficient in
validating and falsifying instances of the probabilistic CI implication
problem.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
The Temporal Logic of Causal Structures,"Computational analysis of time-course data with an underlying causal
structure is needed in a variety of domains, including neural spike trains,
stock price movements, and gene expression levels. However, it can be
challenging to determine from just the numerical time course data alone what is
coordinating the visible processes, to separate the underlying prima facie
causes into genuine and spurious causes and to do so with a feasible
computational complexity. For this purpose, we have been developing a novel
algorithm based on a framework that combines notions of causality in philosophy
with algorithmic approaches built on model checking and statistical techniques
for multiple hypotheses testing. The causal relationships are described in
terms of temporal logic formulae, reframing the inference problem in terms of
model checking. The logic used, PCTL, allows description of both the time
between cause and effect and the probability of this relationship being
observed. We show that equipped with these causal formulae with their
associated probabilities we may compute the average impact a cause makes to its
effect and then discover statistically significant causes through the concepts
of multiple hypothesis testing (treating each causal relationship as a
hypothesis), and false discovery control. By exploring a well-chosen family of
potentially all significant hypotheses with reasonably minimal description
length, it is possible to tame the algorithm's computational complexity while
exploring the nearly complete search-space of all prima facie causes. We have
tested these ideas in a number of domains and illustrate them here with two
examples.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Constraint Processing in Lifted Probabilistic Inference,"First-order probabilistic models combine representational power of
first-order logic with graphical models. There is an ongoing effort to design
lifted inference algorithms for first-order probabilistic models. We analyze
lifted inference from the perspective of constraint processing and, through
this viewpoint, we analyze and compare existing approaches and expose their
advantages and limitations. Our theoretical results show that the wrong choice
of constraint processing method can lead to exponential increase in
computational complexity. Our empirical tests confirm the importance of
constraint processing in lifted inference. This is the first theoretical and
empirical study of constraint processing in lifted inference.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Counting Belief Propagation,"A major benefit of graphical models is that most knowledge is captured in the
model structure. Many models, however, produce inference problems with a lot of
symmetries not reflected in the graphical structure and hence not exploitable
by efficient inference techniques such as belief propagation (BP). In this
paper, we present a new and simple BP algorithm, called counting BP, that
exploits such additional symmetries. Starting from a given factor graph,
counting BP first constructs a compressed factor graph of clusternodes and
clusterfactors, corresponding to sets of nodes and factors that are
indistinguishable given the evidence. Then it runs a modified BP algorithm on
the compressed graph that is equivalent to running BP on the original factor
graph. Our experiments show that counting BP is applicable to a variety of
important AI tasks such as (dynamic) relational models and boolean model
counting, and that significant efficiency gains are obtainable, often by orders
of magnitude.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Improved Mean and Variance Approximations for Belief Net Responses via
  Network Doubling","A Bayesian belief network models a joint distribution with an directed
acyclic graph representing dependencies among variables and network parameters
characterizing conditional distributions. The parameters are viewed as random
variables to quantify uncertainty about their values. Belief nets are used to
compute responses to queries; i.e., conditional probabilities of interest. A
query is a function of the parameters, hence a random variable. Van Allen et
al. (2001, 2008) showed how to quantify uncertainty about a query via a delta
method approximation of its variance. We develop more accurate approximations
for both query mean and variance. The key idea is to extend the query mean
approximation to a ""doubled network"" involving two independent replicates. Our
method assumes complete data and can be applied to discrete, continuous, and
hybrid networks (provided discrete variables have only discrete parents). We
analyze several improvements, and provide empirical studies to demonstrate
their effectiveness.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Generating Optimal Plans in Highly-Dynamic Domains,"Generating optimal plans in highly dynamic environments is challenging. Plans
are predicated on an assumed initial state, but this state can change
unexpectedly during plan generation, potentially invalidating the planning
effort. In this paper we make three contributions: (1) We propose a novel
algorithm for generating optimal plans in settings where frequent, unexpected
events interfere with planning. It is able to quickly distinguish relevant from
irrelevant state changes, and to update the existing planning search tree if
necessary. (2) We argue for a new criterion for evaluating plan adaptation
techniques: the relative running time compared to the ""size"" of changes. This
is significant since during recovery more changes may occur that need to be
recovered from subsequently, and in order for this process of repeated recovery
to terminate, recovery time has to converge. (3) We show empirically that our
approach can converge and find optimal plans in environments that would
ordinarily defy planning due to their high dynamics.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal
  Decision Making","We introduce a challenging real-world planning problem where actions must be
taken at each location in a spatial area at each point in time. We use forestry
planning as the motivating application. In Large Scale Spatial-Temporal (LSST)
planning problems, the state and action spaces are defined as the
cross-products of many local state and action spaces spread over a large
spatial area such as a city or forest. These problems possess state
uncertainty, have complex utility functions involving spatial constraints and
we generally must rely on simulations rather than an explicit transition model.
We define LSST problems as reinforcement learning problems and present a
solution using policy gradients. We compare two different policy formulations:
an explicit policy that identifies each location in space and the action to
take there; and an abstract policy that defines the proportion of actions to
take across all locations in space. We show that the abstract policy is more
robust and achieves higher rewards with far fewer parameters than the
elementary policy. This abstract policy is also a better fit to the properties
that practitioners in LSST problem domains require for such methods to be
widely useful.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Complexity Analysis and Variational Inference for Interpretation-based
  Probabilistic Description Logic","This paper presents complexity analysis and variational methods for inference
in probabilistic description logics featuring Boolean operators,
quantification, qualified number restrictions, nominals, inverse roles and role
hierarchies. Inference is shown to be PEXP-complete, and variational methods
are designed so as to exploit logical inference whenever possible.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Mean Field Variational Approximation for Continuous-Time Bayesian
  Networks","Continuous-time Bayesian networks is a natural structured representation
language for multicomponent stochastic processes that evolve continuously over
time. Despite the compact representation, inference in such models is
intractable even in relatively simple structured networks. Here we introduce a
mean field variational approximation in which we use a product of inhomogeneous
Markov processes to approximate a distribution over trajectories. This
variational approach leads to a globally consistent distribution, which can be
efficiently queried. Additionally, it provides a lower bound on the probability
of observations, thus making it attractive for learning tasks. We provide the
theoretical foundations for the approximation, an efficient implementation that
exploits the wide range of highly optimized ordinary differential equations
(ODE) solvers, experimentally explore characterizations of processes for which
this approximation is suitable, and show applications to a large-scale
realworld inference problem.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Deterministic POMDPs Revisited,"We study a subclass of POMDPs, called Deterministic POMDPs, that is
characterized by deterministic actions and observations. These models do not
provide the same generality of POMDPs yet they capture a number of interesting
and challenging problems, and permit more efficient algorithms. Indeed, some of
the recent work in planning is built around such assumptions mainly by the
quest of amenable models more expressive than the classical deterministic
models. We provide results about the fundamental properties of Deterministic
POMDPs, their relation with AND/OR search problems and algorithms, and their
computational complexity.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
"Lower Bound Bayesian Networks - An Efficient Inference of Lower Bounds
  on Probability Distributions in Bayesian Networks","We present a new method to propagate lower bounds on conditional probability
distributions in conventional Bayesian networks. Our method guarantees to
provide outer approximations of the exact lower bounds. A key advantage is that
we can use any available algorithms and tools for Bayesian networks in order to
represent and infer lower bounds. This new method yields results that are
provable exact for trees with binary variables, and results which are
competitive to existing approximations in credal networks for all other network
structures. Our method is not limited to a specific kind of network structure.
Basically, it is also not restricted to a specific kind of inference, but we
restrict our analysis to prognostic inference in this article. The
computational complexity is superior to that of other existing approaches.","Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)"
Operations on soft sets revisited,"Soft sets, as a mathematical tool for dealing with uncertainty, have recently
gained considerable attention, including some successful applications in
information processing, decision, demand analysis, and forecasting. To
construct new soft sets from given soft sets, some operations on soft sets have
been proposed. Unfortunately, such operations cannot keep all classical
set-theoretic laws true for soft sets. In this paper, we redefine the
intersection, complement, and difference of soft sets and investigate the
algebraic properties of these operations along with a known union operation. We
find that the new operation system on soft sets inherits all basic properties
of operations on classical sets, which justifies our definitions.",8 pages
Approximate Modified Policy Iteration,"Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that
contains the two celebrated policy and value iteration methods. Despite its
generality, MPI has not been thoroughly studied, especially its approximation
form which is used when the state and/or action spaces are large or infinite.
In this paper, we propose three implementations of approximate MPI (AMPI) that
are extensions of well-known approximate DP algorithms: fitted-value iteration,
fitted-Q iteration, and classification-based policy iteration. We provide error
propagation analyses that unify those for approximate policy and value
iteration. On the last classification-based implementation, we develop a
finite-sample analysis that shows that MPI's main parameter allows to control
the balance between the estimation error of the classifier and the overall
value function approximation.",N/A
Machine Recognition of Hand Written Characters using Neural Networks,"Even today in Twenty First Century Handwritten communication has its own
stand and most of the times, in daily life it is globally using as means of
communication and recording the information like to be shared with others.
Challenges in handwritten characters recognition wholly lie in the variation
and distortion of handwritten characters, since different people may use
different style of handwriting, and direction to draw the same shape of the
characters of their known script. This paper demonstrates the nature of
handwritten characters, conversion of handwritten data into electronic data,
and the neural network approach to make machine capable of recognizing hand
written characters.","4 pages, 1 Figure, ISSN:0975 - 8887"
A Simplified Description of Fuzzy TOPSIS,"A simplified description of Fuzzy TOPSIS (Technique for Order Preference by
Similarity to Ideal Situation) is presented. We have adapted the TOPSIS
description from existing Fuzzy theory literature and distilled the bare
minimum concepts required for understanding and applying TOPSIS. An example has
been worked out to illustrate the application of TOPSIS for a multi-criteria
group decision making scenario.",3 pages
"Approximate Equalities on Rough Intuitionistic Fuzzy Sets and an
  Analysis of Approximate Equalities","In order to involve user knowledge in determining equality of sets, which may
not be equal in the mathematical sense, three types of approximate (rough)
equalities were introduced by Novotny and Pawlak ([8, 9, 10]). These notions
were generalized by Tripathy, Mitra and Ojha ([13]), who introduced the
concepts of approximate (rough) equivalences of sets. Rough equivalences
capture equality of sets at a higher level than rough equalities. More
properties of these concepts were established in [14]. Combining the conditions
for the two types of approximate equalities, two more approximate equalities
were introduced by Tripathy [12] and a comparative analysis of their relative
efficiency was provided. In [15], the four types of approximate equalities were
extended by considering rough fuzzy sets instead of only rough sets. In fact
the concepts of leveled approximate equalities were introduced and properties
were studied. In this paper we proceed further by introducing and studying the
approximate equalities based on rough intuitionistic fuzzy sets instead of
rough fuzzy sets. That is we introduce the concepts of approximate
(rough)equalities of intuitionistic fuzzy sets and study their properties. We
provide some real life examples to show the applications of rough equalities of
fuzzy sets and rough equalities of intuitionistic fuzzy sets.",N/A
The Causal Topography of Cognition,"The causal structure of cognition can be simulated but not implemented
computationally, just as the causal structure of a comet can be simulated but
not implemented computationally. The only thing that allows us even to imagine
otherwise is that cognition, unlike a comet, is invisible (to all but the
cognizer).","11 pages, 0 figures, 10 references, Journal of Cognitive Science 13
  2012"
"Fuzzy Knowledge Representation Based on Possibilistic and Necessary
  Bayesian Networks","Within the framework proposed in this paper, we address the issue of
extending the certain networks to a fuzzy certain networks in order to cope
with a vagueness and limitations of existing models for decision under
imprecise and uncertain knowledge. This paper proposes a framework that
combines two disciplines to exploit their own advantages in uncertain and
imprecise knowledge representation problems. The framework proposed is a
possibilistic logic based one in which Bayesian nodes and their properties are
represented by local necessity-valued knowledge base. Data in properties are
interpreted as set of valuated formulas. In our contribution possibilistic
Bayesian networks have a qualitative part and a quantitative part, represented
by local knowledge bases. The general idea is to study how a fusion of these
two formalisms would permit representing compact way to solve efficiently
problems for knowledge representation. We show how to apply possibility and
necessity measures to the problem of knowledge representation with large scale
data. On the other hand fuzzification of crisp certainty degrees to fuzzy
variables improves the quality of the network and tends to bring smoothness and
robustness in the network performance. The general aim is to provide a new
approach for decision under uncertainty that combines three methodologies:
Bayesian networks certainty distribution and fuzzy logic.",ISSN: 1790-0832
"Use of Fuzzy Sets in Semantic Nets for Providing On-Line Assistance to
  User of Technological Systems","The main objective of this paper is to develop a new semantic Network
structure, based on the fuzzy sets theory, used in Artificial Intelligent
system in order to provide effective on-line assistance to users of new
technological systems. This Semantic Networks is used to describe the knowledge
of an ""ideal"" expert while fuzzy sets are used both to describe the approximate
and uncertain knowledge of novice users who intervene to match fuzzy labels of
a query with categories from an ""ideal"" expert. The technical system we
consider is a word processor software, with Objects such as ""Word"" and Goals
such as ""Cut"" or ""Copy"". We suggest to consider the set of the system's Goals
as a set of linguistic variables to which corresponds a set of possible
linguistic values based on the fuzzy set. We consider, therefore, a set of
interpretation's levels for these possible values to which corresponds a set of
membership functions. We also propose a method to measure the similarity degree
between different fuzzy linguistic variables for the partition of the semantic
network in class of similar objects to make easy the diagnosis of the user's
fuzzy queries.",N/A
"Feature Weighting for Improving Document Image Retrieval System
  Performance","Feature weighting is a technique used to approximate the optimal degree of
influence of individual features. This paper presents a feature weighting
method for Document Image Retrieval System (DIRS) based on keyword spotting. In
this method, we weight the feature using coefficient of multiple correlations.
Coefficient of multiple correlations can be used to describe the synthesized
effects and correlation of each feature. The aim of this paper is to show that
feature weighting increases the performance of DIRS. After applying the feature
weighting method to DIRS the average precision is 93.23% and average recall
become 98.66% respectively",N/A
Certain Bayesian Network based on Fuzzy knowledge Bases,"In this paper, we are trying to examine trade offs between fuzzy logic and
certain Bayesian networks and we propose to combine their respective advantages
into fuzzy certain Bayesian networks (FCBN), a certain Bayesian networks of
fuzzy random variables. This paper deals with different definitions and
classifications of uncertainty, sources of uncertainty, and theories and
methodologies presented to deal with uncertainty. Fuzzification of crisp
certainty degrees to fuzzy variables improves the quality of the network and
tends to bring smoothness and robustness in the network performance. The aim is
to provide a new approach for decision under uncertainty that combines three
methodologies: Bayesian networks certainty distribution and fuzzy logic. Within
the framework proposed in this paper, we address the issue of extending the
certain networks to a fuzzy certain networks in order to cope with a vagueness
and limitations of existing models for decision under imprecise and uncertain
knowledge.",arXiv admin note: substantial text overlap with 1206.0918
"An Intelligent Approach for Negotiating between chains in Supply Chain
  Management Systems","Holding commercial negotiations and selecting the best supplier in supply
chain management systems are among weaknesses of producers in production
process. Therefore, applying intelligent systems may have an effective role in
increased speed and improved quality in the selections .This paper introduces a
system which tries to trade using multi-agents systems and holding negotiations
between any agents. In this system, an intelligent agent is considered for each
segment of chains which it tries to send order and receive the response with
attendance in negotiation medium and communication with other agents .This
paper introduces how to communicate between agents, characteristics of
multi-agent and standard registration medium of each agent in the environment.
JADE (Java Application Development Environment) was used for implementation and
simulation of agents cooperation.",10
"A weighted combination similarity measure for mobility patterns in
  wireless networks","The similarity between trajectory patterns in clustering has played an
important role in discovering movement behaviour of different groups of mobile
objects. Several approaches have been proposed to measure the similarity
between sequences in trajectory data. Most of these measures are based on
Euclidean space or on spatial network and some of them have been concerned with
temporal aspect or ordering types. However, they are not appropriate to
characteristics of spatiotemporal mobility patterns in wireless networks. In
this paper, we propose a new similarity measure for mobility patterns in
cellular space of wireless network. The framework for constructing our measure
is composed of two phases as follows. First, we present formal definitions to
capture mathematically two spatial and temporal similarity measures for
mobility patterns. And then, we define the total similarity measure by means of
a weighted combination of these similarities. The truth of the partial and
total similarity measures are proved in mathematics. Furthermore, instead of
the time interval or ordering, our work makes use of the timestamp at which two
mobility patterns share the same cell. A case study is also described to give a
comparison of the combination measure with other ones.","15 pages, 2 figures; International Journal of Computer Networks &
  Communications (IJCNC) http://airccse.org/journal/ijc2012.html"
"Dispelling Classes Gradually to Improve Quality of Feature Reduction
  Approaches","Feature reduction is an important concept which is used for reducing
dimensions to decrease the computation complexity and time of classification.
Since now many approaches have been proposed for solving this problem, but
almost all of them just presented a fix output for each input dataset that some
of them aren't satisfied cases for classification. In this we proposed an
approach as processing input dataset to increase accuracy rate of each feature
extraction methods. First of all, a new concept called dispelling classes
gradually (DCG) is proposed to increase separability of classes based on their
labels. Next, this method is used to process input dataset of the feature
reduction approaches to decrease the misclassification error rate of their
outputs more than when output is achieved without any processing. In addition
our method has a good quality to collate with noise based on adapting dataset
with feature reduction approaches. In the result part, two conditions (With
process and without that) are compared to support our idea by using some of UCI
datasets.","11 Pages, 5 Figure, 7 Tables; Advanced Computing: An International
  Journal (ACIJ), Vol.3, No.3, May 2012"
Software Aging Analysis of Web Server Using Neural Networks,"Software aging is a phenomenon that refers to progressive performance
degradation or transient failures or even crashes in long running software
systems such as web servers. It mainly occurs due to the deterioration of
operating system resource, fragmentation and numerical error accumulation. A
primitive method to fight against software aging is software rejuvenation.
Software rejuvenation is a proactive fault management technique aimed at
cleaning up the system internal state to prevent the occurrence of more severe
crash failures in the future. It involves occasionally stopping the running
software, cleaning its internal state and restarting it. An optimized schedule
for performing the software rejuvenation has to be derived in advance because a
long running application could not be put down now and then as it may lead to
waste of cost. This paper proposes a method to derive an accurate and optimized
schedule for rejuvenation of a web server (Apache) by using Radial Basis
Function (RBF) based Feed Forward Neural Network, a variant of Artificial
Neural Networks (ANN). Aging indicators are obtained through experimental setup
involving Apache web server and clients, which acts as input to the neural
network model. This method is better than existing ones because usage of RBF
leads to better accuracy and speed in convergence.","11 pages, 8 figures, 1 table; International Journal of Artificial
  Intelligence & Applications (IJAIA), Vol.3, No.3, May 2012"
A Distributed Optimized Patient Scheduling using Partial Information,"A software agent may be a member of a Multi-Agent System (MAS) which is
collectively performing a range of complex and intelligent tasks. In the
hospital, scheduling decisions are finding difficult to schedule because of the
dynamic changes and distribution. In order to face this problem with dynamic
changes in the hospital, a new method, Distributed Optimized Patient Scheduling
with Grouping (DOPSG) has been proposed. The goal of this method is that there
is no necessity for knowing patient agents information globally. With minimal
information this method works effectively. Scheduling problem can be solved for
multiple departments in the hospital. Patient agents have been scheduled to the
resource agent based on the patient priority to reduce the waiting time of
patient agent and to reduce idle time of resources.","11 pages, 8 figures"
"Softening Fuzzy Knowledge Representation Tool with the Learning of New
  Words in Natural Language","The approach described here allows using membership function to represent
imprecise and uncertain knowledge by learning in Fuzzy Semantic Networks. This
representation has a great practical interest due to the possibility to realize
on the one hand, the construction of this membership function from a simple
value expressing the degree of interpretation of an Object or a Goal as
compared to an other and on the other hand, the adjustment of the membership
function during the apprenticeship. We show, how to use these membership
functions to represent the interpretation of an Object (respectively of a Goal)
user as compared to an system Object (respectively to a Goal). We also show the
possibility to make decision for each representation of an user Object compared
to a system Object. This decision is taken by determining decision coefficient
calculates according to the nucleus of the membership function of the user
Object.",N/A
"Fuzzy Knowledge Representation, Learning and Optimization with Bayesian
  Analysis in Fuzzy Semantic Networks","This paper presents a method of optimization, based on both Bayesian Analysis
technical and Gallois Lattice, of a Fuzzy Semantic Networks. The technical
System we use learn by interpreting an unknown word using the links created
between this new word and known words. The main link is provided by the context
of the query. When novice's query is confused with an unknown verb (goal)
applied to a known noun denoting either an object in the ideal user's Network
or an object in the user's Network, the system infer that this new verb
corresponds to one of the known goal. With the learning of new words in natural
language as the interpretation, which was produced in agreement with the user,
the system improves its representation scheme at each experiment with a new
user and, in addition, takes advantage of previous discussions with users. The
semantic Net of user objects thus obtained by these kinds of learning is not
always optimal because some relationships between couple of user objects can be
generalized and others suppressed according to values of forces that
characterize them. Indeed, to simplify the obtained Net, we propose to proceed
to an inductive Bayesian analysis, on the Net obtained from Gallois lattice.
The objective of this analysis can be seen as an operation of filtering of the
obtained descriptive graph.",N/A
"Uncertain and Approximative Knowledge Representation to Reasoning on
  Classification with a Fuzzy Networks Based System","The approach described here allows to use the fuzzy Object Based
Representation of imprecise and uncertain knowledge. This representation has a
great practical interest due to the possibility to realize reasoning on
classification with a fuzzy semantic network based system. For instance, the
distinction between necessary, possible and user classes allows to take into
account exceptions that may appear on fuzzy knowledge-base and facilitates
integration of user's Objects in the base. This approach describes the
theoretical aspects of the architecture of the whole experimental A.I. system
we built in order to provide effective on-line assistance to users of new
technological systems: the understanding of ""how it works"" and ""how to complete
tasks"" from queries in quite natural languages. In our model, procedural
semantic networks are used to describe the knowledge of an ""ideal"" expert while
fuzzy sets are used both to describe the approximative and uncertain knowledge
of novice users in fuzzy semantic networks which intervene to match fuzzy
labels of a query with categories from our ""ideal"" expert.",arXiv admin note: text overlap with arXiv:1206.1794
The third open Answer Set Programming competition,"Answer Set Programming (ASP) is a well-established paradigm of declarative
programming in close relationship with other declarative formalisms such as SAT
Modulo Theories, Constraint Handling Rules, FO(.), PDDL and many others. Since
its first informal editions, ASP systems have been compared in the now
well-established ASP Competition. The Third (Open) ASP Competition, as the
sequel to the ASP Competitions Series held at the University of Potsdam in
Germany (2006-2007) and at the University of Leuven in Belgium in 2009, took
place at the University of Calabria (Italy) in the first half of 2011.
Participants competed on a pre-selected collection of benchmark problems, taken
from a variety of domains as well as real world applications. The Competition
ran on two tracks: the Model and Solve (M&S) Track, based on an open problem
encoding, and open language, and open to any kind of system based on a
declarative specification paradigm; and the System Track, run on the basis of
fixed, public problem encodings, written in a standard ASP language. This paper
discusses the format of the Competition and the rationale behind it, then
reports the results for both tracks. Comparison with the second ASP competition
and state-of-the-art solutions for some of the benchmark domains is eventually
discussed.
  To appear in Theory and Practice of Logic Programming (TPLP).","37 pages, 12 figures, 1 table - To appear in Theory and Practice of
  Logic Programming (TPLP)"
AND/OR Importance Sampling,"The paper introduces AND/OR importance sampling for probabilistic graphical
models. In contrast to importance sampling, AND/OR importance sampling caches
samples in the AND/OR space and then extracts a new sample mean from the stored
samples. We prove that AND/OR importance sampling may have lower variance than
importance sampling; thereby providing a theoretical justification for
preferring it over importance sampling. Our empirical evaluation demonstrates
that AND/OR importance sampling is far more accurate than importance sampling
in many cases.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
"Speeding Up Planning in Markov Decision Processes via Automatically
  Constructed Abstractions","In this paper, we consider planning in stochastic shortest path (SSP)
problems, a subclass of Markov Decision Problems (MDP). We focus on medium-size
problems whose state space can be fully enumerated. This problem has numerous
important applications, such as navigation and planning under uncertainty. We
propose a new approach for constructing a multi-level hierarchy of
progressively simpler abstractions of the original problem. Once computed, the
hierarchy can be used to speed up planning by first finding a policy for the
most abstract level and then recursively refining it into a solution to the
original problem. This approach is fully automated and delivers a speed-up of
two orders of magnitude over a state-of-the-art MDP solver on sample problems
while returning near-optimal solutions. We also prove theoretical bounds on the
loss of solution optimality resulting from the use of abstractions.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Bayesian network learning by compiling to weighted MAX-SAT,"The problem of learning discrete Bayesian networks from data is encoded as a
weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to
address it. For each dataset, the per-variable summands of the (BDeu) marginal
likelihood for different choices of parents ('family scores') are computed
prior to applying MaxWalkSat. Each permissible choice of parents for each
variable is encoded as a distinct propositional atom and the associated family
score encoded as a 'soft' weighted single-literal clause. Two approaches to
enforcing acyclicity are considered: either by encoding the ancestor relation
or by attaching a total order to each graph and encoding that. The latter
approach gives better results. Learning experiments have been conducted on 21
synthetic datasets sampled from 7 BNs. The largest dataset has 10,000
datapoints and 60 variables producing (for the 'ancestor' encoding) a weighted
CNF input file with 19,932 atoms and 269,367 clauses. For most datasets,
MaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The
effect of adding prior information is assessed. It is further shown that
Bayesian model averaging can be effected by collecting BNs generated during the
search.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Strategy Selection in Influence Diagrams using Imprecise Probabilities,"This paper describes a new algorithm to solve the decision making problem in
Influence Diagrams based on algorithms for credal networks. Decision nodes are
associated to imprecise probability distributions and a reformulation is
introduced that finds the global maximum strategy with respect to the expected
utility. We work with Limited Memory Influence Diagrams, which generalize most
Influence Diagram proposals and handle simultaneous decisions. Besides the
global optimum method, we explore an anytime approximate solution with a
guaranteed maximum error and show that imprecise probabilities are handled in a
straightforward way. Complexity issues and experiments with random diagrams and
an effects-based military planning problem are discussed.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Knowledge Combination in Graphical Multiagent Model,"A graphical multiagent model (GMM) represents a joint distribution over the
behavior of a set of agents. One source of knowledge about agents' behavior may
come from gametheoretic analysis, as captured by several graphical game
representations developed in recent years. GMMs generalize this approach to
express arbitrary distributions, based on game descriptions or other sources of
knowledge bearing on beliefs about agent behavior. To illustrate the
flexibility of GMMs, we exhibit game-derived models that allow probabilistic
deviation from equilibrium, as well as models based on heuristic action choice.
We investigate three different methods of integrating these models into a
single model representing the combined knowledge sources. To evaluate the
predictive performance of the combined model, we treat as actual outcome the
behavior produced by a reinforcement learning process. We find that combining
the two knowledge sources, using any of the methods, provides better
predictions than either source alone. Among the combination methods, mixing
data outperforms the opinion pool and direct update methods investigated in
this empirical trial.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Almost Optimal Intervention Sets for Causal Discovery,"We conjecture that the worst case number of experiments necessary and
sufficient to discover a causal graph uniquely given its observational Markov
equivalence class can be specified as a function of the largest clique in the
Markov equivalence class. We provide an algorithm that computes intervention
sets that we believe are optimal for the above task. The algorithm builds on
insights gained from the worst case analysis in Eberhardt et al. (2005) for
sequences of experiments when all possible directed acyclic graphs over N
variables are considered. A simulation suggests that our conjecture is correct.
We also show that a generalization of our conjecture to other classes of
possible graph hypotheses cannot be given easily, and in what sense the
algorithm is then no longer optimal.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Sparse Stochastic Finite-State Controllers for POMDPs,"Bounded policy iteration is an approach to solving infinite-horizon POMDPs
that represents policies as stochastic finite-state controllers and iteratively
improves a controller by adjusting the parameters of each node using linear
programming. In the original algorithm, the size of the linear programs, and
thus the complexity of policy improvement, depends on the number of parameters
of each node, which grows with the size of the controller. But in practice, the
number of parameters of a node with non-zero values is often very small, and
does not grow with the size of the controller. Based on this observation, we
develop a version of bounded policy iteration that leverages the sparse
structure of a stochastic finite-state controller. In each iteration, it
improves a policy by the same amount as the original algorithm, but with much
better scalability.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Sampling First Order Logical Particles,"Approximate inference in dynamic systems is the problem of estimating the
state of the system given a sequence of actions and partial observations. High
precision estimation is fundamental in many applications like diagnosis,
natural language processing, tracking, planning, and robotics. In this paper we
present an algorithm that samples possible deterministic executions of a
probabilistic sequence. The algorithm takes advantage of a compact
representation (using first order logic) for actions and world states to
improve the precision of its estimation. Theoretical and empirical results show
that the algorithm's expected error is smaller than propositional sampling and
Sequential Monte Carlo (SMC) sampling techniques.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
"The Computational Complexity of Sensitivity Analysis and Parameter
  Tuning","While known algorithms for sensitivity analysis and parameter tuning in
probabilistic networks have a running time that is exponential in the size of
the network, the exact computational complexity of these problems has not been
established as yet. In this paper we study several variants of the tuning
problem and show that these problems are NPPP-complete in general. We further
show that the problems remain NP-complete or PP-complete, for a number of
restricted variants. These complexity results provide insight in whether or not
recent achievements in sensitivity analysis and tuning can be extended to more
general, practicable methods.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Partitioned Linear Programming Approximations for MDPs,"Approximate linear programming (ALP) is an efficient approach to solving
large factored Markov decision processes (MDPs). The main idea of the method is
to approximate the optimal value function by a set of basis functions and
optimize their weights by linear programming (LP). This paper proposes a new
ALP approximation. Comparing to the standard ALP formulation, we decompose the
constraint space into a set of low-dimensional spaces. This structure allows
for solving the new LP efficiently. In particular, the constraints of the LP
can be satisfied in a compact form without an exponential dependence on the
treewidth of ALP constraints. We study both practical and theoretical aspects
of the proposed approach. Moreover, we demonstrate its scale-up potential on an
MDP with more than 2^100 states.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Learning Arithmetic Circuits,"Graphical models are usually learned without regard to the cost of doing
inference with them. As a result, even if a good model is learned, it may
perform poorly at prediction, because it requires approximate inference. We
propose an alternative: learning models with a score function that directly
penalizes the cost of inference. Specifically, we learn arithmetic circuits
with a penalty on the number of edges in the circuit (in which the cost of
inference is linear). Our algorithm is equivalent to learning a Bayesian
network with context-specific independence by greedily splitting conditional
distributions, at each step scoring the candidates by compiling the resulting
network into an arithmetic circuit, and using its size as the penalty. We show
how this can be done efficiently, without compiling a circuit from scratch for
each candidate. Experiments on several real-world domains show that our
algorithm is able to learn tractable models with very large treewidth, and
yields more accurate predictions than a standard context-specific Bayesian
network learner, in far less time.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Improving Gradient Estimation by Incorporating Sensor Data,"An efficient policy search algorithm should estimate the local gradient of
the objective function, with respect to the policy parameters, from as few
trials as possible. Whereas most policy search methods estimate this gradient
by observing the rewards obtained during policy trials, we show, both
theoretically and empirically, that taking into account the sensor data as well
gives better gradient estimates and hence faster learning. The reason is that
rewards obtained during policy execution vary from trial to trial due to noise
in the environment; sensor data, which correlates with the noise, can be used
to partially correct for this variation, resulting in an estimatorwith lower
variance.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Explanation Trees for Causal Bayesian Networks,"Bayesian networks can be used to extract explanations about the observed
state of a subset of variables. In this paper, we explicate the desiderata of
an explanation and confront them with the concept of explanation proposed by
existing methods. The necessity of taking into account causal approaches when a
causal graph is available is discussed. We then introduce causal explanation
trees, based on the construction of explanation trees using the measure of
causal information ow (Ay and Polani, 2006). This approach is compared to
several other methods on known networks.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Model-Based Bayesian Reinforcement Learning in Large Structured Domains,"Model-based Bayesian reinforcement learning has generated significant
interest in the AI community as it provides an elegant solution to the optimal
exploration-exploitation tradeoff in classical reinforcement learning.
Unfortunately, the applicability of this type of approach has been limited to
small domains due to the high complexity of reasoning about the joint posterior
over model parameters. In this paper, we consider the use of factored
representations combined with online planning techniques, to improve
scalability of these methods. The main contribution of this paper is a Bayesian
framework for learning the structure and parameters of a dynamical system,
while also simultaneously planning a (near-)optimal sequence of actions.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Improving the Accuracy and Efficiency of MAP Inference for Markov Logic,"In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in
two different MAP inference methods: the current method of choice for MAP
inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
"Observation Subset Selection as Local Compilation of Performance
  Profiles","Deciding what to sense is a crucial task, made harder by dependencies and by
a nonadditive utility function. We develop approximation algorithms for
selecting an optimal set of measurements, under a dependency structure modeled
by a tree-shaped Bayesian network (BN). Our approach is a generalization of
composing anytime algorithm represented by conditional performance profiles.
This is done by relaxing the input monotonicity assumption, and extending the
local compilation technique to more general classes of performance profiles
(PPs). We apply the extended scheme to selecting a subset of measurements for
choosing a maximum expectation variable in a binary valued BN, and for
minimizing the worst variance in a Gaussian BN.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Bounding Search Space Size via (Hyper)tree Decompositions,"This paper develops a measure for bounding the performance of AND/OR search
algorithms for solving a variety of queries over graphical models. We show how
drawing a connection to the recent notion of hypertree decompositions allows to
exploit determinism in the problem specification and produce tighter bounds. We
demonstrate on a variety of practical problem instances that we are often able
to improve upon existing bounds by several orders of magnitude.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
New Techniques for Algorithm Portfolio Design,"We present and evaluate new techniques for designing algorithm portfolios. In
our view, the problem has both a scheduling aspect and a machine learning
aspect. Prior work has largely addressed one of the two aspects in isolation.
Building on recent work on the scheduling aspect of the problem, we present a
technique that addresses both aspects simultaneously and has attractive
theoretical guarantees. Experimentally, we show that this technique can be used
to improve the performance of state-of-the-art algorithms for Boolean
satisfiability, zero-one integer programming, and A.I. planning.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Efficient inference in persistent Dynamic Bayesian Networks,"Numerous temporal inference tasks such as fault monitoring and anomaly
detection exhibit a persistence property: for example, if something breaks, it
stays broken until an intervention. When modeled as a Dynamic Bayesian Network,
persistence adds dependencies between adjacent time slices, often making exact
inference over time intractable using standard inference algorithms. However,
we show that persistence implies a regular structure that can be exploited for
efficient inference. We present three successively more general classes of
models: persistent causal chains (PCCs), persistent causal trees (PCTs) and
persistent polytrees (PPTs), and the corresponding exact inference algorithms
that exploit persistence. We show that analytic asymptotic bounds for our
algorithms compare favorably to junction tree inference; and we demonstrate
empirically that we can perform exact smoothing on the order of 100 times
faster than the approximate Boyen-Koller method on randomly generated instances
of persistent tree models. We also show how to handle non-persistent variables
and how persistence can be exploited effectively for approximate filtering.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Hierarchical POMDP Controller Optimization by Likelihood Maximization,"Planning can often be simpli ed by decomposing the task into smaller tasks
arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy
discovery problem can be framed as a non-convex optimization problem. However,
the inherent computational di culty of solving such an optimization problem
makes it hard to scale to realworld problems. In another line of research,
Toussaint et al. [18] developed a method to solve planning problems by
maximumlikelihood estimation. In this paper, we show how the hierarchy
discovery problem in partially observable domains can be tackled using a
similar maximum likelihood approach. Our technique rst transforms the problem
into a dynamic Bayesian network through which a hierarchical structure can
naturally be discovered while optimizing the policy. Experimental results
demonstrate that this approach scales better than previous techniques based on
non-convex optimization.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Identifying Dynamic Sequential Plans,"We address the problem of identifying dynamic sequential plans in the
framework of causal Bayesian networks, and show that the problem is reduced to
identifying causal effects, for which there are complete identi cation
algorithms available in the literature.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Refractor Importance Sampling,"In this paper we introduce Refractor Importance Sampling (RIS), an
improvement to reduce error variance in Bayesian network importance sampling
propagation under evidential reasoning. We prove the existence of a collection
of importance functions that are close to the optimal importance function under
evidential reasoning. Based on this theoretic result we derive the RIS
algorithm. RIS approaches the optimal importance function by applying localized
arc changes to minimize the divergence between the evidence-adjusted importance
function and the optimal importance function. The validity and performance of
RIS is empirically tested with a large setof synthetic Bayesian networks and
two real-world networks.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
Inference for Multiplicative Models,"The paper introduces a generalization for known probabilistic models such as
log-linear and graphical models, called here multiplicative models. These
models, that express probabilities via product of parameters are shown to
capture multiple forms of contextual independence between variables, including
decision graphs and noisy-OR functions. An inference algorithm for
multiplicative models is provided and its correctness is proved. The complexity
analysis of the inference algorithm uses a more refined parameter than the
tree-width of the underlying graph, and shows the computational cost does not
exceed that of the variable elimination algorithm in graphical models. The
paper ends with examples where using the new models and algorithm is
computationally beneficial.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
On Local Regret,"Online learning aims to perform nearly as well as the best hypothesis in
hindsight. For some hypothesis classes, though, even finding the best
hypothesis offline is challenging. In such offline cases, local search
techniques are often employed and only local optimality guaranteed. For online
decision-making with such hypothesis classes, we introduce local regret, a
generalization of regret that aims to perform nearly as well as only nearby
hypotheses. We then present a general algorithm to minimize local regret with
arbitrary locality graphs. We also show how the graph structure can be
exploited to drastically speed learning. These algorithms are then demonstrated
on a diverse set of online problems: online disjunct learning, online Max-SAT,
and online decision tree learning.","This is the longer version of the same-titled paper appearing in the
  Proceedings of the Twenty-Ninth International Conference on Machine Learning
  (ICML), 2012"
Identifying Independence in Relational Models,"The rules of d-separation provide a framework for deriving conditional
independence facts from model structure. However, this theory only applies to
simple directed graphical models. We introduce relational d-separation, a
theory for deriving conditional independence in relational models. We provide a
sound, complete, and computationally efficient method for relational
d-separation, and we present empirical results that demonstrate effectiveness.","This paper has been revised and expanded. See ""Reasoning about
  Independence in Probabilistic Models of Relational Data""
  http://arxiv.org/abs/1302.4381"
Sensitivity analysis in decision circuits,"Decision circuits have been developed to perform efficient evaluation of
influence diagrams [Bhattacharjya and Shachter, 2007], building on the advances
in arithmetic circuits for belief network inference [Darwiche,2003]. In the
process of model building and analysis, we perform sensitivity analysis to
understand how the optimal solution changes in response to changes in the
model. When sequential decision problems under uncertainty are represented as
decision circuits, we can exploit the efficient solution process embodied in
the decision circuit and the wealth of derivative information available to
compute the value of information for the uncertainties in the problem and the
effects of changes to model parameters on the value and the optimal strategy.","Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)"
"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.",N/A
"Studies in Lower Bounding Probabilities of Evidence using the Markov
  Inequality","Computing the probability of evidence even with known error bounds is
NP-hard. In this paper we address this hard problem by settling on an easier
problem. We propose an approximation which provides high confidence lower
bounds on probability of evidence but does not have any guarantees in terms of
relative or absolute error. Our proposed approximation is a randomized
importance sampling scheme that uses the Markov inequality. However, a
straight-forward application of the Markov inequality may lead to poor lower
bounds. We therefore propose several heuristic measures to improve its
performance in practice. Empirical evaluation of our scheme with state-of-
the-art lower bounding schemes reveals the promise of our approach.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Search for Choquet-optimal paths under uncertainty,"Choquet expected utility (CEU) is one of the most sophisticated decision
criteria used in decision theory under uncertainty. It provides a
generalisation of expected utility enhancing both descriptive and prescriptive
possibilities. In this paper, we investigate the use of CEU for path-planning
under uncertainty with a special focus on robust solutions. We first recall the
main features of the CEU model and introduce some examples showing its
descriptive potential. Then we focus on the search for Choquet-optimal paths in
multivalued implicit graphs where costs depend on different scenarios. After
discussing complexity issues, we propose two different heuristic search
algorithms to solve the problem. Finally, numerical experiments are reported,
showing the practical efficiency of the proposed algorithms.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Learning Probabilistic Relational Dynamics for Multiple Tasks,"The ways in which an agent's actions affect the world can often be modeled
compactly using a set of relational probabilistic planning rules. This paper
addresses the problem of learning such rule sets for multiple related tasks. We
take a hierarchical Bayesian approach, in which the system learns a prior
distribution over rule sets. We present a class of prior distributions
parameterized by a rule set prototype that is stochastically modified to
produce a task-specific rule set. We also describe a coordinate ascent
algorithm that iteratively optimizes the task-specific rule sets and the prior
distribution. Experiments using this algorithm show that transferring
information from related tasks significantly reduces the amount of training
data required to predict action effects in blocks-world domains.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
"Node Splitting: A Scheme for Generating Upper Bounds in Bayesian
  Networks","We formulate in this paper the mini-bucket algorithm for approximate
inference in terms of exact inference on an approximate model produced by
splitting nodes in a Bayesian network. The new formulation leads to a number of
theoretical and practical implications. First, we show that branchand- bound
search algorithms that use minibucket bounds may operate in a drastically
reduced search space. Second, we show that the proposed formulation inspires
new minibucket heuristics and allows us to analyze existing heuristics from a
new perspective. Finally, we show that this new formulation allows mini-bucket
approximations to benefit from recent advances in exact inference, allowing one
to significantly increase the reach of these approximations.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Minimax regret based elicitation of generalized additive utilities,"We describe the semantic foundations for elicitation of generalized
additively independent (GAI) utilities using the minimax regret criterion, and
propose several new query types and strategies for this purpose. Computational
feasibility is obtained by exploiting the local GAI structure in the model. Our
results provide a practical approach for implementing preference-based
constrained configuration optimization as well as effective search in
multiattribute product databases.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Evaluating influence diagrams with decision circuits,"Although a number of related algorithms have been developed to evaluate
influence diagrams, exploiting the conditional independence in the diagram, the
exact solution has remained intractable for many important problems. In this
paper we introduce decision circuits as a means to exploit the local structure
usually found in decision problems and to improve the performance of influence
diagram analysis. This work builds on the probabilistic inference algorithms
using arithmetic circuits to represent Bayesian belief networks [Darwiche,
2003]. Once compiled, these arithmetic circuits efficiently evaluate
probabilistic queries on the belief network, and methods have been developed to
exploit both the global and local structure of the network. We show that
decision circuits can be constructed in a similar fashion and promise similar
benefits.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Optimizing Memory-Bounded Controllers for Decentralized POMDPs,"We present a memory-bounded optimization approach for solving
infinite-horizon decentralized POMDPs. Policies for each agent are represented
by stochastic finite state controllers. We formulate the problem of optimizing
these policies as a nonlinear program, leveraging powerful existing nonlinear
optimization techniques for solving the problem. While existing solvers only
guarantee locally optimal solutions, we show that our formulation produces
higher quality controllers than the state-of-the-art approach. We also
incorporate a shared source of randomness in the form of a correlation device
to further increase solution quality with only a limited increase in space and
time. Our experimental results show that nonlinear optimization can be used to
provide high quality, concise solutions to decentralized decision problems
under uncertainty.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Reasoning at the Right Time Granularity,"Most real-world dynamic systems are composed of different components that
often evolve at very different rates. In traditional temporal graphical models,
such as dynamic Bayesian networks, time is modeled at a fixed granularity,
generally selected based on the rate at which the fastest component evolves.
Inference must then be performed at this fastest granularity, potentially at
significant computational cost. Continuous Time Bayesian Networks (CTBNs) avoid
time-slicing in the representation by modeling the system as evolving
continuously over time. The expectation-propagation (EP) inference algorithm of
Nodelman et al. (2005) can then vary the inference granularity over time, but
the granularity is uniform across all parts of the system, and must be selected
in advance. In this paper, we provide a new EP algorithm that utilizes a
general cluster graph architecture where clusters contain distributions that
can overlap in both space (set of variables) and time. This architecture allows
different parts of the system to be modeled at very different time
granularities, according to their current rate of evolution. We also provide an
information-theoretic criterion for dynamically re-partitioning the clusters
during inference to tune the level of approximation to the current rate of
evolution. This avoids the need to hand-select the appropriate granularity, and
allows the granularity to adapt as information is transmitted across the
network. We present experiments demonstrating that this approach can result in
significant computational savings.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
"AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Weighted Graphical
  Models","Compiling graphical models has recently been under intense investigation,
especially for probabilistic modeling and processing. We present here a novel
data structure for compiling weighted graphical models (in particular,
probabilistic models), called AND/OR Multi-Valued Decision Diagram (AOMDD).
This is a generalization of our previous work on constraint networks, to
weighted models. The AOMDD is based on the frameworks of AND/OR search spaces
for graphical models, and Ordered Binary Decision Diagrams (OBDD). The AOMDD is
a canonical representation of a graphical model, and its size and compilation
time are bounded exponentially by the treewidth of the graph, rather than
pathwidth as is known for OBDDs. We discuss a Variable Elimination schedule for
compilation, and present the general APPLY algorithm that combines two weighted
AOMDDs, and also present a search based method for compilation method. The
preliminary experimental evaluation is quite encouraging, showing the potential
of the AOMDD data structure.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Best-First AND/OR Search for Most Probable Explanations,"The paper evaluates the power of best-first search over AND/OR search spaces
for solving the Most Probable Explanation (MPE) task in Bayesian networks. The
main virtue of the AND/OR representation of the search space is its sensitivity
to the structure of the problem, which can translate into significant time
savings. In recent years depth-first AND/OR Branch-and- Bound algorithms were
shown to be very effective when exploring such search spaces, especially when
using caching. Since best-first strategies are known to be superior to
depth-first when memory is utilized, exploring the best-first control strategy
is called for. The main contribution of this paper is in showing that a recent
extension of AND/OR search algorithms from depth-first Branch-and-Bound to
best-first is indeed very effective for computing the MPE in Bayesian networks.
We demonstrate empirically the superiority of the best-first search approach on
various probabilistic networks.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Learning Bayesian Network Structure from Correlation-Immune Data,"Searching the complete space of possible Bayesian networks is intractable for
problems of interesting size, so Bayesian network structure learning
algorithms, such as the commonly used Sparse Candidate algorithm, employ
heuristics. However, these heuristics also restrict the types of relationships
that can be learned exclusively from data. They are unable to learn
relationships that exhibit ""correlation-immunity"", such as parity. To learn
Bayesian networks in the presence of correlation-immune relationships, we
extend the Sparse Candidate algorithm with a technique called ""skewing"". This
technique uses the observation that relationships that are correlation-immune
under a specific input distribution may not be correlation-immune under
another, sufficiently different distribution. We show that by extending Sparse
Candidate with this technique we are able to discover relationships between
random variables that are approximately correlation-immune, with a
significantly lower computational cost than the alternative of considering
multiple parents of a node at a time.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Survey Propagation Revisited,"Survey propagation (SP) is an exciting new technique that has been remarkably
successful at solving very large hard combinatorial problems, such as
determining the satisfiability of Boolean formulas. In a promising attempt at
understanding the success of SP, it was recently shown that SP can be viewed as
a form of belief propagation, computing marginal probabilities over certain
objects called covers of a formula. This explanation was, however, shortly
dismissed by experiments suggesting that non-trivial covers simply do not exist
for large formulas. In this paper, we show that these experiments were
misleading: not only do covers exist for large hard random formulas, SP is
surprisingly accurate at computing marginals over these covers despite the
existence of many cycles in the formulas. This re-opens a potentially simpler
line of reasoning for understanding SP, in contrast to some alternative lines
of explanation that have been proposed assuming covers do not exist.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Template Based Inference in Symmetric Relational Markov Random Fields,"Relational Markov Random Fields are a general and flexible framework for
reasoning about the joint distribution over attributes of a large number of
interacting entities. The main computational difficulty in learning such models
is inference. Even when dealing with complete data, where one can summarize a
large domain by sufficient statistics, learning requires one to compute the
expectation of the sufficient statistics given different parameter choices. The
typical solution to this problem is to resort to approximate inference
procedures, such as loopy belief propagation. Although these procedures are
quite efficient, they still require computation that is on the order of the
number of interactions (or features) in the model. When learning a large
relational model over a complex domain, even such approximations require
unrealistic running time. In this paper we show that for a particular class of
relational MRFs, which have inherent symmetry, we can perform the inference
needed for learning procedures using a template-level belief propagation. This
procedure's running time is proportional to the size of the relational model
rather than the size of the domain. Moreover, we show that this computational
procedure is equivalent to sychronous loopy belief propagation. This enables a
dramatic speedup in inference and learning time. We use this procedure to learn
relational MRFs for capturing the joint distribution of large protein-protein
interaction networks.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
More-or-Less CP-Networks,"Preferences play an important role in our everyday lives. CP-networks, or
CP-nets in short, are graphical models for representing conditional qualitative
preferences under ceteris paribus (""all else being equal"") assumptions. Despite
their intuitive nature and rich representation, dominance testing with CP-nets
is computationally complex, even when the CP-nets are restricted to
binary-valued preferences. Tractable algorithms exist for binary CP-nets, but
these algorithms are incomplete for multi-valued CPnets. In this paper, we
identify a class of multivalued CP-nets, which we call more-or-less CPnets,
that have the same computational complexity as binary CP-nets. More-or-less
CP-nets exploit the monotonicity of the attribute values and use intervals to
aggregate values that induce similar preferences. We then present a search
control rule for dominance testing that effectively prunes the search space
while preserving completeness.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Policy Iteration for Relational MDPs,"Relational Markov Decision Processes are a useful abstraction for complex
reinforcement learning problems and stochastic planning problems. Recent work
developed representation schemes and algorithms for planning in such problems
using the value iteration algorithm. However, exact versions of more complex
algorithms, including policy iteration, have not been developed or analyzed.
The paper investigates this potential and makes several contributions. First we
observe two anomalies for relational representations showing that the value of
some policies is not well defined or cannot be calculated for restricted
representation schemes used in the literature. On the other hand, we develop a
variant of policy iteration that can get around these anomalies. The algorithm
includes an aspect of policy improvement in the process of policy evaluation
and thus differs from the original algorithm. We show that despite this
difference the algorithm converges to the optimal policy.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Markov Logic in Infinite Domains,"Combining first-order logic and probability has long been a goal of AI.
Markov logic (Richardson & Domingos, 2006) accomplishes this by attaching
weights to first-order formulas and viewing them as templates for features of
Markov networks. Unfortunately, it does not have the full power of first-order
logic, because it is only defined for finite domains. This paper extends Markov
logic to infinite domains, by casting it in the framework of Gibbs measures
(Georgii, 1988). We show that a Markov logic network (MLN) admits a Gibbs
measure as long as each ground atom has a finite number of neighbors. Many
interesting cases fall in this category. We also show that an MLN admits a
unique measure if the weights of its non-unit clauses are small enough. We then
examine the structure of the set of consistent measures in the non-unique case.
Many important phenomena, including systems with phase transitions, are
represented by MLNs with non-unique measures. We relate the problem of
satisfiability in first-order logic to the properties of MLN measures, and
discuss how Markov logic relates to previous infinite models.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
What Counterfactuals Can Be Tested,"Counterfactual statements, e.g., ""my headache would be gone had I taken an
aspirin"" are central to scientific discourse, and are formally interpreted as
statements derived from ""alternative worlds"". However, since they invoke
hypothetical states of affairs, often incompatible with what is actually known
or observed, testing counterfactuals is fraught with conceptual and practical
difficulties. In this paper, we provide a complete characterization of
""testable counterfactuals,"" namely, counterfactual statements whose
probabilities can be inferred from physical experiments. We provide complete
procedures for discerning whether a given counterfactual is testable and, if
so, expressing its probability in terms of experimental data.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs,"Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective in
solving decentralized POMDPs with large horizons. We generalize the algorithm
and improve its scalability by reducing the complexity with respect to the
number of observations from exponential to polynomial. We derive error bounds
on solution quality with respect to this new approximation and analyze the
convergence behavior. To evaluate the effectiveness of the improvements, we
introduce a new, larger benchmark problem. Experimental results show that
despite the high complexity of decentralized POMDPs, scalable solution
techniques such as MBDP perform surprisingly well.","Appears in Proceedings of the Twenty-Third Conference on Uncertainty
  in Artificial Intelligence (UAI2007)"
"Relational Approach to Knowledge Engineering for POMDP-based Assistance
  Systems as a Translation of a Psychological Model","Assistive systems for persons with cognitive disabilities (e.g. dementia) are
difficult to build due to the wide range of different approaches people can
take to accomplishing the same task, and the significant uncertainties that
arise from both the unpredictability of client's behaviours and from noise in
sensor readings. Partially observable Markov decision process (POMDP) models
have been used successfully as the reasoning engine behind such assistive
systems for small multi-step tasks such as hand washing. POMDP models are a
powerful, yet flexible framework for modelling assistance that can deal with
uncertainty and utility. Unfortunately, POMDPs usually require a very labour
intensive, manual procedure for their definition and construction. Our previous
work has described a knowledge driven method for automatically generating POMDP
activity recognition and context sensitive prompting systems for complex tasks.
We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The
spreadsheet-like result of the analysis does not correspond to the POMDP model
directly and the translation to a formal POMDP representation is required. To
date, this translation had to be performed manually by a trained POMDP expert.
In this paper, we formalise and automate this translation process using a
probabilistic relational model (PRM) encoded in a relational database. We
demonstrate the method by eliciting three assistance tasks from non-experts. We
validate the resulting POMDP models using case-based simulations to show that
they are reasonable for the domains. We also show a complete case study of a
designer specifying one database, including an evaluation in a real-life
experiment with a human actor.",N/A
Revision of Defeasible Logic Preferences,"There are several contexts of non-monotonic reasoning where a priority
between rules is established whose purpose is preventing conflicts.
  One formalism that has been widely employed for non-monotonic reasoning is
the sceptical one known as Defeasible Logic. In Defeasible Logic the tool used
for conflict resolution is a preference relation between rules, that
establishes the priority among them.
  In this paper we investigate how to modify such a preference relation in a
defeasible logic theory in order to change the conclusions of the theory
itself. We argue that the approach we adopt is applicable to legal reasoning
where users, in general, cannot change facts or rules, but can propose their
preferences about the relative strength of the rules.
  We provide a comprehensive study of the possible combinatorial cases and we
identify and analyse the cases where the revision process is successful.
  After this analysis, we identify three revision/update operators and study
them against the AGM postulates for belief revision operators, to discover that
only a part of these postulates are satisfied by the three operators.",N/A
CAPIR: Collaborative Action Planning with Intention Recognition,"We apply decision theoretic techniques to construct non-player characters
that are able to assist a human player in collaborative games. The method is
based on solving Markov decision processes, which can be difficult when the
game state is described by many variables. To scale to more complex games, the
method allows decomposition of a game task into subtasks, each of which can be
modelled by a Markov decision process. Intention recognition is used to infer
the subtask that the human is currently performing, allowing the helper to
assist the human in performing the correct task. Experiments show that the
method can be effective, giving near-human level performance in helping a human
in a collaborative game.","6 pages, accepted for presentation at AIIDE'11"
Bootstrapping Monte Carlo Tree Search with an Imperfect Heuristic,"We consider the problem of using a heuristic policy to improve the value
approximation by the Upper Confidence Bound applied in Trees (UCT) algorithm in
non-adversarial settings such as planning with large-state space Markov
Decision Processes. Current improvements to UCT focus on either changing the
action selection formula at the internal nodes or the rollout policy at the
leaf nodes of the search tree. In this work, we propose to add an auxiliary arm
to each of the internal nodes, and always use the heuristic policy to roll out
simulations at the auxiliary arms. The method aims to get fast convergence to
optimal values at states where the heuristic policy is optimal, while retaining
similar approximation as the original UCT in other states. We show that
bootstrapping with the proposed method in the new algorithm, UCT-Aux, performs
better compared to the original UCT algorithm and its variants in two benchmark
experiment settings. We also examine conditions under which UCT-Aux works well.","16 pages, accepted for presentation at ECML'12"
"A Variational Approach for Approximating Bayesian Networks by Edge
  Deletion","We consider in this paper the formulation of approximate inference in
Bayesian networks as a problem of exact inference on an approximate network
that results from deleting edges (to reduce treewidth). We have shown in
earlier work that deleting edges calls for introducing auxiliary network
parameters to compensate for lost dependencies, and proposed intuitive
conditions for determining these parameters. We have also shown that our method
corresponds to IBP when enough edges are deleted to yield a polytree, and
corresponds to some generalizations of IBP when fewer edges are deleted. In
this paper, we propose a different criteria for determining auxiliary
parameters based on optimizing the KL-divergence between the original and
approximate networks. We discuss the relationship between the two methods for
selecting parameters, shedding new light on IBP and its generalizations. We
also discuss the application of our new method to approximating inference
problems which are exponential in constrained treewidth, including MAP and
nonmyopic value of information.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
On the Robustness of Most Probable Explanations,"In Bayesian networks, a Most Probable Explanation (MPE) is a complete
variable instantiation with a highest probability given the current evidence.
In this paper, we discuss the problem of finding robustness conditions of the
MPE under single parameter changes. Specifically, we ask the question: How much
change in a single network parameter can we afford to apply while keeping the
MPE unchanged? We will describe a procedure, which is the first of its kind,
that computes this answer for each parameter in the Bayesian network variable
in time O(n exp(w)), where n is the number of network variables and w is its
treewidth.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Cutset Sampling with Likelihood Weighting,"The paper analyzes theoretically and empirically the performance of
likelihood weighting (LW) on a subset of nodes in Bayesian networks. The
proposed scheme requires fewer samples to converge due to reduction in sampling
variance. The method exploits the structure of the network to bound the
complexity of exact inference used to compute sampling distributions, similar
to Gibbs cutset sampling. Yet, the extension of the previosly proposed cutset
sampling principles to likelihood weighting is non-trivial due to differences
in the sampling processes of Gibbs sampler and LW. We demonstrate empirically
that likelihood weighting on a cutset (LWLC) is effective time-wise and has a
lower rejection rate than LW when applied to networks with many deterministic
probabilities. Finally, we show that the performance of likelihood weighting on
a cutset can be improved further by caching computed sampling distributions
and, consequently, learning 'zeros' of the target distribution.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
An Efficient Triplet-based Algorithm for Evidential Reasoning,"Linear-time computational techniques have been developed for combining
evidence which is available on a number of contending hypotheses. They offer a
means of making the computation-intensive calculations involved more efficient
in certain circumstances. Unfortunately, they restrict the orthogonal sum of
evidential functions to the dichotomous structure applies only to elements and
their complements. In this paper, we present a novel evidence structure in
terms of a triplet and a set of algorithms for evidential reasoning. The merit
of this structure is that it divides a set of evidence into three subsets,
distinguishing trivial evidential elements from important ones focusing some
particular elements. It avoids the deficits of the dichotomous structure in
representing the preference of evidence and estimating the basic probability
assignment of evidence. We have established a formalism for this structure and
the general formulae for combining pieces of evidence in the form of the
triplet, which have been theoretically justified.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Linear Algebra Approach to Separable Bayesian Networks,"Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian
Networks in which the conditional probability distribution can be separated
into a function of only the marginal distribution of a node's neighbors,
instead of the joint distributions. In terms of modeling, separable networks
has rendered possible siginificant reduction in complexity, as the state space
is only linear in the number of variables on the network, in contrast to a
typical state space which is exponential. In this work, We describe the
connection between an arbitrary Conditional Probability Table (CPT) and
separable systems using linear algebra. We give an alternate proof on the
equivalence of sufficiency and separability. We present a computational method
for testing whether a given CPT is separable.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Pearl's Calculus of Intervention Is Complete,"This paper is concerned with graphical criteria that can be used to solve the
problem of identifying casual effects from nonexperimental data in a causal
Bayesian network structure, i.e., a directed acyclic graph that represents
causal relationships. We first review Pearl's work on this topic [Pearl, 1995],
in which several useful graphical criteria are presented. Then we present a
complete algorithm [Huang and Valtorta, 2006b] for the identifiability problem.
By exploiting the completeness of this algorithm, we prove that the three basic
do-calculus rules that Pearl presents are complete, in the sense that, if a
causal effect is identifiable, there exists a sequence of applications of the
rules of the do-calculus that transforms the causal effect formula into a
formula that only includes observational quantities.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
A new axiomatization for likelihood gambles,"This paper studies a new and more general axiomatization than one presented
previously for preference on likelihood gambles. Likelihood gambles describe
actions in a situation where a decision maker knows multiple probabilistic
models and a random sample generated from one of those models but does not know
prior probability of models. This new axiom system is inspired by Jensen's
axiomatization of probabilistic gambles. Our approach provides a new
perspective to the role of data in decision making under ambiguity. It avoids
one of the most controversial issue of Bayesian methodology namely the
assumption of prior probability.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
"Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian
  Networks","Continuous-time Bayesian networks (CTBNs) are graphical representations of
multi-component continuous-time Markov processes as directed graphs. The edges
in the network represent direct influences among components. The joint rate
matrix of the multi-component process is specified by means of conditional rate
matrices for each component separately. This paper addresses the situation
where some of the components evolve on a time scale that is much shorter
compared to the time scale of the other components. In this paper, we prove
that in the limit where the separation of scales is infinite, the Markov
process converges (in distribution, or weakly) to a reduced, or effective
Markov process that only involves the slow components. We also demonstrate that
for reasonable separation of scale (an order of magnitude) the reduced process
is a good approximation of the marginal process over the slow components. We
provide a simple procedure for building a reduced CTBN for this effective
process, with conditional rate matrices that can be directly calculated from
the original CTBN, and discuss the implications for approximate reasoning in
large systems.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Methods for computing state similarity in Markov Decision Processes,"A popular approach to solving large probabilistic systems relies on
aggregating states based on a measure of similarity. Many approaches in the
literature are heuristic. A number of recent methods rely instead on metrics
based on the notion of bisimulation, or behavioral equivalence between states
(Givan et al, 2001, 2003; Ferns et al, 2004). An integral component of such
metrics is the Kantorovich metric between probability distributions. However,
while this metric enables many satisfying theoretical properties, it is costly
to compute in practice. In this paper, we use techniques from network
optimization and statistical sampling to overcome this problem. We obtain in
this manner a variety of distance functions for MDP state aggregation, which
differ in the tradeoff between time and space complexity, as well as the
quality of the aggregation. We provide an empirical evaluation of these
trade-offs.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
"Residual Belief Propagation: Informed Scheduling for Asynchronous
  Message Passing","Inference for probabilistic graphical models is still very much a practical
challenge in large domains. The commonly used and effective belief propagation
(BP) algorithm and its generalizations often do not converge when applied to
hard, real-life inference tasks. While it is widely recognized that the
scheduling of messages in these algorithms may have significant consequences,
this issue remains largely unexplored. In this work, we address the question of
how to schedule messages for asynchronous propagation so that a fixed point is
reached faster and more often. We first show that any reasonable asynchronous
BP converges to a unique fixed point under conditions similar to those that
guarantee convergence of synchronous BP. In addition, we show that the
convergence rate of a simple round-robin schedule is at least as good as that
of synchronous propagation. We then propose residual belief propagation (RBP),
a novel, easy-to-implement, asynchronous propagation algorithm that schedules
messages in an informed way, that pushes down a bound on the distance from the
fixed point. Finally, we demonstrate the superiority of RBP over
state-of-the-art methods for a variety of challenging synthetic and real-life
problems: RBP converges significantly more often than other methods; and it
significantly reduces running time until convergence, even when other methods
converge.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Asymmetric separation for local independence graphs,"Directed possibly cyclic graphs have been proposed by Didelez (2000) and
Nodelmann et al. (2002) in order to represent the dynamic dependencies among
stochastic processes. These dependencies are based on a generalization of
Granger-causality to continuous time, first developed by Schweder (1970) for
Markov processes, who called them local dependencies. They deserve special
attention as they are asymmetric unlike stochastic (in)dependence. In this
paper we focus on their graphical representation and develop a suitable, i.e.
asymmetric notion of separation, called delta-separation. The properties of
this graph separation as well as of local independence are investigated in
detail within a framework of asymmetric (semi)graphoids allowing a deeper
insight into what information can be read off these graphs.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
From influence diagrams to multi-operator cluster DAGs,"There exist several architectures to solve influence diagrams using local
computations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation
architectures. They all extend usual variable elimination algorithms thanks to
the use of so-called 'potentials'. In this paper, we introduce a new
architecture, called the Multi-operator Cluster DAG architecture, which can
produce decompositions with an improved constrained induced-width, and
therefore induce potentially exponential gains. Its principle is to benefit
from the composite nature of influence diagrams, instead of using uniform
potentials, in order to better analyze the problem structure.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
General-Purpose MCMC Inference over Relational Structures,"Tasks such as record linkage and multi-target tracking, which involve
reconstructing the set of objects that underlie some observed data, are
particularly challenging for probabilistic inference. Recent work has achieved
efficient and accurate inference on such problems using Markov chain Monte
Carlo (MCMC) techniques with customized proposal distributions. Currently,
implementing such a system requires coding MCMC state representations and
acceptance probability calculations that are specific to a particular
application. An alternative approach, which we pursue in this paper, is to use
a general-purpose probabilistic modeling language (such as BLOG) and a generic
Metropolis-Hastings MCMC algorithm that supports user-supplied proposal
distributions. Our algorithm gains flexibility by using MCMC states that are
only partial descriptions of possible worlds; we provide conditions under which
MCMC over partial worlds yields correct answers to queries. We also show how to
use a context-specific Bayes net to identify the factors in the acceptance
probability that need to be computed for a given proposed move. Experimental
results on a citation matching task show that our general-purpose MCMC engine
compares favorably with an application-specific system.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Belief Update in CLG Bayesian Networks With Lazy Propagation,"In recent years Bayesian networks (BNs) with a mixture of continuous and
discrete variables have received an increasing level of attention. We present
an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG
BNs). The architecture is an extension of lazy propagation using operations of
Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator
potentials into sets of factors, the proposed architecture takes advantage of
independence and irrelevance properties induced by the structure of the graph
and the evidence. The resulting benefits are illustrated by examples. Results
of a preliminary empirical performance evaluation indicate a significant
potential of the proposed architecture.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Reasoning about Uncertainty in Metric Spaces,"We set up a model for reasoning about metric spaces with belief theoretic
measures. The uncertainty in these spaces stems from both probability and
metric. To represent both aspect of uncertainty, we choose an expected distance
function as a measure of uncertainty. A formal logical system is constructed
for the reasoning about expected distance. Soundness and completeness are shown
for this logic. For reasoning on product metric space with uncertainty, a new
metric is defined and shown to have good properties.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Propagation of Delays in the National Airspace System,"The National Airspace System (NAS) is a large and complex system with
thousands of interrelated components: administration, control centers,
airports, airlines, aircraft, passengers, etc. The complexity of the NAS
creates many difficulties in management and control. One of the most pressing
problems is flight delay. Delay creates high cost to airlines, complaints from
passengers, and difficulties for airport operations. As demand on the system
increases, the delay problem becomes more and more prominent. For this reason,
it is essential for the Federal Aviation Administration to understand the
causes of delay and to find ways to reduce delay. Major contributing factors to
delay are congestion at the origin airport, weather, increasing demand, and air
traffic management (ATM) decisions such as the Ground Delay Programs (GDP).
Delay is an inherently stochastic phenomenon. Even if all known causal factors
could be accounted for, macro-level national airspace system (NAS) delays could
not be predicted with certainty from micro-level aircraft information. This
paper presents a stochastic model that uses Bayesian Networks (BNs) to model
the relationships among different components of aircraft delay and the causal
factors that affect delays. A case study on delays of departure flights from
Chicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta
International Airport (ATL) reveals how local and system level environmental
and human-caused factors combine to affect components of delay, and how these
components contribute to the final arrival delay at the destination airport.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Recognizing Activities and Spatial Context Using Wearable Sensors,"We introduce a new dynamic model with the capability of recognizing both
activities that an individual is performing as well as where that ndividual is
located. Our model is novel in that it utilizes a dynamic graphical model to
jointly estimate both activity and spatial context over time based on the
simultaneous use of asynchronous observations consisting of GPS measurements,
and measurements from a small mountable sensor board. Joint inference is quite
desirable as it has the ability to improve accuracy of the model. A key goal,
however, in designing our overall system is to be able to perform accurate
inference decisions while minimizing the amount of hardware an individual must
wear. This minimization leads to greater comfort and flexibility, decreased
power requirements and therefore increased battery life, and reduced cost. We
show results indicating that our joint measurement model outperforms
measurements from either the sensor board or GPS alone, using two types of
probabilistic inference procedures, namely particle filtering and pruned exact
inference.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
"A simple approach for finding the globally optimal Bayesian network
  structure","We study the problem of learning the best Bayesian network structure with
respect to a decomposable score such as BDe, BIC or AIC. This problem is known
to be NP-hard, which means that solving it becomes quickly infeasible as the
number of variables increases. Nevertheless, in this paper we show that it is
possible to learn the best Bayesian network structure with over 30 variables,
which covers many practically interesting cases. Our algorithm is less
complicated and more efficient than the techniques presented earlier. It can be
easily parallelized, and offers a possibility for efficient exploration of the
best networks consistent with different variable orderings. In the experimental
part of the paper we compare the performance of the algorithm to the previous
state-of-the-art algorithm. Free source-code and an online-demo can be found at
http://b-course.hiit.fi/bene.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
Practical Linear Value-approximation Techniques for First-order MDPs,"Recent work on approximate linear programming (ALP) techniques for
first-order Markov Decision Processes (FOMDPs) represents the value function
linearly w.r.t. a set of first-order basis functions and uses linear
programming techniques to determine suitable weights. This approach offers the
advantage that it does not require simplification of the first-order value
function, and allows one to solve FOMDPs independent of a specific domain
instantiation. In this paper, we address several questions to enhance the
applicability of this work: (1) Can we extend the first-order ALP framework to
approximate policy iteration to address performance deficiencies of previous
approaches? (2) Can we automatically generate basis functions and evaluate
their impact on value function quality? (3) How can we decompose intractable
problems with universally quantified rewards into tractable subproblems? We
propose answers to these questions along with a number of novel optimizations
and provide a comparative empirical evaluation on logistics problems from the
ICAPS 2004 Probabilistic Planning Competition.","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)"
"Software Verification and Graph Similarity for Automated Evaluation of
  Students' Assignments","In this paper we promote introducing software verification and control flow
graph similarity measurement in automated evaluation of students' programs. We
present a new grading framework that merges results obtained by combination of
these two approaches with results obtained by automated testing, leading to
improved quality and precision of automated grading. These two approaches are
also useful in providing a comprehensible feedback that can help students to
improve the quality of their programs We also present our corresponding tools
that are publicly available and open source. The tools are based on LLVM
low-level intermediate code representation, so they could be applied to a
number of programming languages. Experimental evaluation of the proposed
grading framework is performed on a corpus of university students' programs
written in programming language C. Results of the experiments show that
automatically generated grades are highly correlated with manually determined
grades suggesting that the presented tools can find real-world applications in
studying and grading.",N/A
Rule Based Expert System for Cerebral Palsy Diagnosis,"The use of Artificial Intelligence is finding prominence not only in core
computer areas, but also in cross disciplinary areas including medical
diagnosis. In this paper, we present a rule based Expert System used in
diagnosis of Cerebral Palsy. The expert system takes user input and depending
on the symptoms of the patient, diagnoses if the patient is suffering from
Cerebral Palsy. The Expert System also classifies the Cerebral Palsy as mild,
moderate or severe based on the presented symptoms.","4 pages, 1 figure, 1 table"
Alternative Restart Strategies for CMA-ES,"This paper focuses on the restart strategy of CMA-ES on multi-modal
functions. A first alternative strategy proceeds by decreasing the initial
step-size of the mutation while doubling the population size at each restart. A
second strategy adaptively allocates the computational budget among the restart
settings in the BIPOP scheme. Both restart strategies are validated on the BBOB
benchmark; their generality is also demonstrated on an independent real-world
problem suite related to spacecraft trajectory optimization.",N/A
"Characteristic matrix of covering and its application to boolean matrix
  decomposition and axiomatization","Covering is an important type of data structure while covering-based rough
sets provide an efficient and systematic theory to deal with covering data. In
this paper, we use boolean matrices to represent and axiomatize three types of
covering approximation operators. First, we define two types of characteristic
matrices of a covering which are essentially square boolean ones, and their
properties are studied. Through the characteristic matrices, three important
types of covering approximation operators are concisely equivalently
represented. Second, matrix representations of covering approximation operators
are used in boolean matrix decomposition. We provide a sufficient and necessary
condition for a square boolean matrix to decompose into the boolean product of
another one and its transpose. And we develop an algorithm for this boolean
matrix decomposition. Finally, based on the above results, these three types of
covering approximation operators are axiomatized using boolean matrices. In a
word, this work borrows extensively from boolean matrices and present a new
view to study covering-based rough sets.",18-page original paper
Robust Principal Component Analysis Using Statistical Estimators,"Principal Component Analysis (PCA) finds a linear mapping and maximizes the
variance of the data which makes PCA sensitive to outliers and may cause wrong
eigendirection. In this paper, we propose techniques to solve this problem; we
use the data-centering method and reestimate the covariance matrix using robust
statistic techniques such as median, robust scaling which is a booster to
data-centering and Huber M-estimator which measures the presentation of
outliers and reweight them with small values. The results on several real world
data sets show that our proposed method handles outliers and gains better
results than the original PCA and provides the same accuracy with lower
computation cost than the Kernel PCA using the polynomial kernel in
classification tasks.","In Proc. of the International Joint Conference on Computer Science
  and Software Engineering (JCSSE) 2009"
"Higher-Order Partial Least Squares (HOPLS): A Generalized Multi-Linear
  Regression Method","A new generalized multilinear regression model, termed the Higher-Order
Partial Least Squares (HOPLS), is introduced with the aim to predict a tensor
(multiway array) $\tensor{Y}$ from a tensor $\tensor{X}$ through projecting the
data onto the latent space and performing regression on the corresponding
latent variables. HOPLS differs substantially from other regression models in
that it explains the data by a sum of orthogonal Tucker tensors, while the
number of orthogonal loadings serves as a parameter to control model complexity
and prevent overfitting. The low dimensional latent space is optimized
sequentially via a deflation operation, yielding the best joint subspace
approximation for both $\tensor{X}$ and $\tensor{Y}$. Instead of decomposing
$\tensor{X}$ and $\tensor{Y}$ individually, higher order singular value
decomposition on a newly defined generalized cross-covariance tensor is
employed to optimize the orthogonal loadings. A systematic comparison on both
synthetic data and real-world decoding of 3D movement trajectories from
electrocorticogram (ECoG) signals demonstrate the advantages of HOPLS over the
existing methods in terms of better predictive ability, suitability to handle
small sample sizes, and robustness to noise.",N/A
Cost Sensitive Reachability Heuristics for Handling State Uncertainty,"While POMDPs provide a general platform for non-deterministic conditional
planning under a variety of quality metrics they have limited scalability. On
the other hand, non-deterministic conditional planners scale very well, but
many lack the ability to optimize plan quality metrics. We present a novel
generalization of planning graph based heuristics that helps conditional
planners both scale and generate high quality plans when using actions with
nonuniform costs. We make empirical comparisons with two state of the art
planners to show the benefit of our techniques.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Stable Independence in Perfect Maps,"With the aid of the concept of stable independence we can construct, in an
efficient way, a compact representation of a semi-graphoid independence
relation. We show that this representation provides a new necessary condition
for the existence of a directed perfect map for the relation. The test for this
condition is based to a large extent on the transitivity property of a special
form of d-separation. The complexity of the test is linear in the size of the
representation. The test, moreover, brings the additional benefit that it can
be used to guide the early stages of network construction.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
'Say EM' for Selecting Probabilistic Models for Logical Sequences,"Many real world sequences such as protein secondary structures or shell logs
exhibit a rich internal structures. Traditional probabilistic models of
sequences, however, consider sequences of flat symbols only. Logical hidden
Markov models have been proposed as one solution. They deal with logical
sequences, i.e., sequences over an alphabet of logical atoms. This comes at the
expense of a more complex model selection problem. Indeed, different
abstraction levels have to be explored. In this paper, we propose a novel
method for selecting logical hidden Markov models from data called SAGEM. SAGEM
combines generalized expectation maximization, which optimizes parameters, with
structure search for model selection using inductive logic programming
refinement operators. We provide convergence and experimental results that show
SAGEM's effectiveness.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Of Starships and Klingons: Bayesian Logic for the 23rd Century,"Intelligent systems in an open world must reason about many interacting
entities related to each other in diverse ways and having uncertain features
and relationships. Traditional probabilistic languages lack the expressive
power to handle relational domains. Classical first-order logic is sufficiently
expressive, but lacks a coherent plausible reasoning capability. Recent years
have seen the emergence of a variety of approaches to integrating first-order
logic, probability, and machine learning. This paper presents Multi-entity
Bayesian networks (MEBN), a formal system that integrates First Order Logic
(FOL) with Bayesian probability theory. MEBN extends ordinary Bayesian networks
to allow representation of graphical models with repeated sub-structures, and
can express a probability distribution over models of any consistent, finitely
axiomatizable first-order theory. We present the logic using an example
inspired by the Paramount Series StarTrek.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
A Differential Semantics of Lazy AR Propagation,"In this paper we present a differential semantics of Lazy AR Propagation
(LARP) in discrete Bayesian networks. We describe how both single and multi
dimensional partial derivatives of the evidence may easily be calculated from a
junction tree in LARP equilibrium. We show that the simplicity of the
calculations stems from the nature of LARP. Based on the differential semantics
we describe how variable propagation in the LARP architecture may give access
to additional partial derivatives. The cautious LARP (cLARP) scheme is derived
to produce a flexible cLARP equilibrium that offers additional opportunities
for calculating single and multidimensional partial derivatives of the evidence
and subsets of the evidence from a single propagation. The results of an
empirical evaluation illustrates how the access to a largely increased number
of partial derivatives comes at a low computational cost.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Modifying Bayesian Networks by Probability Constraints,"This paper deals with the following problem: modify a Bayesian network to
satisfy a given set of probability constraints by only change its conditional
probability tables, and the probability distribution of the resulting network
should be as close as possible to that of the original network. We propose to
solve this problem by extending IPFP (iterative proportional fitting procedure)
to probability distributions represented by Bayesian networks. The resulting
algorithm E-IPFP is further developed to D-IPFP, which reduces the
computational cost by decomposing a global EIPFP into a set of smaller local
E-IPFP problems. Limited analysis is provided, including the convergence proofs
of the two algorithms. Computer experiments were conducted to validate the
algorithms. The results are consistent with the theoretical analysis.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Exploiting Evidence-dependent Sensitivity Bounds,"Studying the effects of one-way variation of any number of parameters on any
number of output probabilities quickly becomes infeasible in practice,
especially if various evidence profiles are to be taken into consideration. To
provide for identifying the parameters that have a potentially large effect
prior to actually performing the analysis, we need properties of sensitivity
functions that are independent of the network under study, of the available
evidence, or of both. In this paper, we study properties that depend upon just
the probability of the entered evidence. We demonstrate that these properties
provide for establishing an upper bound on the sensitivity value for a
parameter; they further provide for establishing the region in which the vertex
of the sensitivity function resides, thereby serving to identify parameters
with a low sensitivity value that may still have a large impact on the
probability of interest for relatively small parameter variations.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs,"We present multi-agent A* (MAA*), the first complete and optimal heuristic
search algorithm for solving decentralized partially-observable Markov decision
problems (DEC-POMDPs) with finite horizon. The algorithm is suitable for
computing optimal plans for a cooperative group of agents that operate in a
stochastic environment such as multirobot coordination, network traffic
control, `or distributed resource allocation. Solving such problems efiectively
is a major challenge in the area of planning under uncertainty. Our solution is
based on a synthesis of classical heuristic search and decentralized control
theory. Experimental results show that MAA* has significant advantages. We
introduce an anytime variant of MAA* and conclude with a discussion of
promising extensions such as an approach to solving infinite horizon problems.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"A unified setting for inference and decision: An argumentation-based
  approach","Inferring from inconsistency and making decisions are two problems which have
always been treated separately by researchers in Artificial Intelligence.
Consequently, different models have been proposed for each category. Different
argumentation systems [2, 7, 10, 11] have been developed for handling
inconsistency in knowledge bases. Recently, other argumentation systems [3, 4,
8] have been defined for making decisions under uncertainty. The aim of this
paper is to present a general argumentation framework in which both inferring
from inconsistency and decision making are captured. The proposed framework can
be used for decision under uncertainty, multiple criteria decision, rule-based
decision and finally case-based decision. Moreover, works on classical decision
suppose that the information about environment is coherent, and this no longer
required by this general framework.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Hybrid Bayesian Networks with Linear Deterministic Variables,"When a hybrid Bayesian network has conditionally deterministic variables with
continuous parents, the joint density function for the continuous variables
does not exist. Conditional linear Gaussian distributions can handle such cases
when the continuous variables have a multi-variate normal distribution and the
discrete variables do not have continuous parents. In this paper, operations
required for performing inference with conditionally deterministic variables in
hybrid Bayesian networks are developed. These methods allow inference in
networks with deterministic variables where continuous variables may be
non-Gaussian, and their density functions can be approximated by mixtures of
truncated exponentials. There are no constraints on the placement of continuous
and discrete nodes in the network.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
On Bayesian Network Approximation by Edge Deletion,"We consider the problem of deleting edges from a Bayesian network for the
purpose of simplifying models in probabilistic inference. In particular, we
propose a new method for deleting network edges, which is based on the evidence
at hand. We provide some interesting bounds on the KL-divergence between
original and approximate networks, which highlight the impact of given evidence
on the quality of approximation and shed some light on good and bad candidates
for edge deletion. We finally demonstrate empirically the promise of the
proposed edge deletion technique as a basis for approximate inference.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Exploiting Evidence in Probabilistic Inference,"We define the notion of compiling a Bayesian network with evidence and
provide a specific approach for evidence-based compilation, which makes use of
logical processing. The approach is practical and advantageous in a number of
application areas-including maximum likelihood estimation, sensitivity
analysis, and MAP computations-and we provide specific empirical results in the
domain of genetic linkage analysis. We also show that the approach is
applicable for networks that do not contain determinism, and show that it
empirically subsumes the performance of the quickscore algorithm when applied
to noisy-or networks.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Use of Dempster-Shafer Conflict Metric to Detect Interpretation
  Inconsistency","A model of the world built from sensor data may be incorrect even if the
sensors are functioning correctly. Possible causes include the use of
inappropriate sensors (e.g. a laser looking through glass walls), sensor
inaccuracies accumulate (e.g. localization errors), the a priori models are
wrong, or the internal representation does not match the world (e.g. a static
occupancy grid used with dynamically moving objects). We are interested in the
case where the constructed model of the world is flawed, but there is no access
to the ground truth that would allow the system to see the discrepancy, such as
a robot entering an unknown environment. This paper considers the problem of
determining when something is wrong using only the sensor data used to
construct the world model. It proposes 11 interpretation inconsistency
indicators based on the Dempster-Shafer conflict metric, Con, and evaluates
these indicators according to three criteria: ability to distinguish true
inconsistency from sensor noise (classification), estimate the magnitude of
discrepancies (estimation), and determine the source(s) (if any) of sensing
problems in the environment (isolation). The evaluation is conducted using data
from a mobile robot with sonar and laser range sensors navigating indoor
environments under controlled conditions. The evaluation shows that the Gambino
indicator performed best in terms of estimation (at best 0.77 correlation),
isolation, and classification of the sensing situation as degraded (7% false
negative rate) or normal (0% false positive rate).","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Nonparametric Bayesian Logic,"The Bayesian Logic (BLOG) language was recently developed for defining
first-order probability models over worlds with unknown numbers of objects. It
handles important problems in AI, including data association and population
estimation. This paper extends BLOG by adopting generative processes over
function spaces - known as nonparametrics in the Bayesian literature. We
introduce syntax for reasoning about arbitrary collections of objects, and
their properties, in an intuitive manner. By exploiting exchangeability,
distributions over unknown objects and their attributes are cast as Dirichlet
processes, which resolve difficulties in model selection and inference caused
by varying numbers of objects. We demonstrate these concepts with application
to citation matching.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Efficient algorithm for estimation of qualitative expected utility in
  possibilistic case-based reasoning","We propose an efficient algorithm for estimation of possibility based
qualitative expected utility. It is useful for decision making mechanisms where
each possible decision is assigned a multi-attribute possibility distribution.
The computational complexity of ordinary methods calculating the expected
utility based on discretization is growing exponentially with the number of
attributes, and may become infeasible with a high number of these attributes.
We present series of theorems and lemmas proving the correctness of our
algorithm that exibits a linear computational complexity. Our algorithm has
been applied in the context of selecting the most prospective partners in
multi-party multi-attribute negotiation, and can also be used in making
decisions about potential offers during the negotiation as other similar
problems.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Local Markov Property for Models Satisfying Composition Axiom,"The local Markov condition for a DAG to be an independence map of a
probability distribution is well known. For DAGs with latent variables,
represented as bi-directed edges in the graph, the local Markov property may
invoke exponential number of conditional independencies. This paper shows that
the number of conditional independence relations required may be reduced if the
probability distributions satisfy the composition axiom. In certain types of
graphs, only linear number of conditional independencies are required. The
result has applications in testing linear structural equation models with
correlated errors.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Unsupervised Activity Discovery and Characterization From Event-Streams,"We present a framework to discover and characterize different classes of
everyday activities from event-streams. We begin by representing activities as
bags of event n-grams. This allows us to analyze the global structural
information of activities, using their local event statistics. We demonstrate
how maximal cliques in an undirected edge-weighted graph of activities, can be
used for activity-class discovery in an unsupervised manner. We show how
modeling an activity as a variable length Markov process, can be used to
discover recurrent event-motifs to characterize the discovered
activity-classes. We present results over extensive data-sets, collected from
multiple active environments, to show the competence and generalizability of
our proposed framework.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Modeling Transportation Routines using Hybrid Dynamic Mixed Networks,"This paper describes a general framework called Hybrid Dynamic Mixed Networks
(HDMNs) which are Hybrid Dynamic Bayesian Networks that allow representation of
discrete deterministic information in the form of constraints. We propose
approximate inference algorithms that integrate and adjust well known
algorithmic principles such as Generalized Belief Propagation,
Rao-Blackwellised Particle Filtering and Constraint Propagation to address the
complexity of modeling and reasoning in HDMNs. We use this framework to model a
person's travel activity over time and to predict destination and routes given
the current location. We present a preliminary empirical evaluation
demonstrating the effectiveness of our modeling framework and algorithms using
several variants of the activity model.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Approximate Inference Algorithms for Hybrid Bayesian Networks with
  Discrete Constraints","In this paper, we consider Hybrid Mixed Networks (HMN) which are Hybrid
Bayesian Networks that allow discrete deterministic information to be modeled
explicitly in the form of constraints. We present two approximate inference
algorithms for HMNs that integrate and adjust well known algorithmic principles
such as Generalized Belief Propagation, Rao-Blackwellised Importance Sampling
and Constraint Propagation to address the complexity of modeling and reasoning
in HMNs. We demonstrate the performance of our approximate inference algorithms
on randomly generated HMNs.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Metrics for Markov Decision Processes with Infinite State Spaces,"We present metrics for measuring state similarity in Markov decision
processes (MDPs) with infinitely many states, including MDPs with continuous
state spaces. Such metrics provide a stable quantitative analogue of the notion
of bisimulation for MDPs, and are suitable for use in MDP approximation. We
show that the optimal value function associated with a discounted infinite
horizon planning task varies continuously with respect to our metric distances.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Existence and Finiteness Conditions for Risk-Sensitive Planning: Results
  and Conjectures","Decision-theoretic planning with risk-sensitive planning objectives is
important for building autonomous agents or decision-support systems for
real-world applications. However, this line of research has been largely
ignored in the artificial intelligence and operations research communities
since planning with risk-sensitive planning objectives is more complicated than
planning with risk-neutral planning objectives. To remedy this situation, we
derive conditions that guarantee that the optimal expected utilities of the
total plan-execution reward exist and are finite for fully observable Markov
decision process models with non-linear utility functions. In case of Markov
decision process models with both positive and negative rewards, most of our
results hold for stationary policies only, but we conjecture that they can be
generalized to non stationary policies.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Near-optimal Nonmyopic Value of Information in Graphical Models,"A fundamental issue in real-world systems, such as sensor networks, is the
selection of observations which most effectively reduce uncertainty. More
specifically, we address the long standing problem of nonmyopically selecting
the most informative subset of variables in a graphical model. We present the
first efficient randomized algorithm providing a constant factor
(1-1/e-epsilon) approximation guarantee for any epsilon > 0 with high
confidence. The algorithm leverages the theory of submodular functions, in
combination with a polynomial bound on sample complexity. We furthermore prove
that no polynomial time algorithm can provide a constant factor approximation
better than (1 - 1/e) unless P = NP. Finally, we provide extensive evidence of
the effectiveness of our method on two complex real-world datasets.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
A Revision-Based Approach to Resolving Conflicting Information,"In this paper, we propose a revision-based approach for conflict resolution
by generalizing the Disjunctive Maxi-Adjustment (DMA) approach (Benferhat et
al. 2004). Revision operators can be classified into two different families:
the model-based ones and the formula-based ones. So the revision-based approach
has two different versions according to which family of revision operators is
chosen. Two particular revision operators are considered, one is the Dalal's
revision operator, which is a model-based revision operator, and the other is
the cardinality-maximal based revision operator, which is a formulabased
revision operator. When the Dalal's revision operator is chosen, the
revision-based approach is independent of the syntactic form in each stratum
and it captures some notion of minimal change. When the cardinalitymaximal
based revision operator is chosen, the revision-based approach is equivalent to
the DMA approach. We also show that both approaches are computationally easier
than the DMA approach.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Asynchronous Dynamic Bayesian Networks,"Systems such as sensor networks and teams of autonomous robots consist of
multiple autonomous entities that interact with each other in a distributed,
asynchronous manner. These entities need to keep track of the state of the
system as it evolves. Asynchronous systems lead to special challenges for
monitoring, as nodes must update their beliefs independently of each other and
no central coordination is possible. Furthermore, the state of the system
continues to change as beliefs are being updated. Previous approaches to
developing distributed asynchronous probabilistic reasoning systems have used
static models. We present an approach using dynamic models, that take into
account the way the system changes state over time. Our approach, which is
based on belief propagation, is fully distributed and asynchronous, and allows
the world to keep on changing as messages are being sent around. Experimental
results show that our approach compares favorably to the factored frontier
algorithm.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Expectation Propagation for Continuous Time Bayesian Networks,"Continuous time Bayesian networks (CTBNs) describe structured stochastic
processes with finitely many states that evolve over continuous time. A CTBN is
a directed (possibly cyclic) dependency graph over a set of variables, each of
which represents a finite state continuous time Markov process whose transition
model is a function of its parents. As shown previously, exact inference in
CTBNs is intractable. We address the problem of approximate inference, allowing
for general queries conditioned on evidence over continuous time intervals and
at discrete time points. We show how CTBNs can be parameterized within the
exponential family, and use that insight to develop a message passing scheme in
cluster graphs and allows us to apply expectation propagation to CTBNs. The
clusters in our cluster graph do not contain distributions over the cluster
variables at individual time points, but distributions over trajectories of the
variables throughout a duration. Thus, unlike discrete time temporal models
such as dynamic Bayesian networks, we can adapt the time granularity at which
we reason for different variables and in different conditions.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Expectation Maximization and Complex Duration Distributions for
  Continuous Time Bayesian Networks","Continuous time Bayesian networks (CTBNs) describe structured stochastic
processes with finitely many states that evolve over continuous time. A CTBN is
a directed (possibly cyclic) dependency graph over a set of variables, each of
which represents a finite state continuous time Markov process whose transition
model is a function of its parents. We address the problem of learning the
parameters and structure of a CTBN from partially observed data. We show how to
apply expectation maximization (EM) and structural expectation maximization
(SEM) to CTBNs. The availability of the EM algorithm allows us to extend the
representation of CTBNs to allow a much richer class of transition durations
distributions, known as phase distributions. This class is a highly expressive
semi-parametric representation, which can approximate any duration distribution
arbitrarily closely. This extension to the CTBN framework addresses one of the
main limitations of both CTBNs and DBNs - the restriction to exponentially /
geometrically distributed duration. We present experimental results on a real
data set of people's life spans, showing that our algorithm learns reasonable
models - structure and parameters - from partially observed data, and, with the
use of phase distributions, achieves better performance than DBNs.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Sufficient conditions for convergence of Loopy Belief Propagation,"We derive novel sufficient conditions for convergence of Loopy Belief
Propagation (also known as the Sum-Product algorithm) to a unique fixed point.
Our results improve upon previously known conditions. For binary variables with
(anti-)ferromagnetic interactions, our conditions seem to be sharp.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
The Relationship Between AND/OR Search and Variable Elimination,"In this paper we compare search and inference in graphical models through the
new framework of AND/OR search. Specifically, we compare Variable Elimination
(VE) and memoryintensive AND/OR Search (AO) and place algorithms such as
graph-based backjumping and no-good and good learning, as well as Recursive
Conditioning [7] and Value Elimination [2] within the AND/OR search framework.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Representation Policy Iteration,"This paper addresses a fundamental issue central to approximation methods for
solving large Markov decision processes (MDPs): how to automatically learn the
underlying representation for value function approximation? A novel
theoretically rigorous framework is proposed that automatically generates
geometrically customized orthonormal sets of basis functions, which can be used
with any approximate MDP solver like least squares policy iteration (LSPI). The
key innovation is a coordinate-free representation of value functions, using
the theory of smooth functions on a Riemannian manifold. Hodge theory yields a
constructive method for generating basis functions for approximating value
functions based on the eigenfunctions of the self-adjoint (Laplace-Beltrami)
operator on manifolds. In effect, this approach performs a global Fourier
analysis on the state space graph to approximate value functions, where the
basis functions reflect the largescale topology of the underlying state space.
A new class of algorithms called Representation Policy Iteration (RPI) are
presented that automatically learn both basis functions and approximately
optimal policies. Illustrative experiments compare the performance of RPI with
that of LSPI using two handcoded basis functions (RBF and polynomial state
encodings).","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Point-Based POMDP Algorithms: Improved Analysis and Implementation,"Existing complexity bounds for point-based POMDP value iteration algorithms
focus either on the curse of dimensionality or the curse of history. We derive
a new bound that relies on both and uses the concept of discounted
reachability; our conclusions may help guide future algorithm design. We also
discuss recent improvements to our (point-based) heuristic search value
iteration algorithm. Our new implementation calculates tighter initial bounds,
avoids solving linear programs, and makes more effective use of sparsity.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Approximate Linear Programming for First-order MDPs,"We introduce a new approximate solution technique for first-order Markov
decision processes (FOMDPs). Representing the value function linearly w.r.t. a
set of first-order basis functions, we compute suitable weights by casting the
corresponding optimization as a first-order linear program and show how
off-the-shelf theorem prover and LP software can be effectively used. This
technique allows one to solve FOMDPs independent of a specific domain
instantiation; furthermore, it allows one to determine bounds on approximation
error that apply equally to all domain instantiations. We apply this solution
technique to the task of elevator scheduling with a rich feature space and
multi-criteria additive reward, and demonstrate that it outperforms a number of
intuitive, heuristicallyguided policies.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Predictive Linear-Gaussian Models of Stochastic Dynamical Systems,"Models of dynamical systems based on predictive state representations (PSRs)
are defined strictly in terms of observable quantities, in contrast with
traditional models (such as Hidden Markov Models) that use latent variables or
statespace representations. In addition, PSRs have an effectively infinite
memory, allowing them to model some systems that finite memory-based models
cannot. Thus far, PSR models have primarily been developed for domains with
discrete observations. Here, we develop the Predictive Linear-Gaussian (PLG)
model, a class of PSR models for domains with continuous observations. We show
that PLG models subsume Linear Dynamical System models (also called Kalman
filter models or state-space models) while using fewer parameters. We also
introduce an algorithm to estimate PLG parameters from data, and contrast it
with standard Expectation Maximization (EM) algorithms used to estimate Kalman
filter parameters. We show that our algorithm is a consistent estimation
procedure and present preliminary empirical results suggesting that our
algorithm outperforms EM, particularly as the model dimension increases.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Efficient Test Selection in Active Diagnosis via Entropy Approximation,"We consider the problem of diagnosing faults in a system represented by a
Bayesian network, where diagnosis corresponds to recovering the most likely
state of unobserved nodes given the outcomes of tests (observed nodes). Finding
an optimal subset of tests in this setting is intractable in general. We show
that it is difficult even to compute the next most-informative test using
greedy test selection, as it involves several entropy terms whose exact
computation is intractable. We propose an approximate approach that utilizes
the loopy belief propagation infrastructure to simultaneously compute
approximations of marginal and conditional entropies on multiple subsets of
nodes. We apply our method to fault diagnosis in computer networks, and show
the algorithm to be very effective on realistic Internet-like topologies. We
also provide theoretical justification for the greedy test selection approach,
along with some performance guarantees.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Importance Sampling in Bayesian Networks: An Influence-Based
  Approximation Strategy for Importance Functions","One of the main problems of importance sampling in Bayesian networks is
representation of the importance function, which should ideally be as close as
possible to the posterior joint distribution. Typically, we represent an
importance function as a factorization, i.e., product of conditional
probability tables (CPTs). Given diagnostic evidence, we do not have explicit
forms for the CPTs in the networks. We first derive the exact form for the CPTs
of the optimal importance function. Since the calculation is hard, we usually
only use their approximations. We review several popular strategies and point
out their limitations. Based on an analysis of the influence of evidence, we
propose a method for approximating the exact form of importance function by
explicitly modeling the most important additional dependence relations
introduced by evidence. Our experimental results show that the new
approximation strategy offers an immediate improvement in the quality of the
importance function.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
Structured Region Graphs: Morphing EP into GBP,"GBP and EP are two successful algorithms for approximate probabilistic
inference, which are based on different approximation strategies. An open
problem in both algorithms has been how to choose an appropriate approximation
structure. We introduce 'structured region graphs', a formalism which marries
these two strategies, reveals a deep connection between them, and suggests how
to choose good approximation structures. In this formalism, each region has an
internal structure which defines an exponential family, whose sufficient
statistics must be matched by the parent region. Reduction operators on these
structures allow conversion between EP and GBP free energies. Thus it is
revealed that all EP approximations on discrete variables are special cases of
GBP, and conversely that some wellknown GBP approximations, such as overlapping
squares, are special cases of EP. Furthermore, region graphs derived from EP
have a number of good structural properties, including maxent-normality and
overall counting number of one. The result is a convenient framework for
producing high-quality approximations with a user-adjustable level of
complexity","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
A Model for Reasoning with Uncertain Rules in Event Composition Systems,"In recent years, there has been an increased need for the use of active
systems - systems required to act automatically based on events, or changes in
the environment. Such systems span many areas, from active databases to
applications that drive the core business processes of today's enterprises.
However, in many cases, the events to which the system must respond are not
generated by monitoring tools, but must be inferred from other events based on
complex temporal predicates. In addition, in many applications, such inference
is inherently uncertain. In this paper, we introduce a formal framework for
knowledge representation and reasoning enabling such event inference. Based on
probability theory, we define the representation of the associated uncertainty.
In addition, we formally define the probability space, and show how the
relevant probabilities can be calculated by dynamically constructing a Bayesian
network. To the best of our knowledge, this is the first work that enables
taking such uncertainty into account in the context of active systems.
herefore, our contribution is twofold: We formally define the representation
and semantics of event composition for probabilistic settings, and show how to
apply these extensions to the quantification of the occurrence probability of
events. These results enable any active system to handle such uncertainty.","Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)"
"Super-Mixed Multiple Attribute Group Decision Making Method Based on
  Hybrid Fuzzy Grey Relation Approach Degree","The feature of our method different from other fuzzy grey relation method for
supermixed multiple attribute group decision-making is that all of the
subjective and objective weights are obtained by interval grey number and that
the group decisionmaking is performed based on the relative approach degree of
grey TOPSIS, the relative approach degree of grey incidence and the relative
membership degree of grey incidence using 4-dimensional Euclidean distance. The
weighted Borda method is used to obtain final rank by using the results of four
methods. An example shows the applicability of the proposed approach.",N/A
"Generalized Hybrid Grey Relation Method for Multiple Attribute Mixed
  Type Decision Making","The multiple attribute mixed type decision making is performed by four
methods, that is, the relative approach degree of grey TOPSIS method, the
relative approach degree of grey incidence, the relative membership degree of
grey incidence and the grey relation relative approach degree method using the
maximum entropy estimation, respectively. In these decision making methods, the
grey incidence degree in four-dimensional Euclidean space is used. The final
arrangement result is obtained by weighted Borda method. An example illustrates
the applicability of the proposed approach.",N/A
The SeqBin Constraint Revisited,"We revisit the SeqBin constraint. This meta-constraint subsumes a number of
important global constraints like Change, Smooth and IncreasingNValue. We show
that the previously proposed filtering algorithm for SeqBin has two drawbacks
even under strong restrictions: it does not detect bounds disentailment and it
is not idempotent. We identify the cause for these problems, and propose a new
propagator that overcomes both issues. Our algorithm is based on a connection
to the problem of finding a path of a given cost in a restricted $n$-partite
graph. Our propagator enforces domain consistency in O(nd^2) and, for special
cases of SeqBin that include Change, Smooth and IncreasingNValue, in O(nd)
time.",Longer version of paper accepted at CP 2012
Arabic CALL system based on pedagogically indexed text,"This article introduces the benefits of using computer as a tool for foreign
language teaching and learning. It describes the effect of using Natural
Language Processing (NLP) tools for learning Arabic. The technique explored in
this particular case is the employment of pedagogically indexed corpora. This
text-based method provides the teacher the advantage of building activities
based on texts adapted to a particular pedagogical situation. This paper also
presents ARAC: a Platform dedicated to language educators allowing them to
create activities within their own pedagogical area of interest.","The 2011 International Conference on Artificial Intelligence
  (ICAI'11), 2011, WORLDCOMP'11, Las Vegas, Nevada, USA"
"Etude de Modèles à base de réseaux Bayésiens pour l'aide au
  diagnostic de tumeurs cérébrales","This article describes different models based on Bayesian networks RB
modeling expertise in the diagnosis of brain tumors. Indeed, they are well
adapted to the representation of the uncertainty in the process of diagnosis of
these tumors. In our work, we first tested several structures derived from the
Bayesian network reasoning performed by doctors on the one hand and structures
generated automatically on the other. This step aims to find the best structure
that increases diagnostic accuracy. The machine learning algorithms relate
MWST-EM algorithms, SEM and SEM + T. To estimate the parameters of the Bayesian
network from a database incomplete, we have proposed an extension of the EM
algorithm by adding a priori knowledge in the form of the thresholds calculated
by the first phase of the algorithm RBE . The very encouraging results obtained
are discussed at the end of the paper","Journ\'ees francophones d'ing\'enierie des connaissances, France
  (2012)"
"Novel Grey Interval Weight Determining and Hybrid Grey Interval Relation
  Method in Multiple Attribute Decision-Making","This paper proposes a grey interval relation TOPSIS for the decision making
in which all of the attribute weights and attribute values are given by the
interval grey numbers. The feature of our method different from other grey
relation decision-making is that all of the subjective and objective weights
are obtained by interval grey number and that decisionmaking is performed based
on the relative approach degree of grey TOPSIS, the relative approach degree of
grey incidence and the relative membership degree of grey incidence using
2-dimensional Euclidean distance. The weighted Borda method is used for
combining the results of three methods. An example shows the applicability of
the proposed approach.",arXiv admin note: substantial text overlap with arXiv:1207.1501
Probabilistic Event Calculus for Event Recognition,"Symbolic event recognition systems have been successfully applied to a
variety of application domains, extracting useful information in the form of
events, allowing experts or other systems to monitor and respond when
significant events are recognised. In a typical event recognition application,
however, these systems often have to deal with a significant amount of
uncertainty. In this paper, we address the issue of uncertainty in logic-based
event recognition by extending the Event Calculus with probabilistic reasoning.
Markov Logic Networks are a natural candidate for our logic-based formalism.
However, the temporal semantics of the Event Calculus introduce a number of
challenges for the proposed model. We show how and under what assumptions we
can overcome these problems. Additionally, we study how probabilistic modelling
changes the behaviour of the formalism, affecting its key property, the inertia
of fluents. Furthermore, we demonstrate the advantages of the probabilistic
Event Calculus through examples and experiments in the domain of activity
recognition, using a publicly available dataset for video surveillance.",N/A
"Classification of Approaches and Challenges of Frequent Subgraphs Mining
  in Biological Networks","Understanding the structure and dynamics of biological networks is one of the
important challenges in system biology. In addition, increasing amount of
experimental data in biological networks necessitate the use of efficient
methods to analyze these huge amounts of data. Such methods require to
recognize common patterns to analyze data. As biological networks can be
modeled by graphs, the problem of common patterns recognition is equivalent
with frequent sub graph mining in a set of graphs. In this paper, at first the
challenges of frequent subgrpahs mining in biological networks are introduced
and the existing approaches are classified for each challenge. then the
algorithms are analyzed on the basis of the type of the approach they apply for
each of the challenges.",N/A
"Hybrid Grey Interval Relation Decision-Making in Artistic Talent
  Evaluation of Player","This paper proposes a grey interval relation TOPSIS method for the decision
making in which all of the attribute weights and attribute values are given by
the interval grey numbers. In this paper, all of the subjective and objective
weights are obtained by interval grey number and decision-making is based on
four methods such as the relative approach degree of grey TOPSIS, the relative
approach degree of grey incidence and the relative approach degree method using
the maximum entropy estimation using 2-dimensional Euclidean distance. A
multiple attribute decision-making example for evaluation of artistic talent of
Kayagum (stringed Korean harp) players is given to show practicability of the
proposed approach.",arXiv admin note: substantial text overlap with arXiv:1207.2592
Qualitative Approximate Behavior Composition,"The behavior composition problem involves automatically building a controller
that is able to realize a desired, but unavailable, target system (e.g., a
house surveillance) by suitably coordinating a set of available components
(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previous
work has almost exclusively aimed at bringing about the desired component in
its totality, which is highly unsatisfactory for unsolvable problems. In this
work, we develop an approach for approximate behavior composition without
departing from the classical setting, thus making the problem applicable to a
much wider range of cases. Based on the notion of simulation, we characterize
what a maximal controller and the ""closest"" implementable target module
(optimal approximation) are, and show how these can be computed using ATL model
checking technology for a special case. We show the uniqueness of optimal
approximations, and prove their soundness and completeness with respect to
their imported controllers.",N/A
Reasoning about Agent Programs using ATL-like Logics,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the
agents' operational know-how, as defined by their libraries of abstract plans.
Inspired by ATLES, a variant itself of ATL, it is possible in our logic to
explicitly refer to ""rational"" strategies for agents developed under the
Belief-Desire-Intention agent programming paradigm. This allows us to express
and verify properties of BDI systems using ATL-type logical frameworks.",N/A
Exploiting First-Order Regression in Inductive Policy Selection,"We consider the problem of computing optimal generalised policies for
relational Markov decision processes. We describe an approach combining some of
the benefits of purely inductive techniques with those of symbolic dynamic
programming methods. The latter reason about the optimal value function using
first-order decision theoretic regression and formula rewriting, while the
former, when provided with a suitable hypotheses language, are capable of
generalising value functions or policies for small instances. Our idea is to
use reasoning and in particular classical first-order regression to
automatically generate a hypotheses language dedicated to the domain at hand,
which is then used as input by an inductive solver. This approach avoids the
more complex reasoning of symbolic dynamic programming while focusing the
inductive solver's attention on concepts that are specifically relevant to the
optimal value function for the domain considered.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Decision Making for Symbolic Probability,"This paper proposes a decision theory for a symbolic generalization of
probability theory (SP). Darwiche and Ginsberg [2,3] proposed SP to relax the
requirement of using numbers for uncertainty while preserving desirable
patterns of Bayesian reasoning. SP represents uncertainty by symbolic supports
that are ordered partially rather than completely as in the case of standard
probability. We show that a preference relation on acts that satisfies a number
of intuitive postulates is represented by a utility function whose domain is a
set of pairs of supports. We argue that a subjective interpretation is as
useful and appropriate for SP as it is for numerical probability. It is useful
because the subjective interpretation provides a basis for uncertainty
elicitation. It is appropriate because we can provide a decision theory that
explains how preference on acts is based on support comparison.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Metrics for Finite Markov Decision Processes,"We present metrics for measuring the similarity of states in a finite Markov
decision process (MDP). The formulation of our metrics is based on the notion
of bisimulation for MDPs, with an aim towards solving discounted infinite
horizon reinforcement learning tasks. Such metrics can be used to aggregate
states, as well as to better structure other value function approximators
(e.g., memory-based or nearest-neighbor approximators). We provide bounds that
relate our metric distances to the optimal values of states in the given MDP.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Dynamic Programming for Structured Continuous Markov Decision Problems,"We describe an approach for exploiting structure in Markov Decision Processes
with continuous state variables. At each step of the dynamic programming, the
state space is dynamically partitioned into regions where the value function is
the same throughout the region. We first describe the algorithm for piecewise
constant representations. We then extend it to piecewise linear
representations, using techniques from POMDPs to represent and reason about
linear surfaces efficiently. We show that for complex, structured problems, our
approach exploits the natural structure so that optimal solutions can be
computed efficiently.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Region-Based Incremental Pruning for POMDPs,"We present a major improvement to the incremental pruning algorithm for
solving partially observable Markov decision processes. Our technique targets
the cross-sum step of the dynamic programming (DP) update, a key source of
complexity in POMDP algorithms. Instead of reasoning about the whole belief
space when pruning the cross-sums, our algorithm divides the belief space into
smaller regions and performs independent pruning in each region. We evaluate
the benefits of the new technique both analytically and experimentally, and
show that it produces very significant performance gains. The results
contribute to the scalability of POMDP algorithms to domains that cannot be
handled by the best existing techniques.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
A Unified framework for order-of-magnitude confidence relations,"The aim of this work is to provide a unified framework for ordinal
representations of uncertainty lying at the crosswords between possibility and
probability theories. Such confidence relations between events are commonly
found in monotonic reasoning, inconsistency management, or qualitative decision
theory. They start either from probability theory, making it more qualitative,
or from possibility theory, making it more expressive. We show these two trends
converge to a class of genuine probability theories. We provide
characterization results for these useful tools that preserve the qualitative
nature of possibility rankings, while enjoying the power of expressivity of
additive representations.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
"Mixtures of Deterministic-Probabilistic Networks and their AND/OR Search
  Space","The paper introduces mixed networks, a new framework for expressing and
reasoning with probabilistic and deterministic information. The framework
combines belief networks with constraint networks, defining the semantics and
graphical representation. We also introduce the AND/OR search space for
graphical models, and develop a new linear space search algorithm. This
provides the basis for understanding the benefits of processing the constraint
information separately, resulting in the pruning of the search space. When the
constraint part is tractable or has a small number of solutions, using the
mixed representation can be exponentially more effective than using pure belief
networks which odel constraints as conditional probability tables.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Stable Independance and Complexity of Representation,"The representation of independence relations generally builds upon the
well-known semigraphoid axioms of independence. Recently, a representation has
been proposed that captures a set of dominant statements of an independence
relation from which any other statement can be generated by means of the
axioms; the cardinality of this set is taken to indicate the complexity of the
relation. Building upon the idea of dominance, we introduce the concept of
stability to provide for a more compact representation of independence. We give
an associated algorithm for establishing such a representation.We show that,
with our concept of stability, many independence relations are found to be of
lower complexity than with existing representations.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
"Propositional and Relational Bayesian Networks Associated with Imprecise
  and Qualitative Probabilistic Assesments","This paper investigates a representation language with flexibility inspired
by probabilistic logic and compactness inspired by relational Bayesian
networks. The goal is to handle propositional and first-order constructs
together with precise, imprecise, indeterminate and qualitative probabilistic
assessments. The paper shows how this can be achieved through the theory of
credal networks. New exact and approximate inference algorithms based on
multilinear programming and iterated/loopy propagation of interval
probabilities are presented; their superior performance, compared to existing
ones, is shown empirically.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
"A Logic Programming Framework for Possibilistic Argumentation with Vague
  Knowledge","Defeasible argumentation frameworks have evolved to become a sound setting to
formalize commonsense, qualitative reasoning from incomplete and potentially
inconsistent knowledge. Defeasible Logic Programming (DeLP) is a defeasible
argumentation formalism based on an extension of logic programming. Although
DeLP has been successfully integrated in a number of different real-world
applications, DeLP cannot deal with explicit uncertainty, nor with vague
knowledge, as defeasibility is directly encoded in the object language. This
paper introduces P-DeLP, a new logic programming language that extends original
DeLP capabilities for qualitative reasoning by incorporating the treatment of
possibilistic uncertainty and fuzzy knowledge. Such features will be formalized
on the basis of PGL, a possibilistic logic based on Godel fuzzy logic.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
"Sensitivity Analysis in Bayesian Networks: From Single to Multiple
  Parameters","Previous work on sensitivity analysis in Bayesian networks has focused on
single parameters, where the goal is to understand the sensitivity of queries
to single parameter changes, and to identify single parameter changes that
would enforce a certain query constraint. In this paper, we expand the work to
multiple parameters which may be in the CPT of a single variable, or the CPTs
of multiple variables. Not only do we identify the solution space of multiple
parameter changes that would be needed to enforce a query constraint, but we
also show how to find the optimal solution, that is, the one which disturbs the
current probability distribution the least (with respect to a specific measure
of disturbance). We characterize the computational complexity of our new
techniques and discuss their applications to developing and debugging Bayesian
networks, and to the problem of reasoning about the value (reliability) of new
information.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Compact Value-Function Representations for Qualitative Preferences,"We consider the challenge of preference elicitation in systems that help
users discover the most desirable item(s) within a given database. Past work on
preference elicitation focused on structured models that provide a factored
representation of users' preferences. Such models require less information to
construct and support efficient reasoning algorithms. This paper makes two
substantial contributions to this area: (1) Strong representation theorems for
factored value functions. (2) A methodology that utilizes our representation
results to address the problem of optimal item selection.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Using arguments for making decisions: A possibilistic logic approach,"Humans currently use arguments for explaining choices which are already made,
or for evaluating potential choices. Each potential choice has usually pros and
cons of various strengths. In spite of the usefulness of arguments in a
decision making process, there have been few formal proposals handling this
idea if we except works by Fox and Parsons and by Bonet and Geffner. In this
paper we propose a possibilistic logic framework where arguments are built from
an uncertain knowledge base and a set of prioritized goals. The proposed
approach can compute two kinds of decisions by distinguishing between
pessimistic and optimistic attitudes. When the available, maybe uncertain,
knowledge is consistent, as well as the set of prioritized goals (which have to
be fulfilled as far as possible), the method for evaluating decisions on the
basis of arguments agrees with the possibility theory-based approach to
decision-making under uncertainty. Taking advantage of its relation with formal
approaches to defeasible argumentation, the proposed framework can be
generalized in case of partially inconsistent knowledge, or goal bases.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Case-Factor Diagrams for Structured Probabilistic Modeling,"We introduce a probabilistic formalism subsuming Markov random fields of
bounded tree width and probabilistic context free grammars. Our models are
based on a representation of Boolean formulas that we call case-factor diagrams
(CFDs). CFDs are similar to binary decision diagrams (BDDs) but are concise for
circuits of bounded tree width (unlike BDDs) and can concisely represent the
set of parse trees over a given string undera given context free grammar (also
unlike BDDs). A probabilistic model consists of aCFD defining a feasible set of
Boolean assignments and a weight (or cost) for each individual Boolean
variable. We give an insideoutside algorithm for simultaneously computing the
marginal of each Boolean variable, and a Viterbi algorithm for finding the
mininum cost variable assignment. Both algorithms run in time proportional to
the size of the CFD.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Convolutional Factor Graphs as Probabilistic Models,"Based on a recent development in the area of error control coding, we
introduce the notion of convolutional factor graphs (CFGs) as a new class of
probabilistic graphical models. In this context, the conventional factor graphs
are referred to as multiplicative factor graphs (MFGs). This paper shows that
CFGs are natural models for probability functions when summation of independent
latent random variables is involved. In particular, CFGs capture a large class
of linear models, where the linearity is in the sense that the observed
variables are obtained as a linear ransformation of the latent variables taking
arbitrary distributions. We use Gaussian models and independent factor models
as examples to emonstrate the use of CFGs. The requirement of a linear
transformation between latent variables (with certain independence restriction)
and the bserved variables, to an extent, limits the modelling flexibility of
CFGs. This structural restriction however provides a powerful analytic tool to
the framework of CFGs; that is, upon taking the Fourier transform of the
function represented by the CFG, the resulting function is represented by a FG
with identical structure. This Fourier transform duality allows inference
problems on a CFG to be solved on the corresponding dual MFG.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
An Empirical Evaluation of Possible Variations of Lazy Propagation,"As real-world Bayesian networks continue to grow larger and more complex, it
is important to investigate the possibilities for improving the performance of
existing algorithms of probabilistic inference. Motivated by examples, we
investigate the dependency of the performance of Lazy propagation on the
message computation algorithm. We show how Symbolic Probabilistic Inference
(SPI) and Arc-Reversal (AR) can be used for computation of clique to clique
messages in the addition to the traditional use of Variable Elimination (VE).
In addition, the paper resents the results of an empirical evaluation of the
performance of Lazy propagation using VE, SPI, and AR as the message
computation algorithm. The results of the empirical evaluation show that for
most networks, the performance of inference did not depend on the choice of
message computation algorithm, but for some randomly generated networks the
choice had an impact on both space and time performance. In the cases where the
choice had an impact, AR produced the best results.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Solving Factored MDPs with Continuous and Discrete Variables,"Although many real-world stochastic planning problems are more naturally
formulated by hybrid models with both discrete and continuous variables,
current state-of-the-art methods cannot adequately address these problems. We
present the first framework that can exploit problem structure for modeling and
solving hybrid problems efficiently. We formulate these problems as hybrid
Markov decision processes (MDPs with continuous and discrete state and action
variables), which we assume can be represented in a factored way using a hybrid
dynamic Bayesian network (hybrid DBN). This formulation also allows us to apply
our methods to collaborative multiagent settings. We present a new linear
program approximation method that exploits the structure of the hybrid MDP and
lets us compute approximate value functions more efficiently. In particular, we
describe a new factored discretization of continuous variables that avoids the
exponential blow-up of traditional approaches. We provide theoretical bounds on
the quality of such an approximation and on its scale-up potential. We support
our theoretical arguments with experiments on a set of control problems with up
to 28-dimensional continuous state space and 22-dimensional action space.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Annealed MAP,"Maximum a Posteriori assignment (MAP) is the problem of finding the most
probable instantiation of a set of variables given the partial evidence on the
other variables in a Bayesian network. MAP has been shown to be a NP-hard
problem [22], even for constrained networks, such as polytrees [18]. Hence,
previous approaches often fail to yield any results for MAP problems in large
complex Bayesian networks. To address this problem, we propose AnnealedMAP
algorithm, a simulated annealing-based MAP algorithm. The AnnealedMAP algorithm
simulates a non-homogeneous Markov chain whose invariant function is a
probability density that concentrates itself on the modes of the target
density. We tested this algorithm on several real Bayesian networks. The
results show that, while maintaining good quality of the MAP solutions, the
AnnealedMAP algorithm is also able to solve many problems that are beyond the
reach of previous approaches.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Monotonicity in Bayesian Networks,"For many real-life Bayesian networks, common knowledge dictates that the
output established for the main variable of interest increases with higher
values for the observable variables. We define two concepts of monotonicity to
capture this type of knowledge. We say that a network is isotone in
distribution if the probability distribution computed for the output variable
given specific observations is stochastically dominated by any such
distribution given higher-ordered observations; a network is isotone in mode if
a probability distribution given higher observations has a higher mode. We show
that establishing whether a network exhibits any of these properties of
monotonicity is coNPPP-complete in general, and remains coNP-complete for
polytrees. We present an approximate algorithm for deciding whether a network
is monotone in distribution and illustrate its application to a real-life
network in oncology.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Heuristic Search Value Iteration for POMDPs,"We present a novel POMDP planning algorithm called heuristic search value
iteration (HSVI).HSVI is an anytime algorithm that returns a policy and a
provable bound on its regret with respect to the optimal policy. HSVI gets its
power by combining two well-known techniques: attention-focusing search
heuristics and piecewise linear convex representations of the value function.
HSVI's soundness and convergence have been proven. On some benchmark problems
from the literature, HSVI displays speedups of greater than 100 with respect to
other state-of-the-art POMDP value iteration algorithms. We also apply HSVI to
a new rover exploration problem 10 times larger than most POMDP problems in the
literature.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
A New Characterization of Probabilities in Bayesian Networks,"We characterize probabilities in Bayesian networks in terms of algebraic
expressions called quasi-probabilities. These are arrived at by casting
Bayesian networks as noisy AND-OR-NOT networks, and viewing the subnetworks
that lead to a node as arguments for or against a node. Quasi-probabilities are
in a sense the ""natural"" algebra of Bayesian networks: we can easily compute
the marginal quasi-probability of any node recursively, in a compact form; and
we can obtain the joint quasi-probability of any set of nodes by multiplying
their marginals (using an idempotent product operator). Quasi-probabilities are
easily manipulated to improve the efficiency of probabilistic inference. They
also turn out to be representable as square-wave pulse trains, and joint and
marginal distributions can be computed by multiplication and complementation of
pulse trains.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Evidence-invariant Sensitivity Bounds,"The sensitivities revealed by a sensitivity analysis of a probabilistic
network typically depend on the entered evidence. For a real-life network
therefore, the analysis is performed a number of times, with different
evidence. Although efficient algorithms for sensitivity analysis exist, a
complete analysis is often infeasible because of the large range of possible
combinations of observations. In this paper we present a method for studying
sensitivities that are invariant to the evidence entered. Our method builds
upon the idea of establishing bounds between which a parameter can be varied
without ever inducing a change in the most likely value of a variable of
interest.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
On Modeling Profiles instead of Values,"We consider the problem of estimating the distribution underlying an observed
sample of data. Instead of maximum likelihood, which maximizes the probability
of the ob served values, we propose a different estimate, the high-profile
distribution, which maximizes the probability of the observed profile the
number of symbols appearing any given number of times. We determine the
high-profile distribution of several data samples, establish some of its
general properties, and show that when the number of distinct symbols observed
is small compared to the data size, the high-profile and maximum-likelihood
distributions are roughly the same, but when the number of symbols is large,
the distributions differ, and high-profile better explains the data.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Learning Diagnostic Policies from Examples by Systematic Search,"A diagnostic policy specifies what test to perform next, based on the results
of previous tests, and when to stop and make a diagnosis. Cost-sensitive
diagnostic policies perform tradeoffs between (a) the cost of tests and (b) the
cost of misdiagnoses. An optimal diagnostic policy minimizes the expected total
cost. We formalize this diagnosis process as a Markov Decision Process (MDP).
We investigate two types of algorithms for solving this MDP: systematic search
based on AO* algorithm and greedy search (particularly the Value of Information
method). We investigate the issue of learning the MDP probabilities from
examples, but only as they are relevant to the search for good policies. We do
not learn nor assume a Bayesian network for the diagnosis process. Regularizers
are developed to control overfitting and speed up the search. This research is
the first that integrates overfitting prevention into systematic search. The
paper has two contributions: it discusses the factors that make systematic
search feasible for diagnosis, and it shows experimentally, on benchmark data
sets, that systematic search methods produce better diagnostic policies than
greedy methods.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Hybrid Influence Diagrams Using Mixtures of Truncated Exponentials,"Mixtures of truncated exponentials (MTE) potentials are an alternative to
discretization for representing continuous chance variables in influence
diagrams. Also, MTE potentials can be used to approximate utility functions.
This paper introduces MTE influence diagrams, which can represent decision
problems without restrictions on the relationships between continuous and
discrete chance variables, without limitations on the distributions of
continuous chance variables, and without limitations on the nature of the
utility functions. In MTE influence diagrams, all probability distributions and
the joint utility function (or its multiplicative factors) are represented by
MTE potentials and decision nodes are assumed to have discrete state spaces.
MTE influence diagrams are solved by variable elimination using a fusion
algorithm.","Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)"
Towards Understanding Triangle Construction Problems,"Straightedge and compass construction problems are one of the oldest and most
challenging problems in elementary mathematics. The central challenge, for a
human or for a computer program, in solving construction problems is a huge
search space. In this paper we analyze one family of triangle construction
problems, aiming at detecting a small core of the underlying geometry
knowledge. The analysis leads to a small set of needed definitions, lemmas and
primitive construction steps, and consequently, to a simple algorithm for
automated solving of problems from this family. The same approach can be
applied to other families of construction problems.",N/A
"The Arcade Learning Environment: An Evaluation Platform for General
  Agents","In this article we introduce the Arcade Learning Environment (ALE): both a
challenge problem and a platform and methodology for evaluating the development
of general, domain-independent AI technology. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and
designed to be a challenge for human players. ALE presents significant research
challenges for reinforcement learning, model learning, model-based planning,
imitation learning, transfer learning, and intrinsic motivation. Most
importantly, it provides a rigorous testbed for evaluating and comparing
approaches to these problems. We illustrate the promise of ALE by developing
and benchmarking domain-independent agents designed using well-established AI
techniques for both reinforcement learning and planning. In doing so, we also
propose an evaluation methodology made possible by ALE, reporting empirical
results on over 55 different games. All of the software, including the
benchmark agents, is publicly available.",N/A
"Exploring the rationality of some syntactic merging operators (extended
  version)","Most merging operators are defined by semantics methods which have very high
computational complexity. In order to have operators with a lower computational
complexity, some merging operators defined in a syntactical way have be
proposed. In this work we define some syntactical merging operators and
exploring its rationality properties. To do that we constrain the belief bases
to be sets of formulas very close to logic programs and the underlying logic is
defined through forward chaining rule (Modus Ponens). We propose two types of
operators: arbitration operators when the inputs are only two bases and fusion
with integrity constraints operators. We introduce a set of postulates inspired
of postulates LS, proposed by Liberatore and Shaerf and then we analyzed the
first class of operators through these postulates. We also introduce a set of
postulates inspired of postulates KP, proposed by Konieczny and Pino P\'erez
and then we analyzed the second class of operators through these postulates.",N/A
Stator flux optimization on direct torque control with fuzzy logic,"The Direct Torque Control (DTC) is well known as an effective control
technique for high performance drives in a wide variety of industrial
applications and conventional DTC technique uses two constant reference value:
torque and stator flux. In this paper, fuzzy logic based stator flux
optimization technique for DTC drives that has been proposed. The proposed
fuzzy logic based stator flux optimizer self-regulates the stator flux
reference using induction motor load situation without need of any motor
parameters. Simulation studies have been carried out with Matlab/Simulink to
compare the proposed system behaviors at vary load conditions. Simulation
results show that the performance of the proposed DTC technique has been
improved and especially at low-load conditions torque ripple are greatly
reduced with respect to the conventional DTC.","8 pages, 8 figures, Itca 2012 conference"
Selecting Computations: Theory and Applications,"Sequential decision problems are often approximately solvable by simulating
possible future action sequences. {\em Metalevel} decision procedures have been
developed for selecting {\em which} action sequences to simulate, based on
estimating the expected improvement in decision quality that would result from
any particular simulation; an example is the recent work on using bandit
algorithms to control Monte Carlo tree search in the game of Go. In this paper
we develop a theoretical basis for metalevel decisions in the statistical
framework of Bayesian {\em selection problems}, arguing (as others have done)
that this is more appropriate than the bandit framework. We derive a number of
basic results applicable to Monte Carlo selection problems, including the first
finite sampling bounds for optimal policies in certain cases; we also provide a
simple counterexample to the intuitive conjecture that an optimal policy will
necessarily reach a decision in all cases. We then derive heuristic
approximations in both Bayesian and distribution-free settings and demonstrate
their superiority to bandit-based heuristics in one-shot decision problems and
in Go.","10 pages, UAI 2012"
Redundant Sudoku Rules,"The rules of Sudoku are often specified using twenty seven
\texttt{all\_different} constraints, referred to as the {\em big} \mrules.
Using graphical proofs and exploratory logic programming, the following main
and new result is obtained: many subsets of six of these big \mrules are
redundant (i.e., they are entailed by the remaining twenty one \mrules), and
six is maximal (i.e., removing more than six \mrules is not possible while
maintaining equivalence). The corresponding result for binary inequality
constraints, referred to as the {\em small} \mrules, is stated as a conjecture.","14 pages, 161 figures, to appear in TPLP"
Earthquake Scenario Reduction by Symmetry Reasoning,"A recently identified problem is that of finding an optimal investment plan
for a transportation network, given that a disaster such as an earthquake may
destroy links in the network. The aim is to strengthen key links to preserve
the expected network connectivity. A network based on the Istanbul highway
system has thirty links and therefore a billion scenarios, but it has been
estimated that sampling a million scenarios gives reasonable accuracy. In this
paper we use symmetry reasoning to reduce the number of scenarios to a much
smaller number, making sampling unnecessary. This result can be used to
facilitate metaheuristic and exact approaches to the problem.","Abstract presented at EURO conference, Vilnius, Lithuania, 2012"
Model-Lite Case-Based Planning,"There is increasing awareness in the planning community that depending on
complete models impedes the applicability of planning technology in many real
world domains where the burden of specifying complete domain models is too
high. In this paper, we consider a novel solution for this challenge that
combines generative planning on incomplete domain models with a library of plan
cases that are known to be correct. While this was arguably the original
motivation for case-based planning, most existing case-based planners assume
(and depend on) from-scratch planners that work on complete domain models. In
contrast, our approach views the plan generated with respect to the incomplete
model as a ""skeletal plan"" and augments it with directed mining of plan
fragments from library cases. We will present the details of our approach and
present an empirical evaluation of our method in comparison to a
state-of-the-art case-based planner that depends on complete domain models.",N/A
Hybrid systems modeling for gas transmission network,"Gas Transmission Networks are large-scale complex systems, and corresponding
design and control problems are challenging. In this paper, we consider the
problem of control and management of these systems in crisis situations. We
present these networks by a hybrid systems framework that provides required
analysis models. Further, we discuss decision-making using computational
discrete and hybrid optimization methods. In particular, several reinforcement
learning methods are employed to explore decision space and achieve the best
policy in a specific crisis situation. Simulations are presented to illustrate
the efficiency of the method.","This paper has been withdrawn by the author due to a crucial citation
  error in introduction section"
Comparison of different T-norm operators in classification problems,"Fuzzy rule based classification systems are one of the most popular fuzzy
modeling systems used in pattern classification problems. This paper
investigates the effect of applying nine different T-norms in fuzzy rule based
classification systems. In the recent researches, fuzzy versions of confidence
and support merits from the field of data mining have been widely used for both
rules selecting and weighting in the construction of fuzzy rule based
classification systems. For calculating these merits the product has been
usually used as a T-norm. In this paper different T-norms have been used for
calculating the confidence and support measures. Therefore, the calculations in
rule selection and rule weighting steps (in the process of constructing the
fuzzy rule based classification systems) are modified by employing these
T-norms. Consequently, these changes in calculation results in altering the
overall accuracy of rule based classification systems. Experimental results
obtained on some well-known data sets show that the best performance is
produced by employing the Aczel-Alsina operator in terms of the classification
accuracy, the second best operator is Dubois-Prade and the third best operator
is Dombi. In experiments, we have used 12 data sets with numerical attributes
from the University of California, Irvine machine learning repository (UCI).","6 pages, 1 figure, 4 tables; International Journal of Fuzzy Logic
  Systems (IJFLS) Vol.2, No.3, July 2012"
"A Novel Fuzzy Logic Based Adaptive Supertwisting Sliding Mode Control
  Algorithm for Dynamic Uncertain Systems","This paper presents a novel fuzzy logic based Adaptive Super-twisting Sliding
Mode Controller for the control of dynamic uncertain systems. The proposed
controller combines the advantages of Second order Sliding Mode Control, Fuzzy
Logic Control and Adaptive Control. The reaching conditions, stability and
robustness of the system with the proposed controller are guaranteed. In
addition, the proposed controller is well suited for simple design and
implementation. The effectiveness of the proposed controller over the first
order Sliding Mode Fuzzy Logic controller is illustrated by Matlab based
simulations performed on a DC-DC Buck converter. Based on this comparison, the
proposed controller is shown to obtain the desired transient response without
causing chattering and error under steady-state conditions. The proposed
controller is able to give robust performance in terms of rejection to input
voltage variations and load variations.",14 pages
Elimination of ISI Using Improved LMS Based Decision Feedback Equalizer,"This paper deals with the implementation of Least Mean Square (LMS) algorithm
in Decision Feedback Equalizer (DFE) for removal of Inter Symbol Interference
(ISI) at the receiver. The channel disrupts the transmitted signal by spreading
it in time. Although, the LMS algorithm is robust and reliable, it is slow in
convergence. In order to increase the speed of convergence, modifications have
been made in the algorithm where the weights get updated depending on the
severity of disturbance.",N/A
The Complexity of Planning Revisited - A Parameterized Analysis,"The early classifications of the computational complexity of planning under
various restrictions in STRIPS (Bylander) and SAS+ (Baeckstroem and Nebel) have
influenced following research in planning in many ways. We go back and
reanalyse their subclasses, but this time using the more modern tool of
parameterized complexity analysis. This provides new results that together with
the old results give a more detailed picture of the complexity landscape. We
demonstrate separation results not possible with standard complexity theory,
which contributes to explaining why certain cases of planning have seemed
simpler in practice than theory has predicted. In particular, we show that
certain restrictions of practical interest are tractable in the parameterized
sense of the term, and that a simple heuristic is sufficient to make a
well-known partial-order planner exploit this fact.",(author's self-archived copy)
"Explaining Time-Table-Edge-Finding Propagation for the Cumulative
  Resource Constraint","Cumulative resource constraints can model scarce resources in scheduling
problems or a dimension in packing and cutting problems. In order to
efficiently solve such problems with a constraint programming solver, it is
important to have strong and fast propagators for cumulative resource
constraints. One such propagator is the recently developed
time-table-edge-finding propagator, which considers the current resource
profile during the edge-finding propagation. Recently, lazy clause generation
solvers, i.e. constraint programming solvers incorporating nogood learning,
have proved to be excellent at solving scheduling and cutting problems. For
such solvers, concise and accurate explanations of the reasons for propagation
are essential for strong nogood learning. In this paper, we develop the first
explaining version of time-table-edge-finding propagation and show preliminary
results on resource-constrained project scheduling problems from various
standard benchmark suites. On the standard benchmark suite PSPLib, we were able
to close one open instance and to improve the lower bound of about 60% of the
remaining open instances. Moreover, 6 of those instances were closed.","22 pages, 3 figures, 11 tables, 2 algorithms"
"Evaluating Ontology Matching Systems on Large, Multilingual and
  Real-world Test Cases","In the field of ontology matching, the most systematic evaluation of matching
systems is established by the Ontology Alignment Evaluation Initiative (OAEI),
which is an annual campaign for evaluating ontology matching systems organized
by different groups of researchers. In this paper, we report on the results of
an intermediary OAEI campaign called OAEI 2011.5. The evaluations of this
campaign are divided in five tracks. Three of these tracks are new or have been
improved compared to previous OAEI campaigns. Overall, we evaluated 18 matching
systems. We discuss lessons learned, in terms of scalability, multilingual
issues and the ability do deal with real world cases from different domains.",Technical Report of the OAEI 2011.5 Evaluation Campaign
OntoAna: Domain Ontology for Human Anatomy,"Today, we can find many search engines which provide us with information
which is more operational in nature. None of the search engines provide domain
specific information. This becomes very troublesome to a novice user who wishes
to have information in a particular domain. In this paper, we have developed an
ontology which can be used by a domain specific search engine. We have
developed an ontology on human anatomy, which captures information regarding
cardiovascular system, digestive system, skeleton and nervous system. This
information can be used by people working in medical and health care domain.","Proceedings of 5th CSI National Conference on Education and Research.
  Organized by Lingayay University, Faridabad. Sponsored by Computer Society of
  India and IEEE Delhi Chapter. Proceedings published by Lingayay University
  Press"
Lifted Variable Elimination: A Novel Operator and Completeness Results,"Various methods for lifted probabilistic inference have been proposed, but
our understanding of these methods and the relationships between them is still
limited, compared to their propositional counterparts. The only existing
theoretical characterization of lifting is for weighted first-order model
counting (WFOMC), which was shown to be complete domain-lifted for the class of
2-logvar models. This paper makes two contributions to lifted variable
elimination (LVE). First, we introduce a novel inference operator called group
inversion. Second, we prove that LVE augmented with this operator is complete
in the same sense as WFOMC.",N/A
"A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based
  Approaches for E-GTSP","The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard
combinatorial optimization problems. A variant of GTSP is E-GTSP where E,
meaning equality, has the constraint: exactly one node from a cluster of a
graph partition is visited. The main objective of the E-GTSP is to find a
minimum cost tour passing through exactly one node from each cluster of an
undirected graph. Agent-based approaches involving are successfully used
nowadays for solving real life complex problems. The aim of the current paper
is to illustrate some variants of agent-based algorithms including ant-based
models with specific properties for solving E-GTSP.","9 pages, 2 figures"
Parallel ACO with a Ring Neighborhood for Dynamic TSP,"The current paper introduces a new parallel computing technique based on ant
colony optimization for a dynamic routing problem. In the dynamic traveling
salesman problem the distances between cities as travel times are no longer
fixed. The new technique uses a parallel model for a problem variant that
allows a slight movement of nodes within their Neighborhoods. The algorithm is
tested with success on several large data sets.","8 pages, 1 figure; accepted J. Information Technology Research"
"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.",N/A
"Proceedings of the Twenty-Third Conference on Uncertainty in Artificial
  Intelligence (2007)","This is the Proceedings of the Twenty-Third Conference on Uncertainty in
Artificial Intelligence, which was held in Vancouver, British Columbia, July 19
- 22 2007.",N/A
"Proceedings of the Twenty-First Conference on Uncertainty in Artificial
  Intelligence (2005)","This is the Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29
2005.",N/A
"Proceedings of the Twenty-Second Conference on Uncertainty in Artificial
  Intelligence (2006)","This is the Proceedings of the Twenty-Second Conference on Uncertainty in
Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006.",N/A
"Proceedings of the Twentieth Conference on Uncertainty in Artificial
  Intelligence (2004)","This is the Proceedings of the Twentieth Conference on Uncertainty in
Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004.",N/A
Distributed Pharaoh System for Network Routing,"In this paper it is introduced a biobjective ant algorithm for constructing
low cost routing networks. The new algorithm is called the Distributed Pharaoh
System (DPS). DPS is based on AntNet algorithm. The algorithm is using Pharaoh
Ant System (PAS) with an extra-exploration phase and a 'no-entry' condition in
order to improve the solutions for the Low Cost Network Routing problem.
Additionally it is used a cost model for overlay network construction that
includes network traffic demands. The Pharaoh ants (Monomorium pharaonis)
includes negative pheromones with signals concentrated at decision points where
trails fork. The negative pheromones may complement positive pheromone or could
help ants to escape from an unnecessarily long route to food that is being
reinforced by attractive signals. Numerical experiments were made for a random
10-node network. The average node degree of the network tested was 4.0. The
results are encouraging. The algorithm converges to the shortest path while
converging on a low cost overlay routing network topology.","4 pages, 4 figures"
Soft Computing approaches on the Bandwidth Problem,"The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous
reordering of the rows and the columns of a square matrix such that the nonzero
entries are collected within a band of small width close to the main diagonal.
The MBMP is a NP-complete problem, with applications in many scientific
domains, linear systems, artificial intelligence, and real-life situations in
industry, logistics, information recovery. The complex problems are hard to
solve, that is why any attempt to improve their solutions is beneficent.
Genetic algorithms and ant-based systems are Soft Computing methods used in
this paper in order to solve some MBMP instances. Our approach is based on a
learning agent-based model involving a local search procedure. The algorithm is
compared with the classical Cuthill-McKee algorithm, and with a hybrid genetic
algorithm, using several instances from Matrix Market collection. Computational
experiments confirm a good performance of the proposed algorithms for the
considered set of MBMP instances. On Soft Computing basis, we also propose a
new theoretical Reinforcement Learning model for solving the MBMP problem.","6 pages, 1 figure; accepted to Informatica"
"Automatic firewall rules generator for anomaly detection systems with
  Apriori algorithm","Network intrusion detection systems have become a crucial issue for computer
systems security infrastructures. Different methods and algorithms are
developed and proposed in recent years to improve intrusion detection systems.
The most important issue in current systems is that they are poor at detecting
novel anomaly attacks. These kinds of attacks refer to any action that
significantly deviates from the normal behaviour which is considered intrusion.
This paper proposed a model to improve this problem based on data mining
techniques. Apriori algorithm is used to predict novel attacks and generate
real-time rules for firewall. Apriori algorithm extracts interesting
correlation relationships among large set of data items. This paper illustrates
how to use Apriori algorithm in intrusion detection systems to cerate a
automatic firewall rules generator to detect novel anomaly attack. Apriori is
the best-known algorithm to mine association rules. This is an innovative way
to find association rules on large scale.",4 Pages
"On Solving the Oriented Two-Dimensional Bin Packing Problem under Free
  Guillotine Cutting: Exploiting the Power of Probabilistic Solution
  Construction","Two-dimensional bin packing problems are highly relevant combinatorial
optimization problems. They find a large number of applications, for example,
in the context of transportation or warehousing, and for the cutting of
different materials such as glass, wood or metal. In this work we deal with the
oriented two-dimensional bin packing problem under free guillotine cutting. In
this specific problem a set of oriented rectangular items is given which must
be packed into a minimum number of bins of equal size. The first algorithm
proposed in this work is a randomized multi-start version of a constructive
one-pass heuristic from the literature. Additionally we propose the use of this
randomized one-pass heuristic within an evolutionary algorithm. The results of
the two proposed algorithms are compared to the best approaches from the
literature. In particular the evolutionary algorithm compares very favorably to
current state-of-the-art approaches. The optimal solution for 4 previously
unsolved instances could be found.",N/A
Direct computation of diagnoses for ontology debugging,"Modern ontology debugging methods allow efficient identification and
localization of faulty axioms defined by a user while developing an ontology.
The ontology development process in this case is characterized by rather
frequent and regular calls to a reasoner resulting in an early user awareness
of modeling errors. In such a scenario an ontology usually includes only a
small number of conflict sets, i.e. sets of axioms preserving the faults. This
property allows efficient use of standard model-based diagnosis techniques
based on the application of hitting set algorithms to a number of given
conflict sets. However, in many use cases such as ontology alignment the
ontologies might include many more conflict sets than in usual ontology
development settings, thus making precomputation of conflict sets and
consequently ontology diagnosis infeasible. In this paper we suggest a
debugging approach based on a direct computation of diagnoses that omits
calculation of conflict sets. Embedded in an ontology debugger, the proposed
algorithm is able to identify diagnoses for an ontology which includes a large
number of faults and for which application of standard diagnosis methods fails.
The evaluation results show that the approach is practicable and is able to
identify a fault in adequate time.",16 pages
A matrix approach for computing extensions of argumentation frameworks,"The matrices and their sub-blocks are introduced into the study of
determining various extensions in the sense of Dung's theory of argumentation
frameworks. It is showed that each argumentation framework has its matrix
representations, and the core semantics defined by Dung can be characterized by
specific sub-blocks of the matrix. Furthermore, the elementary permutations of
a matrix are employed by which an efficient matrix approach for finding out all
extensions under a given semantics is obtained. Different from several
established approaches, such as the graph labelling algorithm, Constraint
Satisfaction Problem algorithm, the matrix approach not only put the mathematic
idea into the investigation for finding out various extensions, but also
completely achieve the goal to compute all the extensions needed.",arXiv admin note: substantial text overlap with arXiv:1110.1416
"On firm specific characteristics of pharmaceutical generics and
  incentives to permanence under fuzzy conditions","The aim of this paper is to develop a methodology that is useful for
analysing from a microeconomic perspective the incentives to entry, permanence
and exit in the market for pharmaceutical generics under fuzzy conditions. In
an empirical application of our proposed methodology, the potential towards
permanence of labs with different characteristics has been estimated. The case
we deal with is set in an open market where global players diversify into
different national markets of pharmaceutical generics. Risk issues are
significantly important in deterring decision makers from expanding in the
generic pharmaceutical business. However, not all players are affected in the
same way and/or to the same extent. Small, non-diversified generics labs are in
the worse position. We have highlighted that the expected NPV and the number of
generics in the portfolio of a pharmaceutical lab are important variables, but
that it is also important to consider the degree of diversification. Labs with
a higher potential for diversification across markets have an advantage over
smaller labs. We have described a fuzzy decision support system based on the
Mamdani model in order to determine the incentives for a laboratory to remain
in the market both when it is stable and when it is growing.",N/A
"Tractable Optimization Problems through Hypergraph-Based Structural
  Restrictions","Several variants of the Constraint Satisfaction Problem have been proposed
and investigated in the literature for modelling those scenarios where
solutions are associated with some given costs. Within these frameworks
computing an optimal solution is an NP-hard problem in general; yet, when
restricted over classes of instances whose constraint interactions can be
modelled via (nearly-)acyclic graphs, this problem is known to be solvable in
polynomial time. In this paper, larger classes of tractable instances are
singled out, by discussing solution approaches based on exploiting hypergraph
acyclicity and, more generally, structural decomposition methods, such as
(hyper)tree decompositions.",N/A
RIO: Minimizing User Interaction in Ontology Debugging,"Efficient ontology debugging is a cornerstone for many activities in the
context of the Semantic Web, especially when automatic tools produce (parts of)
ontologies such as in the field of ontology matching. The best currently known
interactive debugging systems rely upon some meta information in terms of fault
probabilities, which can speed up the debugging procedure in the good case, but
can also have negative impact on the performance in the bad case. The problem
is that assessment of the meta information is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactive diagnoses discrimination. As an alternative, one
might prefer to rely on a tool which pursues a no-risk strategy. In this case,
however, possibly well-chosen meta information cannot be exploited, resulting
again in inefficient debugging actions. In this work we present a reinforcement
learning strategy that continuously adapts its behavior depending on the
performance achieved and minimizes the risk of using low-quality meta
information. Therefore, this method is suitable for application scenarios where
reliable a-priori fault estimates are difficult to obtain. Using problematic
ontologies in the field of ontology matching, we show that the proposed
risk-aware query strategy outperforms both active learning approaches and
no-risk strategies on average in terms of required amount of user interaction.",N/A
Textual Features for Programming by Example,"In Programming by Example, a system attempts to infer a program from input
and output examples, generally by searching for a composition of certain base
functions. Performing a naive brute force search is infeasible for even mildly
involved tasks. We note that the examples themselves often present clues as to
which functions to compose, and how to rank the resulting programs. In text
processing, which is our domain of interest, clues arise from simple textual
features: for example, if parts of the input and output strings are
permutations of one another, this suggests that sorting may be useful. We
describe a system that learns the reliability of such clues, allowing for
faster search and a principled ranking over programs. Experiments on a
prototype of this system show that this learning scheme facilitates efficient
inference on a range of text processing tasks.",N/A
"Hybrid technique for effective knowledge representation & a comparative
  study","Knowledge representation (KR) and inference mechanism are most desirable
thing to make the system intelligent. System is known to an intelligent if its
intelligence is equivalent to the intelligence of human being for a particular
domain or general. Because of incomplete ambiguous and uncertain information
the task of making intelligent system is very difficult. The objective of this
paper is to present the hybrid KR technique for making the system effective &
Optimistic. The requirement for (effective & optimistic) is because the system
must be able to reply the answer with a confidence of some factor. This paper
also presents the comparison between various hybrid KR techniques with the
proposed one.","15 pages,9 figures, 1 table, Pablished in IJCSES,International
  Journal of Computer Science & Engineering Survey Vol.3, No.4, August 2012"
Cognitive Bias for Universal Algorithmic Intelligence,"Existing theoretical universal algorithmic intelligence models are not
practically realizable. More pragmatic approach to artificial general
intelligence is based on cognitive architectures, which are, however,
non-universal in sense that they can construct and use models of the
environment only from Turing-incomplete model spaces. We believe that the way
to the real AGI consists in bridging the gap between these two approaches. This
is possible if one considers cognitive functions as a ""cognitive bias"" (priors
and search heuristics) that should be incorporated into the models of universal
algorithmic intelligence without violating their universality. Earlier reported
results suiting this approach and its overall feasibility are discussed on the
example of perception, planning, knowledge representation, attention, theory of
mind, language, and some others.",10 pages
Speech Signal Filters based on Soft Computing Techniques: A Comparison,"The paper presents a comparison of various soft computing techniques used for
filtering and enhancing speech signals. The three major techniques that fall
under soft computing are neural networks, fuzzy systems and genetic algorithms.
Other hybrid techniques such as neuro-fuzzy systems are also available. In
general, soft computing techniques have been experimentally observed to give
far superior performance as compared to non-soft computing techniques in terms
of robustness and accuracy.",5 pages
"Applicability of Crisp and Fuzzy Logic in Intelligent Response
  Generation","This paper discusses the merits and demerits of crisp logic and fuzzy logic
with respect to their applicability in intelligent response generation by a
human being and by a robot. Intelligent systems must have the capability of
taking decisions that are wise and handle situations intelligently. A direct
relationship exists between the level of perfection in handling a situation and
the level of completeness of the available knowledge or information or data
required to handle the situation. The paper concludes that the use of crisp
logic with complete knowledge leads to perfection in handling situations
whereas fuzzy logic can handle situations imperfectly only. However, in the
light of availability of incomplete knowledge fuzzy theory is more effective
but may be disadvantageous as compared to crisp logic.","4 pages, 1 table"
"Application of Fuzzy Mathematics to Speech-to-Text Conversion by
  Elimination of Paralinguistic Content","For the past few decades, man has been trying to create an intelligent
computer which can talk and respond like he can. The task of creating a system
that can talk like a human being is the primary objective of Automatic Speech
Recognition. Various Speech Recognition techniques have been developed in
theory and have been applied in practice. This paper discusses the problems
that have been encountered in developing Speech Recognition, the techniques
that have been applied to automate the task, and a representation of the core
problems of present day Speech Recognition by using Fuzzy Mathematics.","6 pages, 3 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1001.2267 by other authors"
Formal Definition of AI,"A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word ""Human"" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.",N/A
"Matroidal structure of rough sets based on serial and transitive
  relations","The theory of rough sets is concerned with the lower and upper approximations
of objects through a binary relation on a universe. It has been applied to
machine learning, knowledge discovery and data mining. The theory of matroids
is a generalization of linear independence in vector spaces. It has been used
in combinatorial optimization and algorithm design. In order to take advantages
of both rough sets and matroids, in this paper we propose a matroidal structure
of rough sets based on a serial and transitive relation on a universe. We
define the family of all minimal neighborhoods of a relation on a universe, and
prove it satisfy the circuit axioms of matroids when the relation is serial and
transitive. In order to further study this matroidal structure, we investigate
the inverse of this construction: inducing a relation by a matroid. The
relationships between the upper approximation operators of rough sets based on
relations and the closure operators of matroids in the above two constructions
are studied. Moreover, we investigate the connections between the above two
constructions.",16 pages
Covering matroid,"In this paper, we propose a new type of matroids, namely covering matroids,
and investigate the connections with the second type of covering-based rough
sets and some existing special matroids. Firstly, as an extension of
partitions, coverings are more natural combinatorial objects and can sometimes
be more efficient to deal with problems in the real world. Through extending
partitions to coverings, we propose a new type of matroids called covering
matroids and prove them to be an extension of partition matroids. Secondly,
since some researchers have successfully applied partition matroids to
classical rough sets, we study the relationships between covering matroids and
covering-based rough sets which are an extension of classical rough sets.
Thirdly, in matroid theory, there are many special matroids, such as
transversal matroids, partition matroids, 2-circuit matroid and
partition-circuit matroids. The relationships among several special matroids
and covering matroids are studied.",15 pages
"Relation matroid and its relationship with generalized rough set based
  on relation","Recently, the relationship between matroids and generalized rough sets based
on relations has been studied from the viewpoint of linear independence of
matrices. In this paper, we reveal more relationships by the predecessor and
successor neighborhoods from relations. First, through these two neighborhoods,
we propose a pair of matroids, namely predecessor relation matroid and
successor relation matroid, respectively. Basic characteristics of this pair of
matroids, such as dependent sets, circuits, the rank function and the closure
operator, are described by the predecessor and successor neighborhoods from
relations. Second, we induce a relation from a matroid through the circuits of
the matroid. We prove that the induced relation is always an equivalence
relation. With these two inductions, a relation induces a relation matroid, and
the relation matroid induces an equivalence relation, then the connection
between the original relation and the induced equivalence relation is studied.
Moreover, the relationships between the upper approximation operator in
generalized rough sets and the closure operator in matroids are investigated.",15 pages
"Matroidal structure of generalized rough sets based on symmetric and
  transitive relations","Rough sets are efficient for data pre-process in data mining. Lower and upper
approximations are two core concepts of rough sets. This paper studies
generalized rough sets based on symmetric and transitive relations from the
operator-oriented view by matroidal approaches. We firstly construct a
matroidal structure of generalized rough sets based on symmetric and transitive
relations, and provide an approach to study the matroid induced by a symmetric
and transitive relation. Secondly, this paper establishes a close relationship
between matroids and generalized rough sets. Approximation quality and
roughness of generalized rough sets can be computed by the circuit of matroid
theory. At last, a symmetric and transitive relation can be constructed by a
matroid with some special properties.",5 pages
Some characteristics of matroids through rough sets,"At present, practical application and theoretical discussion of rough sets
are two hot problems in computer science. The core concepts of rough set theory
are upper and lower approximation operators based on equivalence relations.
Matroid, as a branch of mathematics, is a structure that generalizes linear
independence in vector spaces. Further, matroid theory borrows extensively from
the terminology of linear algebra and graph theory. We can combine rough set
theory with matroid theory through using rough sets to study some
characteristics of matroids. In this paper, we apply rough sets to matroids
through defining a family of sets which are constructed from the upper
approximation operator with respect to an equivalence relation. First, we prove
the family of sets satisfies the support set axioms of matroids, and then we
obtain a matroid. We say the matroids induced by the equivalence relation and a
type of matroid, namely support matroid, is induced. Second, through rough
sets, some characteristics of matroids such as independent sets, support sets,
bases, hyperplanes and closed sets are investigated.",13 pages
"Condition for neighborhoods in covering based rough sets to form a
  partition","Neighborhood is an important concept in covering based rough sets. That under
what condition neighborhoods form a partition is a meaningful issue induced by
this concept. Many scholars have paid attention to this issue and presented
some necessary and sufficient conditions. However, there exists one common
trait among these conditions, that is they are established on the basis of all
neighborhoods have been obtained. In this paper, we provide a necessary and
sufficient condition directly based on the covering itself. First, we
investigate the influence of that there are reducible elements in the covering
on neighborhoods. Second, we propose the definition of uniform block and obtain
a sufficient condition from it. Third, we propose the definitions of repeat
degree and excluded number. By means of the two concepts, we obtain a necessary
and sufficient condition for neighborhoods to form a partition. In a word, we
have gained a deeper and more direct understanding of the essence over that
neighborhoods form a partition.",12 pages
Rough sets and matroidal contraction,"Rough sets are efficient for data pre-processing in data mining. As a
generalization of the linear independence in vector spaces, matroids provide
well-established platforms for greedy algorithms. In this paper, we apply rough
sets to matroids and study the contraction of the dual of the corresponding
matroid. First, for an equivalence relation on a universe, a matroidal
structure of the rough set is established through the lower approximation
operator. Second, the dual of the matroid and its properties such as
independent sets, bases and rank function are investigated. Finally, the
relationships between the contraction of the dual matroid to the complement of
a single point set and the contraction of the dual matroid to the complement of
the equivalence class of this point are studied.",11 pages
"Condition for neighborhoods induced by a covering to be equal to the
  covering itself","It is a meaningful issue that under what condition neighborhoods induced by a
covering are equal to the covering itself. A necessary and sufficient condition
for this issue has been provided by some scholars. In this paper, through a
counter-example, we firstly point out the necessary and sufficient condition is
false. Second, we present a necessary and sufficient condition for this issue.
Third, we concentrate on the inverse issue of computing neighborhoods by a
covering, namely giving an arbitrary covering, whether or not there exists
another covering such that the neighborhoods induced by it is just the former
covering. We present a necessary and sufficient condition for this issue as
well. In a word, through the study on the two fundamental issues induced by
neighborhoods, we have gained a deeper understanding of the relationship
between neighborhoods and the covering which induce the neighborhoods.",11 pages
"Closed-set lattice of regular sets based on a serial and transitive
  relation through matroids","Rough sets are efficient for data pre-processing in data mining. Matroids are
based on linear algebra and graph theory, and have a variety of applications in
many fields. Both rough sets and matroids are closely related to lattices. For
a serial and transitive relation on a universe, the collection of all the
regular sets of the generalized rough set is a lattice. In this paper, we use
the lattice to construct a matroid and then study relationships between the
lattice and the closed-set lattice of the matroid. First, the collection of all
the regular sets based on a serial and transitive relation is proved to be a
semimodular lattice. Then, a matroid is constructed through the height function
of the semimodular lattice. Finally, we propose an approach to obtain all the
closed sets of the matroid from the semimodular lattice. Borrowing from
matroids, results show that lattice theory provides an interesting view to
investigate rough sets.",12 pages
"Lattice structures of fixed points of the lower approximations of two
  types of covering-based rough sets","Covering is a common type of data structure and covering-based rough set
theory is an efficient tool to process this data. Lattice is an important
algebraic structure and used extensively in investigating some types of
generalized rough sets. In this paper, we propose two family of sets and study
the conditions that these two sets become some lattice structures. These two
sets are consisted by the fixed point of the lower approximations of the first
type and the sixth type of covering-based rough sets, respectively. These two
sets are called the fixed point set of neighborhoods and the fixed point set of
covering, respectively. First, for any covering, the fixed point set of
neighborhoods is a complete and distributive lattice, at the same time, it is
also a double p-algebra. Especially, when the neighborhood forms a partition of
the universe, the fixed point set of neighborhoods is both a boolean lattice
and a double Stone algebra. Second, for any covering, the fixed point set of
covering is a complete lattice.When the covering is unary, the fixed point set
of covering becomes a distributive lattice and a double p-algebra. a
distributive lattice and a double p-algebra when the covering is unary.
Especially, when the reduction of the covering forms a partition of the
universe, the fixed point set of covering is both a boolean lattice and a
double Stone algebra.",17 pages
"Semi-automatic annotation process for procedural texts: An application
  on cooking recipes","Taaable is a case-based reasoning system that adapts cooking recipes to user
constraints. Within it, the preparation part of recipes is formalised as a
graph. This graph is a semantic representation of the sequence of instructions
composing the cooking process and is used to compute the procedure adaptation,
conjointly with the textual adaptation. It is composed of cooking actions and
ingredients, among others, represented as vertices, and semantic relations
between those, shown as arcs, and is built automatically thanks to natural
language processing. The results of the automatic annotation process is often a
disconnected graph, representing an incomplete annotation, or may contain
errors. Therefore, a validating and correcting step is required. In this paper,
we present an existing graphic tool named \kcatos, conceived for representing
and editing decision trees, and show how it has been adapted and integrated in
WikiTaaable, the semantic wiki in which the knowledge used by Taaable is
stored. This interface provides the wiki users with a way to correct the case
representation of the cooking process, improving at the same time the quality
of the knowledge about cooking procedures stored in WikiTaaable.",N/A
Efficient Natural Evolution Strategies,"Efficient Natural Evolution Strategies (eNES) is a novel alternative to
conventional evolutionary algorithms, using the natural gradient to adapt the
mutation distribution. Unlike previous methods based on natural gradients, eNES
uses a fast algorithm to calculate the inverse of the exact Fisher information
matrix, thus increasing both robustness and performance of its evolution
gradient estimation, even in higher dimensions. Additional novel aspects of
eNES include optimal fitness baselines and importance mixing (a procedure for
updating the population with very few fitness evaluations). The algorithm
yields competitive results on both unimodal and multimodal benchmarks.",Puslished in GECCO'2009
"Examples of Artificial Perceptions in Optical Character Recognition and
  Iris Recognition","This paper assumes the hypothesis that human learning is perception based,
and consequently, the learning process and perceptions should not be
represented and investigated independently or modeled in different simulation
spaces. In order to keep the analogy between the artificial and human learning,
the former is assumed here as being based on the artificial perception. Hence,
instead of choosing to apply or develop a Computational Theory of (human)
Perceptions, we choose to mirror the human perceptions in a numeric
(computational) space as artificial perceptions and to analyze the
interdependence between artificial learning and artificial perception in the
same numeric space, using one of the simplest tools of Artificial Intelligence
and Soft Computing, namely the perceptrons. As practical applications, we
choose to work around two examples: Optical Character Recognition and Iris
Recognition. In both cases a simple Turing test shows that artificial
perceptions of the difference between two characters and between two irides are
fuzzy, whereas the corresponding human perceptions are, in fact, crisp.","5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24
  Aug 2012"
"Multi-Agents Dynamic Case Based Reasoning and The Inverse Longest Common
  Sub-Sequence And Individualized Follow-up of Learners in The CEHL","In E-learning, there is still the problem of knowing how to ensure an
individualized and continuous learner's follow-up during learning process,
indeed among the numerous tools proposed, very few systems concentrate on a
real time learner's follow-up. Our work in this field develops the design and
implementation of a Multi-Agents System Based on Dynamic Case Based Reasoning
which can initiate learning and provide an individualized follow-up of learner.
When interacting with the platform, every learner leaves his/her traces in the
machine. These traces are stored in a basis under the form of scenarios which
enrich collective past experience. The system monitors, compares and analyses
these traces to keep a constant intelligent watch and therefore detect
difficulties hindering progress and/or avoid possible dropping out. The system
can support any learning subject. The success of a case-based reasoning system
depends critically on the performance of the retrieval step used and, more
specifically, on similarity measure used to retrieve scenarios that are similar
to the course of the learner (traces in progress). We propose a complementary
similarity measure, named Inverse Longest Common Sub-Sequence (ILCSS). To help
and guide the learner, the system is equipped with combined virtual and human
tutors.","International Journal of Computer Science Issues, Volume 9, Issue 4,
  No 2, July 2012"
"Topological characterizations to three types of covering approximation
  operators","Covering-based rough set theory is a useful tool to deal with inexact,
uncertain or vague knowledge in information systems. Topology, one of the most
important subjects in mathematics, provides mathematical tools and interesting
topics in studying information systems and rough sets. In this paper, we
present the topological characterizations to three types of covering
approximation operators. First, we study the properties of topology induced by
the sixth type of covering lower approximation operator. Second, some
topological characterizations to the covering lower approximation operator to
be an interior operator are established. We find that the topologies induced by
this operator and by the sixth type of covering lower approximation operator
are the same. Third, we study the conditions which make the first type of
covering upper approximation operator be a closure operator, and find that the
topology induced by the operator is the same as the topology induced by the
fifth type of covering upper approximation operator. Forth, the conditions of
the second type of covering upper approximation operator to be a closure
operator and the properties of topology induced by it are established. Finally,
these three topologies space are compared. In a word, topology provides a
useful method to study the covering-based rough sets.",N/A
"Geometric lattice structure of covering-based rough sets through
  matroids","Covering-based rough set theory is a useful tool to deal with inexact,
uncertain or vague knowledge in information systems. Geometric lattice has
widely used in diverse fields, especially search algorithm design which plays
important role in covering reductions. In this paper, we construct four
geometric lattice structures of covering-based rough sets through matroids, and
compare their relationships. First, a geometric lattice structure of
covering-based rough sets is established through the transversal matroid
induced by the covering, and its characteristics including atoms, modular
elements and modular pairs are studied. We also construct a one-to-one
correspondence between this type of geometric lattices and transversal matroids
in the context of covering-based rough sets. Second, sufficient and necessary
conditions for three types of covering upper approximation operators to be
closure operators of matroids are presented. We exhibit three types of matroids
through closure axioms, and then obtain three geometric lattice structures of
covering-based rough sets. Third, these four geometric lattice structures are
compared. Some core concepts such as reducible elements in covering-based rough
sets are investigated with geometric lattices. In a word, this work points out
an interesting view, namely geometric lattice, to study covering-based rough
sets.",N/A
"Test-cost-sensitive attribute reduction of data with normal distribution
  measurement errors","The measurement error with normal distribution is universal in applications.
Generally, smaller measurement error requires better instrument and higher test
cost. In decision making based on attribute values of objects, we shall select
an attribute subset with appropriate measurement error to minimize the total
test cost. Recently, error-range-based covering rough set with uniform
distribution error was proposed to investigate this issue. However, the
measurement errors satisfy normal distribution instead of uniform distribution
which is rather simple for most applications. In this paper, we introduce
normal distribution measurement errors to covering-based rough set model, and
deal with test-cost-sensitive attribute reduction problem in this new model.
The major contributions of this paper are four-fold. First, we build a new data
model based on normal distribution measurement errors. With the new data model,
the error range is an ellipse in a two-dimension space. Second, the
covering-based rough set with normal distribution measurement errors is
constructed through the ""3-sigma"" rule. Third, the test-cost-sensitive
attribute reduction problem is redefined on this covering-based rough set.
Fourth, a heuristic algorithm is proposed to deal with this problem. The
algorithm is tested on ten UCI (University of California - Irvine) datasets.
The experimental results show that the algorithm is more effective and
efficient than the existing one. This study is a step toward realistic
applications of cost-sensitive learning.","This paper has been withdrawn by the author due to the error of the
  title"
"Relationship between the second type of covering-based rough set and
  matroid via closure operator","Recently, in order to broad the application and theoretical areas of rough
sets and matroids, some authors have combined them from many different
viewpoints, such as circuits, rank function, spanning sets and so on. In this
paper, we connect the second type of covering-based rough sets and matroids
from the view of closure operators. On one hand, we establish a closure system
through the fixed point family of the second type of covering lower
approximation operator, and then construct a closure operator. For a covering
of a universe, the closure operator is a closure one of a matroid if and only
if the reduct of the covering is a partition of the universe. On the other
hand, we investigate the sufficient and necessary condition that the second
type of covering upper approximation operation is a closure one of a matroid.",10 pages
The Definition of AI in Terms of Multi Agent Systems,"The questions which we will consider here are ""What is AI?"" and ""How can we
make AI?"". Here we will present the definition of AI in terms of multi-agent
systems. This means that here you will not find a new answer to the question
""What is AI?"", but an old answer in a new form.
  This new form of the definition of AI is of interest for the theory of
multi-agent systems because it gives us better understanding of this theory.
More important is that this work will help us answer the second question. We
want to make a program which is capable of constructing a model of its
environment. Every multi-agent model is equivalent to a single-agent model but
multi-agent models are more natural and accordingly more easily discoverable.",N/A
A Definition of Artificial Intelligence,"In this paper we offer a formal definition of Artificial Intelligence and
this directly gives us an algorithm for construction of this object. Really,
this algorithm is useless due to the combinatory explosion.
  The main innovation in our definition is that it does not include the
knowledge as a part of the intelligence. So according to our definition a newly
born baby also is an Intellect. Here we differs with Turing's definition which
suggests that an Intellect is a person with knowledge gained through the years.",N/A
Conflict-driven ASP Solving with External Sources,"Answer Set Programming (ASP) is a well-known problem solving approach based
on nonmonotonic logic programs and efficient solvers. To enable access to
external information, HEX-programs extend programs with external atoms, which
allow for a bidirectional communication between the logic program and external
sources of computation (e.g., description logic reasoners and Web resources).
Current solvers evaluate HEX-programs by a translation to ASP itself, in which
values of external atoms are guessed and verified after the ordinary answer set
computation. This elegant approach does not scale with the number of external
accesses in general, in particular in presence of nondeterminism (which is
instrumental for ASP). In this paper, we present a novel, native algorithm for
evaluating HEX-programs which uses learning techniques. In particular, we
extend conflict-driven ASP solving techniques, which prevent the solver from
running into the same conflict again, from ordinary to HEX-programs. We show
how to gain additional knowledge from external source evaluations and how to
use it in a conflict-driven algorithm. We first target the uninformed case,
i.e., when we have no extra information on external sources, and then extend
our approach to the case where additional meta-information is available.
Experiments show that learning from external sources can significantly decrease
both the runtime and the number of considered candidate compatible sets.",To appear in Theory and Practice of Logic Programming
AI in arbitrary world,"In order to build AI we have to create a program which copes well in an
arbitrary world. In this paper we will restrict our attention on one concrete
world, which represents the game Tick-Tack-Toe. This world is a very simple one
but it is sufficiently complicated for our task because most people cannot
manage with it. The main difficulty in this world is that the player cannot see
the entire internal state of the world so he has to build a model in order to
understand the world. The model which we will offer will consist of final
automata and first order formulas.",N/A
An Agent-based framework for cooperation in Supply Chain,"Supply Chain coordination has become a critical success factor for Supply
Chain management (SCM) and effectively improving the performance of
organizations in various industries. Companies are increasingly located at the
intersection of one or more corporate networks which are designated by ""Supply
Chain"". Managing this chain is mainly based on an 'information sharing' and
redeployment activities between the various links that comprise it. Several
attempts have been made by industrialists and researchers to educate
policymakers about the gains to be made by the implementation of cooperative
relationships. The approach presented in this paper here is among the works
that aim to propose solutions related to information systems distributed Supply
Chains to enable the different actors of the chain to improve their
performance. We propose in particular solutions that focus on cooperation
between actors in the Supply Chain.","IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  5, No 3, September 2012"
Local optima networks and the performance of iterated local search,"Local Optima Networks (LONs) have been recently proposed as an alternative
model of combinatorial fitness landscapes. The model compresses the information
given by the whole search space into a smaller mathematical object that is the
graph having as vertices the local optima and as edges the possible weighted
transitions between them. A new set of metrics can be derived from this model
that capture the distribution and connectivity of the local optima in the
underlying configuration space. This paper departs from the descriptive
analysis of local optima networks, and actively studies the correlation between
network features and the performance of a local search heuristic. The NK family
of landscapes and the Iterated Local Search metaheuristic are considered. With
a statistically-sound approach based on multiple linear regression, it is shown
that some LONs' features strongly influence and can even partly predict the
performance of a heuristic search algorithm. This study validates the
expressive power of LONs as a model of combinatorial fitness landscapes.","Proceedings of the fourteenth international conference on Genetic and
  evolutionary computation conference, Philadelphia : United States (2012)"
"Lifted Relax, Compensate and then Recover: From Approximate to Exact
  Lifted Probabilistic Inference","We propose an approach to lifted approximate inference for first-order
probabilistic models, such as Markov logic networks. It is based on performing
exact lifted inference in a simplified first-order model, which is found by
relaxing first-order constraints, and then compensating for the relaxation.
These simplified models can be incrementally improved by carefully recovering
constraints that have been relaxed, also at the first-order level. This leads
to a spectrum of approximations, with lifted belief propagation on one end, and
exact lifted inference on the other. We discuss how relaxation, compensation,
and recovery can be performed, all at the firstorder level, and show
empirically that our approach substantially improves on the approximations of
both propositional solvers and lifted belief propagation.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Exploiting Uniform Assignments in First-Order MPE,"The MPE (Most Probable Explanation) query plays an important role in
probabilistic inference. MPE solution algorithms for probabilistic relational
models essentially adapt existing belief assessment method, replacing summation
with maximization. But the rich structure and symmetries captured by relational
models together with the properties of the maximization operator offer an
opportunity for additional simplification with potentially significant
computational ramifications. Specifically, these models often have groups of
variables that define symmetric distributions over some population of formulas.
The maximizing choice for different elements of this group is the same. If we
can realize this ahead of time, we can significantly reduce the size of the
model by eliminating a potentially significant portion of random variables.
This paper defines the notion of uniformly assigned and partially uniformly
assigned sets of variables, shows how one can recognize these sets efficiently,
and how the model can be greatly simplified once we recognize them, with little
computational effort. We demonstrate the effectiveness of these ideas
empirically on a number of models.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Generalized Belief Propagation on Tree Robust Structured Region Graphs,"This paper provides some new guidance in the construction of region graphs
for Generalized Belief Propagation (GBP). We connect the problem of choosing
the outer regions of a LoopStructured Region Graph (SRG) to that of finding a
fundamental cycle basis of the corresponding Markov network. We also define a
new class of tree-robust Loop-SRG for which GBP on any induced (spanning) tree
of the Markov network, obtained by setting to zero the off-tree interactions,
is exact. This class of SRG is then mapped to an equivalent class of
tree-robust cycle bases on the Markov network. We show that a treerobust cycle
basis can be identified by proving that for every subset of cycles, the graph
obtained from the edges that participate in a single cycle only, is multiply
connected. Using this we identify two classes of tree-robust cycle bases:
planar cycle bases and ""star"" cycle bases. In experiments we show that
tree-robustness can be successfully exploited as a design principle to improve
the accuracy and convergence of GBP.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Uniform Solution Sampling Using a Constraint Solver As an Oracle,"We consider the problem of sampling from solutions defined by a set of hard
constraints on a combinatorial space. We propose a new sampling technique that,
while enforcing a uniform exploration of the search space, leverages the
reasoning power of a systematic constraint solver in a black-box scheme. We
present a series of challenging domains, such as energy barriers and highly
asymmetric spaces, that reveal the difficulties introduced by hard constraints.
We demonstrate that standard approaches such as Simulated Annealing and Gibbs
Sampling are greatly affected, while our new technique can overcome many of
these difficulties. Finally, we show that our sampling scheme naturally defines
a new approximate model counting technique, which we empirically show to be
very accurate on a range of benchmark problems.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
A Theory of Goal-Oriented MDPs with Dead Ends,"Stochastic Shortest Path (SSP) MDPs is a problem class widely studied in AI,
especially in probabilistic planning. They describe a wide range of scenarios
but make the restrictive assumption that the goal is reachable from any state,
i.e., that dead-end states do not exist. Because of this, SSPs are unable to
model various scenarios that may have catastrophic events (e.g., an airplane
possibly crashing if it flies into a storm). Even though MDP algorithms have
been used for solving problems with dead ends, a principled theory of SSP
extensions that would allow dead ends, including theoretically sound algorithms
for solving such MDPs, has been lacking. In this paper, we propose three new
MDP classes that admit dead ends under increasingly weaker assumptions. We
present Value Iteration-based as well as the more efficient heuristic search
algorithms for optimally solving each class, and explore theoretical
relationships between these classes. We also conduct a preliminary empirical
study comparing the performance of our algorithms on different MDP classes,
especially on scenarios with unavoidable dead ends.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Join-graph based cost-shifting schemes,"We develop several algorithms taking advantage of two common approaches for
bounding MPE queries in graphical models: minibucket elimination and
message-passing updates for linear programming relaxations. Both methods are
quite similar, and offer useful perspectives for the other; our hybrid
approaches attempt to balance the advantages of each. We demonstrate the power
of our hybrid algorithms through extensive empirical evaluation. Most notably,
a Branch and Bound search guided by the heuristic function calculated by one of
our new algorithms has recently won first place in the PASCAL2 inference
challenge.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
A Maximum Likelihood Approach For Selecting Sets of Alternatives,"We consider the problem of selecting a subset of alternatives given noisy
evaluations of the relative strength of different alternatives. We wish to
select a k-subset (for a given k) that provides a maximum likelihood estimate
for one of several objectives, e.g., containing the strongest alternative.
Although this problem is NP-hard, we show that when the noise level is
sufficiently high, intuitive methods provide the optimal solution. We thus
generalize classical results about singling out one alternative and identifying
the hidden ranking of alternatives by strength. Extensive experiments show that
our methods perform well in practical settings.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
"A Case Study in Complexity Estimation: Towards Parallel Branch-and-Bound
  over Graphical Models","We study the problem of complexity estimation in the context of parallelizing
an advanced Branch and Bound-type algorithm over graphical models. The
algorithm's pruning power makes load balancing, one crucial element of every
distributed system, very challenging. We propose using a statistical regression
model to identify and tackle disproportionally complex parallel subproblems,
the cause of load imbalance, ahead of time. The proposed model is evaluated and
analyzed on various levels and shown to yield robust predictions. We then
demonstrate its effectiveness for load balancing in practice.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
The Complexity of Approximately Solving Influence Diagrams,"Influence diagrams allow for intuitive and yet precise description of complex
situations involving decision making under uncertainty. Unfortunately, most of
the problems described by influence diagrams are hard to solve. In this paper
we discuss the complexity of approximately solving influence diagrams. We do
not assume no-forgetting or regularity, which makes the class of problems we
address very broad. Remarkably, we show that when both the tree-width and the
cardinality of the variables are bounded the problem admits a fully
polynomial-time approximation scheme.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Belief Propagation for Structured Decision Making,"Variational inference algorithms such as belief propagation have had
tremendous impact on our ability to learn and use graphical models, and give
many insights for developing or understanding exact and approximate inference.
However, variational approaches have not been widely adoped for decision making
in graphical models, often formulated through influence diagrams and including
both centralized and decentralized (or multi-agent) decisions. In this work, we
present a general variational framework for solving structured cooperative
decision-making problems, use it to propose several belief propagation-like
algorithms, and analyze them both theoretically and empirically.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Multi-objective Influence Diagrams,"We describe multi-objective influence diagrams, based on a set of p
objectives, where utility values are vectors in Rp, and are typically only
partially ordered. These can still be solved by a variable elimination
algorithm, leading to a set of maximal values of expected utility. If the
Pareto ordering is used this set can often be prohibitively large. We consider
approximate representations of the Pareto set based on e-coverings, allowing
much larger problems to be solved. In addition, we define a method for
incorporating user tradeoffs, which also greatly improves the efficiency.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
"FHHOP: A Factored Hybrid Heuristic Online Planning Algorithm for Large
  POMDPs","Planning in partially observable Markov decision processes (POMDPs) remains a
challenging topic in the artificial intelligence community, in spite of recent
impressive progress in approximation techniques. Previous research has
indicated that online planning approaches are promising in handling large-scale
POMDP domains efficiently as they make decisions ""on demand"" instead of
proactively for the entire state space. We present a Factored Hybrid Heuristic
Online Planning (FHHOP) algorithm for large POMDPs. FHHOP gets its power by
combining a novel hybrid heuristic search strategy with a recently developed
factored state representation. On several benchmark problems, FHHOP
substantially outperformed state-of-the-art online heuristic search approaches
in terms of both scalability and quality.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
A Cluster-Cumulant Expansion at the Fixed Points of Belief Propagation,"We introduce a new cluster-cumulant expansion (CCE) based on the fixed points
of iterative belief propagation (IBP). This expansion is similar in spirit to
the loop-series (LS) recently introduced in [1]. However, in contrast to the
latter, the CCE enjoys the following important qualities: 1) it is defined for
arbitrary state spaces 2) it is easily extended to fixed points of generalized
belief propagation (GBP), 3) disconnected groups of variables will not
contribute to the CCE and 4) the accuracy of the expansion empirically improves
upon that of the LS. The CCE is based on the same M\""obius transform as the
Kikuchi approximation, but unlike GBP does not require storing the beliefs of
the GBP-clusters nor does it suffer from convergence issues during belief
updating.","Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)"
Characteristic of partition-circuit matroid through approximation number,"Rough set theory is a useful tool to deal with uncertain, granular and
incomplete knowledge in information systems. And it is based on equivalence
relations or partitions. Matroid theory is a structure that generalizes linear
independence in vector spaces, and has a variety of applications in many
fields. In this paper, we propose a new type of matroids, namely,
partition-circuit matroids, which are induced by partitions. Firstly, a
partition satisfies circuit axioms in matroid theory, then it can induce a
matroid which is called a partition-circuit matroid. A partition and an
equivalence relation on the same universe are one-to-one corresponding, then
some characteristics of partition-circuit matroids are studied through rough
sets. Secondly, similar to the upper approximation number which is proposed by
Wang and Zhu, we define the lower approximation number. Some characteristics of
partition-circuit matroids and the dual matroids of them are investigated
through the lower approximation number and the upper approximation number.",12 pages
Ambiente de Planejamento Ipê,"In this work we investigate the systems that implements algorithms for the
planning problem in Artificial Intelligence, called planners, with especial
attention to the planners based on the plan graph. We analyze the problem of
comparing the performance of the different algorithms and we propose an
environment for the development and analysis of planners.","MSc dissertation involving Artificial Intelligence, Planning, Petri
  Net, Plangraph, Intelig\^encia Artificial, Planejamento, Redes de Petri e
  Grafo de Planos"
Lex-Partitioning: A New Option for BDD Search,"For the exploration of large state spaces, symbolic search using binary
decision diagrams (BDDs) can save huge amounts of memory and computation time.
State sets are represented and modified by accessing and manipulating their
characteristic functions. BDD partitioning is used to compute the image as the
disjunction of smaller subimages.
  In this paper, we propose a novel BDD partitioning option. The partitioning
is lexicographical in the binary representation of the states contained in the
set that is represented by a BDD and uniform with respect to the number of
states represented. The motivation of controlling the state set sizes in the
partitioning is to eventually bridge the gap between explicit and symbolic
search.
  Let n be the size of the binary state vector. We propose an O(n) ranking and
unranking scheme that supports negated edges and operates on top of precomputed
satcount values. For the uniform split of a BDD, we then use unranking to
provide paths along which we partition the BDDs. In a shared BDD representation
the efforts are O(n). The algorithms are fully integrated in the CUDD library
and evaluated in strongly solving general game playing benchmarks.","In Proceedings GRAPHITE 2012, arXiv:1210.6118"
"A Biomimetic Approach Based on Immune Systems for Classification of
  Unstructured Data","In this paper we present the results of unstructured data clustering in this
case a textual data from Reuters 21578 corpus with a new biomimetic approach
using immune system. Before experimenting our immune system, we digitalized
textual data by the n-grams approach. The novelty lies on hybridization of
n-grams and immune systems for clustering. The experimental results show that
the recommended ideas are promising and prove that this method can solve the
text clustering problem.","10 pages, 4 figures"
"Get my pizza right: Repairing missing is-a relations in ALC ontologies
  (extended version)","With the increased use of ontologies in semantically-enabled applications,
the issue of debugging defects in ontologies has become increasingly important.
These defects can lead to wrong or incomplete results for the applications.
Debugging consists of the phases of detection and repairing. In this paper we
focus on the repairing phase of a particular kind of defects, i.e. the missing
relations in the is-a hierarchy. Previous work has dealt with the case of
taxonomies. In this work we extend the scope to deal with ALC ontologies that
can be represented using acyclic terminologies. We present algorithms and
discuss a system.",N/A
Algorithm Selection for Combinatorial Search Problems: A Survey,"The Algorithm Selection Problem is concerned with selecting the best
algorithm to solve a given problem on a case-by-case basis. It has become
especially relevant in the last decade, as researchers are increasingly
investigating how to identify the most suitable existing algorithm for solving
a problem instead of developing new algorithms. This survey presents an
overview of this work focusing on the contributions made in the area of
combinatorial search problems, where Algorithm Selection techniques have
achieved significant performance improvements. We unify and organise the vast
literature according to criteria that determine Algorithm Selection systems in
practice. The comprehensive classification of approaches identifies and
analyses the different directions from which Algorithm Selection has been
approached. This paper contrasts and compares different methods for solving the
problem as well as ways of using these solutions. It closes by identifying
directions of current and future research.",N/A
Matrix approach to rough sets through vector matroids over a field,"Rough sets were proposed to deal with the vagueness and incompleteness of
knowledge in information systems. There are may optimization issues in this
field such as attribute reduction. Matroids generalized from matrices are
widely used in optimization. Therefore, it is necessary to connect matroids
with rough sets. In this paper, we take field into consideration and introduce
matrix to study rough sets through vector matroids. First, a matrix
representation of an equivalence relation is proposed, and then a matroidal
structure of rough sets over a field is presented by the matrix. Second, the
properties of the matroidal structure including circuits, bases and so on are
studied through two special matrix solution spaces, especially null space.
Third, over a binary field, we construct an equivalence relation from matrix
null space, and establish an algebra isomorphism from the collection of
equivalence relations to the collection of sets, which any member is a family
of the minimal non-empty sets that are supports of members of null space of a
binary dependence matrix. In a word, matrix provides a new viewpoint to study
rough sets.",N/A
Hybrid Systems for Knowledge Representation in Artificial Intelligence,"There are few knowledge representation (KR) techniques available for
efficiently representing knowledge. However, with the increase in complexity,
better methods are needed. Some researchers came up with hybrid mechanisms by
combining two or more methods. In an effort to construct an intelligent
computer system, a primary consideration is to represent large amounts of
knowledge in a way that allows effective use and efficiently organizing
information to facilitate making the recommended inferences. There are merits
and demerits of combinations, and standardized method of KR is needed. In this
paper, various hybrid schemes of KR were explored at length and details
presented.",6 pages
Segregating event streams and noise with a Markov renewal process model,"We describe an inference task in which a set of timestamped event
observations must be clustered into an unknown number of temporal sequences
with independent and varying rates of observations. Various existing approaches
to multi-object tracking assume a fixed number of sources and/or a fixed
observation rate; we develop an approach to inferring structure in timestamped
data produced by a mixture of an unknown and varying number of similar Markov
renewal processes, plus independent clutter noise. The inference simultaneously
distinguishes signal from noise as well as clustering signal observations into
separate source streams. We illustrate the technique via a synthetic experiment
as well as an experiment to track a mixture of singing birds.",N/A
Cost-sensitive C4.5 with post-pruning and competition,"Decision tree is an effective classification approach in data mining and
machine learning. In applications, test costs and misclassification costs
should be considered while inducing decision trees. Recently, some
cost-sensitive learning algorithms based on ID3 such as CS-ID3, IDX,
\lambda-ID3 have been proposed to deal with the issue. These algorithms deal
with only symbolic data. In this paper, we develop a decision tree algorithm
inspired by C4.5 for numeric data. There are two major issues for our
algorithm. First, we develop the test cost weighted information gain ratio as
the heuristic information. According to this heuristic information, our
algorithm is to pick the attribute that provides more gain ratio and costs less
for each selection. Second, we design a post-pruning strategy through
considering the tradeoff between test costs and misclassification costs of the
generated decision tree. In this way, the total cost is reduced. Experimental
results indicate that (1) our algorithm is stable and effective; (2) the
post-pruning technique reduces the total cost significantly; (3) the
competition strategy is effective to obtain a cost-sensitive decision tree with
low cost.",N/A
A Logic and Adaptive Approach for Efficient Diagnosis Systems using CBR,"Case Based Reasoning (CBR) is an intelligent way of thinking based on
experience and capitalization of already solved cases (source cases) to find a
solution to a new problem (target case). Retrieval phase consists on
identifying source cases that are similar to the target case. This phase may
lead to erroneous results if the existing knowledge imperfections are not taken
into account. This work presents a novel solution based on Fuzzy logic
techniques and adaptation measures which aggregate weighted similarities to
improve the retrieval results. To confirm the efficiency of our solution, we
have applied it to the industrial diagnosis domain. The obtained results are
more efficient results than those obtained by applying typical measures.","5 pages,3 figures, 1 table"
A Dataset for StarCraft AI \& an Example of Armies Clustering,"This paper advocates the exploration of the full state of recorded real-time
strategy (RTS) games, by human or robotic players, to discover how to reason
about tactics and strategy. We present a dataset of StarCraft games
encompassing the most of the games' state (not only player's orders). We
explain one of the possible usages of this dataset by clustering armies on
their compositions. This reduction of armies compositions to mixtures of
Gaussian allow for strategic reasoning at the level of the components. We
evaluated this clustering method by predicting the outcomes of battles based on
armies compositions' mixtures components","Artificial Intelligence in Adversarial Real-Time Games 2012, Palo
  Alto : United States (2012)"
"Shadows and headless shadows: a worlds-based, autobiographical approach
  to reasoning","Many cognitive systems deploy multiple, closed, individually consistent
models which can represent interpretations of the present state of the world,
moments in the past, possible futures or alternate versions of reality. While
they appear under different names, these structures can be grouped under the
general term of worlds. The Xapagy architecture is a story-oriented cognitive
system which relies exclusively on the autobiographical memory implemented as a
raw collection of events organized into world-type structures called {\em
scenes}. The system performs reasoning by shadowing current events with events
from the autobiography. The shadows are then extrapolated into headless shadows
corresponding to predictions, hidden events or inferred relations.",N/A
Modeling problems of identity in Little Red Riding Hood,"This paper argues that the problem of identity is a critical challenge in
agents which are able to reason about stories. The Xapagy architecture has been
built from scratch to perform narrative reasoning and relies on a somewhat
unusual approach to represent instances and identity. We illustrate the
approach by a representation of the story of Little Red Riding Hood in the
architecture, with a focus on the problem of identity raised by the narrative.",arXiv admin note: text overlap with arXiv:1105.3486
"Shadows and Headless Shadows: an Autobiographical Approach to Narrative
  Reasoning","The Xapagy architecture is a story-oriented cognitive system which relies
exclusively on the autobiographical memory implemented as a raw collection of
events. Reasoning is performed by shadowing current events with events from the
autobiography. The shadows are then extrapolated into headless shadows (HLSs).
In a story following mood, HLSs can be used to track the level of surprise of
the agent, to infer hidden actions or relations between the participants, and
to summarize ongoing events. In recall mood, the HLSs can be used to create new
stories ranging from exact recall to free-form confabulation.",arXiv admin note: substantial text overlap with arXiv:1211.5643
Problem Solving and Computational Thinking in a Learning Environment,"Computational thinking is a new problem soling method named for its extensive
use of computer science techniques. It synthesizes critical thinking and
existing knowledge and applies them in solving complex technological problems.
The term was coined by J. Wing, but the relationship between computational and
critical thinking, the two modes of thiking in solving problems, has not been
yet learly established. This paper aims at shedding some light into this
relationship. We also present two classroom experiments performed recently at
the Graduate Technological Educational Institute of Patras in Greece. The
results of these experiments give a strong indication that the use of computers
as a tool for problem solving enchances the students' abilities in solving real
world problems involving mathematical modelling. This is also crossed by
earlier findings of other researchers for the problem solving process in
general (not only for mathematical problems).","19 pages, 2 figures"
"An ontology-based approach to relax traffic regulation for autonomous
  vehicle assistance","Traffic regulation must be respected by all vehicles, either human- or
computer- driven. However, extreme traffic situations might exhibit practical
cases in which a vehicle should safely and reasonably relax traffic regulation,
e.g., in order not to be indefinitely blocked and to keep circulating. In this
paper, we propose a high-level representation of an automated vehicle, other
vehicles and their environment, which can assist drivers in taking such
""illegal"" but practical relaxation decisions. This high-level representation
(an ontology) includes topological knowledge and inference rules, in order to
compute the next high-level motion an automated vehicle should take, as
assistance to a driver. Results on practical cases are presented.",N/A
"Soft Constraint Logic Programming for Electric Vehicle Travel
  Optimization","Soft Constraint Logic Programming is a natural and flexible declarative
programming formalism, which allows to model and solve real-life problems
involving constraints of different types.
  In this paper, after providing a slightly more general and elegant
presentation of the framework, we show how we can apply it to the e-mobility
problem of coordinating electric vehicles in order to overcome both energetic
and temporal constraints and so to reduce their running cost. In particular, we
focus on the journey optimization sub-problem, considering sequences of trips
from a user's appointment to another one. Solutions provide the best
alternatives in terms of time and energy consumption, including route sequences
and possible charging events.",17 pages; 26th Workshop on Logic Programming - 2012
On revising fuzzy belief bases,"We look at the problem of revising fuzzy belief bases, i.e., belief base
revision in which both formulas in the base as well as revision-input formulas
can come attached with varying truth-degrees. Working within a very general
framework for fuzzy logic which is able to capture a variety of types of
inference under uncertainty, such as truth-functional fuzzy logics and certain
types of probabilistic inference, we show how the idea of rational change from
'crisp' base revision, as embodied by the idea of partial meet revision, can be
faithfully extended to revising fuzzy belief bases. We present and axiomatise
an operation of partial meet fuzzy revision and illustrate how the operation
works in several important special instances of the framework.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Upgrading Ambiguous Signs in QPNs,"WA qualitative probabilistic network models the probabilistic relationships
between its variables by means of signs. Non-monotonic influences have
associated an ambiguous sign. These ambiguous signs typically lead to
uninformative results upon inference. A non-monotonic influence can, however,
be associated with a, more informative, sign that indicates its effect in the
current state of the network. To capture this effect, we introduce the concept
of situational sign. Furthermore, if the network converts to a state in which
all variables that provoke the non-monotonicity have been observed, a
non-monotonic influence reduces to a monotonic influence. We study the
persistence and propagation of situational signs upon inference and give a
method to establish the sign of a reduced influence.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Parametric Dependability Analysis through Probabilistic Horn Abduction,"Dependability modeling and evaluation is aimed at investigating that a system
performs its function correctly in time. A usual way to achieve a high
reliability, is to design redundant systems that contain several replicas of
the same subsystem or component. State space methods for dependability analysis
may suffer of the state space explosion problem in such a kind of situation.
Combinatorial models, on the other hand, require the simplified assumption of
statistical independence; however, in case of redundant systems, this does not
guarantee a reduced number of modeled elements. In order to provide a more
compact system representation, parametric system modeling has been investigated
in the literature, in such a way that a set of replicas of a given subsystem is
parameterized so that only one representative instance is explicitly included.
While modeling aspects can be suitably addressed by these approaches,
analytical tools working on parametric characterizations are often more
difficult to be defined and the standard approach is to 'unfold' the parametric
model, in order to exploit standard analysis algorithms working at the unfolded
'ground' level. Moreover, parameterized combinatorial methods still require the
statistical independence assumption. In the present paper we consider the
formalism of Parametric Fault Tree (PFT) and we show how it can be related to
Probabilistic Horn Abduction (PHA). Since PHA is a framework where both
modeling and analysis can be performed in a restricted first-order language, we
aim at showing that converting a PFT into a PHA knowledge base will allow an
approach to dependability analysis directly exploiting parametric
representation. We will show that classical qualitative and quantitative
dependability measures can be characterized within PHA. Furthermore, additional
modeling aspects (such as noisy gates and local dependencies) as well as
additional reliability measures (such as posterior probability analysis) can be
naturally addressed by this conversion. A simple example of a multi-processor
system with several replicated units is used to illustrate the approach.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
On Triangulating Dynamic Graphical Models,"This paper introduces new methodology to triangulate dynamic Bayesian
networks (DBNs) and dynamic graphical models (DGMs). While most methods to
triangulate such networks use some form of constrained elimination scheme based
on properties of the underlying directed graph, we find it useful to view
triangulation and elimination using properties only of the resulting undirected
graph, obtained after the moralization step. We first briefly introduce the
Graphical model toolkit (GMTK) and its notion of dynamic graphical models, one
that slightly extends the standard notion of a DBN. We next introduce the
'boundary algorithm', a method to find the best boundary between partitions in
a dynamic model. We find that using this algorithm, the notions of forward- and
backward-interface become moot - namely, the size and fill-in of the best
forward- and backward- interface are identical. Moreover, we observe that
finding a good partition boundary allows for constrained elimination orders
(and therefore graph triangulations) that are not possible using standard
slice-by-slice constrained eliminations. More interestingly, with certain
boundaries it is possible to obtain constrained elimination schemes that lie
outside the space of possible triangulations using only unconstrained
elimination. Lastly, we report triangulation results on invented graphs,
standard DBNs from the literature, novel DBNs used in speech recognition
research systems, and also random graphs. Using a number of different
triangulation quality measures (max clique size, state-space, etc.), we find
that with our boundary algorithm the triangulation quality can dramatically
improve.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
An Empirical Study of w-Cutset Sampling for Bayesian Networks,"The paper studies empirically the time-space trade-off between sampling and
inference in a sl cutset sampling algorithm. The algorithm samples over a
subset of nodes in a Bayesian network and applies exact inference over the
rest. Consequently, while the size of the sampling space decreases, requiring
less samples for convergence, the time for generating each single sample
increases. The w-cutset sampling selects a sampling set such that the
induced-width of the network when the sampling set is observed is bounded by w,
thus requiring inference whose complexity is exponential in w. In this paper,
we investigate performance of w-cutset sampling over a range of w values and
measure the accuracy of w-cutset sampling as a function of w. Our experiments
demonstrate that the cutset sampling idea is quite powerful showing that an
optimal balance between inference and sampling benefits substantially from
restricting the cutset size, even at the cost of more complex inference.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
A possibilistic handling of partially ordered information,"In a standard possibilistic logic, prioritized information are encoded by
means of weighted knowledge base. This paper proposes an extension of
possibilistic logic for dealing with partially ordered information. We Show
that all basic notions of standard possibilitic logic (sumbsumption, syntactic
and semantic inference, etc.) have natural couterparts when dealing with
partially ordered information. We also propose an algorithm which computes
possibilistic conclusions of a partial knowledge base of a partially ordered
knowlege base.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Value Elimination: Bayesian Inference via Backtracking Search,"Backtracking search is a powerful algorithmic paradigm that can be used to
solve many problems. It is in a certain sense the dual of variable elimination;
but on many problems, e.g., SAT, it is vastly superior to variable elimination
in practice. Motivated by this we investigate the application of backtracking
search to the problem of Bayesian inference (Bayes). We show that natural
generalizations of known techniques allow backtracking search to achieve
performance guarantees similar to standard algorithms for Bayes, and that there
exist problems on which backtracking can in fact do much better. We also
demonstrate that these ideas can be applied to implement a Bayesian inference
engine whose performance is competitive with standard algorithms. Since
backtracking search can very naturally take advantage of context specific
structure, the potential exists for performance superior to standard algorithms
on many problems.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
New Advances in Inference by Recursive Conditioning,"Recursive Conditioning (RC) was introduced recently as the first any-space
algorithm for inference in Bayesian networks which can trade time for space by
varying the size of its cache at the increment needed to store a floating point
number. Under full caching, RC has an asymptotic time and space complexity
which is comparable to mainstream algorithms based on variable elimination and
clustering (exponential in the network treewidth and linear in its size). We
show two main results about RC in this paper. First, we show that its actual
space requirements under full caching are much more modest than those needed by
mainstream methods and study the implications of this finding. Second, we show
that RC can effectively deal with determinism in Bayesian networks by employing
standard logical techniques, such as unit resolution, allowing a significant
reduction in its time requirements in certain cases. We illustrate our results
using a number of benchmark networks, including the very challenging ones that
arise in genetic linkage analysis.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Incremental Compilation of Bayesian networks,"Most methods of exact probability propagation in Bayesian networks do not
carry out the inference directly over the network, but over a secondary
structure known as a junction tree or a join tree (JT). The process of
obtaining a JT is usually termed {sl compilation}. As compilation is usually
viewed as a whole process; each time the network is modified, a new compilation
process has to be carried out. The possibility of reusing an already existing
JT, in order to obtain the new one regarding only the modifications in the
network has received only little attention in the literature. In this paper we
present a method for incremental compilation of a Bayesian network, following
the classical scheme in which triangulation plays the key role. In order to
perform incremental compilation we propose to recompile only those parts of the
JT which can have been affected by the networks modifications. To do so, we
exploit the technique OF maximal prime subgraph decomposition in determining
the minimal subgraph(s) that have to be recompiled, and thereby the minimal
subtree(s) of the JT that should be replaced by new subtree(s).We focus on
structural modifications : addition and deletion of links and variables.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Structure-Based Causes and Explanations in the Independent Choice Logic,"This paper is directed towards combining Pearl's structural-model approach to
causal reasoning with high-level formalisms for reasoning about actions. More
precisely, we present a combination of Pearl's structural-model approach with
Poole's independent choice logic. We show how probabilistic theories in the
independent choice logic can be mapped to probabilistic causal models. This
mapping provides the independent choice logic with appealing concepts of
causality and explanation from the structural-model approach. We illustrate
this along Halpern and Pearl's sophisticated notions of actual cause,
explanation, and partial explanation. This mapping also adds first-order
modeling capabilities and explicit actions to the structural-model approach.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Symbolic Generalization for On-line Planning,"Symbolic representations have been used successfully in off-line planning
algorithms for Markov decision processes. We show that they can also improve
the performance of on-line planners. In addition to reducing computation time,
symbolic generalization can reduce the amount of costly real-world interactions
required for convergence. We introduce Symbolic Real-Time Dynamic Programming
(or sRTDP), an extension of RTDP. After each step of on-line interaction with
an environment, sRTDP uses symbolic model-checking techniques to generalizes
its experience by updating a group of states rather than a single state. We
examine two heuristic approaches to dynamic grouping of states and show that
they accelerate the planning process significantly in terms of both CPU time
and the number of steps of interaction with the environment.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories,"We present the language {m P}{cal C}+ for probabilistic reasoning about
actions, which is a generalization of the action language {cal C}+ that allows
to deal with probabilistic as well as nondeterministic effects of actions. We
define a formal semantics of {m P}{cal C}+ in terms of probabilistic
transitions between sets of states. Using a concept of a history and its belief
state, we then show how several important problems in reasoning about actions
can be concisely formulated in our formalism.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
A Simple Insight into Iterative Belief Propagation's Success,"In Non - ergodic belief networks the posterior belief OF many queries given
evidence may become zero.The paper shows that WHEN belief propagation IS
applied iteratively OVER arbitrary networks(the so called, iterative OR loopy
belief propagation(IBP)) it IS identical TO an arc - consistency algorithm
relative TO zero - belief queries(namely assessing zero posterior
probabilities). This implies that zero - belief conclusions derived BY belief
propagation converge AND are sound.More importantly it suggests that the
inference power OF IBP IS AS strong AND AS weak, AS that OF arc -
consistency.This allows the synthesis OF belief networks FOR which belief
propagation IS useless ON one hand, AND focuses the investigation OF classes OF
belief network FOR which belief propagation may be zero - complete.Finally, ALL
the above conclusions apply also TO Generalized belief propagation algorithms
that extend loopy belief propagation AND allow a crisper understanding OF their
power.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"Using the structure of d-connecting paths as a qualitative measure of
  the strength of dependence","Pearls concept OF a d - connecting path IS one OF the foundations OF the
modern theory OF graphical models : the absence OF a d - connecting path IN a
DAG indicates that conditional independence will hold IN ANY distribution
factorising according TO that graph. IN this paper we show that IN singly -
connected Gaussian DAGs it IS possible TO USE the form OF a d - connection TO
obtain qualitative information about the strength OF conditional
dependence.More precisely, the squared partial correlations BETWEEN two given
variables, conditioned ON different subsets may be partially ordered BY
examining the relationship BETWEEN the d - connecting path AND the SET OF
variables conditioned upon.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"Approximate Decomposition: A Method for Bounding and Estimating
  Probabilistic and Deterministic Queries","In this paper, we introduce a method for approximating the solution to
inference and optimization tasks in uncertain and deterministic reasoning. Such
tasks are in general intractable for exact algorithms because of the large
number of dependency relationships in their structure. Our method effectively
maps such a dense problem to a sparser one which is in some sense ""closest"".
Exact methods can be run on the sparser problem to derive bounds on the
original answer, which can be quite sharp. We present empirical results
demonstrating that our method works well on the tasks of belief inference and
finding the probability of the most probable explanation in belief networks,
and finding the cost of the solution that violates the smallest number of
constraints in constraint satisfaction problems. On one large CPCS network, for
example, we were able to calculate upper and lower bounds on the conditional
probability of a variable, given evidence, that were almost identical in the
average case.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"Monte-Carlo optimizations for resource allocation problems in stochastic
  network systems","Real-world distributed systems and networks are often unreliable and subject
to random failures of its components. Such a stochastic behavior affects
adversely the complexity of optimization tasks performed routinely upon such
systems, in particular, various resource allocation tasks. In this work we
investigate and develop Monte Carlo solutions for a class of two-stage
optimization problems in stochastic networks in which the expected value of
resource allocations before and after stochastic failures needs to be
optimized. The limitation of these problems is that their exact solutions are
exponential in the number of unreliable network components: thus, exact methods
do not scale-up well to large networks often seen in practice. We first prove
that Monte Carlo optimization methods can overcome the exponential bottleneck
of exact methods. Next we support our theoretical findings on resource
allocation experiments and show a very good scale-up potential of the new
methods to large stochastic networks.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"Implementation and Comparison of Solution Methods for Decision Processes
  with Non-Markovian Rewards","This paper examines a number of solution methods for decision processes with
non-Markovian rewards (NMRDPs). They all exploit a temporal logic specification
of the reward function to automatically translate the NMRDP into an equivalent
Markov decision process (MDP) amenable to well-known MDP solution methods. They
differ however in the representation of the target MDP and the class of MDP
solution methods to which they are suited. As a result, they adopt different
temporal logics and different translations. Unfortunately, no implementation of
these methods nor experimental let alone comparative results have ever been
reported. This paper is the first step towards filling this gap. We describe an
integrated system for solving NMRDPs which implements these methods and several
variants under a common interface; we use it to compare the various approaches
and identify the problem features favoring one over the other.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Decision Making with Partially Consonant Belief Functions,"This paper studies decision making for Walley's partially consonant belief
functions (pcb). In a pcb, the set of foci are partitioned. Within each
partition, the foci are nested. The pcb class includes probability functions
and possibility functions as extreme cases. Unlike earlier proposals for a
decision theory with belief functions, we employ an axiomatic approach. We
adopt an axiom system similar in spirit to von Neumann - Morgenstern's linear
utility theory for a preference relation on pcb lotteries. We prove a
representation theorem for this relation. Utility for a pcb lottery is a
combination of linear utility for probabilistic lottery and binary utility for
possibilistic lottery.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"Extending Factor Graphs so as to Unify Directed and Undirected Graphical
  Models","The two most popular types of graphical model are directed models (Bayesian
networks) and undirected models (Markov random fields, or MRFs). Directed and
undirected models offer complementary properties in model construction,
expressing conditional independencies, expressing arbitrary factorizations of
joint distributions, and formulating message-passing inference algorithms. We
show that the strengths of these two representations can be combined in a
single type of graphical model called a 'factor graph'. Every Bayesian network
or MRF can be easily converted to a factor graph that expresses the same
conditional independencies, expresses the same factorization of the joint
distribution, and can be used for probabilistic inference through application
of a single, simple message-passing algorithm. In contrast to chain graphs,
where message-passing is implemented on a hypergraph, message-passing can be
directly implemented on the factor graph. We describe a modified 'Bayes-ball'
algorithm for establishing conditional independence in factor graphs, and we
show that factor graphs form a strict superset of Bayesian networks and MRFs.
In particular, we give an example of a commonly-used 'mixture of experts' model
fragment, whose independencies cannot be represented in a Bayesian network or
an MRF, but can be represented in a factor graph. We finish by giving examples
of real-world problems that are not well suited to representation in Bayesian
networks and MRFs, but are well-suited to representation in factor graphs.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
"An Axiomatic Approach to Robustness in Search Problems with Multiple
  Scenarios","This paper is devoted to the search of robust solutions in state space graphs
when costs depend on scenarios. We first present axiomatic requirements for
preference compatibility with the intuitive idea of robustness.This leads us to
propose the Lorenz dominance rule as a basis for robustness analysis. Then,
after presenting complexity results about the determination of robust
solutions, we propose a new sophistication of A* specially designed to
determine the set of robust paths in a state space graph. The behavior of the
algorithm is illustrated on a small example. Finally, an axiomatic
justification of the refinement of robustness by an OWA criterion is provided.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Solving MAP Exactly using Systematic Search,"MAP is the problem of finding a most probable instantiation of a set of
variables in a Bayesian network given some evidence. Unlike computing posterior
probabilities, or MPE (a special case of MAP), the time and space complexity of
structural solutions for MAP are not only exponential in the network treewidth,
but in a larger parameter known as the ""constrained"" treewidth. In practice,
this means that computing MAP can be orders of magnitude more expensive than
computing posterior probabilities or MPE. This paper introduces a new, simple
upper bound on the probability of a MAP solution, which admits a tradeoff
between the bound quality and the time needed to compute it. The bound is shown
to be generally much tighter than those of other methods of comparable
complexity. We use this proposed upper bound to develop a branch-and-bound
search algorithm for solving MAP exactly. Experimental results demonstrate that
the search algorithm is able to solve many problems that are far beyond the
reach of any structure-based method for MAP. For example, we show that the
proposed algorithm can compute MAP exactly and efficiently for some networks
whose constrained treewidth is more than 40.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Dealing with uncertainty in fuzzy inductive reasoning methodology,"The aim of this research is to develop a reasoning under uncertainty strategy
in the context of the Fuzzy Inductive Reasoning (FIR) methodology. FIR emerged
from the General Systems Problem Solving developed by G. Klir. It is a data
driven methodology based on systems behavior rather than on structural
knowledge. It is a very useful tool for both the modeling and the prediction of
those systems for which no previous structural knowledge is available. FIR
reasoning is based on pattern rules synthesized from the available data. The
size of the pattern rule base can be very large making the prediction process
quite difficult. In order to reduce the size of the pattern rule base, it is
possible to automatically extract classical Sugeno fuzzy rules starting from
the set of pattern rules. The Sugeno rule base preserves pattern rules
knowledge as much as possible. In this process some information is lost but
robustness is considerably increased. In the forecasting process either the
pattern rule base or the Sugeno fuzzy rule base can be used. The first option
is desirable when the computational resources make it possible to deal with the
overall pattern rule base or when the extracted fuzzy rules are not accurate
enough due to uncertainty associated to the original data. In the second
option, the prediction process is done by means of the classical Sugeno
inference system. If the amount of uncertainty associated to the data is small,
the predictions obtained using the Sugeno fuzzy rule base will be very
accurate. In this paper a mixed pattern/fuzzy rules strategy is proposed to
deal with uncertainty in such a way that the best of both perspectives is used.
Areas in the data space with a higher level of uncertainty are identified by
means of the so-called error models. The prediction process in these areas
makes use of a mixed pattern/fuzzy rules scheme, whereas areas identified with
a lower level of uncertainty only use the Sugeno fuzzy rule base. The proposed
strategy is applied to a real biomedical system, i.e., the central nervous
system control of the cardiovascular system.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Optimal Limited Contingency Planning,"For a given problem, the optimal Markov policy can be considerred as a
conditional or contingent plan containing a (potentially large) number of
branches. Unfortunately, there are applications where it is desirable to
strictly limit the number of decision points and branches in a plan. For
example, it may be that plans must later undergo more detailed simulation to
verify correctness and safety, or that they must be simple enough to be
understood and analyzed by humans. As a result, it may be necessary to limit
consideration to plans with only a small number of branches. This raises the
question of how one goes about finding optimal plans containing only a limited
number of branches. In this paper, we present an any-time algorithm for optimal
k-contingency planning (OKP). It is the first optimal algorithm for limited
contingency planning that is not an explicit enumeration of possible contingent
plans. By modelling the problem as a Partially Observable Markov Decision
Process, it implements the Bellman optimality principle and prunes the solution
space. We present experimental results of applying this algorithm to some
simple test cases.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Systematic vs. Non-systematic Algorithms for Solving the MPE Task,"The paper continues the study of partitioning based inference of heuristics
for search in the context of solving the Most Probable Explanation task in
Bayesian Networks. We compare two systematic Branch and Bound search
algorithms, BBBT (for which the heuristic information is constructed during
search and allows dynamic variable/value ordering) and its predecessor BBMB
(for which the heuristic information is pre-compiled), against a number of
popular local search algorithms for the MPE problem. We show empirically that,
when viewed as approximation schemes, BBBT/BBMB are superior to all of these
best known SLS algorithms, especially when the domain sizes increase beyond 2.
This is in contrast with the performance of SLS vs. systematic search on
CSP/SAT problems, where SLS often significantly outperforms systematic
algorithms. As far as we know, BBBT/BBMB are currently the best performing
algorithms for solving the MPE task.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
An Importance Sampling Algorithm Based on Evidence Pre-propagation,"Precision achieved by stochastic sampling algorithms for Bayesian networks
typically deteriorates in face of extremely unlikely evidence. To address this
problem, we propose the Evidence Pre-propagation Importance Sampling algorithm
(EPIS-BN), an importance sampling algorithm that computes an approximate
importance function by the heuristic methods: loopy belief Propagation and
e-cutoff. We tested the performance of e-cutoff on three large real Bayesian
networks: ANDES, CPCS, and PATHFINDER. We observed that on each of these
networks the EPIS-BN algorithm gives us a considerable improvement over the
current state of the art algorithm, the AIS-BN algorithm. In addition, it
avoids the costly learning stage of the AIS-BN algorithm.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
Efficient Inference in Large Discrete Domains,"In this paper we examine the problem of inference in Bayesian Networks with
discrete random variables that have very large or even unbounded domains. For
example, in a domain where we are trying to identify a person, we may have
variables that have as domains, the set of all names, the set of all postal
codes, or the set of all credit card numbers. We cannot just have big tables of
the conditional probabilities, but need compact representations. We provide an
inference algorithm, based on variable elimination, for belief networks
containing both large domain and normal discrete random variables. We use
intensional (i.e., in terms of procedures) and extensional (in terms of listing
the elements) representations of conditional probabilities and of the
intermediate factors.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
CLP(BN): Constraint Logic Programming for Probabilistic Knowledge,"We present CLP(BN), a novel approach that aims at expressing Bayesian
networks through the constraint logic programming framework. Arguably, an
important limitation of traditional Bayesian networks is that they are
propositional, and thus cannot represent relations between multiple similar
objects in multiple contexts. Several researchers have thus proposed
first-order languages to describe such networks. Namely, one very successful
example of this approach are the Probabilistic Relational Models (PRMs), that
combine Bayesian networks with relational database technology. The key
difficulty that we had to address when designing CLP(cal{BN}) is that logic
based representations use ground terms to denote objects. With probabilitic
data, we need to be able to uniquely represent an object whose value we are not
sure about. We use {sl Skolem functions} as unique new symbols that uniquely
represent objects with unknown value. The semantics of CLP(cal{BN}) programs
then naturally follow from the general framework of constraint logic
programming, as applied to a specific domain where we have probabilistic data.
This paper introduces and defines CLP(cal{BN}), and it describes an
implementation and initial experiments. The paper also shows how CLP(cal{BN})
relates to Probabilistic Relational Models (PRMs), Ngo and Haddawys
Probabilistic Logic Programs, AND Kersting AND De Raedts Bayesian Logic
Programs.","Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)"
A Study on Fuzzy Systems,"We use princiles of fuzzy logic to develop a general model representing
several processes in a system's operation characterized by a degree of
vagueness and/or uncertainy. Further, we introduce three altenative measures of
a fuzzy system's effectiveness connected to the above model. An applcation is
also developed for the Mathematical Modelling process illustrating our results.","9 pages, 3 figures, 1 table"
Study: Symmetry breaking for ASP,"In their nature configuration problems are combinatorial (optimization)
problems. In order to find a configuration a solver has to instantiate a number
of components of a some type and each of these components can be used in a
relation defined for a type. Therefore, many solutions of a configuration
problem have symmetric ones which can be obtained by replacing some component
of a solution by another one of the same type. These symmetric solutions
decrease performance of optimization algorithms because of two reasons: a) they
satisfy all requirements and cannot be pruned out from the search space; and b)
existence of symmetric optimal solutions does not allow to prove the optimum in
a feasible time.",N/A
Performance Analysis of ANFIS in short term Wind Speed Prediction,"Results are presented on the performance of Adaptive Neuro-Fuzzy Inference
system (ANFIS) for wind velocity forecasts in the Isthmus of Tehuantepec region
in the state of Oaxaca, Mexico. The data bank was provided by the
meteorological station located at the University of Isthmus, Tehuantepec
campus, and this data bank covers the period from 2008 to 2011. Three data
models were constructed to carry out 16, 24 and 48 hours forecasts using the
following variables: wind velocity, temperature, barometric pressure, and date.
The performance measure for the three models is the mean standard error (MSE).
In this work, performance analysis in short-term prediction is presented,
because it is essential in order to define an adequate wind speed model for
eolian parks, where a right planning provide economic benefits.","9 pages, 11 figures, 1 table; IJCSI International Journal of Computer
  Science Issues, Vol. 9, Issue 5, No 3, September 2012. ISSN (Online):
  1694-0814. www.IJCSI.org"
"ConArg: a Tool to Solve (Weighted) Abstract Argumentation Frameworks
  with (Soft) Constraints","ConArg is a Constraint Programming-based tool that can be used to model and
solve different problems related to Abstract Argumentation Frameworks (AFs). To
implement this tool we have used JaCoP, a Java library that provides the user
with a Finite Domain Constraint Programming paradigm. ConArg is able to
randomly generate networks with small-world properties in order to find
conflict-free, admissible, complete, stable grounded, preferred, semi-stable,
stage and ideal extensions on such interaction graphs. We present the main
features of ConArg and we report the performance in time, showing also a
comparison with ASPARTIX [1], a similar tool using Answer Set Programming. The
use of techniques for constraint solving can tackle the complexity of the
problems presented in [2]. Moreover we suggest semiring-based soft constraints
as a mean to parametrically represent and solve Weighted Argumentation
Frameworks: different kinds of preference levels related to attacks, e.g., a
score representing a ""fuzziness"", a ""cost"" or a probability, can be represented
by choosing different instantiation of the semiring algebraic structure. The
basic idea is to provide a common computational and quantitative framework.",N/A
Modeling in OWL 2 without Restrictions,"The Semantic Web ontology language OWL 2 DL comes with a variety of language
features that enable sophisticated and practically useful modeling. However,
the use of these features has been severely restricted in order to retain
decidability of the language. For example, OWL 2 DL does not allow a property
to be both transitive and asymmetric, which would be desirable, e.g., for
representing an ancestor relation. In this paper, we argue that the so-called
global restrictions of OWL 2 DL preclude many useful forms of modeling, by
providing a catalog of basic modeling patterns that would be available in OWL 2
DL if the global restrictions were discarded. We then report on the results of
evaluating several state-of-the-art OWL 2 DL reasoners on problems that use
combinations of features in a way that the global restrictions are violated.
The systems turn out to rely heavily on the global restrictions and are thus
largely incapable of coping with the modeling patterns. Next we show how
off-the-shelf first-order logic theorem proving technology can be used to
perform reasoning in the OWL 2 direct semantics, the semantics that underlies
OWL 2 DL, but without requiring the global restrictions. Applying a naive
proof-of-concept implementation of this approach to the test problems was
successful in all cases. Based on our observations, we make suggestions for
future lines of research on expressive description logic-style OWL reasoning.",Technical Report
Multi-Objective AI Planning: Evaluating DAE-YAHSP on a Tunable Benchmark,"All standard AI planners to-date can only handle a single objective, and the
only way for them to take into account multiple objectives is by aggregation of
the objectives. Furthermore, and in deep contrast with the single objective
case, there exists no benchmark problems on which to test the algorithms for
multi-objective planning. Divide and Evolve (DAE) is an evolutionary planner
that won the (single-objective) deterministic temporal satisficing track in the
last International Planning Competition. Even though it uses intensively the
classical (and hence single-objective) planner YAHSP, it is possible to turn
DAE-YAHSP into a multi-objective evolutionary planner. A tunable benchmark
suite for multi-objective planning is first proposed, and the performances of
several variants of multi-objective DAE-YAHSP are compared on different
instances of this benchmark, hopefully paving the road to further
multi-objective competitions in AI planning.","7th International Conference on Evolutionary Multi-Criterion
  Optimization (2013) To appearr. arXiv admin note: text overlap with
  arXiv:0804.3965 by other authors"
Improving problem solving by exploiting the concept of symmetry,"We investigate the concept of symmetry and its role in problem solving. This
paper first defines precisely the elements that constitute a ""problem"" and its
""solution,"" and gives several examples to illustrate these definitions. Given
precise definitions of problems, it is relatively straightforward to construct
a search process for finding solutions. Finally this paper attempts to exploit
the concept of symmetry in improving problem solving.",N/A
"Irrespective Priority-Based Regular Properties of High-Intensity Virtual
  Environments","We have a lot of relation to the encoding and the Theory of Information, when
considering thinking. This is a natural process and, at once, the complex thing
we investigate. This always was a challenge - to understand how our mind works,
and we are trying to find some universal models for this. A lot of ways have
been considered so far, but we are looking for Something, we seek for
approaches. And the goal is to find a consistent, noncontradictory view, which
should at once be enough flexible in any dimensions to allow to represent
various kinds of processes and environments, matters of different nature and
diverse objects. Developing of such a model is the destination of this article.","4 pages, 2 figures; ISBN: 978-1-4673-2984-2"
A Frequency-Domain Encoding for Neuroevolution,"Neuroevolution has yet to scale up to complex reinforcement learning tasks
that require large networks. Networks with many inputs (e.g. raw video) imply a
very high dimensional search space if encoded directly. Indirect methods use a
more compact genotype representation that is transformed into networks of
potentially arbitrary size. In this paper, we present an indirect method where
networks are encoded by a set of Fourier coefficients which are transformed
into network weight matrices via an inverse Fourier-type transform. Because
there often exist network solutions whose weight matrices contain regularity
(i.e. adjacent weights are correlated), the number of coefficients required to
represent these networks in the frequency domain is much smaller than the
number of weights (in the same way that natural images can be compressed by
ignore high-frequency components). This ""compressed"" encoding is compared to
the direct approach where search is conducted in the weight space on the
high-dimensional octopus arm task. The results show that representing networks
in the frequency domain can reduce the search-space dimensionality by as much
as two orders of magnitude, both accelerating convergence and yielding more
general solutions.",N/A
Alternating Directions Dual Decomposition,"We propose AD3, a new algorithm for approximate maximum a posteriori (MAP)
inference on factor graphs based on the alternating directions method of
multipliers. Like dual decomposition algorithms, AD3 uses worker nodes to
iteratively solve local subproblems and a controller node to combine these
local solutions into a global update. The key characteristic of AD3 is that
each local subproblem has a quadratic regularizer, leading to a faster
consensus than subgradient-based dual decomposition, both theoretically and in
practice. We provide closed-form solutions for these AD3 subproblems for binary
pairwise factors and factors imposing first-order logic constraints. For
arbitrary factors (large or combinatorial), we introduce an active set method
which requires only an oracle for computing a local MAP configuration, making
AD3 applicable to a wide range of problems. Experiments on synthetic and
realworld problems show that AD3 compares favorably with the state-of-the-art.",N/A
Comparison between the two definitions of AI,"Two different definitions of the Artificial Intelligence concept have been
proposed in papers [1] and [2]. The first definition is informal. It says that
any program that is cleverer than a human being, is acknowledged as Artificial
Intelligence. The second definition is formal because it avoids reference to
the concept of human being. The readers of papers [1] and [2] might be left
with the impression that both definitions are equivalent and the definition in
[2] is simply a formal version of that in [1]. This paper will compare both
definitions of Artificial Intelligence and, hopefully, will bring a better
understanding of the concept.",added four new sections
Class Algebra for Ontology Reasoning,"Class algebra provides a natural framework for sharing of ISA hierarchies
between users that may be unaware of each other's definitions. This permits
data from relational databases, object-oriented databases, and tagged XML
documents to be unioned into one distributed ontology, sharable by all users
without the need for prior negotiation or the development of a ""standard""
ontology for each field. Moreover, class algebra produces a functional
correspondence between a class's class algebraic definition (i.e. its ""intent"")
and the set of all instances which satisfy the expression (i.e. its ""extent"").
The framework thus provides assistance in quickly locating examples and
counterexamples of various definitions. This kind of information is very
valuable when developing models of the real world, and serves as an invaluable
tool assisting in the proof of theorems concerning these class algebra
expressions. Finally, the relative frequencies of objects in the ISA hierarchy
can produce a useful Boolean algebra of probabilities. The probabilities can be
used by traditional information-theoretic classification methodologies to
obtain optimal ways of classifying objects in the database.",pp.2-13
"An Effective Procedure for Computing ""Uncomputable"" Functions","We give an effective procedure that produces a natural number in its output
from any natural number in its input, that is, it computes a total function.
The elementary operations of the procedure are Turing-computable. The procedure
has a second input which can contain the Goedel number of any Turing-computable
total function whose range is a subset of the set of the Goedel numbers of all
Turing-computable total functions. We prove that the second input cannot be set
to the Goedel number of any Turing-computable function that computes the output
from any natural number in its first input. In this sense, there is no Turing
program that computes the output from its first input. The procedure is used to
define creative procedures which compute functions that are not
Turing-computable. We argue that creative procedures model an aspect of
reasoning that cannot be modeled by Turing machines.",N/A
Principles of modal and vector theory of formal intelligence systems,"The paper considers the class of information systems capable of solving
heuristic problems on basis of formal theory that was termed modal and vector
theory of formal intelligent systems (FIS). The paper justifies the
construction of FIS resolution algorithm, defines the main features of these
systems and proves theorems that underlie the theory. The principle of
representation diversity of FIS construction is formulated. The paper deals
with the main principles of constructing and functioning formal intelligent
system (FIS) on basis of FIS modal and vector theory. The following phenomena
are considered: modular architecture of FIS presentation sub-system, algorithms
of data processing at every step of the stage of creating presentations.
Besides the paper suggests the structure of neural elements, i.e. zone
detectors and processors that are the basis for FIS construction.","34 pages, 8 figures"
Bayes Networks for Sonar Sensor Fusion,"Wide-angle sonar mapping of the environment by mobile robot is nontrivial due
to several sources of uncertainty: dropouts due to ""specular"" reflections,
obstacle location uncertainty due to the wide beam, and distance measurement
error. Earlier papers address the latter problems, but dropouts remain a
problem in many environments. We present an approach that lifts the
overoptimistic independence assumption used in earlier work, and use Bayes nets
to represent the dependencies between objects of the model. Objects of the
model consist of readings, and of regions in which ""quasi location invariance""
of the (possible) obstacles exists, with respect to the readings. Simulation
supports the method's feasibility. The model is readily extensible to allow for
prior distributions, as well as other types of sensing operations.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Exploiting Uncertain and Temporal Information in Correlation,"A modelling language is described which is suitable for the correlation of
information when the underlying functional model of the system is incomplete or
uncertain and the temporal dependencies are imprecise. An efficient and
incremental implementation is outlined which depends on cost functions
satisfying certain criteria. Possibilistic logic and probability theory (as it
is used in the applications targetted) satisfy these criteria.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Correlated Action Effects in Decision Theoretic Regression,"Much recent research in decision theoretic planning has adopted Markov
decision processes (MDPs) as the model of choice, and has attempted to make
their solution more tractable by exploiting problem structure. One particular
algorithm, structured policy construction achieves this by means of a decision
theoretic analog of goal regression using action descriptions based on Bayesian
networks with tree-structured conditional probability tables. The algorithm as
presented is not able to deal with actions with correlated effects. We describe
a new decision theoretic regression operator that corrects this weakness. While
conceptually straightforward, this extension requires a somewhat more
complicated technical approach.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Corporate Evidential Decision Making in Performance Prediction Domains,"Performance prediction or forecasting sporting outcomes involves a great deal
of insight into the particular area one is dealing with, and a considerable
amount of intuition about the factors that bear on such outcomes and
performances. The mathematical Theory of Evidence offers representation
formalisms which grant experts a high degree of freedom when expressing their
subjective beliefs in the context of decision-making situations like
performance prediction. Furthermore, this reasoning framework incorporates a
powerful mechanism to systematically pool the decisions made by individual
subject matter experts. The idea behind such a combination of knowledge is to
improve the competence (quality) of the overall decision-making process. This
paper reports on a performance prediction experiment carried out during the
European Football Championship in 1996. Relying on the knowledge of four
predictors, Evidence Theory was used to forecast the final scores of all 31
matches. The results of this empirical study are very encouraging.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Algorithms for Learning Decomposable Models and Chordal Graphs,"Decomposable dependency models and their graphical counterparts, i.e.,
chordal graphs, possess a number of interesting and useful properties. On the
basis of two characterizations of decomposable models in terms of independence
relationships, we develop an exact algorithm for recovering the chordal
graphical representation of any given decomposable model. We also propose an
algorithm for learning chordal approximations of dependency models isomorphic
to general undirected graphs.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"Incremental Pruning: A Simple, Fast, Exact Method for Partially
  Observable Markov Decision Processes","Most exact algorithms for general partially observable Markov decision
processes (POMDPs) use a form of dynamic programming in which a
piecewise-linear and convex representation of one value function is transformed
into another. We examine variations of the ""incremental pruning"" method for
solving this problem and compare them to earlier algorithms from theoretical
and empirical perspectives. We find that incremental pruning is presently the
most efficient exact method for solving POMDPs.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Defining Explanation in Probabilistic Systems,"As probabilistic systems gain popularity and are coming into wider use, the
need for a mechanism that explains the system's findings and recommendations
becomes more critical. The system will also need a mechanism for ordering
competing explanations. We examine two representative approaches to explanation
in the literature - one due to G\""ardenfors and one due to Pearl - and show
that both suffer from significant problems. We propose an approach to defining
a notion of ""better explanation"" that combines some of the features of both
together with more recent work by Pearl and others on causality.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Structured Arc Reversal and Simulation of Dynamic Probabilistic Networks,"We present an algorithm for arc reversal in Bayesian networks with
tree-structured conditional probability tables, and consider some of its
advantages, especially for the simulation of dynamic probabilistic networks. In
particular, the method allows one to produce CPTs for nodes involved in the
reversal that exploit regularities in the conditional distributions. We argue
that this approach alleviates some of the overhead associated with arc
reversal, plays an important role in evidence integration and can be used to
restrict sampling of variables in DPNs. We also provide an algorithm that
detects the dynamic irrelevance of state variables in forward simulation. This
algorithm exploits the structured CPTs in a reversed network to determine, in a
time-independent fashion, the conditions under which a variable does or does
not need to be sampled.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"Robustness Analysis of Bayesian Networks with Local Convex Sets of
  Distributions","Robust Bayesian inference is the calculation of posterior probability bounds
given perturbations in a probabilistic model. This paper focuses on
perturbations that can be expressed locally in Bayesian networks through convex
sets of distributions. Two approaches for combination of local models are
considered. The first approach takes the largest set of joint distributions
that is compatible with the local sets of distributions; we show how to reduce
this type of robust inference to a linear programming problem. The second
approach takes the convex hull of joint distributions generated from the local
sets of distributions; we demonstrate how to apply interior-point optimization
methods to generate posterior bounds and how to generate approximations that
are guaranteed to converge to correct posterior bounds. We also discuss
calculation of bounds for expected utilities and variances, and global
perturbation models.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"A Standard Approach for Optimizing Belief Network Inference using Query
  DAGs","This paper proposes a novel, algorithm-independent approach to optimizing
belief network inference. rather than designing optimizations on an algorithm
by algorithm basis, we argue that one should use an unoptimized algorithm to
generate a Q-DAG, a compiled graphical representation of the belief network,
and then optimize the Q-DAG and its evaluator instead. We present a set of
Q-DAG optimizations that supplant optimizations designed for traditional
inference algorithms, including zero compression, network pruning and caching.
We show that our Q-DAG optimizations require time linear in the Q-DAG size, and
significantly simplify the process of designing algorithms for optimizing
belief network inference.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"Model Reduction Techniques for Computing Approximately Optimal Solutions
  for Markov Decision Processes","We present a method for solving implicit (factored) Markov decision processes
(MDPs) with very large state spaces. We introduce a property of state space
partitions which we call epsilon-homogeneity. Intuitively, an
epsilon-homogeneous partition groups together states that behave approximately
the same under all or some subset of policies. Borrowing from recent work on
model minimization in computer-aided software verification, we present an
algorithm that takes a factored representation of an MDP and an 0<=epsilon<=1
and computes a factored epsilon-homogeneous partition of the state space. This
partition defines a family of related MDPs - those MDPs with state space equal
to the blocks of the partition, and transition probabilities ""approximately""
like those of any (original MDP) state in the source block. To formally study
such families of MDPs, we introduce the new notion of a ""bounded parameter MDP""
(BMDP), which is a family of (traditional) MDPs defined by specifying upper and
lower bounds on the transition probabilities and rewards. We describe
algorithms that operate on BMDPs to find policies that are approximately
optimal with respect to the original MDP. In combination, our method for
reducing a large implicit MDP to a possibly much smaller BMDP using an
epsilon-homogeneous partition, and our methods for selecting actions in BMDPs
constitute a new approach for analyzing large implicit MDPs. Among its
advantages, this new approach provides insight into existing algorithms to
solving implicit MDPs, provides useful connections to work in automata theory
and model minimization, and suggests methods, which involve varying epsilon, to
trade time and space (specifically in terms of the size of the corresponding
state space) for solution quality.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
A Scheme for Approximating Probabilistic Inference,"This paper describes a class of probabilistic approximation algorithms based
on bucket elimination which offer adjustable levels of accuracy and efficiency.
We analyze the approximation for several tasks: finding the most probable
explanation, belief updating and finding the maximum a posteriori hypothesis.
We identify regions of completeness and provide preliminary empirical
evaluation on randomly generated networks.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Myopic Value of Information in Influence Diagrams,"We present a method for calculation of myopic value of information in
influence diagrams (Howard & Matheson, 1981) based on the strong junction tree
framework (Jensen, Jensen & Dittmer, 1994). The difference in instantiation
order in the influence diagrams is reflected in the corresponding junction
trees by the order in which the chance nodes are marginalized. This order of
marginalization can be changed by table expansion and in effect the same
junction tree with expanded tables may be used for calculating the expected
utility for scenarios with different instantiation order. We also compare our
method to the classic method of modeling different instantiation orders in the
same influence diagram.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Limitations of Skeptical Default Reasoning,"Poole has shown that nonmonotonic logics do not handle the lottery paradox
correctly. In this paper we will show that Pollock's theory of defeasible
reasoning fails for the same reason: defeasible reasoning is incompatible with
the skeptical notion of derivability.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Decision-making Under Ordinal Preferences and Comparative Uncertainty,"This paper investigates the problem of finding a preference relation on a set
of acts from the knowledge of an ordering on events (subsets of states of the
world) describing the decision-maker (DM)s uncertainty and an ordering of
consequences of acts, describing the DMs preferences. However, contrary to
classical approaches to decision theory, we try to do it without resorting to
any numerical representation of utility nor uncertainty, and without even using
any qualitative scale on which both uncertainty and preference could be mapped.
It is shown that although many axioms of Savage theory can be preserved and
despite the intuitive appeal of the method for constructing a preference over
acts, the approach is inconsistent with a probabilistic representation of
uncertainty, but leads to the kind of uncertainty theory encountered in
non-monotonic reasoning (especially preferential and rational inference),
closely related to possibility theory. Moreover the method turns out to be
either very little decisive or to lead to very risky decisions, although its
basic principles look sound. This paper raises the question of the very
possibility of purely symbolic approaches to Savage-like decision-making under
uncertainty and obtains preliminary negative results.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
The Complexity of Plan Existence and Evaluation in Probabilistic Domains,"We examine the computational complexity of testing and finding small plans in
probabilistic planning domains with succinct representations. We find that many
problems of interest are complete for a variety of complexity classes: NP,
co-NP, PP, NP^PP, co-NP^PP, and PSPACE. Of these, the probabilistic classes PP
and NP^PP are likely to be of special interest in the field of uncertainty in
artificial intelligence and are deserving of additional study. These results
suggest a fruitful direction of future algorithmic development.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Algorithm Portfolio Design: Theory vs. Practice,"Stochastic algorithms are among the best for solving computationally hard
search and reasoning problems. The runtime of such procedures is characterized
by a random variable. Different algorithms give rise to different probability
distributions. One can take advantage of such differences by combining several
algorithms into a portfolio, and running them in parallel or interleaving them
on a single processor. We provide a detailed evaluation of the portfolio
approach on distributions of hard combinatorial search problems. We show under
what conditions the protfolio approach can have a dramatic computational
advantage over the best traditional methods.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Probability Update: Conditioning vs. Cross-Entropy,"Conditioning is the generally agreed-upon method for updating probability
distributions when one learns that an event is certainly true. But it has been
argued that we need other rules, in particular the rule of cross-entropy
minimization, to handle updates that involve uncertain information. In this
paper we re-examine such a case: van Fraassen's Judy Benjamin problem, which in
essence asks how one might update given the value of a conditional probability.
We argue that -- contrary to the suggestions in the literature -- it is
possible to use simple conditionalization in this case, and thereby obtain
answers that agree fully with intuition. This contrasts with proposals such as
cross-entropy, which are easier to apply but can give unsatisfactory answers.
Based on the lessons from this example, we speculate on some general
philosophical issues concerning probability update.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Inference with Idempotent Valuations,"Valuation based systems verifying an idempotent property are studied. A
partial order is defined between the valuations giving them a lattice
structure. Then, two different strategies are introduced to represent
valuations: as infimum of the most informative valuations or as supremum of the
least informative ones. It is studied how to carry out computations with both
representations in an efficient way. The particular cases of finite sets and
convex polytopes are considered.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Time-Critical Reasoning: Representations and Application,"We review the problem of time-critical action and discuss a reformulation
that shifts knowledge acquisition from the assessment of complex temporal
probabilistic dependencies to the direct assessment of time-dependent utilities
over key outcomes of interest. We dwell on a class of decision problems
characterized by the centrality of diagnosing and reacting in a timely manner
to pathological processes. We motivate key ideas in the context of trauma-care
triage and transportation decisions.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Relational Bayesian Networks,"A new method is developed to represent probabilistic relations on multiple
random events. Where previously knowledge bases containing probabilistic rules
were used for this purpose, here a probability distribution over the relations
is directly represented by a Bayesian network. By using a powerful way of
specifying conditional probability distributions in these networks, the
resulting formalism is more expressive than the previous ones. Particularly, it
provides for constraints on equalities of events, and it allows to define
complex, nested combination functions.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Composition of Probability Measures on Finite Spaces,"Decomposable models and Bayesian networks can be defined as sequences of
oligo-dimensional probability measures connected with operators of composition.
The preliminary results suggest that the probabilistic models allowing for
effective computational procedures are represented by sequences possessing a
special property; we shall call them perfect sequences. The paper lays down the
elementary foundation necessary for further study of iterative application of
operators of composition. We believe to develop a technique describing several
graph models in a unifying way. We are convinced that practically all
theoretical results and procedures connected with decomposable models and
Bayesian networks can be translated into the terminology introduced in this
paper. For example, complexity of computational procedures in these models is
closely dependent on possibility to change the ordering of oligo-dimensional
measures defining the model. Therefore, in this paper, lot of attention is paid
to possibility to change ordering of the operators of composition.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Nested Junction Trees,"The efficiency of inference in both the Hugin and, most notably, the
Shafer-Shenoy architectures can be improved by exploiting the independence
relations induced by the incoming messages of a clique. That is, the message to
be sent from a clique can be computed via a factorization of the clique
potential in the form of a junction tree. In this paper we show that by
exploiting such nested junction trees in the computation of messages both space
and time costs of the conventional propagation methods may be reduced. The
paper presents a structured way of exploiting the nested junction trees
technique to achieve such reductions. The usefulness of the method is
emphasized through a thorough empirical evaluation involving ten large
real-world Bayesian networks and the Hugin inference algorithm.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Object-Oriented Bayesian Networks,"Bayesian networks provide a modeling language and associated inference
algorithm for stochastic domains. They have been successfully applied in a
variety of medium-scale applications. However, when faced with a large complex
domain, the task of modeling using Bayesian networks begins to resemble the
task of programming using logical circuits. In this paper, we describe an
object-oriented Bayesian network (OOBN) language, which allows complex domains
to be described in terms of inter-related objects. We use a Bayesian network
fragment to describe the probabilistic relations between the attributes of an
object. These attributes can themselves be objects, providing a natural
framework for encoding part-of hierarchies. Classes are used to provide a
reusable probabilistic model which can be applied to multiple similar objects.
Classes also support inheritance of model fragments from a class to a subclass,
allowing the common aspects of related classes to be defined only once. Our
language has clear declarative semantics: an OOBN can be interpreted as a
stochastic functional program, so that it uniquely specifies a probabilistic
model. We provide an inference algorithm for OOBNs, and show that much of the
structural information encoded by an OOBN--particularly the encapsulation of
variables within an object and the reuse of model fragments in different
contexts--can also be used to speed up the inference process.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Nonuniform Dynamic Discretization in Hybrid Networks,"We consider probabilistic inference in general hybrid networks, which include
continuous and discrete variables in an arbitrary topology. We reexamine the
question of variable discretization in a hybrid network aiming at minimizing
the information loss induced by the discretization. We show that a nonuniform
partition across all variables as opposed to uniform partition of each variable
separately reduces the size of the data structures needed to represent a
continuous function. We also provide a simple but efficient procedure for
nonuniform partition. To represent a nonuniform discretization in the computer
memory, we introduce a new data structure, which we call a Binary Split
Partition (BSP) tree. We show that BSP trees can be an exponential factor
smaller than the data structures in the standard uniform discretization in
multiple dimensions and show how the BSP trees can be used in the standard join
tree algorithm. We show that the accuracy of the inference process can be
significantly improved by adjusting discretization with evidence. We construct
an iterative anytime algorithm that gradually improves the quality of the
discretization and the accuracy of the answer on a query. We provide empirical
evidence that the algorithm converges.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Probabilistic Acceptance,"The idea of fully accepting statements when the evidence has rendered them
probable enough faces a number of difficulties. We leave the interpretation of
probability largely open, but attempt to suggest a contextual approach to full
belief. We show that the difficulties of probabilistic acceptance are not as
severe as they are sometimes painted, and that though there are oddities
associated with probabilistic acceptance they are in some instances less
awkward than the difficulties associated with other nonmonotonic formalisms. We
show that the structure at which we arrive provides a natural home for
statistical inference.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"Network Fragments: Representing Knowledge for Constructing Probabilistic
  Models","In most current applications of belief networks, domain knowledge is
represented by a single belief network that applies to all problem instances in
the domain. In more complex domains, problem-specific models must be
constructed from a knowledge base encoding probabilistic relationships in the
domain. Most work in knowledge-based model construction takes the rule as the
basic unit of knowledge. We present a knowledge representation framework that
permits the knowledge base designer to specify knowledge in larger semantically
meaningful units which we call network fragments. Our framework provides for
representation of asymmetric independence and canonical intercausal
interaction. We discuss the combination of network fragments to form
problem-specific models to reason about particular problem instances. The
framework is illustrated using examples from the domain of military situation
awareness.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
"Computational Advantages of Relevance Reasoning in Bayesian Belief
  Networks","This paper introduces a computational framework for reasoning in Bayesian
belief networks that derives significant advantages from focused inference and
relevance reasoning. This framework is based on d -separation and other simple
and computationally efficient techniques for pruning irrelevant parts of a
network. Our main contribution is a technique that we call relevance-based
decomposition. Relevance-based decomposition approaches belief updating in
large networks by focusing on their parts and decomposing them into partially
overlapping subnetworks. This makes reasoning in some intractable networks
possible and, in addition, often results in significant speedup, as the total
time taken to update all subnetworks is in practice often considerably less
than the time taken to update the network as a whole. We report results of
empirical tests that demonstrate practical significance of our approach.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
A Target Classification Decision Aid,"A submarine's sonar team is responsible for detecting, localising and
classifying targets using information provided by the platform's sensor suite.
The information used to make these assessments is typically uncertain and/or
incomplete and is likely to require a measure of confidence in its reliability.
Moreover, improvements in sensor and communication technology are resulting in
increased amounts of on-platform and off-platform information available for
evaluation. This proliferation of imprecise information increases the risk of
overwhelming the operator. To assist the task of localisation and
classification a concept demonstration decision aid (Horizon), based on
evidential reasoning, has been developed. Horizon is an information fusion
software package for representing and fusing imprecise information about the
state of the world, expressed across suitable frames of reference. The Horizon
software is currently at prototype stage.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Support and Plausibility Degrees in Generalized Functional Models,"By discussing several examples, the theory of generalized functional models
is shown to be very natural for modeling some situations of reasoning under
uncertainty. A generalized functional model is a pair (f, P) where f is a
function describing the interactions between a parameter variable, an
observation variable and a random source, and P is a probability distribution
for the random source. Unlike traditional functional models, generalized
functional models do not require that there is only one value of the parameter
variable that is compatible with an observation and a realization of the random
source. As a consequence, the results of the analysis of a generalized
functional model are not expressed in terms of probability distributions but
rather by support and plausibility functions. The analysis of a generalized
functional model is very logical and is inspired from ideas already put forward
by R.A. Fisher in his theory of fiducial probability.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
The Cognitive Processing of Causal Knowledge,"There is a brief description of the probabilistic causal graph model for
representing, reasoning with, and learning causal structure using Bayesian
networks. It is then argued that this model is closely related to how humans
reason with and learn causal structure. It is shown that studies in psychology
on discounting (reasoning concerning how the presence of one cause of an effect
makes another cause less probable) support the hypothesis that humans reach the
same judgments as algorithms for doing inference in Bayesian networks. Next, it
is shown how studies by Piaget indicate that humans learn causal structure by
observing the same independencies and dependencies as those used by certain
algorithms for learning the structure of a Bayesian network. Based on this
indication, a subjective definition of causality is forwarded. Finally, methods
for further testing the accuracy of these claims are discussed.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Cost-Sharing in Bayesian Knowledge Bases,"Bayesian knowledge bases (BKBs) are a generalization of Bayes networks and
weighted proof graphs (WAODAGs), that allow cycles in the causal graph.
Reasoning in BKBs requires finding the most probable inferences consistent with
the evidence. The cost-sharing heuristic for finding least-cost explanations in
WAODAGs was presented and shown to be effective by Charniak and Husain.
However, the cycles in BKBs would make the definition of cost-sharing cyclic as
well, if applied directly to BKBs. By treating the defining equations of
cost-sharing as a system of equations, one can properly define an admissible
cost-sharing heuristic for BKBs. Empirical evaluation shows that cost-sharing
improves performance significantly when applied to BKBs.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Sequential Thresholds: Context Sensitive Default Extensions,"Default logic encounters some conceptual difficulties in representing common
sense reasoning tasks. We argue that we should not try to formulate modular
default rules that are presumed to work in all or most circumstances. We need
to take into account the importance of the context which is continuously
evolving during the reasoning process. Sequential thresholding is a
quantitative counterpart of default logic which makes explicit the role context
plays in the construction of a non-monotonic extension. We present a semantic
characterization of generic non-monotonic reasoning, as well as the
instantiations pertaining to default logic and sequential thresholding. This
provides a link between the two mechanisms as well as a way to integrate the
two that can be beneficial to both.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
On Stable Multi-Agent Behavior in Face of Uncertainty,"A stable joint plan should guarantee the achievement of a designer's goal in
a multi-agent environment, while ensuring that deviations from the prescribed
plan would be detected. We present a computational framework where stable joint
plans can be studied, as well as several basic results about the
representation, verification and synthesis of stable joint plans.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Region-Based Approximations for Planning in Stochastic Domains,"This paper is concerned with planning in stochastic domains by means of
partially observable Markov decision processes (POMDPs). POMDPs are difficult
to solve. This paper identifies a subclass of POMDPs called region observable
POMDPs, which are easier to solve and can be used to approximate general POMDPs
to arbitrary accuracy.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Independence of Causal Influence and Clique Tree Propagation,"This paper explores the role of independence of causal influence (ICI) in
Bayesian network inference. ICI allows one to factorize a conditional
probability table into smaller pieces. We describe a method for exploiting the
factorization in clique tree propagation (CTP) - the state-of-the-art exact
inference algorithm for Bayesian networks. We also present empirical results
showing that the resulting algorithm is significantly more efficient than the
combination of CTP and previous techniques for exploiting ICI.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Fast Value Iteration for Goal-Directed Markov Decision Processes,"Planning problems where effects of actions are non-deterministic can be
modeled as Markov decision processes. Planning problems are usually
goal-directed. This paper proposes several techniques for exploiting the
goal-directedness to accelerate value iteration, a standard algorithm for
solving Markov decision processes. Empirical studies have shown that the
techniques can bring about significant speedups.","Appears in Proceedings of the Thirteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1997)"
Complexity distribution of agent policies,"We analyse the complexity of environments according to the policies that need
to be used to achieve high performance. The performance results for a
population of policies leads to a distribution that is examined in terms of
policy complexity and analysed through several diagrams and indicators. The
notion of environment response curve is also introduced, by inverting the
performance results into an ability scale. We apply all these concepts,
diagrams and indicators to a minimalistic environment class, agent-populated
elementary cellular automata, showing how the difficulty, discriminating power
and ranges (previous to normalisation) may vary for several environments.",N/A
RIO: Minimizing User Interaction in Debugging of Knowledge Bases,"The best currently known interactive debugging systems rely upon some
meta-information in terms of fault probabilities in order to improve their
efficiency. However, misleading meta information might result in a dramatic
decrease of the performance and its assessment is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactions. In this work we present a reinforcement learning
strategy that continuously adapts its behavior depending on the performance
achieved and minimizes the risk of using low-quality meta information.
Therefore, this method is suitable for application scenarios where reliable
prior fault estimates are difficult to obtain. Using diverse real-world
knowledge bases, we show that the proposed interactive query strategy is
scalable, features decent reaction time, and outperforms both entropy-based and
no-risk strategies on average w.r.t. required amount of user interaction.",arXiv admin note: substantial text overlap with arXiv:1209.3734
An Algorithm for Finding Minimum d-Separating Sets in Belief Networks,"The criterion commonly used in directed acyclic graphs (dags) for testing
graphical independence is the well-known d-separation criterion. It allows us
to build graphical representations of dependency models (usually probabilistic
dependency models) in the form of belief networks, which make easy
interpretation and management of independence relationships possible, without
reference to numerical parameters (conditional probabilities). In this paper,
we study the following combinatorial problem: finding the minimum d-separating
set for two nodes in a dag. This set would represent the minimum information
(in the sense of minimum number of variables) necessary to prevent these two
nodes from influencing each other. The solution to this basic problem and some
of its extensions can be useful in several ways, as we shall see later. Our
solution is based on a two-step process: first, we reduce the original problem
to the simpler one of finding a minimum separating set in an undirected graph,
and second, we develop an algorithm for solving it.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Constraining Influence Diagram Structure by Generative Planning: An
  Application to the Optimization of Oil Spill Response","This paper works through the optimization of a real world planning problem,
with a combination of a generative planning tool and an influence diagram
solver. The problem is taken from an existing application in the domain of oil
spill emergency response. The planning agent manages constraints that order
sets of feasible equipment employment actions. This is mapped at an
intermediate level of abstraction onto an influence diagram. In addition, the
planner can apply a surveillance operator that determines observability of the
state---the unknown trajectory of the oil. The uncertain world state and the
objective function properties are part of the influence diagram structure, but
not represented in the planning agent domain. By exploiting this structure
under the constraints generated by the planning agent, the influence diagram
solution complexity simplifies considerably, and an optimum solution to the
employment problem based on the objective function is found. Finding this
optimum is equivalent to the simultaneous evaluation of a range of plans. This
result is an example of bounded optimality, within the limitations of this
hybrid generative planner and influence diagram architecture.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Inference Using Message Propagation and Topology Transformation in
  Vector Gaussian Continuous Networks","We extend Gaussian networks - directed acyclic graphs that encode
probabilistic relationships between variables - to its vector form. Vector
Gaussian continuous networks consist of composite nodes representing
multivariates, that take continuous values. These vector or composite nodes can
represent correlations between parents, as opposed to conventional univariate
nodes. We derive rules for inference in these networks based on two methods:
message propagation and topology transformation. These two approaches lead to
the development of algorithms, that can be implemented in either a centralized
or a decentralized manner. The domain of application of these networks are
monitoring and estimation problems. This new representation along with the
rules for inference developed here can be used to derive current Bayesian
algorithms such as the Kalman filter, and provide a rich foundation to develop
new algorithms. We illustrate this process by deriving the decentralized form
of the Kalman filter. This work unifies concepts from artificial intelligence
and modern control theory.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"A Structurally and Temporally Extended Bayesian Belief Network Model:
  Definitions, Properties, and Modeling Techniques","We developed the language of Modifiable Temporal Belief Networks (MTBNs) as a
structural and temporal extension of Bayesian Belief Networks (BNs) to
facilitate normative temporal and causal modeling under uncertainty. In this
paper we present definitions of the model, its components, and its fundamental
properties. We also discuss how to represent various types of temporal
knowledge, with an emphasis on hybrid temporal-explicit time modeling, dynamic
structures, avoiding causal temporal inconsistencies, and dealing with models
that involve simultaneously actions (decisions) and causal and non-causal
associations. We examine the relationships among BNs, Modifiable Belief
Networks, and MTBNs with a single temporal granularity, and suggest areas of
application suitable to each one of them.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
An Alternative Markov Property for Chain Graphs,"Graphical Markov models use graphs, either undirected, directed, or mixed, to
represent possible dependences among statistical variables. Applications of
undirected graphs (UDGs) include models for spatial dependence and image
analysis, while acyclic directed graphs (ADGs), which are especially convenient
for statistical analysis, arise in such fields as genetics and psychometrics
and as models for expert systems and Bayesian belief networks. Lauritzen,
Wermuth and Frydenberg (LWF) introduced a Markov property for chain graphs,
which are mixed graphs that can be used to represent simultaneously both causal
and associative dependencies and which include both UDGs and ADGs as special
cases. In this paper an alternative Markov property (AMP) for chain graphs is
introduced, which in some ways is a more direct extension of the ADG Markov
property than is the LWF property for chain graph.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Plan Development using Local Probabilistic Models,"Approximate models of world state transitions are necessary when building
plans for complex systems operating in dynamic environments. External event
probabilities can depend on state feature values as well as time spent in that
particular state. We assign temporally -dependent probability functions to
state transitions. These functions are used to locally compute state
probabilities, which are then used to select highly probable goal paths and
eliminate improbable states. This probabilistic model has been implemented in
the Cooperative Intelligent Real-time Control Architecture (CIRCA), which
combines an AI planner with a separate real-time system such that plans are
developed, scheduled, and executed with real-time guarantees. We present flight
simulation tests that demonstrate how our probabilistic model may improve CIRCA
performance.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Entailment in Probability of Thresholded Generalizations,"A nonmonotonic logic of thresholded generalizations is presented. Given
propositions A and B from a language L and a positive integer k, the
thresholded generalization A=>B{k} means that the conditional probability
P(B|A) falls short of one by no more than c*d^k. A two-level probability
structure is defined. At the lower level, a model is defined to be a
probability function on L. At the upper level, there is a probability
distribution over models. A definition is given of what it means for a
collection of thresholded generalizations to entail another thresholded
generalization. This nonmonotonic entailment relation, called ""entailment in
probability"", has the feature that its conclusions are ""probabilistically
trustworthy"" meaning that, given true premises, it is improbable that an
entailed conclusion would be false. A procedure is presented for ascertaining
whether any given collection of premises entails any given conclusion. It is
shown that entailment in probability is closely related to Goldszmidt and
Pearl's System-Z^+, thereby demonstrating that the conclusions of System-Z^+
are probabilistically trustworthy.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Approximations for Decision Making in the Dempster-Shafer Theory of
  Evidence","The computational complexity of reasoning within the Dempster-Shafer theory
of evidence is one of the main points of criticism this formalism has to face.
To overcome this difficulty various approximation algorithms have been
suggested that aim at reducing the number of focal elements in the belief
functions involved. Besides introducing a new algorithm using this method, this
paper describes an empirical study that examines the appropriateness of these
approximation procedures in decision making situations. It presents the
empirical findings and discusses the various tradeoffs that have to be taken
into account when actually applying one of these methods.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Coping with the Limitations of Rational Inference in the Framework of
  Possibility Theory","Possibility theory offers a framework where both Lehmann's ""preferential
inference"" and the more productive (but less cautious) ""rational closure
inference"" can be represented. However, there are situations where the second
inference does not provide expected results either because it cannot produce
them, or even provide counter-intuitive conclusions. This state of facts is not
due to the principle of selecting a unique ordering of interpretations (which
can be encoded by one possibility distribution), but rather to the absence of
constraints expressing pieces of knowledge we have implicitly in mind. It is
advocated in this paper that constraints induced by independence information
can help finding the right ordering of interpretations. In particular,
independence constraints can be systematically assumed with respect to formulas
composed of literals which do not appear in the conditional knowledge base, or
for default rules with respect to situations which are ""normal"" according to
the other default rules in the base. The notion of independence which is used
can be easily expressed in the qualitative setting of possibility theory.
Moreover, when a counter-intuitive plausible conclusion of a set of defaults,
is in its rational closure, but not in its preferential closure, it is always
possible to repair the set of defaults so as to produce the desired conclusion.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Arguing for Decisions: A Qualitative Model of Decision Making,"We develop a qualitative model of decision making with two aims: to describe
how people make simple decisions and to enable computer programs to do the
same. Current approaches based on Planning or Decisions Theory either ignore
uncertainty and tradeoffs, or provide languages and algorithms that are too
complex for this task. The proposed model provides a language based on rules, a
semantics based on high probabilities and lexicographical preferences, and a
transparent decision procedure where reasons for and against decisions
interact. The model is no substitude for Decision Theory, yet for decisions
that people find easy to explain it may provide an appealing alternative.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Context-Specific Independence in Bayesian Networks,"Bayesian networks provide a language for qualitatively representing the
conditional independence properties of a distribution. This allows a natural
and compact representation of the distribution, eases knowledge acquisition,
and supports effective inference algorithms. It is well-known, however, that
there are certain independencies that we cannot capture qualitatively within
the Bayesian network structure: independencies that hold only in certain
contexts, i.e., given a specific assignment of values to certain variables. In
this paper, we propose a formal notion of context-specific independence (CSI),
based on regularities in the conditional probability tables (CPTs) at a node.
We present a technique, analogous to (and based on) d-separation, for
determining when such independence holds in a given network. We then focus on a
particular qualitative representation scheme - tree-structured CPTs - for
capturing CSI. We suggest ways in which this representation can be used to
support effective inference algorithms. In particular, we present a structural
decomposition of the resulting network which can improve the performance of
clustering algorithms, and an alternative algorithm based on cutset
conditioning.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Decision-Theoretic Troubleshooting: A Framework for Repair and
  Experiment","We develop and extend existing decision-theoretic methods for troubleshooting
a nonfunctioning device. Traditionally, diagnosis with Bayesian networks has
focused on belief updating---determining the probabilities of various faults
given current observations. In this paper, we extend this paradigm to include
taking actions. In particular, we consider three classes of actions: (1) we can
make observations regarding the behavior of a device and infer likely faults as
in traditional diagnosis, (2) we can repair a component and then observe the
behavior of the device to infer likely faults, and (3) we can change the
configuration of the device, observe its new behavior, and infer the likelihood
of faults. Analysis of latter two classes of troubleshooting actions requires
incorporating notions of persistence into the belief-network formalism used for
probabilistic inference.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Independence with Lower and Upper Probabilities,"It is shown that the ability of the interval probability representation to
capture epistemological independence is severely limited. Two events are
epistemologically independent if knowledge of the first event does not alter
belief (i.e., probability bounds) about the second. However, independence in
this form can only exist in a 2-monotone probability function in degenerate
cases i.e., if the prior bounds are either point probabilities or entirely
vacuous. Additional limitations are characterized for other classes of lower
probabilities as well. It is argued that these phenomena are simply a matter of
interpretation. They appear to be limitations when one interprets probability
bounds as a measure of epistemological indeterminacy (i.e., uncertainty arising
from a lack of knowledge), but are exactly as one would expect when probability
intervals are interpreted as representations of ontological indeterminacy
(indeterminacy introduced by structural approximations). The ontological
interpretation is introduced and discussed.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Propagation of 2-Monotone Lower Probabilities on an Undirected Graph,"Lower and upper probabilities, also known as Choquet capacities, are widely
used as a convenient representation for sets of probability distributions. This
paper presents a graphical decomposition and exact propagation algorithm for
computing marginal posteriors of 2-monotone lower probabilities (equivalently,
2-alternating upper probabilities).","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Quasi-Bayesian Strategies for Efficient Plan Generation: Application to
  the Planning to Observe Problem","Quasi-Bayesian theory uses convex sets of probability distributions and
expected loss to represent preferences about plans. The theory focuses on
decision robustness, i.e., the extent to which plans are affected by deviations
in subjective assessments of probability. The present work presents solutions
for plan generation when robustness of probability assessments must be
included: plans contain information about the robustness of certain actions.
The surprising result is that some problems can be solved faster in the
Quasi-Bayesian framework than within usual Bayesian theory. We investigate this
on the planning to observe problem, i.e., an agent must decide whether to take
new observations or not. The fundamental question is: How, and how much, to
search for a ""best"" plan, based on the robustness of probability assessments?
Plan generation algorithms are derived in the context of material
classification with an acoustic robotic probe. A package that constructs
Quasi-Bayesian plans is available through anonymous ftp.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Some Experiments with Real-Time Decision Algorithms,"Real-time Decision algorithms are a class of incremental resource-bounded
[Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence
diagrams. We present a test domain for real-time decision algorithms, and the
results of experiments with several Real-time Decision Algorithms in this
domain. The results demonstrate high performance for two algorithms, a
decision-evaluation variant of Incremental Probabilisitic Inference [D'Ambrosio
93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95],
PK-reduced. We discuss the implications of these experimental results and
explore the broader applicability of these algorithms.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Bucket Elimination: A Unifying Framework for Several Probabilistic
  Inference","Probabilistic inference algorithms for finding the most probable explanation,
the maximum aposteriori hypothesis, and the maximum expected utility and for
updating belief are reformulated as an elimination--type algorithm called
bucket elimination. This emphasizes the principle common to many of the
algorithms appearing in that literature and clarifies their relationship to
nonserial dynamic programming algorithms. We also present a general way of
combining conditioning and elimination within this framework. Bounds on
complexity are given for all the algorithms as a function of the problem's
structure.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Topological Parameters for Time-Space Tradeoff,"In this paper we propose a family of algorithms combining tree-clustering
with conditioning that trade space for time. Such algorithms are useful for
reasoning in probabilistic and deterministic networks as well as for
accomplishing optimization tasks. By analyzing the problem structure it will be
possible to select from a spectrum the algorithm that best meets a given
time-space specification.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Sound Abstraction of Probabilistic Actions in The Constraint Mass
  Assignment Framework","This paper provides a formal and practical framework for sound abstraction of
probabilistic actions. We start by precisely defining the concept of sound
abstraction within the context of finite-horizon planning (where each plan is a
finite sequence of actions). Next we show that such abstraction cannot be
performed within the traditional probabilistic action representation, which
models a world with a single probability distribution over the state space. We
then present the constraint mass assignment representation, which models the
world with a set of probability distributions and is a generalization of mass
assignment representations. Within this framework, we present sound abstraction
procedures for three types of action abstraction. We end the paper with
discussions and related work on sound and approximate abstraction. We give
pointers to papers in which we discuss other sound abstraction-related issues,
including applications, estimating loss due to abstraction, and automatically
generating abstraction hierarchies.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Belief Revision with Uncertain Inputs in the Possibilistic Setting,"This paper discusses belief revision under uncertain inputs in the framework
of possibility theory. Revision can be based on two possible definitions of the
conditioning operation, one based on min operator which requires a purely
ordinal scale only, and another based on product, for which a richer structure
is needed, and which is a particular case of Dempster's rule of conditioning.
Besides, revision under uncertain inputs can be understood in two different
ways depending on whether the input is viewed, or not, as a constraint to
enforce. Moreover, it is shown that M.A. Williams' transmutations, originally
defined in the setting of Spohn's functions, can be captured in this framework,
as well as Boutilier's natural revision.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"An Evaluation of Structural Parameters for Probabilistic Reasoning:
  Results on Benchmark Circuits","Many algorithms for processing probabilistic networks are dependent on the
topological properties of the problem's structure. Such algorithms (e.g.,
clustering, conditioning) are effective only if the problem has a sparse graph
captured by parameters such as tree width and cycle-cut set size. In this paper
we initiate a study to determine the potential of structure-based algorithms in
real-life applications. We analyze empirically the structural properties of
problems coming from the circuit diagnosis domain. Specifically, we locate
those properties that capture the effectiveness of clustering and conditioning
as well as of a family of conditioning+clustering algorithms designed to
gradually trade space for time. We perform our analysis on 11 benchmark
circuits widely used in the testing community. We also report on the effect of
ordering heuristics on tree-clustering and show that, on our benchmarks, the
well-known max-cardinality ordering is substantially inferior to an ordering
called min-degree.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
A Qualitative Markov Assumption and its Implications for Belief Change,"The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change, belief revision and belief
update, have been studied in detail. Roughly, revision treats a surprising
observation as a sign that previous beliefs were wrong, while update treats a
surprising observation as an indication that the world has changed. In general,
we would expect that an agent making an observation may both want to revise
some earlier beliefs and assume that some change has occurred in the world. We
define a novel approach to belief change that allows us to do this, by applying
ideas from probability theory in a qualitative setting. The key idea is to use
a qualitative Markov assumption, which says that state transitions are
independent. We show that a recent approach to modeling qualitative uncertainty
using plausibility measures allows us to make such a qualitative Markov
assumption in a relatively straightforward way, and show how the Markov
assumption can be used to provide an attractive belief-change model.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Theoretical Foundations for Abstraction-Based Probabilistic Planning,"Modeling worlds and actions under uncertainty is one of the central problems
in the framework of decision-theoretic planning. The representation must be
general enough to capture real-world problems but at the same time it must
provide a basis upon which theoretical results can be derived. The central
notion in the framework we propose here is that of the affine-operator, which
serves as a tool for constructing (convex) sets of probability distributions,
and which can be considered as a generalization of belief functions and
interval mass assignments. Uncertainty in the state of the worlds is modeled
with sets of probability distributions, represented by affine-trees while
actions are defined as tree-manipulators. A small set of key properties of the
affine-operator is presented, forming the basis for most existing
operator-based definitions of probabilistic action projection and action
abstraction. We derive and prove correct three projection rules, which vividly
illustrate the precision-complexity tradeoff in plan projection. Finally, we
show how the three types of action abstraction identified by Haddawy and Doan
are manifested in the present framework.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Why Is Diagnosis Using Belief Networks Insensitive to Imprecision In
  Probabilities?","Recent research has found that diagnostic performance with Bayesian belief
networks is often surprisingly insensitive to imprecision in the numerical
probabilities. For example, the authors have recently completed an extensive
study in which they applied random noise to the numerical probabilities in a
set of belief networks for medical diagnosis, subsets of the CPCS network, a
subset of the QMR (Quick Medical Reference) focused on liver and bile diseases.
The diagnostic performance in terms of the average probabilities assigned to
the actual diseases showed small sensitivity even to large amounts of noise. In
this paper, we summarize the findings of this study and discuss possible
explanations of this low sensitivity. One reason is that the criterion for
performance is average probability of the true hypotheses, rather than average
error in probability, which is insensitive to symmetric noise distributions.
But, we show that even asymmetric, logodds-normal noise has modest effects. A
second reason is that the gold-standard posterior probabilities are often near
zero or one, and are little disturbed by noise.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Flexible Policy Construction by Information Refinement,"We report on work towards flexible algorithms for solving decision problems
represented as influence diagrams. An algorithm is given to construct a tree
structure for each decision node in an influence diagram. Each tree represents
a decision function and is constructed incrementally. The improvements to the
tree converge to the optimal decision function (neglecting computational costs)
and the asymptotic behaviour is only a constant factor worse than dynamic
programming techniques, counting the number of Bayesian network queries.
Empirical results show how expected utility increases with the size of the tree
and the number of Bayesian net calculations.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Efficient Search-Based Inference for Noisy-OR Belief Networks:
  TopEpsilon","Inference algorithms for arbitrary belief networks are impractical for large,
complex belief networks. Inference algorithms for specialized classes of belief
networks have been shown to be more efficient. In this paper, we present a
search-based algorithm for approximate inference on arbitrary, noisy-OR belief
networks, generalizing earlier work on search-based inference for two-level,
noisy-OR belief networks. Initial experimental results appear promising.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
A Probabilistic Model For Sensor Validation,"The validation of data from sensors has become an important issue in the
operation and control of modern industrial plants. One approach is to use
knowledge based techniques to detect inconsistencies in measured data. This
article presents a probabilistic model for the detection of such
inconsistencies. Based on probability propagation, this method is able to find
the existence of a possible fault among the set of sensors. That is, if an
error exists, many sensors present an apparent fault due to the propagation
from the sensor(s) with a real fault. So the fault detection mechanism can only
tell if a sensor has a potential fault, but it can not tell if the fault is
real or apparent. So the central problem is to develop a theory, and then an
algorithm, for distinguishing real and apparent faults, given that one or more
sensors can fail at the same time. This article then, presents an approach
based on two levels: (i) probabilistic reasoning, to detect a potential fault,
and (ii) constraint management, to distinguish the real fault from the apparent
ones. The proposed approach is exemplified by applying it to a power plant
model.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Computing Upper and Lower Bounds on Likelihoods in Intractable Networks,"We present deterministic techniques for computing upper and lower bounds on
marginal probabilities in sigmoid and noisy-OR networks. These techniques
become useful when the size of the network (or clique size) precludes exact
computations. We illustrate the tightness of the bounds by numerical
experiments.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
MIDAS - An Influence Diagram for Management of Mildew in Winter Wheat,"We present a prototype of a decision support system for management of the
fungal disease mildew in winter wheat. The prototype is based on an influence
diagram which is used to determine the optimal time and dose of mildew
treatments. This involves multiple decision opportunities over time,
stochasticity, inaccurate information and incomplete knowledge. The paper
describes the practical and theoretical problems encountered during the
construction of the influence diagram, and also the experience with the
prototype.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Computational Complexity Reduction for BN2O Networks Using Similarity of
  States","Although probabilistic inference in a general Bayesian belief network is an
NP-hard problem, computation time for inference can be reduced in most
practical cases by exploiting domain knowledge and by making approximations in
the knowledge representation. In this paper we introduce the property of
similarity of states and a new method for approximate knowledge representation
and inference which is based on this property. We define two or more states of
a node to be similar when the ratio of their probabilities, the likelihood
ratio, does not depend on the instantiations of the other nodes in the network.
We show that the similarity of states exposes redundancies in the joint
probability distribution which can be exploited to reduce the computation time
of probabilistic inference in networks with multiple similar states, and that
the computational complexity in the networks with exponentially many similar
states might be polynomial. We demonstrate our ideas on the example of a BN2O
network -- a two layer network often used in diagnostic problems -- by reducing
it to a very close network with multiple similar states. We show that the
answers to practical queries converge very fast to the answers obtained with
the original network. The maximum error is as low as 5% for models that require
only 10% of the computation time needed by the original BN2O model.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Uncertain Inferences and Uncertain Conclusions,"Uncertainty may be taken to characterize inferences, their conclusions, their
premises or all three. Under some treatments of uncertainty, the inferences
itself is never characterized by uncertainty. We explore both the significance
of uncertainty in the premises and in the conclusion of an argument that
involves uncertainty. We argue that for uncertainty to characterize the
conclusion of an inference is natural, but that there is an interplay between
uncertainty in the premises and uncertainty in the procedure of argument
itself. We show that it is possible in principle to incorporate all uncertainty
in the premises, rendering uncertainty arguments deductively valid. But we then
argue (1) that this does not reflect human argument, (2) that it is
computationally costly, and (3) that the gain in simplicity obtained by
allowing uncertainty inference can sometimes outweigh the loss of flexibility
it entails.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Network Engineering for Complex Belief Networks,"Like any large system development effort, the construction of a complex
belief network model requires systems engineering to manage the design and
construction process. We propose a rapid prototyping approach to network
engineering. We describe criteria for identifying network modules and the use
of ""stubs"" to represent not-yet-constructed modules. We propose an object
oriented representation for belief networks which captures the semantics of the
problem in addition to conditional independencies and probabilities. Methods
for evaluating complex belief network models are discussed. The ideas are
illustrated with examples from a large belief network construction problem in
the military intelligence domain.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Probabilistic Disjunctive Logic Programming,"In this paper we propose a framework for combining Disjunctive Logic
Programming and Poole's Probabilistic Horn Abduction. We use the concept of
hypothesis to specify the probability structure. We consider the case in which
probabilistic information is not available. Instead of using probability
intervals, we allow for the specification of the probabilities of disjunctions.
Because minimal models are used as characteristic models in disjunctive logic
programming, we apply the principle of indifference on the set of minimal
models to derive default probability values. We define the concepts of
explanation and partial explanation of a formula, and use them to determine the
default probability distribution(s) induced by a program. An algorithm for
calculating the default probability of a goal is presented.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Geometric Implications of the Naive Bayes Assumption,"A naive (or Idiot) Bayes network is a network with a single hypothesis node
and several observations that are conditionally independent given the
hypothesis. We recently surveyed a number of members of the UAI community and
discovered a general lack of understanding of the implications of the Naive
Bayes assumption on the kinds of problems that can be solved by these networks.
It has long been recognized [Minsky 61] that if observations are binary, the
decision surfaces in these networks are hyperplanes. We extend this result
(hyperplane separability) to Naive Bayes networks with m-ary observations. In
addition, we illustrate the effect of observation-observation dependencies on
decision surfaces. Finally, we discuss the implications of these results on
knowledge acquisition and research in learning.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Identifying Independencies in Causal Graphs with Feedback,"We show that the d -separation criterion constitutes a valid test for
conditional independence relationships that are induced by feedback systems
involving discrete variables.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
A Graph-Theoretic Analysis of Information Value,"We derive qualitative relationships about the informational relevance of
variables in graphical decision models based on a consideration of the topology
of the models. Specifically, we identify dominance relations for the expected
value of information on chance variables in terms of their position and
relationships in influence diagrams. The qualitative relationships can be
harnessed to generate nonnumerical procedures for ordering uncertain variables
in a decision model by their informational relevance.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"A Framework for Decision-Theoretic Planning I: Combining the Situation
  Calculus, Conditional Plans, Probability and Utility","This paper shows how we can combine logical representations of actions and
decision theory in such a manner that seems natural for both. In particular we
assume an axiomatization of the domain in terms of situation calculus, using
what is essentially Reiter's solution to the frame problem, in terms of the
completion of the axioms defining the state change. Uncertainty is handled in
terms of the independent choice logic, which allows for independent choices and
a logic program that gives the consequences of the choices. As part of the
consequences are a specification of the utility of (final) states. The robot
adopts robot plans, similar to the GOLOG programming language. Within this
logic, we can define the expected utility of a conditional plan, based on the
axiomatization of the actions, the uncertainty and the utility. The ?planning'
problem is to find the plan with the highest expected utility. This is related
to recent structured representations for POMDPs; here we use stochastic
situation calculus rules to specify the state transition function and the
reward/value function. Finally we show that with stochastic frame axioms,
actions representations in probabilistic STRIPS are exponentially larger than
using the representation proposed here.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Optimal Monte Carlo Estimation of Belief Network Inference,"We present two Monte Carlo sampling algorithms for probabilistic inference
that guarantee polynomial-time convergence for a larger class of network than
current sampling algorithms provide. These new methods are variants of the
known likelihood weighting algorithm. We use of recent advances in the theory
of optimal stopping rules for Monte Carlo simulation to obtain an inference
approximation with relative error epsilon and a small failure probability
delta. We present an empirical evaluation of the algorithms which demonstrates
their improved performance.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
A Discovery Algorithm for Directed Cyclic Graphs,"Directed acyclic graphs have been used fruitfully to represent causal
strucures (Pearl 1988). However, in the social sciences and elsewhere models
are often used which correspond both causally and statistically to directed
graphs with directed cycles (Spirtes 1995). Pearl (1993) discussed predicting
the effects of intervention in models of this kind, so-called linear
non-recursive structural equation models. This raises the question of whether
it is possible to make inferences about causal structure with cycles, form
sample data. In particular do there exist general, informative, feasible and
reliable precedures for inferring causal structure from conditional
independence relations among variables in a sample generated by an unknown
causal structure? In this paper I present a discovery algorithm that is correct
in the large sample limit, given commonly (but often implicitly) made plausible
assumptions, and which provides information about the existence or
non-existence of causal pathways from one variable to another. The algorithm is
polynomial on sparse graphs.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"A Polynomial-Time Algorithm for Deciding Markov Equivalence of Directed
  Cyclic Graphical Models","Although the concept of d-separation was originally defined for directed
acyclic graphs (see Pearl 1988), there is a natural extension of he concept to
directed cyclic graphs. When exactly the same set of d-separation relations
hold in two directed graphs, no matter whether respectively cyclic or acyclic,
we say that they are Markov equivalent. In other words, when two directed
cyclic graphs are Markov equivalent, the set of distributions that satisfy a
natural extension of the Global Directed Markov condition (Lauritzen et al.
1990) is exactly the same for each graph. There is an obvious exponential (in
the number of vertices) time algorithm for deciding Markov equivalence of two
directed cyclic graphs; simply chech all of the d-separation relations in each
graph. In this paper I state a theorem that gives necessary and sufficient
conditions for the Markov equivalence of two directed cyclic graphs, where each
of the conditions can be checked in polynomial time. Hence, the theorem can be
easily adapted into a polynomial time algorithm for deciding the Markov
equivalence of two directed cyclic graphs. Although space prohibits inclusion
of correctness proofs, they are fully described in Richardson (1994b).","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Coherent Knowledge Processing at Maximum Entropy by SPIRIT,"SPIRIT is an expert system shell for probabilistic knowledge bases. Knowledge
acquisition is performed by processing facts and rules on discrete variables in
a rich syntax. The shell generates a probability distribution which respects
all acquired facts and rules and which maximizes entropy. The user-friendly
devices of SPIRIT to define variables, formulate rules and create the knowledge
base are revealed in detail. Inductive learning is possible. Medium sized
applications show the power of the system.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Sample-and-Accumulate Algorithms for Belief Updating in Bayes Networks,"Belief updating in Bayes nets, a well known computationally hard problem, has
recently been approximated by several deterministic algorithms, and by various
randomized approximation algorithms. Deterministic algorithms usually provide
probability bounds, but have an exponential runtime. Some randomized schemes
have a polynomial runtime, but provide only probability estimates. We present
randomized algorithms that enumerate high-probability partial instantiations,
resulting in probability bounds. Some of these algorithms are also sampling
algorithms. Specifically, we introduce and evaluate a variant of backward
sampling, both as a sampling algorithm and as a randomized enumeration
algorithm. We also relax the implicit assumption used by both sampling and
accumulation algorithms, that query nodes must be instantiated in all the
samples.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
A Measure of Decision Flexibility,"We propose a decision-analytical approach to comparing the flexibility of
decision situations from the perspective of a decision-maker who exhibits
constant risk-aversion over a monetary value model. Our approach is simple yet
seems to be consistent with a variety of flexibility concepts, including robust
and adaptive alternatives. We try to compensate within the model for
uncertainty that was not anticipated or not modeled. This approach not only
allows one to compare the flexibility of plans, but also guides the search for
new, more flexible alternatives.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Binary Join Trees,"The main goal of this paper is to describe a data structure called binary
join trees that are useful in computing multiple marginals efficiently using
the Shenoy-Shafer architecture. We define binary join trees, describe their
utility, and sketch a procedure for constructing them.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Efficient Enumeration of Instantiations in Bayesian Networks,"Over the past several years Bayesian networks have been applied to a wide
variety of problems. A central problem in applying Bayesian networks is that of
finding one or more of the most probable instantiations of a network. In this
paper we develop an efficient algorithm that incrementally enumerates the
instantiations of a Bayesian network in decreasing order of probability. Such
enumeration algorithms are applicable in a variety of applications ranging from
medical expert systems to model-based diagnosis. Fundamentally, our algorithm
is simply performing a lazy enumeration of the sorted list of all
instantiations of the network. This insight leads to a very concise algorithm
statement which is both easily understood and implemented. We show that for
singly connected networks, our algorithm generates the next instantiation in
time polynomial in the size of the network. The algorithm extends to arbitrary
Bayesian networks using standard conditioning techniques. We empirically
evaluate the enumeration algorithm and demonstrate its practicality.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
On Separation Criterion and Recovery Algorithm for Chain Graphs,"Chain graphs give a natural unifying point of view on Markov and Bayesian
networks and enlarge the potential of graphical models for description of
conditional independence structures. In the paper a direct graphical separation
criterion for chain graphs, called c-separation, which generalizes the
d-separation criterion for Bayesian networks is introduced (recalled). It is
equivalent to the classic moralization criterion for chain graphs and complete
in sense that for every chain graph there exists a probability distribution
satisfying exactly conditional independencies derivable from the chain graph by
the c-separation criterion. Every class of Markov equivalent chain graphs can
be uniquely described by a natural representative, called the largest chain
graph. A recovery algorithm, which on basis of the (conditional) dependency
model induced by an unknown chain graph finds the corresponding largest chain
graph, is presented.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Possible World Partition Sequences: A Unifying Framework for Uncertain
  Reasoning","When we work with information from multiple sources, the formalism each
employs to handle uncertainty may not be uniform. In order to be able to
combine these knowledge bases of different formats, we need to first establish
a common basis for characterizing and evaluating the different formalisms, and
provide a semantics for the combined mechanism. A common framework can provide
an infrastructure for building an integrated system, and is essential if we are
to understand its behavior. We present a unifying framework based on an ordered
partition of possible worlds called partition sequences, which corresponds to
our intuitive notion of biasing towards certain possible scenarios when we are
uncertain of the actual situation. We show that some of the existing
formalisms, namely, default logic, autoepistemic logic, probabilistic
conditioning and thresholding (generalized conditioning), and possibility
theory can be incorporated into this general framework.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
"Supply Restoration in Power Distribution Systems - A Case Study in
  Integrating Model-Based Diagnosis and Repair Planning","Integrating diagnosis and repair is particularly crucial when gaining
sufficient information to discriminate between several candidate diagnoses
requires carrying out some repair actions. A typical case is supply restoration
in a faulty power distribution system. This problem, which is a major concern
for electricity distributors, features partial observability, and stochastic
repair actions which are more elaborate than simple replacement of components.
This paper analyses the difficulties in applying existing work on integrating
model-based diagnosis and repair and on planning in partially observable
stochastic domains to this real-world problem, and describes the pragmatic
approach we have retained so far.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Real Time Estimation of Bayesian Networks,"For real time evaluation of a Bayesian network when there is not sufficient
time to obtain an exact solution, a guaranteed response time, approximate
solution is required. It is shown that nontraditional methods utilizing
estimators based on an archive of trial solutions and genetic search can
provide an approximate solution that is considerably superior to the
traditional Monte Carlo simulation methods.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Testing Implication of Probabilistic Dependencies,"Axiomatization has been widely used for testing logical implications. This
paper suggests a non-axiomatic method, the chase, to test if a new dependency
follows from a given set of probabilistic dependencies. Although the chase
computation may require exponential time in some cases, this technique is a
powerful tool for establishing nontrivial theoretical results. More
importantly, this approach provides valuable insight into the intriguing
connection between relational databases and probabilistic reasoning systems.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Optimal Factory Scheduling using Stochastic Dominance A*,"We examine a standard factory scheduling problem with stochastic processing
and setup times, minimizing the expectation of the weighted number of tardy
jobs. Because the costs of operators in the schedule are stochastic and
sequence dependent, standard dynamic programming algorithms such as A* may fail
to find the optimal schedule. The SDA* (Stochastic Dominance A*) algorithm
remedies this difficulty by relaxing the pruning condition. We present an
improved state-space search formulation for these problems and discuss the
conditions under which stochastic scheduling problems can be solved optimally
using SDA*. In empirical testing on randomly generated problems, we found that
in 70%, the expected cost of the optimal stochastic solution is lower than that
of the solution derived using a deterministic approximation, with comparable
search effort.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Critical Remarks on Single Link Search in Learning Belief Networks,"In learning belief networks, the single link lookahead search is widely
adopted to reduce the search space. We show that there exists a class of
probabilistic domain models which displays a special pattern of dependency. We
analyze the behavior of several learning algorithms using different scoring
metrics such as the entropy, conditional independence, minimal description
length and Bayesian metrics. We demonstrate that single link lookahead search
procedures (employed in these algorithms) cannot learn these models correctly.
Thus, when the underlying domain model actually belongs to this class, the use
of a single link search procedure will result in learning of an incorrect
model. This may lead to inference errors when the model is used. Our analysis
suggests that if the prior knowledge about a domain does not rule out the
possible existence of these models, a multi-link lookahead search or other
heuristics should be used for the learning process.","Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)"
Reasoning about Independence in Probabilistic Models of Relational Data,"We extend the theory of d-separation to cases in which data instances are not
independent and identically distributed. We show that applying the rules of
d-separation directly to the structure of probabilistic models of relational
data inaccurately infers conditional independence. We introduce relational
d-separation, a theory for deriving conditional independence facts from
relational models. We provide a new representation, the abstract ground graph,
that enables a sound, complete, and computationally efficient method for
answering d-separation queries about relational models, and we present
empirical results that demonstrate effectiveness.","61 pages, substantial revisions to formalisms, theory, and related
  work"
Graphical Models for Preference and Utility,"Probabilistic independence can dramatically simplify the task of eliciting,
representing, and computing with probabilities in large domains. A key
technique in achieving these benefits is the idea of graphical modeling. We
survey existing notions of independence for utility functions in a
multi-attribute space, and suggest that these can be used to achieve similar
advantages. Our new results concern conditional additive independence, which we
show always has a perfect representation as separation in an undirected graph
(a Markov network). Conditional additive independencies entail a particular
functional for the utility function that is analogous to a product
decomposition of a probability function, and confers analogous benefits. This
functional form has been utilized in the Bayesian network and influence diagram
literature, but generally without an explanation in terms of independence. The
functional form yields a decomposition of the utility function that can greatly
speed up expected utility calculations, particularly when the utility graph has
a similar topology to the probabilistic network being used.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Counterfactuals and Policy Analysis in Structural Models,"Evaluation of counterfactual queries (e.g., ""If A were true, would C have
been true?"") is important to fault diagnosis, planning, determination of
liability, and policy analysis. We present a method of revaluating
counterfactuals when the underlying causal model is represented by structural
models - a nonlinear generalization of the simultaneous equations models
commonly used in econometrics and social sciences. This new method provides a
coherent means for evaluating policies involving the control of variables
which, prior to enacting the policy were influenced by other variables in the
system.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Belief Functions and Default Reasoning,"We present a new approach to dealing with default information based on the
theory of belief functions. Our semantic structures, inspired by Adams'
epsilon-semantics, are epsilon-belief assignments, where values committed to
focal elements are either close to 0 or close to 1. We define two systems based
on these structures, and relate them to other non-monotonic systems presented
in the literature. We show that our second system correctly addresses the
well-known problems of specificity, irrelevance, blocking of inheritance,
ambiguity, and redundancy.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Automating Computer Bottleneck Detection with Belief Nets,"We describe an application of belief networks to the diagnosis of bottlenecks
in computer systems. The technique relies on a high-level functional model of
the interaction between application workloads, the Windows NT operating system,
and system hardware. Given a workload description, the model predicts the
values of observable system counters available from the Windows NT performance
monitoring tool. Uncertainty in workloads, predictions, and counter values are
characterized with Gaussian distributions. During diagnostic inference, we use
observed performance monitor values to find the most probable assignment to the
workload parameters. In this paper we provide some background on automated
bottleneck detection, describe the structure of the system model, and discuss
empirical procedures for model calibration and verification. Part of the
calibration process includes generating a dataset to estimate a multivariate
Gaussian error model. Initial results in diagnosing bottlenecks are presented.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Chain Graphs for Learning,"Chain graphs combine directed and undirected graphs and their underlying
mathematics combines properties of the two. This paper gives a simplified
definition of chain graphs based on a hierarchical combination of Bayesian
(directed) and Markov (undirected) networks. Examples of a chain graph are
multivariate feed-forward networks, clustering with conditional interaction
between variables, and forms of Bayes classifiers. Chain graphs are then
extended using the notation of plates so that samples and data analysis
problems can be represented in a graphical model as well. Implications for
learning are discussed in the conclusion.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Error Estimation in Approximate Bayesian Belief Network Inference,"We can perform inference in Bayesian belief networks by enumerating
instantiations with high probability thus approximating the marginals. In this
paper, we present a method for determining the fraction of instantiations that
has to be considered such that the absolute error in the marginals does not
exceed a predefined value. The method is based on extreme value theory.
Essentially, the proposed method uses the reversed generalized Pareto
distribution to model probabilities of instantiations below a given threshold.
Based on this distribution, an estimate of the maximal absolute error if
instantiations with probability smaller than u are disregarded can be made.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Generating the Structure of a Fuzzy Rule under Uncertainty,"The aim of this paper is to present a method for identifying the structure of
a rule in a fuzzy model. For this purpose, an ATMS shall be used (Zurita 1994).
An algorithm obtaining the identification of the structure will be suggested
(Castro 1995). The minimal structure of the rule (with respect to the number of
variables that must appear in the rule) will be found by this algorithm.
Furthermore, the identification parameters shall be obtained simultaneously.
The proposed method shall be applied for classification to an example. The {em
Iris Plant Database} shall be learnt for all three kinds of plants.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Practical Model-Based Diagnosis with Qualitative Possibilistic
  Uncertainty","An approach to fault isolation that exploits vastly incomplete models is
presented. It relies on separate descriptions of each component behavior,
together with the links between them, which enables focusing of the reasoning
to the relevant part of the system. As normal observations do not need
explanation, the behavior of the components is limited to anomaly propagation.
Diagnostic solutions are disorders (fault modes or abnormal signatures) that
are consistent with the observations, as well as abductive explanations. An
ordinal representation of uncertainty based on possibility theory provides a
simple exception-tolerant description of the component behaviors. We can for
instance distinguish between effects that are more or less certainly present
(or absent) and effects that are more or less certainly present (or absent)
when a given anomaly is present. A realistic example illustrates the benefits
of this approach.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Decision Flexibility,"The development of new methods and representations for temporal
decision-making requires a principled basis for characterizing and measuring
the flexibility of decision strategies in the face of uncertainty. Our goal in
this paper is to provide a framework - not a theory - for observing how
decision policies behave in the face of informational perturbations, to gain
clues as to how they might behave in the face of unanticipated, possibly
unarticulated uncertainties. To this end, we find it beneficial to distinguish
between two types of uncertainty: ""Small World"" and ""Large World"" uncertainty.
The first type can be resolved by posing an unambiguous question to a
""clairvoyant,"" and is anchored on some well-defined aspect of a decision frame.
The second type is more troublesome, yet it is often of greater interest when
we address the issue of flexibility; this type of uncertainty can be resolved
only by consulting a ""psychic."" We next observe that one approach to
flexibility used in the economics literature is already implicitly accounted
for in the Maximum Expected Utility (MEU) principle from decision theory.
Though simple, the observation establishes the context for a more illuminating
notion of flexibility, what we term flexibility with respect to information
revelation. We show how to perform flexibility analysis of a static (i.e.,
single period) decision problem using a simple example, and we observe that the
most flexible alternative thus identified is not necessarily the MEU
alternative. We extend our analysis for a dynamic (i.e., multi-period) model,
and we demonstrate how to calculate the value of flexibility for decision
strategies that allow downstream revision of an upstream commitment decision.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"A Transformational Characterization of Equivalent Bayesian Network
  Structures","We present a simple characterization of equivalent Bayesian network
structures based on local transformations. The significance of the
characterization is twofold. First, we are able to easily prove several new
invariant properties of theoretical interest for equivalent structures. Second,
we use the characterization to derive an efficient algorithm that identifies
all of the compelled edges in a structure. Compelled edge identification is of
particular importance for learning Bayesian network structures from data
because these edges indicate causal relationships when certain assumptions
hold.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Conditioning Methods for Exact and Approximate Inference in Causal
  Networks","We present two algorithms for exact and approximate inference in causal
networks. The first algorithm, dynamic conditioning, is a refinement of cutset
conditioning that has linear complexity on some networks for which cutset
conditioning is exponential. The second algorithm, B-conditioning, is an
algorithm for approximate inference that allows one to trade-off the quality of
approximations with the computation time. We also present some experimental
results illustrating the properties of the proposed algorithms.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Independence Concepts for Convex Sets of Probabilities,"In this paper we study different concepts of independence for convex sets of
probabilities. There will be two basic ideas for independence. The first is
irrelevance. Two variables are independent when a change on the knowledge about
one variable does not affect the other. The second one is factorization. Two
variables are independent when the joint convex set of probabilities can be
decomposed on the product of marginal convex sets. In the case of the Theory of
Probability, these two starting points give rise to the same definition. In the
case of convex sets of probabilities, the resulting concepts will be strongly
related, but they will not be equivalent. As application of the concept of
independence, we shall consider the problem of building a global convex set
from marginal convex sets of probabilities.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Clustering Without (Thinking About) Triangulation,"The undirected technique for evaluating belief networks [Jensen, et.al.,
1990, Lauritzen and Spiegelhalter, 1988] requires clustering the nodes in the
network into a junction tree. In the traditional view, the junction tree is
constructed from the cliques of the moralized and triangulated belief network:
triangulation is taken to be the primitive concept, the goal towards which any
clustering algorithm (e.g. node elimination) is directed. In this paper, we
present an alternative conception of clustering, in which clusters and the
junction tree property play the role of primitives: given a graph (not a tree)
of clusters which obey (a modified version of) the junction tree property, we
transform this graph until we have obtained a tree. There are several
advantages to this approach: it is much clearer and easier to understand, which
is important for humans who are constructing belief networks; it admits a wider
range of heuristics which may enable more efficient or superior clustering
algorithms; and it serves as the natural basis for an incremental clustering
scheme, which we describe.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Implementation of Continuous Bayesian Networks Using Sums of Weighted
  Gaussians","Bayesian networks provide a method of representing conditional independence
between random variables and computing the probability distributions associated
with these random variables. In this paper, we extend Bayesian network
structures to compute probability density functions for continuous random
variables. We make this extension by approximating prior and conditional
densities using sums of weighted Gaussian distributions and then finding the
propagation rules for updating the densities in terms of these weights. We
present a simple example that illustrates the Bayesian network for continuous
variables; this example shows the effect of the network structure and
approximation errors on the computation of densities for variables in the
network.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Elicitation of Probabilities for Belief Networks: Combining Qualitative
  and Quantitative Information","Although the usefulness of belief networks for reasoning under uncertainty is
widely accepted, obtaining numerical probabilities that they require is still
perceived a major obstacle. Often not enough statistical data is available to
allow for reliable probability estimation. Available information may not be
directly amenable for encoding in the network. Finally, domain experts may be
reluctant to provide numerical probabilities. In this paper, we propose a
method for elicitation of probabilities from a domain expert that is
non-invasive and accommodates whatever probabilistic information the expert is
willing to state. We express all available information, whether qualitative or
quantitative in nature, in a canonical form consisting of (in) equalities
expressing constraints on the hyperspace of possible joint probability
distributions. We then use this canonical form to derive second-order
probability distributions over the desired probabilities.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Numerical Representations of Acceptance,"Accepting a proposition means that our confidence in this proposition is
strictly greater than the confidence in its negation. This paper investigates
the subclass of uncertainty measures, expressing confidence, that capture the
idea of acceptance, what we call acceptance functions. Due to the monotonicity
property of confidence measures, the acceptance of a proposition entails the
acceptance of any of its logical consequences. In agreement with the idea that
a belief set (in the sense of Gardenfors) must be closed under logical
consequence, it is also required that the separate acceptance o two
propositions entail the acceptance of their conjunction. Necessity (and
possibility) measures agree with this view of acceptance while probability and
belief functions generally do not. General properties of acceptance functions
are estabilished. The motivation behind this work is the investigation of a
setting for belief revision more general than the one proposed by Alchourron,
Gardenfors and Makinson, in connection with the notion of conditioning.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Fraud/Uncollectible Debt Detection Using a Bayesian Network Based
  Learning System: A Rare Binary Outcome with Mixed Data Structures","The fraud/uncollectible debt problem in the telecommunications industry
presents two technical challenges: the detection and the treatment of the
account given the detection. In this paper, we focus on the first problem of
detection using Bayesian network models, and we briefly discuss the application
of a normative expert system for the treatment at the end. We apply Bayesian
network models to the problem of fraud/uncollectible debt detection for
telecommunication services. In addition to being quite successful at predicting
rare event outcomes, it is able to handle a mixture of categorical and
continuous data. We present a performance comparison using linear and
non-linear discriminant analysis, classification and regression trees, and
Bayesian network models","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
A Constraint Satisfaction Approach to Decision under Uncertainty,"The Constraint Satisfaction Problem (CSP) framework offers a simple and sound
basis for representing and solving simple decision problems, without
uncertainty. This paper is devoted to an extension of the CSP framework
enabling us to deal with some decisions problems under uncertainty. This
extension relies on a differentiation between the agent-controllable decision
variables and the uncontrollable parameters whose values depend on the
occurrence of uncertain events. The uncertainty on the values of the parameters
is assumed to be given under the form of a probability distribution. Two
algorithms are given, for computing respectively decisions solving the problem
with a maximal probability, and conditional decisions mapping the largest
possible amount of possible cases to actual decisions.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Plausibility Measures: A User's Guide,"We examine a new approach to modeling uncertainty based on plausibility
measures, where a plausibility measure just associates with an event its
plausibility, an element is some partially ordered set. This approach is easily
seen to generalize other approaches to modeling uncertainty, such as
probability measures, belief functions, and possibility measures. The lack of
structure in a plausibility measure makes it easy for us to add structure on an
""as needed"" basis, letting us examine what is required to ensure that a
plausibility measure has certain properties of interest. This gives us insight
into the essential features of the properties in question, while allowing us to
prove general results that apply to many approaches to reasoning about
uncertainty. Plausibility measures have already proved useful in analyzing
default reasoning. In this paper, we examine their ""algebraic properties,""
analogues to the use of + and * in probability theory. An understanding of such
properties will be essential if plausibility measures are to be used in
practice as a representation tool.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Testing Identifiability of Causal Effects,"This paper concerns the probabilistic evaluation of the effects of actions in
the presence of unmeasured variables. We show that the identification of causal
effect between a singleton variable X and a set of variables Y can be
accomplished systematically, in time polynomial in the number of variables in
the graph. When the causal effect is identifiable, a closed-form expression can
be obtained for the probability that the action will achieve a specified goal,
or a set of goals.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Fast Belief Update Using Order-of-Magnitude Probabilities,"We present an algorithm, called Predict, for updating beliefs in causal
networks quantified with order-of-magnitude probabilities. The algorithm takes
advantage of both the structure and the quantification of the network and
presents a polynomial asymptotic complexity. Predict exhibits a conservative
behavior in that it is always sound but not always complete. We provide
sufficient conditions for completeness and present algorithms for testing these
conditions and for computing a complete set of plausible values. We propose
Predict as an efficient method to estimate probabilistic values and illustrate
its use in conjunction with two known algorithms for probabilistic inference.
Finally, we describe an application of Predict to plan evaluation, present
experimental results, and discuss issues regarding its use with conditional
logics of belief, and in the characterization of irrelevance.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Transforming Prioritized Defaults and Specificity into Parallel Defaults,"We show how to transform any set of prioritized propositional defaults into
an equivalent set of parallel (i.e., unprioritized) defaults, in
circumscription. We give an algorithm to implement the transform. We show how
to use the transform algorithm as a generator of a whole family of inferencing
algorithms for circumscription. The method is to employ the transform algorithm
as a front end to any inferencing algorithm, e.g., one of the previously
available, that handles the parallel (empty) case of prioritization. Our
algorithms provide not just coverage of a new expressive class, but also
alternatives to previous algorithms for implementing the previously covered
class (?layered?) of prioritization. In particular, we give a new
query-answering algorithm for prioritized cirumscription which is sound and
complete for the full expressive class of unrestricted finite prioritization
partial orders, for propositional defaults (or minimized predicates). By
contrast, previous algorithms required that the prioritization partial order be
layered, i.e., structured similar to the system of rank in the military. Our
algorithm enables, for the first time, the implementation of the most useful
class of prioritization: non-layered prioritization partial orders. Default
inheritance, for example, typically requires non-layered prioritization to
represent specificity adequately. Our algorithm enables not only the
implementation of default inheritance (and specificity) within prioritized
circumscription, but also the extension and combination of default inheritance
with other kinds of prioritized default reasoning, e.g.: with stratified logic
programs with negation-as-failure. Such logic programs are previously known to
be representable equivalently as layered-priority predicate circumscriptions.
Worst-case, the transform increases the number of defaults exponentially. We
discuss how inferencing is practically implementable nevertheless in two kinds
of situations: general expressiveness but small numbers of defaults, or
expressive special cases with larger numbers of defaults. One such expressive
special case is non-?top-heaviness? of the prioritization partial order. In
addition to its direct implementation, the transform can also be exploited
analytically to generate special case algorithms, e.g., a tractable transform
for a class within default inheritance (detailed in another, forthcoming
paper). We discuss other aspects of the significance of the fundamental result.
One can view the transform as reducing n degrees of partially ordered belief
confidence to just 2 degrees of confidence: for-sure and (unprioritized)
default. Ordinary, parallel default reasoning, e.g., in parallel
circumscription or Poole's Theorist, can be viewed in these terms as reducing 2
degrees of confidence to just 1 degree of confidence: that of the non-monotonic
theory's conclusions. The expressive reduction's computational complexity
suggests that prioritization is valuable for its expressive conciseness, just
as defaults are for theirs. For Reiter's Default Logic and Poole's Theorist,
the transform implies how to extend those formalisms so as to equip them with a
concept of prioritization that is exactly equivalent to that in
circumscription. This provides an interesting alternative to Brewka's approach
to equipping them with prioritization-type precedence.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Efficient Decision-Theoretic Planning: Techniques and Empirical Analysis,"This paper discusses techniques for performing efficient decision-theoretic
planning. We give an overview of the DRIPS decision-theoretic refinement
planning system, which uses abstraction to efficiently identify optimal plans.
We present techniques for automatically generating search control information,
which can significantly improve the planner's performance. We evaluate the
efficiency of DRIPS both with and without the search control rules on a complex
medical planning problem and compare its performance to that of a
branch-and-bound decision tree algorithm.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Fuzzy Logic and Probability,"In this paper we deal with a new approach to probabilistic reasoning in a
logical framework. Nearly almost all logics of probability that have been
proposed in the literature are based on classical two-valued logic. After
making clear the differences between fuzzy logic and probability theory, here
we propose a {em fuzzy} logic of probability for which completeness results (in
a probabilistic sense) are provided. The main idea behind this approach is that
probability values of crisp propositions can be understood as truth-values of
some suitable fuzzy propositions associated to the crisp ones. Moreover,
suggestions and examples of how to extend the formalism to cope with
conditional probabilities and with other uncertainty formalisms are also
provided.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Probabilistic Temporal Reasoning with Endogenous Change,"This paper presents a probabilistic model for reasoning about the state of a
system as it changes over time, both due to exogenous and endogenous
influences. Our target domain is a class of medical prediction problems that
are neither so urgent as to preclude careful diagnosis nor progress so slowly
as to allow arbitrary testing and treatment options. In these domains there is
typically enough time to gather information about the patient's state and
consider alternative diagnoses and treatments, but the temporal interaction
between the timing of tests, treatments, and the course of the disease must
also be considered. Our approach is to elicit a qualitative structural model of
the patient from a human expert---the model identifies important attributes,
the way in which exogenous changes affect attribute values, and the way in
which the patient's condition changes endogenously. We then elicit
probabilistic information to capture the expert's uncertainty about the effects
of tests and treatments and the nature and timing of endogenous state changes.
This paper describes the model in the context of a problem in treating vehicle
accident trauma, and suggests a method for solving the model based on the
technique of sequential imputation. A complementary goal of this work is to
understand and synthesize a disparate collection of research efforts all using
the name ?probabilistic temporal reasoning.? This paper analyzes related work
and points out essential differences between our proposed model and other
approaches in the literature.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Toward a Characterization of Uncertainty Measure for the Dempster-Shafer
  Theory","This is a working paper summarizing results of an ongoing research project
whose aim is to uniquely characterize the uncertainty measure for the
Dempster-Shafer Theory. A set of intuitive axiomatic requirements is presented,
some of their implications are shown, and the proof is given of the minimality
of recently proposed measure AU among all measures satisfying the proposed
requirements.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
A Definition and Graphical Representation for Causality,"We present a precise definition of cause and effect in terms of a fundamental
notion called unresponsiveness. Our definition is based on Savage's (1954)
formulation of decision theory and departs from the traditional view of
causation in that our causal assertions are made relative to a set of
decisions. An important consequence of this departure is that we can reason
about cause locally, not requiring a causal explanation for every dependency.
Such local reasoning can be beneficial because it may not be necessary to
determine whether a particular dependency is causal to make a decision. Also in
this paper, we examine the graphical encoding of causal relationships. We show
that influence diagrams in canonical form are an accurate and efficient
representation of causal relationships. In addition, we establish a
correspondence between canonical form and Pearl's causal theory.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Learning Bayesian Networks: A Unification for Discrete and Gaussian
  Domains","We examine Bayesian methods for learning Bayesian networks from a combination
of prior knowledge and statistical data. In particular, we unify the approaches
we presented at last year's conference for discrete and Gaussian domains. We
derive a general Bayesian scoring metric, appropriate for both domains. We then
use this metric in combination with well-known statistical facts about the
Dirichlet and normal--Wishart distributions to derive our metrics for discrete
and Gaussian domains.",This version has improved pointers to the literature
A Bayesian Approach to Learning Causal Networks,"Whereas acausal Bayesian networks represent probabilistic independence,
causal Bayesian networks represent causal relationships. In this paper, we
examine Bayesian methods for learning both types of networks. Bayesian methods
for learning acausal networks are fairly well developed. These methods often
employ assumptions to facilitate the construction of priors, including the
assumptions of parameter independence, parameter modularity, and likelihood
equivalence. We show that although these assumptions also can be appropriate
for learning causal networks, we need additional assumptions in order to learn
causal networks. We introduce two sufficient assumptions, called {em mechanism
independence} and {em component independence}. We show that these new
assumptions, when combined with parameter independence, parameter modularity,
and likelihood equivalence, allow us to apply methods for learning acausal
networks to learn causal networks.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Display of Information for Time-Critical Decision Making,"We describe methods for managing the complexity of information displayed to
people responsible for making high-stakes, time-critical decisions. The
techniques provide tools for real-time control of the configuration and
quantity of information displayed to a user, and a methodology for designing
flexible human-computer interfaces for monitoring applications. After defining
a prototypical set of display decision problems, we introduce the expected
value of revealed information (EVRI) and the related measure of expected value
of displayed information (EVDI). We describe how these measures can be used to
enhance computer displays used for monitoring complex systems. We motivate the
presentation by discussing our efforts to employ decision-theoretic control of
displays for a time-critical monitoring application at the NASA Mission Control
Center in Houston.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Reasoning, Metareasoning, and Mathematical Truth: Studies of Theorem
  Proving under Limited Resources","In earlier work, we introduced flexible inference and decision-theoretic
metareasoning to address the intractability of normative inference. Here,
rather than pursuing the task of computing beliefs and actions with decision
models composed of distinctions about uncertain events, we examine methods for
inferring beliefs about mathematical truth before an automated theorem prover
completes a proof. We employ a Bayesian analysis to update belief in truth,
given theorem-proving progress, and show how decision-theoretic methods can be
used to determine the value of continuing to deliberate versus taking immediate
action in time-critical situations.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Improved Sampling for Diagnostic Reasoning in Bayesian Networks,"Bayesian networks offer great potential for use in automating large scale
diagnostic reasoning tasks. Gibbs sampling is the main technique used to
perform diagnostic reasoning in large richly interconnected Bayesian networks.
Unfortunately Gibbs sampling can take an excessive time to generate a
representative sample. In this paper we describe and test a number of heuristic
strategies for improving sampling in noisy-or Bayesian networks. The strategies
include Monte Carlo Markov chain sampling techniques other than Gibbs sampling.
Emphasis is put on strategies that can be implemented in distributed systems.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Cautious Propagation in Bayesian Networks,"Consider the situation where some evidence e has been entered to a Bayesian
network. When performing conflict analysis, sensitivity analysis, or when
answering questions like ""What if the finding on X had been y instead of x?""
you need probabilities P (e'| h), where e' is a subset of e, and h is a
configuration of a (possibly empty) set of variables. Cautious propagation is a
modification of HUGIN propagation into a Shafer-Shenoy-like architecture. It is
less efficient than HUGIN propagation; however, it provides easy access to P
(e'| h) for a great deal of relevant subsets e'.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Information/Relevance Influence Diagrams,"In this paper we extend the influence diagram (ID) representation for
decisions under uncertainty. In the standard ID, arrows into a decision node
are only informational; they do not represent constraints on what the decision
maker can do. We can represent such constraints only indirectly, using arrows
to the children of the decision and sometimes adding more variables to the
influence diagram, thus making the ID more complicated. Users of influence
diagrams often want to represent constraints by arrows into decision nodes. We
represent constraints on decisions by allowing relevance arrows into decision
nodes. We call the resulting representation information/relevance influence
diagrams (IRIDs). Information/relevance influence diagrams allow for direct
representation and specification of constrained decisions. We use a combination
of stochastic dynamic programming and Gibbs sampling to solve IRIDs. This
method is especially useful when exact methods for solving IDs fail.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Stochastic Simulation Algorithms for Dynamic Probabilistic Networks,"Stochastic simulation algorithms such as likelihood weighting often give
fast, accurate approximations to posterior probabilities in probabilistic
networks, and are the methods of choice for very large networks. Unfortunately,
the special characteristics of dynamic probabilistic networks (DPNs), which are
used to represent stochastic temporal processes, mean that standard simulation
algorithms perform very poorly. In essence, the simulation trials diverge
further and further from reality as the process is observed over time. In this
paper, we present simulation algorithms that use the evidence observed at each
time step to push the set of trials back towards reality. The first algorithm,
""evidence reversal"" (ER) restructures each time slice of the DPN so that the
evidence nodes for the slice become ancestors of the state variables. The
second algorithm, called ""survival of the fittest"" sampling (SOF),
""repopulates"" the set of trials at each time step using a stochastic
reproduction rate weighted by the likelihood of the evidence according to each
trial. We compare the performance of each algorithm with likelihood weighting
on the original network, and also investigate the benefits of combining the ER
and SOF methods. The ER/SOF combination appears to maintain bounded error
independent of the number of time steps in the simulation.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Probabilistic Exploration in Planning while Learning,"Sequential decision tasks with incomplete information are characterized by
the exploration problem; namely the trade-off between further exploration for
learning more about the environment and immediate exploitation of the accrued
information for decision-making. Within artificial intelligence, there has been
an increasing interest in studying planning-while-learning algorithms for these
decision tasks. In this paper we focus on the exploration problem in
reinforcement learning and Q-learning in particular. The existing exploration
strategies for Q-learning are of a heuristic nature and they exhibit limited
scaleability in tasks with large (or infinite) state and action spaces.
Efficient experimentation is needed for resolving uncertainties when possible
plans are compared (i.e. exploration). The experimentation should be sufficient
for selecting with statistical significance a locally optimal plan (i.e.
exploitation). For this purpose, we develop a probabilistic hill-climbing
algorithm that uses a statistical selection procedure to decide how much
exploration is needed for selecting a plan which is, with arbitrarily high
probability, arbitrarily close to a locally optimal one. Due to its generality
the algorithm can be employed for the exploration strategy of robust
Q-learning. An experiment on a relatively complex control task shows that the
proposed exploration strategy performs better than a typical exploration
strategy.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"On the Detection of Conflicts in Diagnostic Bayesian Networks Using
  Abstraction","An important issue in the use of expert systems is the so-called brittleness
problem. Expert systems model only a limited part of the world. While the
explicit management of uncertainty in expert systems itigates the brittleness
problem, it is still possible for a system to be used, unwittingly, in ways
that the system is not prepared to address. Such a situation may be detected by
the method of straw models, first presented by Jensen et al. [1990] and later
generalized and justified by Laskey [1991]. We describe an algorithm, which we
have implemented, that takes as input an annotated diagnostic Bayesian network
(the base model) and constructs, without assistance, a bipartite network to be
used as a straw model. We show that in some cases this straw model is better
that the independent straw model of Jensen et al., the only other straw model
for which a construction algorithm has been designed and implemented.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
HUGS: Combining Exact Inference and Gibbs Sampling in Junction Trees,"Dawid, Kjaerulff and Lauritzen (1994) provided a preliminary description of a
hybrid between Monte-Carlo sampling methods and exact local computations in
junction trees. Utilizing the strengths of both methods, such hybrid inference
methods has the potential of expanding the class of problems which can be
solved under bounded resources as well as solving problems which otherwise
resist exact solutions. The paper provides a detailed description of a
particular instance of such a hybrid scheme; namely, combination of exact
inference and Gibbs sampling in discrete Bayesian networks. We argue that this
combination calls for an extension of the usual message passing scheme of
ordinary junction trees.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Sensitivities: An Alternative to Conditional Probabilities for Bayesian
  Belief Networks","We show an alternative way of representing a Bayesian belief network by
sensitivities and probability distributions. This representation is equivalent
to the traditional representation by conditional probabilities, but makes
dependencies between nodes apparent and intuitively easy to understand. We also
propose a QR matrix representation for the sensitivities and/or conditional
probabilities which is more efficient, in both memory requirements and
computational speed, than the traditional representation for computer-based
implementations of probabilistic inference. We use sensitivities to show that
for a certain class of binary networks, the computation time for approximate
probabilistic inference with any positive upper bound on the error of the
result is independent of the size of the network. Finally, as an alternative to
traditional algorithms that use conditional probabilities, we describe an exact
algorithm for probabilistic inference that uses the QR-representation for
sensitivities and updates probability distributions of nodes in a network
according to messages from the neighbors.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Is There a Role for Qualitative Risk Assessment?,"Classically, risk is characterized by a point value probability indicating
the likelihood of occurrence of an adverse effect. However, there are domains
where the attainability of objective numerical risk characterizations is
increasingly being questioned. This paper reviews the arguments in favour of
extending classical techniques of risk assessment to incorporate meaningful
qualitative and weak quantitative risk characterizations. A technique in which
linguistic uncertainty terms are defined in terms of patterns of argument is
then proposed. The technique is demonstrated using a prototype computer-based
system for predicting the carcinogenic risk due to novel chemical compounds.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
On the Complexity of Solving Markov Decision Problems,"Markov decision problems (MDPs) provide the foundations for a number of
problems of interest to AI researchers studying automated planning and
reinforcement learning. In this paper, we summarize results regarding the
complexity of solving MDPs and the running time of MDP solution algorithms. We
argue that, although MDPs can be solved efficiently in theory, more study is
needed to reveal practical algorithms for solving large problems quickly. To
encourage future research, we sketch some alternative methods of analysis that
rely on the structure of MDPs.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Causal Inference and Causal Explanation with Background Knowledge,"This paper presents correct algorithms for answering the following two
questions; (i) Does there exist a causal explanation consistent with a set of
background knowledge which explains all of the observed independence facts in a
sample? (ii) Given that there is such a causal explanation what are the causal
relationships common to every such causal explanation?","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Strong Completeness and Faithfulness in Bayesian Networks,"A completeness result for d-separation applied to discrete Bayesian networks
is presented and it is shown that in a strong measure-theoretic sense almost
all discrete distributions for a given network structure are faithful; i.e. the
independence facts true of the distribution are all and only those entailed by
the network structure.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"A Theoretical Framework for Context-Sensitive Temporal Probability Model
  Construction with Application to Plan Projection","We define a context-sensitive temporal probability logic for representing
classes of discrete-time temporal Bayesian networks. Context constraints allow
inference to be focused on only the relevant portions of the probabilistic
knowledge. We provide a declarative semantics for our language. We present a
Bayesian network construction algorithm whose generated networks give sound and
complete answers to queries. We use related concepts in logic programming to
justify our approach. We have implemented a Bayesian network construction
algorithm for a subset of the theory and demonstrate it's application to the
problem of evaluating the effectiveness of treatments for acute cardiac
conditions.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Refining Reasoning in Qualitative Probabilistic Networks,"In recent years there has been a spate of papers describing systems for
probabilisitic reasoning which do not use numerical probabilities. In some
cases the simple set of values used by these systems make it impossible to
predict how a probability will change or which hypothesis is most likely given
certain evidence. This paper concentrates on such situations, and suggests a
number of ways in which they may be resolved by refining the representation.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"On the Testability of Causal Models with Latent and Instrumental
  Variables","Certain causal models involving unmeasured variables induce no independence
constraints among the observed variables but imply, nevertheless, inequality
contraints on the observed distribution. This paper derives a general formula
for such instrumental variables, that is, exogenous variables that directly
affect some variables but not all. With the help of this formula, it is
possible to test whether a model involving instrumental variables may account
for the data, or, conversely, whether a given variables can be deemed
instrumental.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Probabilistic Evaluation of Sequential Plans from Causal Models with
  Hidden Variables","The paper concerns the probabilistic evaluation of plans in the presence of
unmeasured variables, each plan consisting of several concurrent or sequential
actions. We establish a graphical criterion for recognizing when the effects of
a given plan can be predicted from passive observations on measured variables
only. When the criterion is satisfied, a closed-form expression is provided for
the probability that the plan will achieve a specified goal.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Exploiting the Rule Structure for Decision Making within the Independent
  Choice Logic","This paper introduces the independent choice logic, and in particular the
""single agent with nature"" instance of the independent choice logic, namely
ICLdt. This is a logical framework for decision making uncertainty that extends
both logic programming and stochastic models such as influence diagrams. This
paper shows how the representation of a decision problem within the independent
choice logic can be exploited to cut down the combinatorics of dynamic
programming. One of the main problems with influence diagram evaluation
techniques is the need to optimise a decision for all values of the 'parents'
of a decision variable. In this paper we show how the rule based nature of the
ICLdt can be exploited so that we only make distinctions in the values of the
information available for a decision that will make a difference to utility.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Abstraction in Belief Networks: The Role of Intermediate States in
  Diagnostic Reasoning","Bayesian belief networks are bing increasingly used as a knowledge
representation for diagnostic reasoning. One simple method for conducting
diagnostic reasoning is to represent system faults and observations only. In
this paper, we investigate how having intermediate nodes-nodes other than fault
and observation nodes affects the diagnostic performance of a Bayesian belief
network. We conducted a series of experiments on a set of real belief networks
for medical diagnosis in liver and bile disease. We compared the effects on
diagnostic performance of a two-level network consisting just of disease and
finding nodes with that of a network which models intermediate
pathophysiological disease states as well. We provide some theoretical evidence
for differences observed between the abstracted two-level network and the full
network.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Accounting for Context in Plan Recognition, with Application to Traffic
  Monitoring","Typical approaches to plan recognition start from a representation of an
agent's possible plans, and reason evidentially from observations of the
agent's actions to assess the plausibility of the various candidates. A more
expansive view of the task (consistent with some prior work) accounts for the
context in which the plan was generated, the mental state and planning process
of the agent, and consequences of the agent's actions in the world. We present
a general Bayesian framework encompassing this view, and focus on how context
can be exploited in plan recognition. We demonstrate the approach on a problem
in traffic monitoring, where the objective is to induce the plan of the driver
from observation of vehicle movements. Starting from a model of how the driver
generates plans, we show how the highway context can appropriately influence
the recognizer's interpretation of observed driver behavior.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
A New Pruning Method for Solving Decision Trees and Game Trees,"The main goal of this paper is to describe a new pruning method for solving
decision trees and game trees. The pruning method for decision trees suggests a
slight variant of decision trees that we call scenario trees. In scenario
trees, we do not need a conditional probability for each edge emanating from a
chance node. Instead, we require a joint probability for each path from the
root node to a leaf node. We compare the pruning method to the traditional
rollback method for decision trees and game trees. For problems that require
Bayesian revision of probabilities, a scenario tree representation with the
pruning method is more efficient than a decision tree representation with the
rollback method. For game trees, the pruning method is more efficient than the
rollback method.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Directed Cyclic Graphical Representations of Feedback Models,"The use of directed acyclic graphs (DAGs) to represent conditional
independence relations among random variables has proved fruitful in a variety
of ways. Recursive structural equation models are one kind of DAG model.
However, non-recursive structural equation models of the kinds used to model
economic processes are naturally represented by directed cyclic graphs with
independent errors, a characterization of conditional independence errors, a
characterization of conditional independence constraints is obtained, and it is
shown that the result generalizes in a natural way to systems in which the
error variables or noises are statistically dependent. For non-linear systems
with independent errors a sufficient condition for conditional independence of
variables in associated distributions is obtained.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Causal Inference in the Presence of Latent Variables and Selection Bias,"We show that there is a general, informative and reliable procedure for
discovering causal relations when, for all the investigator knows, both latent
variables and selection bias may be at work. Given information about
conditional independence and dependence relations between measured variables,
even when latent variables and selection bias may be present, there are
sufficient conditions for reliably concluding that there is a causal path from
one variable to another, and sufficient conditions for reliably concluding when
no such causal path exists.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Modeling Failure Priors and Persistence in Model-Based Diagnosis,"Probabilistic model-based diagnosis computes the posterior probabilities of
failure of components from the prior probabilities of component failure and
observations of system behavior. One problem with this method is that such
priors are almost never directly available. One of the reasons is that the
prior probability estimates include an implicit notion of a time interval over
which they are specified -- for example, if the probability of failure of a
component is 0.05, is this over the period of a day or is this over a week? A
second problem facing probabilistic model-based diagnosis is the modeling of
persistence. Say we have an observation about a system at time t_1 and then
another observation at a later time t_2. To compute posterior probabilities
that take into account both the observations, we need some model of how the
state of the system changes from time t_1 to t_2. In this paper, we address
these problems using techniques from Reliability theory. We show how to compute
the failure prior of a component from an empirical measure of its reliability
-- the Mean Time Between Failure (MTBF). We also develop a scheme to model
persistence when handling multiple time tagged observations.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"A Polynomial Algorithm for Computing the Optimal Repair Strategy in a
  System with Independent Component Failures","The goal of diagnosis is to compute good repair strategies in response to
anomalous system behavior. In a decision theoretic framework, a good repair
strategy has low expected cost. In a general formulation of the problem, the
computation of the optimal (lowest expected cost) repair strategy for a system
with multiple faults is intractable. In this paper, we consider an interesting
and natural restriction on the behavior of the system being diagnosed: (a) the
system exhibits faulty behavior if and only if one or more components is
malfunctioning. (b) The failures of the system components are independent.
Given this restriction on system behavior, we develop a polynomial time
algorithm for computing the optimal repair strategy. We then go on to introduce
a system hierarchy and the notion of inspecting (testing) components before
repair. We develop a linear time algorithm for computing an optimal repair
strategy for the hierarchical system which includes both repair and inspection.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Exploiting System Hierarchy to Compute Repair Plans in Probabilistic
  Model-based Diagnosis","The goal of model-based diagnosis is to isolate causes of anomalous system
behavior and recommend inexpensive repair actions in response. In general,
precomputing optimal repair policies is intractable. To date, investigators
addressing this problem have explored approximations that either impose
restrictions on the system model (such as a single fault assumption) or compute
an immediate best action with limited lookahead. In this paper, we develop a
formulation of repair in model-based diagnosis and a repair algorithm that
computes optimal sequences of actions. This optimal approach is costly but can
be applied to precompute an optimal repair strategy for compact systems. We
show how we can exploit a hierarchical system specification to make this
approach tractable for large systems. When introducing hierarchy, we also
consider the tradeoff between simply replacing a component and decomposing it
to repair its subcomponents. The hierarchical repair algorithm is suitable for
off-line precomputation of an optimal repair strategy. A modification of the
algorithm takes advantage of an iterative deepening scheme to trade off
inference time and the quality of the computed strategy.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Path Planning under Time-Dependent Uncertainty,"Standard algorithms for finding the shortest path in a graph require that the
cost of a path be additive in edge costs, and typically assume that costs are
deterministic. We consider the problem of uncertain edge costs, with potential
probabilistic dependencies among the costs. Although these dependencies violate
the standard dynamic-programming decomposition, we identify a weaker stochastic
consistency condition that justifies a generalized dynamic-programming approach
based on stochastic dominance. We present a revised path-planning algorithm and
prove that it produces optimal paths under time-dependent uncertain costs. We
test the algorithm by applying it to a model of stochastic bus networks, and
present empirical performance results comparing it to some alternatives.
Finally, we consider extensions of these concepts to a more general class of
problems of heuristic search under uncertainty.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Defaults and Infinitesimals: Defeasible Inference by Nonarchimedean
  Entropy-Maximization","We develop a new semantics for defeasible inference based on extended
probability measures allowed to take infinitesimal values, on the
interpretation of defaults as generalized conditional probability constraints
and on a preferred-model implementation of entropy maximization.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
An Order of Magnitude Calculus,"This paper develops a simple calculus for order of magnitude reasoning. A
semantics is given with soundness and completeness results. Order of magnitude
probability functions are easily defined and turn out to be equivalent to kappa
functions, which are slight generalizations of Spohn's Natural Conditional
Functions. The calculus also gives rise to an order of magnitude decision
theory, which can be used to justify an amended version of Pearl's decision
theory for kappa functions, although the latter is weaker and less expressive.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
A Method for Implementing a Probabilistic Model as a Relational Database,"This paper discusses a method for implementing a probabilistic inference
system based on an extended relational data model. This model provides a
unified approach for a variety of applications such as dynamic programming,
solving sparse linear equations, and constraint propagation. In this framework,
the probability model is represented as a generalized relational database.
Subsequent probabilistic requests can be processed as standard relational
queries. Conventional database management systems can be easily adopted for
implementing such an approximate reasoning system.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"Optimization of Inter-Subnet Belief Updating in Multiply Sectioned
  Bayesian Networks","Recent developments show that Multiply Sectioned Bayesian Networks (MSBNs)
can be used for diagnosis of natural systems as well as for model-based
diagnosis of artificial systems. They can be applied to single-agent oriented
reasoning systems as well as multi-agent distributed probabilistic reasoning
systems. Belief propagation between a pair of subnets plays a central role in
maintenance of global consistency in a MSBN. This paper studies the operation
UpdateBelief, presented originally with MSBNs, for inter-subnet propagation. We
analyze how the operation achieves its intended functionality, which provides
hints as for how its efficiency can be improved. We then define two new
versions of UpdateBelief that reduce the computation time for inter-subnet
propagation. One of them is optimal in the sense that the minimum amount of
computation for coordinating multi-linkage belief propagation is required. The
optimization problem is solved through the solution of a graph-theoretic
problem: the minimum weight open tour in a tree.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Generating Explanations for Evidential Reasoning,"In this paper, we present two methods to provide explanations for reasoning
with belief functions in the valuation-based systems. One approach, inspired by
Strat's method, is based on sensitivity analysis, but its computation is
simpler thus easier to implement than Strat's. The other one is to examine the
impact of evidence on the conclusion based on the measure of the information
content in the evidence. We show the property of additivity for the pieces of
evidence that are conditional independent within the context of the
valuation-based systems. We will give an example to show how these approaches
are applied in an evidential network.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
Inference with Causal Independence in the CPSC Network,"This paper reports experiments with the causal independence inference
algorithm proposed by Zhang and Poole (1994b) on the CPSC network created by
Pradhan et al. (1994). It is found that the algorithm is able to answer 420 of
the 422 possible zero-observation queries, 94 of 100 randomly generated
five-observation queries, 87 of 100 randomly generated ten-observation queries,
and 69 of 100 randomly generated twenty-observation queries.","Appears in Proceedings of the Eleventh Conference on Uncertainty in
  Artificial Intelligence (UAI1995)"
"An Ontology Construction Approach for the Domain Of Poultry Science
  Using Protege","The information retrieval systems that are present nowadays are mainly based
on full text matching of keywords or topic based classification. This matching
of keywords often returns a large number of irrelevant information and this
does not meet the users query requirement. In order to solve this problem and
to enhance the search using semantic environment, a technique named ontology is
implemented for the field of poultry in this paper. Ontology is an emerging
technique in the current field of research in semantic environment. This paper
constructs ontology using the tool named Protege version 4.0 and this also
generates Resource Description Framework schema and XML scripts for using
poultry ontology in web.",arXiv admin note: text overlap with arXiv:1302.5215
Measuring Visual Complexity of Cluster-Based Visualizations,"Handling visual complexity is a challenging problem in visualization owing to
the subjectiveness of its definition and the difficulty in devising
generalizable quantitative metrics. In this paper we address this challenge by
measuring the visual complexity of two common forms of cluster-based
visualizations: scatter plots and parallel coordinatess. We conceptualize
visual complexity as a form of visual uncertainty, which is a measure of the
degree of difficulty for humans to interpret a visual representation correctly.
We propose an algorithm for estimating visual complexity for the aforementioned
visualizations using Allen's interval algebra. We first establish a set of
primitive 2-cluster cases in scatter plots and another set for parallel
coordinatess based on symmetric isomorphism. We confirm that both are the
minimal sets and verify the correctness of their members computationally. We
score the uncertainty of each primitive case based on its topological
properties, including the existence of overlapping regions, splitting regions
and meeting points or edges. We compare a few optional scoring schemes against
a set of subjective scores by humans, and identify the one that is the most
consistent with the subjective scores. Finally, we extend the 2-cluster measure
to k-cluster measure as a general purpose estimator of visual complexity for
these two forms of cluster-based visualization.",N/A
"Modification of conceptual clustering algorithm Cobweb for numerical
  data using fuzzy membership function","Modification of a conceptual clustering algorithm Cobweb for the purpose of
its application for numerical data is offered. Keywords: clustering, algorithm
Cobweb, numerical data, fuzzy membership function.",in Russian
A Modelling Approach Based on Fuzzy Agents,"Modelling of complex systems is mainly based on the decomposition of these
systems in autonomous elements, and the identification and definitio9n of
possible interactions between these elements. For this, the agent-based
approach is a modelling solution often proposed. Complexity can also be due to
external events or internal to systems, whose main characteristics are
uncertainty, imprecision, or whose perception is subjective (i.e. interpreted).
Insofar as fuzzy logic provides a solution for modelling uncertainty, the
concept of fuzzy agent can model both the complexity and uncertainty. This
paper focuses on introducing the concept of fuzzy agent: a classical
architecture of agent is redefined according to a fuzzy perspective. A
pedagogical illustration of fuzzy agentification of a smart watering system is
then proposed.","10 pages, 8 figures, 35 references"
"An Evaluation of an Algorithm for Inductive Learning of Bayesian Belief
  Networks Usin","Bayesian learning of belief networks (BLN) is a method for automatically
constructing belief networks (BNs) from data using search and Bayesian scoring
techniques. K2 is a particular instantiation of the method that implements a
greedy search strategy. To evaluate the accuracy of K2, we randomly generated a
number of BNs and for each of those we simulated data sets. K2 was then used to
induce the generating BNs from the simulated data. We examine the performance
of the program, and the factors that influence it. We also present a simple BN
model, developed from our results, which predicts the accuracy of K2, when
given various characteristics of the data set.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Probabilistic Constraint Satisfaction with Non-Gaussian Noise,"We have previously reported a Bayesian algorithm for determining the
coordinates of points in three-dimensional space from uncertain constraints.
This method is useful in the determination of biological molecular structure.
It is limited, however, by the requirement that the uncertainty in the
constraints be normally distributed. In this paper, we present an extension of
the original algorithm that allows constraint uncertainty to be represented as
a mixture of Gaussians, and thereby allows arbitrary constraint distributions.
We illustrate the performance of this algorithm on a problem drawn from the
domain of molecular structure determination, in which a multicomponent
constraint representation produces a much more accurate solution than the old
single component mechanism. The new mechanism uses mixture distributions to
decompose the problem into a set of independent problems with unimodal
constraint uncertainty. The results of the unimodal subproblems are
periodically recombined using Bayes' law, to avoid combinatorial explosion. The
new algorithm is particularly suited for parallel implementation.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Bayesian Method Reexamined,"This paper examines the ""K2"" network scoring metric of Cooper and Herskovits.
It shows counterintuitive results from applying this metric to simple networks.
One family of noninformative priors is suggested for assigning equal scores to
equivalent networks.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Laplace's Method Approximations for Probabilistic Inference in Belief
  Networks with Continuous Variables","Laplace's method, a family of asymptotic methods used to approximate
integrals, is presented as a potential candidate for the tool box of techniques
used for knowledge acquisition and probabilistic inference in belief networks
with continuous variables. This technique approximates posterior moments and
marginal posterior distributions with reasonable accuracy [errors are O(n^-2)
for posterior means] in many interesting cases. The method also seems promising
for computing approximations for Bayes factors for use in the context of model
selection, model uncertainty and mixtures of pdfs. The limitations, regularity
conditions and computational difficulties for the implementation of Laplace's
method are comparable to those associated with the methods of maximum
likelihood and posterior mode analysis.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Generating New Beliefs From Old,"In previous work [BGHK92, BGHK93], we have studied the random-worlds approach
-- a particular (and quite powerful) method for generating degrees of belief
(i.e., subjective probabilities) from a knowledge base consisting of objective
(first-order, statistical, and default) information. But allowing a knowledge
base to contain only objective information is sometimes limiting. We
occasionally wish to include information about degrees of belief in the
knowledge base as well, because there are contexts in which old beliefs
represent important information that should influence new beliefs. In this
paper, we describe three quite general techniques for extending a method that
generates degrees of belief from objective information to one that can make use
of degrees of belief as well. All of our techniques are bloused on well-known
approaches, such as cross-entropy. We discuss general connections between the
techniques and in particular show that, although conceptually and technically
quite different, all of the techniques give the same answer when applied to the
random-worlds method.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Counterfactual Probabilities: Computational Methods, Bounds and
  Applications","Evaluation of counterfactual queries (e.g., ""If A were true, would C have
been true?"") is important to fault diagnosis, planning, and determination of
liability. In this paper we present methods for computing the probabilities of
such queries using the formulation proposed in [Balke and Pearl, 1994], where
the antecedent of the query is interpreted as an external action that forces
the proposition A to be true. When a prior probability is available on the
causal mechanisms governing the domain, counterfactual probabilities can be
evaluated precisely. However, when causal knowledge is specified as conditional
probabilities on the observables, only bounds can computed. This paper develops
techniques for evaluating these bounds, and demonstrates their use in two
applications: (1) the determination of treatment efficacy from studies in which
subjects may choose their own treatment, and (2) the determination of liability
in product-safety litigation.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Modus Ponens Generating Function in the Class of ^-valuations of
  Plausibility","We discuss the problem of construction of inference procedures which can
manipulate with uncertainties measured in ordinal scales and fulfill to the
property of strict monotonicity of conclusion. The class of A-valuations of
plausibility is considered where operations based only on information about
linear ordering of plausibility values are used. In this class the modus ponens
generating function fulfiling to the property of strict monotonicity of
conclusions is introduced.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Possibility and Necessity Functions over Non-classical Logics,"We propose an integration of possibility theory into non-classical logics. We
obtain many formal results that generalize the case where possibility and
necessity functions are based on classical logic. We show how useful such an
approach is by applying it to reasoning under uncertain and inconsistent
information.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Exploratory Model Building,"Some instances of creative thinking require an agent to build and test
hypothetical theories. Such a reasoner needs to explore the space of not only
those situations that have occurred in the past, but also those that are
rationally conceivable. In this paper we present a formalism for exploring the
space of conceivable situation-models for those domains in which the knowledge
is primarily probabilistic in nature. The formalism seeks to construct
consistent, minimal, and desirable situation-descriptions by selecting suitable
domain-attributes and dependency relationships from the available domain
knowledge.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Planning with External Events,"I describe a planning methodology for domains with uncertainty in the form of
external events that are not completely predictable. The events are represented
by enabling conditions and probabilities of occurrence. The planner is
goal-directed and backward chaining, but the subgoals are suggested by
analyzing the probability of success of the partial plan rather than being
simply the open conditions of the operators in the plan. The partial plan is
represented as a Bayesian belief net to compute its probability of success.
Since calculating the probability of success of a plan can be very expensive I
introduce two other techniques for computing it, one that uses Monte Carlo
simulation to estimate it and one based on a Markov chain representation that
uses knowledge about the dependencies between the predicates describing the
domain.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Properties of Bayesian Belief Network Learning Algorithms,"Bayesian belief network learning algorithms have three basic components: a
measure of a network structure and a database, a search heuristic that chooses
network structures to be considered, and a method of estimating the probability
tables from the database. This paper contributes to all these three topics. The
behavior of the Bayesian measure of Cooper and Herskovits and a minimum
description length (MDL) measure are compared with respect to their properties
for both limiting size and finite size databases. It is shown that the MDL
measure has more desirable properties than the Bayesian measure when a
distribution is to be learned. It is shown that selecting belief networks with
certain minimallity properties is NP-hard. This result justifies the use of
search heuristics instead of exact algorithms for choosing network structures
to be considered. In some cases, a collection of belief networks can be
represented by a single belief network which leads to a new kind of probability
table estimation called smoothing. We argue that smoothing can be efficiently
implemented by incorporating it in the search heuristic. Experimental results
suggest that for learning probabilities of belief networks smoothing is
helpful.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Stratified Simulation Scheme for Inference in Bayesian Belief Networks,"Simulation schemes for probabilistic inference in Bayesian belief networks
offer many advantages over exact algorithms; for example, these schemes have a
linear and thus predictable runtime while exact algorithms have exponential
runtime. Experiments have shown that likelihood weighting is one of the most
promising simulation schemes. In this paper, we present a new simulation scheme
that generates samples more evenly spread in the sample space than the
likelihood weighting scheme. We show both theoretically and experimentally that
the stratified scheme outperforms likelihood weighting in average runtime and
error in estimates of beliefs.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Efficient Estimation of the Value of Information in Monte Carlo Models,"The expected value of information (EVI) is the most powerful measure of
sensitivity to uncertainty in a decision model: it measures the potential of
information to improve the decision, and hence measures the expected value of
outcome. Standard methods for computing EVI use discrete variables and are
computationally intractable for models that contain more than a few variables.
Monte Carlo simulation provides the basis for more tractable evaluation of
large predictive models with continuous and discrete variables, but so far
computation of EVI in a Monte Carlo setting also has appeared impractical. We
introduce an approximate approach based on pre-posterior analysis for
estimating EVI in Monte Carlo models. Our method uses a linear approximation to
the value function and multiple linear regression to estimate the linear model
from the samples. The approach is efficient and practical for extremely large
models. It allows easy estimation of EVI for perfect or partial information on
individual variables or on combinations of variables. We illustrate its
implementation within Demos (a decision modeling system), and its application
to a large model for crisis transportation planning.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Symbolic Probabilitistic Inference in Large BN2O Networks,"A BN2O network is a two level belief net in which the parent interactions are
modeled using the noisy-or interaction model. In this paper we discuss
application of the SPI local expression language to efficient inference in
large BN2O networks. In particular, we show that there is significant
structure, which can be exploited to improve over the Quickscore result. We
further describe how symbolic techniques can provide information which can
significantly reduce the computation required for computing all cause posterior
marginals. Finally, we present a novel approximation technique with preliminary
experimental results.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Action Networks: A Framework for Reasoning about Actions and Change
  under Uncertainty","This work proposes action networks as a semantically well-founded framework
for reasoning about actions and change under uncertainty. Action networks add
two primitives to probabilistic causal networks: controllable variables and
persistent variables. Controllable variables allow the representation of
actions as directly setting the value of specific events in the domain, subject
to preconditions. Persistent variables provide a canonical model of persistence
according to which both the state of a variable and the causal mechanism
dictating its value persist over time unless intervened upon by an action (or
its consequences). Action networks also allow different methods for quantifying
the uncertainty in causal relationships, which go beyond traditional
probabilistic quantification. This paper describes both recent results and work
in progress.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
On the Relation between Kappa Calculus and Probabilistic Reasoning,"We study the connection between kappa calculus and probabilistic reasoning in
diagnosis applications. Specifically, we abstract a probabilistic belief
network for diagnosing faults into a kappa network and compare the ordering of
faults computed using both methods. We show that, at least for the example
examined, the ordering of faults coincide as long as all the causal relations
in the original probabilistic network are taken into account. We also provide a
formal analysis of some network structures where the two methods will differ.
Both kappa rankings and infinitesimal probabilities have been used extensively
to study default reasoning and belief revision. But little has been done on
utilizing their connection as outlined above. This is partly because the
relation between kappa and probability calculi assumes that probabilities are
arbitrarily close to one (or zero). The experiments in this paper investigate
this relation when this assumption is not satisfied. The reported results have
important implications on the use of kappa rankings to enhance the knowledge
engineering of uncertainty models.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"A Structured, Probabilistic Representation of Action","When agents devise plans for execution in the real world, they face two
important forms of uncertainty: they can never have complete knowledge about
the state of the world, and they do not have complete control, as the effects
of their actions are uncertain. While most classical planning methods avoid
explicit uncertainty reasoning, we believe that uncertainty should be
explicitly represented and reasoned about. We develop a probabilistic
representation for states and actions, based on belief networks. We define
conditional belief nets (CBNs) to capture the probabilistic dependency of the
effects of an action upon the state of the world. We also use a CBN to
represent the intrinsic relationships among entities in the environment, which
persist from state to state. We present a simple projection algorithm to
construct the belief network of the state succeeding an action, using the
environment CBN model to infer indirect effects. We discuss how the qualitative
aspects of belief networks and CBNs make them appropriate for the various
stages of the problem solving process, from model construction to the design of
planning algorithms.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Integrating Planning and Execution in Stochastic Domains,"We investigate planning in time-critical domains represented as Markov
Decision Processes, showing that search based techniques can be a very powerful
method for finding close to optimal plans. To reduce the computational cost of
planning in these domains, we execute actions as we construct the plan, and
sacrifice optimality by searching to a fixed depth and using a heuristic
function to estimate the value of states. Although this paper concentrates on
the search algorithm, we also discuss ways of constructing heuristic functions
suitable for this approach. Our results show that by interleaving search and
execution, close to optimal policies can be found without the computational
requirements of other approaches.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Localized Partial Evaluation of Belief Networks,"Most algorithms for propagating evidence through belief networks have been
exact and exhaustive: they produce an exact (point-valued) marginal probability
for every node in the network. Often, however, an application will not need
information about every n ode in the network nor will it need exact
probabilities. We present the localized partial evaluation (LPE) propagation
algorithm, which computes interval bounds on the marginal probability of a
specified query node by examining a subset of the nodes in the entire network.
Conceptually, LPE ignores parts of the network that are ""too far away"" from the
queried node to have much impact on its value. LPE has the ""anytime"" property
of being able to produce better solutions (tighter intervals) given more time
to consider more of the network.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"A Probabilistic Model of Action for Least-Commitment Planning with
  Information Gather","AI planning algorithms have addressed the problem of generating sequences of
operators that achieve some input goal, usually assuming that the planning
agent has perfect control over and information about the world. Relaxing these
assumptions requires an extension to the action representation that allows
reasoning both about the changes an action makes and the information it
provides. This paper presents an action representation that extends the
deterministic STRIPS model, allowing actions to have both causal and
informational effects, both of which can be context dependent and noisy. We
also demonstrate how a standard least-commitment planning algorithm can be
extended to include informational actions and contingent execution.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Some Properties of Joint Probability Distributions,"Several Artificial Intelligence schemes for reasoning under uncertainty
explore either explicitly or implicitly asymmetries among probabilities of
various states of their uncertain domain models. Even though the correct
working of these schemes is practically contingent upon the existence of a
small number of probable states, no formal justification has been proposed of
why this should be the case. This paper attempts to fill this apparent gap by
studying asymmetries among probabilities of various states of uncertain models.
By rewriting the joint probability distribution over a model's variables into a
product of individual variables' prior and conditional probability
distributions, and applying central limit theorem to this product, we can
demonstrate that the probabilities of individual states of the model can be
expected to be drawn from highly skewed, log-normal distributions. With
sufficient asymmetry in individual prior and conditional probability
distributions, a small fraction of states can be expected to cover a large
portion of the total probability space with the remaining states having
practically negligible probability. Theoretical discussion is supplemented by
simulation results and an illustrative real-world example.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
An Ordinal View of Independence with Application to Plausible Reasoning,"An ordinal view of independence is studied in the framework of possibility
theory. We investigate three possible definitions of dependence, of increasing
strength. One of them is the counterpart to the multiplication law in
probability theory, and the two others are based on the notion of conditional
possibility. These two have enough expressive power to support the whole
possibility theory, and a complete axiomatization is provided for the strongest
one. Moreover we show that weak independence is well-suited to the problems of
belief change and plausible reasoning, especially to address the problem of
blocking of property inheritance in exception-tolerant taxonomic reasoning.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Penalty logic and its Link with Dempster-Shafer Theory,"Penalty logic, introduced by Pinkas, associates to each formula of a
knowledge base the price to pay if this formula is violated. Penalties may be
used as a criterion for selecting preferred consistent subsets in an
inconsistent knowledge base, thus inducing a non-monotonic inference relation.
A precise formalization and the main properties of penalty logic and of its
associated non-monotonic inference relation are given in the first part. We
also show that penalty logic and Dempster-Shafer theory are related, especially
in the infinitesimal case.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Value of Evidence on Influence Diagrams,"In this paper, we introduce evidence propagation operations on influence
diagrams and a concept of value of evidence, which measures the value of
experimentation. Evidence propagation operations are critical for the
computation of the value of evidence, general update and inference operations
in normative expert systems which are based on the influence diagram
(generalized Bayesian network) paradigm. The value of evidence allows us to
compute directly an outcome sensitivity, a value of perfect information and a
value of control which are used in decision analysis (the science of decision
making under uncertainty). More specifically, the outcome sensitivity is the
maximum difference among the values of evidence, the value of perfect
information is the expected value of the values of evidence, and the value of
control is the optimal value of the values of evidence. We also discuss an
implementation and a relative computational efficiency issues related to the
value of evidence and the value of perfect information.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Conditional Independence in Possibility Theory,"Possibilistic conditional independence is investigated: we propose a
definition of this notion similar to the one used in probability theory. The
links between independence and non-interactivity are investigated, and
properties of these relations are given. The influence of the conjunction used
to define a conditional measure of possibility is also highlighted: we examine
three types of conjunctions: Lukasiewicz - like T-norms, product-like T-norms
and the minimum operator.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Backward Simulation in Bayesian Networks,"Backward simulation is an approximate inference technique for Bayesian belief
networks. It differs from existing simulation methods in that it starts
simulation from the known evidence and works backward (i.e., contrary to the
direction of the arcs). The technique's focus on the evidence leads to improved
convergence in situations where the posterior beliefs are dominated by the
evidence rather than by the prior probabilities. Since this class of situations
is large, the technique may make practical the application of approximate
inference in Bayesian belief networks to many real-world problems.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"On Testing Whether an Embedded Bayesian Network Represents a Probability
  Model","Testing the validity of probabilistic models containing unmeasured (hidden)
variables is shown to be a hard task. We show that the task of testing whether
models are structurally incompatible with the data at hand, requires an
exponential number of independence evaluations, each of the form: ""X is
conditionally independent of Y, given Z."" In contrast, a linear number of such
evaluations is required to test a standard Bayesian network (one per vertex).
On the positive side, we show that if a network with hidden variables G has a
tree skeleton, checking whether G represents a given probability model P
requires the polynomial number of such independence evaluations. Moreover, we
provide an algorithm that efficiently constructs a tree-structured Bayesian
network (with hidden variables) that represents P if such a network exists, and
further recognizes when such a network does not exist.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Epsilon-Safe Planning,"We introduce an approach to high-level conditional planning we call
epsilon-safe planning. This probabilistic approach commits us to planning to
meet some specified goal with a probability of success of at least 1-epsilon
for some user-supplied epsilon. We describe several algorithms for epsilon-safe
planning based on conditional planners. The two conditional planners we discuss
are Peot and Smith's nonlinear conditional planner, CNLP, and our own linear
conditional planner, PLINTH. We present a straightforward extension to
conditional planners for which computing the necessary probabilities is simple,
employing a commonly-made but perhaps overly-strong independence assumption. We
also discuss a second approach to epsilon-safe planning which relaxes this
independence assumption, involving the incremental construction of a
probability dependence model in conjunction with the construction of the plan
graph.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Generating Bayesian Networks from Probability Logic Knowledge Bases,"We present a method for dynamically generating Bayesian networks from
knowledge bases consisting of first-order probability logic sentences. We
present a subset of probability logic sufficient for representing the class of
Bayesian networks with discrete-valued nodes. We impose constraints on the form
of the sentences that guarantee that the knowledge base contains all the
probabilistic information necessary to generate a network. We define the
concept of d-separation for knowledge bases and prove that a knowledge base
with independence conditions defined by d-separation is a complete
specification of a probability distribution. We present a network generation
algorithm that, given an inference problem in the form of a query Q and a set
of evidence E, generates a network to compute P(Q|E). We prove the algorithm to
be correct.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Abstracting Probabilistic Actions,"This paper discusses the problem of abstracting conditional probabilistic
actions. We identify two distinct types of abstraction: intra-action
abstraction and inter-action abstraction. We define what it means for the
abstraction of an action to be correct and then derive two methods of
intra-action abstraction and two methods of inter-action abstraction which are
correct according to this criterion. We illustrate the developed techniques by
applying them to actions described with the temporal action representation used
in the DRIPS decision-theoretic planner and we describe how the planner uses
abstraction to reduce the complexity of planning.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A New Look at Causal Independence,"Heckerman (1993) defined causal independence in terms of a set of temporal
conditional independence statements. These statements formalized certain types
of causal interaction where (1) the effect is independent of the order that
causes are introduced and (2) the impact of a single cause on the effect does
not depend on what other causes have previously been applied. In this paper, we
introduce an equivalent a temporal characterization of causal independence
based on a functional representation of the relationship between causes and the
effect. In this representation, the interaction between causes and effect can
be written as a nested decomposition of functions. Causal independence can be
exploited by representing this decomposition in the belief network, resulting
in representations that are more efficient for inference than general causal
models. We present empirical results showing the benefits of a
causal-independence representation for belief-network inference.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Learning Bayesian Networks: The Combination of Knowledge and Statistical
  Data","We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Our contributions are threefold. First, we identify two important
properties of metrics, which we call event equivalence and parameter
modularity. These properties have been mostly ignored, but when combined,
greatly simplify the encoding of a user's prior knowledge. In particular, a
user can express her knowledge-for the most part-as a single prior Bayesian
network for the domain. Second, we describe local search and annealing
algorithms to be used in conjunction with scoring metrics. In the special case
where each node has at most one parent, we show that heuristic search can be
replaced with a polynomial algorithm to identify the networks with the highest
score. Third, we describe a methodology for evaluating Bayesian-network
learning algorithms. We apply this approach to a comparison of metrics and
search procedures.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Decision-Based View of Causality,"Most traditional models of uncertainty have focused on the associational
relationship among variables as captured by conditional dependence. In order to
successfully manage intelligent systems for decision making, however, we must
be able to predict the effects of actions. In this paper, we attempt to unite
two branches of research that address such predictions: causal modeling and
decision analysis. First, we provide a definition of causal dependence in
decision-analytic terms, which we derive from consequences of causal dependence
cited in the literature. Using this definition, we show how causal dependence
can be represented within an influence diagram. In particular, we identify two
inadequacies of an ordinary influence diagram as a representation for cause. We
introduce a special class of influence diagrams, called causal influence
diagrams, which corrects one of these problems, and identify situations where
the other inadequacy can be eliminated. In addition, we describe the
relationships between Howard Canonical Form and existing graphical
representations of cause.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Probabilistic Description Logics,"On the one hand, classical terminological knowledge representation excludes
the possibility of handling uncertain concept descriptions involving, e.g.,
""usually true"" concept properties, generalized quantifiers, or exceptions. On
the other hand, purely numerical approaches for handling uncertainty in general
are unable to consider terminological knowledge. This paper presents the
language ACP which is a probabilistic extension of terminological logics and
aims at closing the gap between the two areas of research. We present the
formal semantics underlying the language ALUP and introduce the probabilistic
formalism that is based on classes of probabilities and is realized by means of
probabilistic constraints. Besides inferring implicitly existent probabilistic
relationships, the constraints guarantee terminological and probabilistic
consistency. Altogether, the new language ALUP applies to domains where both
term descriptions and uncertainty have to be handled.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"An Experimental Comparison of Numerical and Qualitative Probabilistic
  Reasoning","Qualitative and infinitesimal probability schemes are consistent with the
axioms of probability theory, but avoid the need for precise numerical
probabilities. Using qualitative probabilities could substantially reduce the
effort for knowledge engineering and improve the robustness of results. We
examine experimentally how well infinitesimal probabilities (the kappa-calculus
of Goldszmidt and Pearl) perform a diagnostic task - troubleshooting a car that
will not start by comparison with a conventional numerical belief network. We
found the infinitesimal scheme to be as good as the numerical scheme in
identifying the true fault. The performance of the infinitesimal scheme worsens
significantly for prior fault probabilities greater than 0.03. These results
suggest that infinitesimal probability methods may be of substantial practical
value for machine diagnosis with small prior fault probabilities.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"An Alternative Proof Method for Possibilistic Logic and its Application
  to Terminological Logics","Possibilistic logic, an extension of first-order logic, deals with
uncertainty that can be estimated in terms of possibility and necessity
measures. Syntactically, this means that a first-order formula is equipped with
a possibility degree or a necessity degree that expresses to what extent the
formula is possibly or necessarily true. Possibilistic resolution yields a
calculus for possibilistic logic which respects the semantics developed for
possibilistic logic. A drawback, which possibilistic resolution inherits from
classical resolution, is that it may not terminate if applied to formulas
belonging to decidable fragments of first-order logic. Therefore we propose an
alternative proof method for possibilistic logic. The main feature of this
method is that it completely abstracts from a concrete calculus but uses as
basic operation a test for classical entailment. We then instantiate
possibilistic logic with a terminological logic, which is a decidable subclass
o f first-order logic but nevertheless much more expressive than propositional
logic. This yields an extension of terminological logics towards the
representation of uncertain knowledge which is satisfactory from a semantic as
well as algorithmic point of view.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Possibilistic Conditioning and Propagation,"We give an axiomatization of confidence transfer - a known conditioning
scheme - from the perspective of expectation-based inference in the sense of
Gardenfors and Makinson. Then, we use the notion of belief independence to
""filter out"" different proposal s of possibilistic conditioning rules, all are
variations of confidence transfer. Among the three rules that we consider, only
Dempster's rule of conditioning passes the test of supporting the notion of
belief independence. With the use of this conditioning rule, we then show that
we can use local computation for computing desired conditional marginal
possibilities of the joint possibility satisfying the given constraints. It
turns out that our local computation scheme is already proposed by Shenoy.
However, our intuitions are completely different from that of Shenoy. While
Shenoy just defines a local computation scheme that fits his framework of
valuation-based systems, we derive that local computation scheme from II(,8) =
tI(,8 I a) * II(a) and appropriate independence assumptions, just like how the
Bayesians derive their local computation scheme.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
The Automated Mapping of Plans for Plan Recognition,"To coordinate with other agents in its environment, an agent needs models of
what the other agents are trying to do. When communication is impossible or
expensive, this information must be acquired indirectly via plan recognition.
Typical approaches to plan recognition start with a specification of the
possible plans the other agents may be following, and develop special
techniques for discriminating among the possibilities. Perhaps more desirable
would be a uniform procedure for mapping plans to general structures supporting
inference based on uncertain and incomplete observations. In this paper, we
describe a set of methods for converting plans represented in a flexible
procedural language to observation models represented as probabilistic belief
networks.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Logic for Default Reasoning About Probabilities,"A logic is defined that allows to express information about statistical
probabilities and about degrees of belief in specific propositions. By
interpreting the two types of probabilities in one common probability space,
the semantics given are well suited to model the influence of statistical
information on the formation of subjective beliefs. Cross entropy minimization
is a key element in these semantics, the use of which is justified by showing
that the resulting logic exhibits some very reasonable properties.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Optimal Junction Trees,"The paper deals with optimality issues in connection with updating beliefs in
networks. We address two processes: triangulation and construction of junction
trees. In the first part, we give a simple algorithm for constructing an
optimal junction tree from a triangulated network. In the second part, we argue
that any exact method based on local calculations must either be less efficient
than the junction tree method, or it has an optimality problem equivalent to
that of triangulation.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
From Influence Diagrams to Junction Trees,"We present an approach to the solution of decision problems formulated as
influence diagrams. This approach involves a special triangulation of the
underlying graph, the construction of a junction tree with special properties,
and a message passing algorithm operating on the junction tree for computation
of expected utilities and optimal decision policies.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Reduction of Computational Complexity in Bayesian Networks through
  Removal of Weak Dependencies","The paper presents a method for reducing the computational complexity of
Bayesian networks through identification and removal of weak dependencies
(removal of links from the (moralized) independence graph). The removal of a
small number of links may reduce the computational complexity dramatically,
since several fill-ins and moral links may be rendered superfluous by the
removal. The method is described in terms of impact on the independence graph,
the junction tree, and the potential functions associated with these. An
empirical evaluation of the method using large real-world networks demonstrates
the applicability of the method. Further, the method, which has been
implemented in Hugin, complements the approximation method suggested by Jensen
& Andersen (1990).","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Using New Data to Refine a Bayesian Network,"We explore the issue of refining an existent Bayesian network structure using
new data which might mention only a subset of the variables. Most previous
works have only considered the refinement of the network's conditional
probability parameters, and have not addressed the issue of refining the
network's structure. We develop a new approach for refining the network's
structure. Our approach is based on the Minimal Description Length (MDL)
principle, and it employs an adapted version of a Bayesian network learning
algorithm developed in our previous work. One of the adaptations required is to
modify the previous algorithm to account for the structure of the existent
network. The learning algorithm generates a partial network structure which can
then be used to improve the existent network. We also present experimental
evidence demonstrating the effectiveness of our approach.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Syntax-based Default Reasoning as Probabilistic Model-based Diagnosis,"We view the syntax-based approaches to default reasoning as a model-based
diagnosis problem, where each source giving a piece of information is
considered as a component. It is formalized in the ATMS framework (each source
corresponds to an assumption). We assume then that all sources are independent
and ""fail"" with a very small probability. This leads to a probability
assignment on the set of candidates, or equivalently on the set of consistent
environments. This probability assignment induces a Dempster-Shafer belief
function which measures the probability that a proposition can be deduced from
the evidence. This belief function can be used in several different ways to
define a non-monotonic consequence relation. We study and compare these
consequence relations. The -case of prioritized knowledge bases is briefly
considered.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Fuzzy Geometric Relations to Represent Hierarchical Spatial Information,"A model to represent spatial information is presented in this paper. It is
based on fuzzy constraints represented as fuzzy geometric relations that can be
hierarchically structured. The concept of spatial template is introduced to
capture the idea of interrelated objects in two-dimensional space. The
representation model is used to specify imprecise or vague information
consisting in relative locations and orientations of template objects. It is
shown in this paper how a template represented by this model can be matched
against a crisp situation to recognize a particular instance of this template.
Furthermore, the proximity measure (fuzzy measure) between the instance and the
template is worked out - this measure can be interpreted as a degree of
similarity. In this context, template recognition can be viewed as a case of
fuzzy pattern recognition. The results of this work have been implemented and
applied to a complex military problem from which this work originated.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Constructing Belief Networks to Evaluate Plans,"This paper examines the problem of constructing belief networks to evaluate
plans produced by an knowledge-based planner. Techniques are presented for
handling various types of complicating plan features. These include plans with
context-dependent consequences, indirect consequences, actions with
preconditions that must be true during the execution of an action,
contingencies, multiple levels of abstraction multiple execution agents with
partially-ordered and temporally overlapping actions, and plans which reference
specific times and time durations.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Operator Selection While Planning Under Uncertainty,"This paper describes the best first search strategy used by U-Plan (Mansell
1993a), a planning system that constructs quantitatively ranked plans given an
incomplete description of an uncertain environment. U-Plan uses uncertain and
incomplete evidence de scribing the environment, characterizes it using a
Dempster-Shafer interval, and generates a set of possible world states. Plan
construction takes place in an abstraction hierarchy where strategic decisions
are made before tactical decisions. Search through this abstraction hierarchy
is guided by a quantitative measure (expected fulfillment) based on decision
theory. The search strategy is best first with the provision to update expected
fulfillment and review previous decisions in the light of planning
developments. U-Plan generates multiple plans for multiple possible worlds, and
attempts to use existing plans for new world situations. A super-plan is then
constructed, based on merging the set of plans and appropriately timed
knowledge acquisition operators, which are used to decide between plan
alternatives during plan execution.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Model-Based Diagnosis with Qualitative Temporal Uncertainty,"In this paper we describe a framework for model-based diagnosis of dynamic
systems, which extends previous work in this field by using and expressing
temporal uncertainty in the form of qualitative interval relations a la Allen.
Based on a logical framework extended by qualitative and quantitative temporal
constraints we show how to describe behavioral models (both consistency- and
abductive-based), discuss how to use abstract observations and show how
abstract temporal diagnoses are computed. This yields an expressive framework,
which allows the representation of complex temporal behavior allowing us to
represent temporal uncertainty. Due to its abstraction capabilities computation
is made independent of the number of observations and time points in a temporal
setting. An example of hepatitis diagnosis is used throughout the paper.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Incremental Dynamic Construction of Layered Polytree Networks,"Certain classes of problems, including perceptual data understanding,
robotics, discovery, and learning, can be represented as incremental,
dynamically constructed belief networks. These automatically constructed
networks can be dynamically extended and modified as evidence of new
individuals becomes available. The main result of this paper is the incremental
extension of the singly connected polytree network in such a way that the
network retains its singly connected polytree structure after the changes. The
algorithm is deterministic and is guaranteed to have a complexity of single
node addition that is at most of order proportional to the number of nodes (or
size) of the network. Additional speed-up can be achieved by maintaining the
path information. Despite its incremental and dynamic nature, the algorithm can
also be used for probabilistic inference in belief networks in a fashion
similar to other exact inference algorithms.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Probabilistic Calculus of Actions,"We present a symbolic machinery that admits both probabilistic and causal
information about a given domain and produces probabilistic statements about
the effect of actions and the impact of observations. The calculus admits two
types of conditioning operators: ordinary Bayes conditioning, P(y|X = x), which
represents the observation X = x, and causal conditioning, P(y|do(X = x)), read
the probability of Y = y conditioned on holding X constant (at x) by deliberate
action. Given a mixture of such observational and causal sentences, together
with the topology of the causal graph, the calculus derives new conditional
probabilities of both types, thus enabling one to quantify the effects of
actions (and policies) from partially specified knowledge bases, such as
Bayesian networks in which some conditional probabilities may not be available.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Robust Planning in Uncertain Environments,"This paper describes a novel approach to planning which takes advantage of
decision theory to greatly improve robustness in an uncertain environment. We
present an algorithm which computes conditional plans of maximum expected
utility. This algorithm relies on a representation of the search space as an
AND/OR tree and employs a depth-limit to control computation costs. A numeric
robustness factor, which parameterizes the utility function, allows the user to
modulate the degree of risk-aversion employed by the planner. Via a look-ahead
search, the planning algorithm seeks to find an optimal plan using expected
utility as its optimization criterion. We present experimental results obtained
by applying our algorithm to a non-deterministic extension of the blocks world
domain. Our results demonstrate that the robustness factor governs the degree
of risk embodied in the conditional plans computed by our algorithm.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Anytime Decision Making with Imprecise Probabilities,"This paper examines methods of decision making that are able to accommodate
limitations on both the form in which uncertainty pertaining to a decision
problem can be realistically represented and the amount of computing time
available before a decision must be made. The methods are anytime algorithms in
the sense of Boddy and Dean 1991. Techniques are presented for use with Frisch
and Haddawy's [1992] anytime deduction system, with an anytime adaptation of
Nilsson's [1986] probabilistic logic, and with a probabilistic database model.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Knowledge Engineering for Large Belief Networks,"We present several techniques for knowledge engineering of large belief
networks (BNs) based on the our experiences with a network derived from a large
medical knowledge base. The noisyMAX, a generalization of the noisy-OR gate, is
used to model causal in dependence in a BN with multi-valued variables. We
describe the use of leak probabilities to enforce the closed-world assumption
in our model. We present Netview, a visualization tool based on causal
independence and the use of leak probabilities. The Netview software allows
knowledge engineers to dynamically view sub-networks for knowledge engineering,
and it provides version control for editing a BN. Netview generates
sub-networks in which leak probabilities are dynamically updated to reflect the
missing portions of the network.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Solving Asymmetric Decision Problems with Influence Diagrams,"While influence diagrams have many advantages as a representation framework
for Bayesian decision problems, they have a serious drawback in handling
asymmetric decision problems. To be represented in an influence diagram, an
asymmetric decision problem must be symmetrized. A considerable amount of
unnecessary computation may be involved when a symmetrized influence diagram is
evaluated by conventional algorithms. In this paper we present an approach for
avoiding such unnecessary computation in influence diagram evaluation.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Belief Maintenance in Bayesian Networks,"Bayesian Belief Networks (BBNs) are a powerful formalism for reasoning under
uncertainty but bear some severe limitations: they require a large amount of
information before any reasoning process can start, they have limited
contradiction handling capabilities, and their ability to provide explanations
for their conclusion is still controversial. There exists a class of reasoning
systems, called Truth Maintenance Systems (TMSs), which are able to deal with
partially specified knowledge, to provide well-founded explanation for their
conclusions, and to detect and handle contradictions. TMSs incorporating
measure of uncertainty are called Belief Maintenance Systems (BMSs). This paper
describes how a BMS based on probabilistic logic can be applied to BBNs, thus
introducing a new class of BBNs, called Ignorant Belief Networks, able to
incrementally deal with partially specified conditional dependencies, to
provide explanations, and to detect and handle contradictions.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Belief Updating by Enumerating High-Probability Independence-Based
  Assignments","Independence-based (IB) assignments to Bayesian belief networks were
originally proposed as abductive explanations. IB assignments assign fewer
variables in abductive explanations than do schemes assigning values to all
evidentially supported variables. We use IB assignments to approximate marginal
probabilities in Bayesian belief networks. Recent work in belief updating for
Bayes networks attempts to approximate posterior probabilities by finding a
small number of the highest probability complete (or perhaps evidentially
supported) assignments. Under certain assumptions, the probability mass in the
union of these assignments is sufficient to obtain a good approximation. Such
methods are especially useful for highly-connected networks, where the maximum
clique size or the cutset size make the standard algorithms intractable. Since
IB assignments contain fewer assigned variables, the probability mass in each
assignment is greater than in the respective complete assignment. Thus, fewer
IB assignments are sufficient, and a good approximation can be obtained more
efficiently. IB assignments can be used for efficiently approximating posterior
node probabilities even in cases which do not obey the rather strict skewness
assumptions used in previous research. Two algorithms for finding the high
probability IB assignments are suggested: one by doing a best-first heuristic
search, and another by special-purpose integer linear programming. Experimental
results show that this approach is feasible for highly connected belief
networks.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Global Conditioning for Probabilistic Inference in Belief Networks,"In this paper we propose a new approach to probabilistic inference on belief
networks, global conditioning, which is a simple generalization of Pearl's
(1986b) method of loopcutset conditioning. We show that global conditioning, as
well as loop-cutset conditioning, can be thought of as a special case of the
method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (199Oa;
1990b). Nonetheless, this approach provides new opportunities for parallel
processing and, in the case of sequential processing, a tradeoff of time for
memory. We also show how a hybrid method (Suermondt and others 1990) combining
loop-cutset conditioning with Jensen's method can be viewed within our
framework. By exploring the relationships between these methods, we develop a
unifying framework in which the advantages of each approach can be combined
successfully.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Belief Induced by the Partial Knowledge of the Probabilities,"We construct the belief function that quantifies the agent, beliefs about
which event of Q will occurred when he knows that the event is selected by a
chance set-up and that the probability function associated to the chance set up
is only partially known.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Ignorance and the Expressiveness of Single- and Set-Valued Probability
  Models of Belief","Over time, there have hen refinements in the way that probability
distributions are used for representing beliefs. Models which rely on single
probability distributions depict a complete ordering among the propositions of
interest, yet human beliefs are sometimes not completely ordered. Non-singleton
sets of probability distributions can represent partially ordered beliefs.
Convex sets are particularly convenient and expressive, but it is known that
there are reasonable patterns of belief whose faithful representation require
less restrictive sets. The present paper shows that prior ignorance about three
or more exclusive alternatives and the emergence of partially ordered beliefs
when evidence is obtained defy representation by any single set of
distributions, but yield to a representation baud on several uts. The partial
order is shown to be a partial qualitative probability which shares some
intuitively appealing attributes with probability distributions.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Probabilistic Approach to Hierarchical Model-based Diagnosis,"Model-based diagnosis reasons backwards from a functional schematic of a
system to isolate faults given observations of anomalous behavior. We develop a
fully probabilistic approach to model based diagnosis and extend it to support
hierarchical models. Our scheme translates the functional schematic into a
Bayesian network and diagnostic inference takes place in the Bayesian network.
A Bayesian network diagnostic inference algorithm is modified to take advantage
of the hierarchy to give computational gains.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
"Semigraphoids Are Two-Antecedental Approximations of Stochastic
  Conditional Independence Models","The semigraphoid closure of every couple of CI-statements (GI=conditional
independence) is a stochastic CI-model. As a consequence of this result it is
shown that every probabilistically sound inference rule for CI-model, having at
most two antecedents, is derivable from the semigraphoid inference rules. This
justifies the use of semigraphoids as approximations of stochastic CI-models in
probabilistic reasoning. The list of all 19 potential dominant elements of the
mentioned semigraphoid closure is given as a byproduct.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Exceptional Subclasses in Qualitative Probability,"System Z+ [Goldszmidt and Pearl, 1991, Goldszmidt, 1992] is a formalism for
reasoning with normality defaults of the form ""typically if phi then + (with
strength cf)"" where 6 is a positive integer. The system has a critical
shortcoming in that it does not sanction inheritance across exceptional
subclasses. In this paper we propose an extension to System Z+ that rectifies
this shortcoming by extracting additional conditions between worlds from the
defaults database. We show that the additional constraints do not change the
notion of the consistency of a database. We also make comparisons with
competing default reasoning systems.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
A Defect in Dempster-Shafer Theory,"By analyzing the relationships among chance, weight of evidence and degree of
beliefwe show that the assertion ""probability functions are special cases of
belief functions"" and the assertion ""Dempster's rule can be used to combine
belief functions based on distinct bodies of evidence"" together lead to an
inconsistency in Dempster-Shafer theory. To solve this problem, we must reject
some fundamental postulates of the theory. We introduce a new approach for
uncertainty management that shares many intuitive ideas with D-S theory, while
avoiding this problem.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
State-space Abstraction for Anytime Evaluation of Probabilistic Networks,"One important factor determining the computational complexity of evaluating a
probabilistic network is the cardinality of the state spaces of the nodes. By
varying the granularity of the state spaces, one can trade off accuracy in the
result for computational efficiency. We present an anytime procedure for
approximate evaluation of probabilistic networks based on this idea. On
application to some simple networks, the procedure exhibits a smooth
improvement in approximation quality as computation time increases. This
suggests that state-space abstraction is one more useful control parameter for
designing real-time probabilistic reasoners.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
General Belief Measures,"Probability measures by themselves, are known to be inappropriate for
modeling the dynamics of plain belief and their excessively strong
measurability constraints make them unsuitable for some representational tasks,
e.g. in the context of firstorder knowledge. In this paper, we are therefore
going to look for possible alternatives and extensions. We begin by delimiting
the general area of interest, proposing a minimal list of assumptions to be
satisfied by any reasonable quasi-probabilistic valuation concept. Within this
framework, we investigate two particularly interesting kinds of quasi-measures
which are not or much less affected by the traditional problems. * Ranking
measures, which generalize Spohn-type and possibility measures. * Cumulative
measures, which combine the probabilistic and the ranking philosophy, allowing
thereby a fine-grained account of static and dynamic belief.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Generating Graphoids from Generalised Conditional Probability,"We take a general approach to uncertainty on product spaces, and give
sufficient conditions for the independence structures of uncertainty measures
to satisfy graphoid properties. Since these conditions are arguably more
intuitive than some of the graphoid properties, they can be viewed as
explanations why probability and certain other formalisms generate graphoids.
The conditions include a sufficient condition for the Intersection property
which can still apply even if there is a strong logical relations hip between
the variables. We indicate how these results can be used to produce theories of
qualitative conditional probability which are semi-graphoids and graphoids.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
On Axiomatization of Probabilistic Conditional Independencies,"This paper studies the connection between probabilistic conditional
independence in uncertain reasoning and data dependency in relational
databases. As a demonstration of the usefulness of this preliminary
investigation, an alternate proof is presented for refuting the conjecture
suggested by Pearl and Paz that probabilistic conditional independencies have a
complete axiomatization.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Evidential Reasoning with Conditional Belief Functions,"In the existing evidential networks with belief functions, the relations
among the variables are always represented by joint belief functions on the
product space of the involved variables. In this paper, we use conditional
belief functions to represent such relations in the network and show some
relations of these two kinds of representations. We also present a propagation
algorithm for such networks. By analyzing the properties of some special
evidential networks with conditional belief functions, we show that the
reasoning process can be simplified in such kinds of networks.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Inter-causal Independence and Heterogeneous Factorization,"It is well known that conditional independence can be used to factorize a
joint probability into a multiplication of conditional probabilities. This
paper proposes a constructive definition of inter-causal independence, which
can be used to further factorize a conditional probability. An inference
algorithm is developed, which makes use of both conditional independence and
inter-causal independence to reduce inference complexity in Bayesian networks.","Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)"
Causality in Bayesian Belief Networks,"We address the problem of causal interpretation of the graphical structure of
Bayesian belief networks (BBNs). We review the concept of causality explicated
in the domain of structural equations models and show that it is applicable to
BBNs. In this view, which we call mechanism-based, causality is defined within
models and causal asymmetries arise when mechanisms are placed in the context
of a system. We lay the link between structural equations models and BBNs
models and formulate the conditions under which the latter can be given causal
interpretation.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
From Conditional Oughts to Qualitative Decision Theory,"The primary theme of this investigation is a decision theoretic account of
conditional ought statements (e.g., ""You ought to do A, if C"") that rectifies
glaring deficiencies in classical deontic logic. The resulting account forms a
sound basis for qualitative decision theory, thus providing a framework for
qualitative planning under uncertainty. In particular, we show that adding
causal relationships (in the form of a single graph) as part of an epistemic
state is sufficient to facilitate the analysis of action sequences, their
consequences, their interaction with observations, their expected utilities
and, hence, the synthesis of plans and strategies under uncertainty.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A Probabilistic Algorithm for Calculating Structure: Borrowing from
  Simulated Annealing","We have developed a general Bayesian algorithm for determining the
coordinates of points in a three-dimensional space. The algorithm takes as
input a set of probabilistic constraints on the coordinates of the points, and
an a priori distribution for each point location. The output is a
maximum-likelihood estimate of the location of each point. We use the extended,
iterated Kalman filter, and add a search heuristic for optimizing its solution
under nonlinear conditions. This heuristic is based on the same principle as
the simulated annealing heuristic for other optimization problems. Our method
uses any probabilistic constraints that can be expressed as a function of the
point coordinates (for example, distance, angles, dihedral angles, and
planarity). It assumes that all constraints have Gaussian noise. In this paper,
we describe the algorithm and show its performance on a set of synthetic data
to illustrate its convergence properties, and its applicability to domains such
ng molecular structure determination.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A Study of Scaling Issues in Bayesian Belief Networks for Ship
  Classification","The problems associated with scaling involve active and challenging research
topics in the area of artificial intelligence. The purpose is to solve real
world problems by means of AI technologies, in cases where the complexity of
representation of the real world problem is potentially combinatorial. In this
paper, we present a novel approach to cope with the scaling issues in Bayesian
belief networks for ship classification. The proposed approach divides the
conceptual model of a complex ship classification problem into a set of small
modules that work together to solve the classification problem while preserving
the functionality of the original model. The possible ways of explaining sensor
returns (e.g., the evidence) for some features, such as portholes along the
length of a ship, are sometimes combinatorial. Thus, using an exhaustive
approach, which entails the enumeration of all possible explanations, is
impractical for larger problems. We present a network structure (referred to as
Sequential Decomposition, SD) in which each observation is associated with a
set of legitimate outcomes which are consistent with the explanation of each
observed piece of evidence. The results show that the SD approach allows one to
represent feature-observation relations in a manageable way and achieve the
same explanatory power as an exhaustive approach.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams,"This paper addresses the tradeoffs which need to be considered in reasoning
using probabilistic network representations, such as Influence Diagrams (IDs).
In particular, we examine the tradeoffs entailed in using Temporal Influence
Diagrams (TIDs) which adequately capture the temporal evolution of a dynamic
system without prohibitive data and computational requirements. Three
approaches for TID construction which make different tradeoffs are examined:
(1) tailoring the network at each time interval to the data available (rather
then just copying the original Bayes Network for all time intervals); (2)
modeling the evolution of a parsimonious subset of variables (rather than all
variables); and (3) model selection approaches, which seek to minimize some
measure of the predictive accuracy of the model without introducing too many
parameters, which might cause ""overfitting"" of the model. Methods of evaluating
the accuracy/efficiency of the tradeoffs are proposed.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
End-User Construction of Influence Diagrams for Bayesian Statistics,"Influence diagrams are ideal knowledge representations for Bayesian
statistical models. However, these diagrams are difficult for end users to
interpret and to manipulate. We present a user-based architecture that enables
end users to create and to manipulate the knowledge representation. We use the
problem of physicians' interpretation of two-arm parallel randomized clinical
trials (TAPRCT) to illustrate the architecture and its use. There are three
primary data structures. Elements of statistical models are encoded as
subgraphs of a restricted class of influence diagram. The interpretations of
those elements are mapped into users' language in a domain-specific, user-based
semantic interface, called a patient-flow diagram, in the TAPRCT problem.
Pennitted transformations of the statistical model that maintain the semantic
relationships of the model are encoded in a metadata-state diagram, called the
cohort-state diagram, in the TAPRCT problem. The algorithm that runs the system
uses modular actions called construction steps. This framework has been
implemented in a system called THOMAS, that allows physicians to interpret the
data reported from a TAPRCT.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Forecasting Sleep Apnea with Dynamic Network Models,"Dynamic network models (DNMs) are belief networks for temporal reasoning. The
DNM methodology combines techniques from time series analysis and probabilistic
reasoning to provide (1) a knowledge representation that integrates
noncontemporaneous and contemporaneous dependencies and (2) methods for
iteratively refining these dependencies in response to the effects of exogenous
influences. We use belief-network inference algorithms to perform forecasting,
control, and discrete event simulation on DNMs. The belief network formulation
allows us to move beyond the traditional assumptions of linearity in the
relationships among time-dependent variables and of normality in their
probability distributions. We demonstrate the DNM methodology on an important
forecasting problem in medicine. We conclude with a discussion of how the
methodology addresses several limitations found in traditional time series
analyses.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Normative Engineering Risk Management Systems,"This paper describes a normative system design that incorporates diagnosis,
dynamic evolution, decision making, and information gathering. A single
influence diagram demonstrates the design's coherence, yet each activity is
more effectively modeled and evaluated separately. Application to offshore oil
platforms illustrates the design. For this application, the normative system is
embedded in a real-time expert system.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Diagnosis of Multiple Faults: A Sensitivity Analysis,"We compare the diagnostic accuracy of three diagnostic inference models: the
simple Bayes model, the multimembership Bayes model, which is isomorphic to the
parallel combination function in the certainty-factor model, and a model that
incorporates the noisy OR-gate interaction. The comparison is done on 20
clinicopathological conference (CPC) cases from the American Journal of
Medicine-challenging cases describing actual patients often with multiple
disorders. We find that the distributions produced by the noisy OR model agree
most closely with the gold-standard diagnoses, although substantial differences
exist between the distributions and the diagnoses. In addition, we find that
the multimembership Bayes model tends to significantly overestimate the
posterior probabilities of diseases, whereas the simple Bayes model tends to
significantly underestimate the posterior probabilities. Our results suggest
that additional work to refine the noisy OR model for internal medicine will be
worthwhile.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Additive Belief-Network Models,"The inherent intractability of probabilistic inference has hindered the
application of belief networks to large domains. Noisy OR-gates [30] and
probabilistic similarity networks [18, 17] escape the complexity of inference
by restricting model expressiveness. Recent work in the application of
belief-network models to time-series analysis and forecasting [9, 10] has given
rise to the additive belief network model (ABNM). We (1) discuss the nature and
implications of the approximations made by an additive decomposition of a
belief network, (2) show greater efficiency in the induction of additive models
when available data are scarce, (3) generalize probabilistic inference
algorithms to exploit the additive decomposition of ABNMs, (4) show greater
efficiency of inference, and (5) compare results on inference with a simple
additive belief network.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate,"Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian
networks and proposed three models for the representation of conditional
probabilities. A forth model, shown here, assumes that the parameter
distribution is given by a product of Gaussian functions and updates them from
the _ and _r messages of evidence propagation. We also generalize the noisy
OR-gate for multivalued variables, develop the algorithm to compute probability
in time proportional to the number of parents (even in networks with loops) and
apply the learning model to this gate.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A fuzzy relation-based extension of Reggia's relational model for
  diagnosis handling uncertain and incomplete information","Relational models for diagnosis are based on a direct description of the
association between disorders and manifestations. This type of model has been
specially used and developed by Reggia and his co-workers in the late eighties
as a basic starting point for approaching diagnosis problems. The paper
proposes a new relational model which includes Reggia's model as a particular
case and which allows for a more expressive representation of the observations
and of the manifestations associated with disorders. The model distinguishes,
i) between manifestations which are certainly absent and those which are not
(yet) observed, and ii) between manifestations which cannot be caused by a
given disorder and manifestations for which we do not know if they can or
cannot be caused by this disorder. This new model, which can handle uncertainty
in a non-probabilistic way, is based on possibility theory and so-called
twofold fuzzy sets, previously introduced by the authors.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Dialectic Reasoning with Inconsistent Information,"From an inconsistent database non-trivial arguments may be constructed both
for a proposition, and for the contrary of that proposition. Therefore,
inconsistency in a logical database causes uncertainty about which conclusions
to accept. This kind of uncertainty is called logical uncertainty. We define a
concept of ""acceptability"", which induces a means for differentiating
arguments. The more acceptable an argument, the more confident we are in it. A
specific interest is to use the acceptability classes to assign linguistic
qualifiers to propositions, such that the qualifier assigned to a propositions
reflects its logical uncertainty. A more general interest is to understand how
classes of acceptability can be defined for arguments constructed from an
inconsistent database, and how this notion of acceptability can be devised to
reflect different criteria. Whilst concentrating on the aspects of assigning
linguistic qualifiers to propositions, we also indicate the more general
significance of the notion of acceptability.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Causal Independence for Knowledge Acquisition and Inference,"I introduce a temporal belief-network representation of causal independence
that a knowledge engineer can use to elicit probabilistic models. Like the
current, atemporal belief-network representation of causal independence, the
new representation makes knowledge acquisition tractable. Unlike the atemproal
representation, however, the temporal representation can simplify inference,
and does not require the use of unobservable variables. The representation is
less general than is the atemporal representation, but appears to be useful for
many practical applications.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Utility-Based Abstraction and Categorization,"We take a utility-based approach to categorization. We construct
generalizations about events and actions by considering losses associated with
failing to distinguish among detailed distinctions in a decision model. The
utility-based methods transform detailed states of the world into more abstract
categories comprised of disjunctions of the states. We show how we can cluster
distinctions into groups of distinctions at progressively higher levels of
abstraction, and describe rules for decision making with the abstractions. The
techniques introduce a utility-based perspective on the nature of concepts, and
provide a means of simplifying decision models used in automated reasoning
systems. We demonstrate the techniques by describing the capabilities and
output of TUBA, a program for utility-based abstraction.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Sensitivity Analysis for Probability Assessments in Bayesian Networks,"When eliciting probability models from experts, knowledge engineers may
compare the results of the model with expert judgment on test scenarios, then
adjust model parameters to bring the behavior of the model more in line with
the expert's intuition. This paper presents a methodology for analytic
computation of sensitivity values to measure the impact of small changes in a
network parameter on a target probability value or distribution. These values
can be used to guide knowledge elicitation. They can also be used in a gradient
descent algorithm to estimate parameter values that maximize a measure of
goodness-of-fit to both local and holistic probability assessments.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Causal Modeling,"Causal Models are like Dependency Graphs and Belief Nets in that they provide
a structure and a set of assumptions from which a joint distribution can, in
principle, be computed. Unlike Dependency Graphs, Causal Models are models of
hierarchical and/or parallel processes, rather than models of distributions
(partially) known to a model builder through some sort of gestalt. As such,
Causal Models are more modular, easier to build, more intuitive, and easier to
understand than Dependency Graph Models. Causal Models are formally defined and
Dependency Graph Models are shown to be a special case of them. Algorithms
supporting inference are presented. Parsimonious methods for eliciting
dependent probabilities are presented.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Some Complexity Considerations in the Combination of Belief Networks,"One topic that is likely to attract an increasing amount of attention within
the Knowledge-base systems research community is the coordination of
information provided by multiple experts. We envision a situation in which
several experts independently encode information as belief networks. A
potential user must then coordinate the conclusions and recommendations of
these networks to derive some sort of consensus. One approach to such a
consensus is the fusion of the contributed networks into a single, consensus
model prior to the consideration of any case-specific data (specific
observations, test results). This approach requires two types of combination
procedures, one for probabilities, and one for graphs. Since the combination of
probabilities is relatively well understood, the key barriers to this approach
lie in the realm of graph theory. This paper provides formal definitions of
some of the operations necessary to effect the necessary graphical
combinations, and provides complexity analyses of these procedures. The paper's
key result is that most of these operations are NP-hard, and its primary
message is that the derivation of ?good? consensus networks must be done
heuristically.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Deriving a Minimal I-map of a Belief Network Relative to a Target
  Ordering of its Nodes","This paper identifies and solves a new optimization problem: Given a belief
network (BN) and a target ordering on its variables, how can we efficiently
derive its minimal I-map whose arcs are consistent with the target ordering? We
present three solutions to this problem, all of which lead to directed acyclic
graphs based on the original BN's recursive basis relative to the specified
ordering (such a DAG is sometimes termed the boundary DAG drawn from the given
BN relative to the said ordering [5]). Along the way, we also uncover an
important general principal about arc reversals: when reordering a BN according
to some target ordering, (while attempting to minimize the number of arcs
generated), the sequence of arc reversals should follow the topological
ordering induced by the original belief network's arcs to as great an extent as
possible. These results promise to have a significant impact on the derivation
of consensus models, as well as on other algorithms that require the
reconfiguration and/or combination of BN's.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Probabilistic Conceptual Network: A Belief Representation Scheme for
  Utility-Based Categorization","Probabilistic conceptual network is a knowledge representation scheme
designed for reasoning about concepts and categorical abstractions in
utility-based categorization. The scheme combines the formalisms of abstraction
and inheritance hierarchies from artificial intelligence, and probabilistic
networks from decision analysis. It provides a common framework for
representing conceptual knowledge, hierarchical knowledge, and uncertainty. It
facilitates dynamic construction of categorization decision models at varying
levels of abstraction. The scheme is applied to an automated machining problem
for reasoning about the state of the machine at varying levels of abstraction
in support of actions for maintaining competitiveness of the plant.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Reasoning about the Value of Decision-Model Refinement: Methods and
  Application","We investigate the value of extending the completeness of a decision model
along different dimensions of refinement. Specifically, we analyze the expected
value of quantitative, conceptual, and structural refinement of decision
models. We illustrate the key dimensions of refinement with examples. The
analyses of value of model refinement can be used to focus the attention of an
analyst or an automated reasoning system on extensions of a decision model
associated with the greatest expected value.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Mixtures of Gaussians and Minimum Relative Entropy Techniques for
  Modeling Continuous Uncertainties","Problems of probabilistic inference and decision making under uncertainty
commonly involve continuous random variables. Often these are discretized to a
few points, to simplify assessments and computations. An alternative
approximation is to fit analytically tractable continuous probability
distributions. This approach has potential simplicity and accuracy advantages,
especially if variables can be transformed first. This paper shows how a
minimum relative entropy criterion can drive both transformation and fitting,
illustrating with a power and logarithm family of transformations and mixtures
of Gaussian (normal) distributions, which allow use of efficient influence
diagram methods. The fitting procedure in this case is the well-known EM
algorithm. The selection of the number of components in a fitted mixture
distribution is automated with an objective that trades off accuracy and
computational cost.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Valuation Networks and Conditional Independence,"Valuation networks have been proposed as graphical representations of
valuation-based systems (VBSs). The VBS framework is able to capture many
uncertainty calculi including probability theory, Dempster-Shafer's
belief-function theory, Spohn's epistemic belief theory, and Zadeh's
possibility theory. In this paper, we show how valuation networks encode
conditional independence relations. For the probabilistic case, the class of
probability models encoded by valuation networks includes undirected graph
models, directed acyclic graph models, directed balloon graph models, and
recursive causal graph models.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Relevant Explanations: Allowing Disjunctive Assignments,"Relevance-based explanation is a scheme in which partial assignments to
Bayesian belief network variables are explanations (abductive conclusions). We
allow variables to remain unassigned in explanations as long as they are
irrelevant to the explanation, where irrelevance is defined in terms of
statistical independence. When multiple-valued variables exist in the system,
especially when subsets of values correspond to natural types of events, the
over specification problem, alleviated by independence-based explanation,
resurfaces. As a solution to that, as well as for addressing the question of
explanation specificity, it is desirable to collapse such a subset of values
into a single value on the fly. The equivalent method, which is adopted here,
is to generalize the notion of assignments to allow disjunctive assignments. We
proceed to define generalized independence based explanations as maximum
posterior probability independence based generalized assignments (GIB-MAPs).
GIB assignments are shown to have certain properties that ease the design of
algorithms for computing GIB-MAPs. One such algorithm is discussed here, as
well as suggestions for how other algorithms may be adapted to compute
GIB-MAPs. GIB-MAP explanations still suffer from instability, a problem which
may be addressed using ?approximate? conditional independence as a condition
for irrelevance.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
A Generalization of the Noisy-Or Model,"The Noisy-Or model is convenient for describing a class of uncertain
relationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or
model for Boolean variables. Here we generalize the model to nary input and
output variables and to arbitrary functions other than the Boolean OR function.
This generalization is a useful modeling aid for construction of Bayesian
networks. We illustrate with some examples including digital circuit diagnosis
and network reliability analysis.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Using First-Order Probability Logic for the Construction of Bayesian
  Networks","We present a mechanism for constructing graphical models, specifically
Bayesian networks, from a knowledge base of general probabilistic information.
The unique feature of our approach is that it uses a powerful first-order
probabilistic logic for expressing the general knowledge base. This logic
allows for the representation of a wide range of logical and probabilistic
information. The model construction procedure we propose uses notions from
direct inference to identify pieces of local statistical information from the
knowledge base that are most appropriate to the particular event we want to
reason about. These pieces are composed to generate a joint probability
distribution specified as a Bayesian network. Although there are fundamental
difficulties in dealing with fully general knowledge, our procedure is
practical for quite rich knowledge bases and it supports the construction of a
far wider range of networks than allowed for by current template technology.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Representing and Reasoning With Probabilistic Knowledge: A Bayesian
  Approach","PAGODA (Probabilistic Autonomous Goal-Directed Agent) is a model for
autonomous learning in probabilistic domains [desJardins, 1992] that
incorporates innovative techniques for using the agent's existing knowledge to
guide and constrain the learning process and for representing, reasoning with,
and learning probabilistic knowledge. This paper describes the probabilistic
representation and inference mechanism used in PAGODA. PAGODA forms theories
about the effects of its actions and the world state on the environment over
time. These theories are represented as conditional probability distributions.
A restriction is imposed on the structure of the theories that allows the
inference mechanism to find a unique predicted distribution for any action and
world state description. These restricted theories are called uniquely
predictive theories. The inference mechanism, Probability Combination using
Independence (PCI), uses minimal independence assumptions to combine the
probabilities in a theory to make probabilistic predictions.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Graph-Grammar Assistance for Automated Generation of Influence Diagrams,"One of the most difficult aspects of modeling complex dilemmas in
decision-analytic terms is composing a diagram of relevance relations from a
set of domain concepts. Decision models in domains such as medicine, however,
exhibit certain prototypical patterns that can guide the modeling process.
Medical concepts can be classified according to semantic types that have
characteristic positions and typical roles in an influence-diagram model. We
have developed a graph-grammar production system that uses such inherent
interrelationships among medical terms to facilitate the modeling of medical
decisions.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Using Causal Information and Local Measures to Learn Bayesian Networks,"In previous work we developed a method of learning Bayesian Network models
from raw data. This method relies on the well known minimal description length
(MDL) principle. The MDL principle is particularly well suited to this task as
it allows us to tradeoff, in a principled way, the accuracy of the learned
network against its practical usefulness. In this paper we present some new
results that have arisen from our work. In particular, we present a new local
way of computing the description length. This allows us to make significant
improvements in our search algorithm. In addition, we modify our algorithm so
that it can take into account partial domain information that might be provided
by a domain expert. The local computation of description length also opens the
door for local refinement of an existent network. The feasibility of our
approach is demonstrated by experiments involving networks of a practical size.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Minimal Assumption Distribution Propagation in Belief Networks,"As belief networks are used to model increasingly complex situations, the
need to automatically construct them from large databases will become
paramount. This paper concentrates on solving a part of the belief network
induction problem: that of learning the quantitative structure (the conditional
probabilities), given the qualitative structure. In particular, a theory is
presented that shows how to propagate inference distributions in a belief
network, with the only assumption being that the given qualitative structure is
correct. Most inference algorithms must make at least this assumption. The
theory is based on four network transformations that are sufficient for any
inference in a belief network. Furthermore, the claim is made that contrary to
popular belief, error will not necessarily grow as the inference chain grows.
Instead, for QBN belief nets induced from large enough samples, the error is
more likely to decrease as the size of the inference chain increases.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"An Algorithm for the Construction of Bayesian Network Structures from
  Data","Previous algorithms for the construction of Bayesian belief network
structures from data have been either highly dependent on conditional
independence (CI) tests, or have required an ordering on the nodes to be
supplied by the user. We present an algorithm that integrates these two
approaches - CI tests are used to generate an ordering on the nodes from the
database which is then used to recover the underlying Bayesian network
structure using a non CI based method. Results of preliminary evaluation of the
algorithm on two networks (ALARM and LED) are presented. We also discuss some
algorithm performance issues and open problems.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A Construction of Bayesian Networks from Databases Based on an MDL
  Principle","This paper addresses learning stochastic rules especially on an
inter-attribute relation based on a Minimum Description Length (MDL) principle
with a finite number of examples, assuming an application to the design of
intelligent relational database systems. The stochastic rule in this paper
consists of a model giving the structure like the dependencies of a Bayesian
Belief Network (BBN) and some stochastic parameters each indicating a
conditional probability of an attribute value given the state determined by the
other attributes' values in the same record. Especially, we propose the
extended version of the algorithm of Chow and Liu in that our learning
algorithm selects the model in the range where the dependencies among the
attributes are represented by some general plural number of trees.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Knowledge-Based Decision Model Construction for Hierarchical Diagnosis:
  A Preliminary Report","Numerous methods for probabilistic reasoning in large, complex belief or
decision networks are currently being developed. There has been little research
on automating the dynamic, incremental construction of decision models. A
uniform value-driven method of decision model construction is proposed for the
hierarchical complete diagnosis. Hierarchical complete diagnostic reasoning is
formulated as a stochastic process and modeled using influence diagrams. Given
observations, this method creates decision models in order to obtain the best
actions sequentially for locating and repairing a fault at minimum cost. This
method construct decision models incrementally, interleaving probe actions with
model construction and evaluation. The method treats meta-level and baselevel
tasks uniformly. That is, the method takes a decision-theoretic look at the
control of search in causal pathways and structural hierarchies.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A Synthesis of Logical and Probabilistic Reasoning for Program
  Understanding and Debugging","We describe the integration of logical and uncertain reasoning methods to
identify the likely source and location of software problems. To date, software
engineers have had few tools for identifying the sources of error in complex
software packages. We describe a method for diagnosing software problems
through combining logical and uncertain reasoning analyses. Our preliminary
results suggest that such methods can be of value in directing the attention of
software engineers to paths of an algorithm that have the highest likelihood of
harboring a programming error.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"An Implementation of a Method for Computing the Uncertainty in Inferred
  Probabilities in Belief Networks","In recent years the belief network has been used increasingly to model
systems in Al that must perform uncertain inference. The development of
efficient algorithms for probabilistic inference in belief networks has been a
focus of much research in AI. Efficient algorithms for certain classes of
belief networks have been developed, but the problem of reporting the
uncertainty in inferred probabilities has received little attention. A system
should not only be capable of reporting the values of inferred probabilities
and/or the favorable choices of a decision; it should report the range of
possible error in the inferred probabilities and/or choices. Two methods have
been developed and implemented for determining the variance in inferred
probabilities in belief networks. These methods, the Approximate Propagation
Method and the Monte Carlo Integration Method are discussed and compared in
this paper.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Incremental Probabilistic Inference,"Propositional representation services such as truth maintenance systems offer
powerful support for incremental, interleaved, problem-model construction and
evaluation. Probabilistic inference systems, in contrast, have lagged behind in
supporting this incrementality typically demanded by problem solvers. The
problem, we argue, is that the basic task of probabilistic inference is
typically formulated at too large a grain-size. We show how a system built
around a smaller grain-size inference task can have the desired incrementality
and serve as the basis for a low-level (propositional) probabilistic
representation service.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Deliberation Scheduling for Time-Critical Sequential Decision Making,"We describe a method for time-critical decision making involving sequential
tasks and stochastic processes. The method employs several iterative refinement
routines for solving different aspects of the decision making problem. This
paper concentrates on the meta-level control problem of deliberation
scheduling, allocating computational resources to these routines. We provide
different models corresponding to optimization problems that capture the
different circumstances and computational strategies for decision making under
time constraints. We consider precursor models in which all decision making is
performed prior to execution and recurrent models in which decision making is
performed in parallel with execution, accounting for the states observed during
execution and anticipating future states. We describe algorithms for precursor
and recurrent models and provide the results of our empirical investigations to
date.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Intercausal Reasoning with Uninstantiated Ancestor Nodes,"Intercausal reasoning is a common inference pattern involving probabilistic
dependence of causes of an observed common effect. The sign of this dependence
is captured by a qualitative property called product synergy. The current
definition of product synergy is insufficient for intercausal reasoning where
there are additional uninstantiated causes of the common effect. We propose a
new definition of product synergy and prove its adequacy for intercausal
reasoning with direct and indirect evidence for the common effect. The new
definition is based on a new property matrix half positive semi-definiteness, a
weakened form of matrix positive semi-definiteness.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Inference Algorithms for Similarity Networks,"We examine two types of similarity networks each based on a distinct notion
of relevance. For both types of similarity networks we present an efficient
inference algorithm that works under the assumption that every event has a
nonzero probability of occurrence. Another inference algorithm is developed for
type 1 similarity networks that works under no restriction, albeit less
efficiently.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Two Procedures for Compiling Influence Diagrams,"Two algorithms are presented for ""compiling"" influence diagrams into a set of
simple decision rules. These decision rules define simple-to-execute, complete,
consistent, and near-optimal decision procedures. These compilation algorithms
can be used to derive decision procedures for human teams solving time
constrained decision problems.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
An efficient approach for finding the MPE in belief networks,"Given a belief network with evidence, the task of finding the I most probable
explanations (MPE) in the belief network is that of identifying and ordering
the I most probable instantiations of the non-evidence nodes of the belief
network. Although many approaches have been proposed for solving this problem,
most work only for restricted topologies (i.e., singly connected belief
networks). In this paper, we will present a new approach for finding I MPEs in
an arbitrary belief network. First, we will present an algorithm for finding
the MPE in a belief network. Then, we will present a linear time algorithm for
finding the next MPE after finding the first MPE. And finally, we will discuss
the problem of finding the MPE for a subset of variables of a belief network,
and show that the problem can be efficiently solved by this approach.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
A Method for Planning Given Uncertain and Incomplete Information,"This paper describes ongoing research into planning in an uncertain
environment. In particular, it introduces U-Plan, a planning system that
constructs quantitatively ranked plans given an incomplete description of the
state of the world. U-Plan uses a DempsterShafer interval to characterise
uncertain and incomplete information about the state of the world. The planner
takes as input what is known about the world, and constructs a number of
possible initial states with representations at different abstraction levels. A
plan is constructed for the initial state with the greatest support, and this
plan is tested to see if it will work for other possible initial states. All,
part, or none of the existing plans may be used in the generation of the plans
for the remaining possible worlds. Planning takes place in an abstraction
hierarchy where strategic decisions are made before tactical decisions. A
super-plan is then constructed, based on merging the set of plans and the
appropriately timed acquisition of essential knowledge, which is used to decide
between plan alternatives. U-Plan usually produces a super-plan in less time
than a classical planner would take to produce a set of plans, one for each
possible world.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
The use of conflicts in searching Bayesian networks,"This paper discusses how conflicts (as used by the consistency-based
diagnosis community) can be adapted to be used in a search-based algorithm for
computing prior and posterior probabilities in discrete Bayesian Networks. This
is an ""anytime"" algorithm, that at any stage can estimate the probabilities and
give an error bound. Whereas the most popular Bayesian net algorithms exploit
the structure of the network for efficiency, we exploit probability
distributions for efficiency; this algorithm is most suited to the case with
extreme probabilities. This paper presents a solution to the inefficiencies
found in naive algorithms, and shows how the tools of the consistency-based
diagnosis community (namely conflicts) can be used effectively to improve the
efficiency. Empirical results with networks having tens of thousands of nodes
are presented.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"GALGO: A Genetic ALGOrithm Decision Support Tool for Complex Uncertain
  Systems Modeled with Bayesian Belief Networks","Bayesian belief networks can be used to represent and to reason about complex
systems with uncertain, incomplete and conflicting information. Belief networks
are graphs encoding and quantifying probabilistic dependence and conditional
independence among variables. One type of reasoning of interest in diagnosis is
called abductive inference (determination of the global most probable system
description given the values of any partial subset of variables). In some
cases, abductive inference can be performed with exact algorithms using
distributed network computations but it is an NP-hard problem and complexity
increases drastically with the presence of undirected cycles, number of
discrete states per variable, and number of variables in the network. This
paper describes an approximate method based on genetic algorithms to perform
abductive inference in large, multiply connected networks for which complexity
is a concern when using most exact methods and for which systematic search
methods are not feasible. The theoretical adequacy of the method is discussed
and preliminary experimental results are presented.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Using Tree-Decomposable Structures to Approximate Belief Networks,"Tree structures have been shown to provide an efficient framework for
propagating beliefs [Pearl,1986]. This paper studies the problem of finding an
optimal approximating tree. The star decomposition scheme for sets of three
binary variables [Lazarsfeld,1966; Pearl,1986] is shown to enhance the class of
probability distributions that can support tree structures; such structures are
called tree-decomposable structures. The logarithm scoring rule is found to be
an appropriate optimality criterion to evaluate different tree-decomposable
structures. Characteristics of such structures closest to the actual belief
network are identified using the logarithm rule, and greedy and exact
techniques are developed to find the optimal approximation.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Using Potential Influence Diagrams for Probabilistic Inference and
  Decision Making","The potential influence diagram is a generalization of the standard
""conditional"" influence diagram, a directed network representation for
probabilistic inference and decision analysis [Ndilikilikesha, 1991]. It allows
efficient inference calculations corresponding exactly to those on undirected
graphs. In this paper, we explore the relationship between potential and
conditional influence diagrams and provide insight into the properties of the
potential influence diagram. In particular, we show how to convert a potential
influence diagram into a conditional influence diagram, and how to view the
potential influence diagram operations in terms of the conditional influence
diagram.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Deciding Morality of Graphs is NP-complete,"In order to find a causal explanation for data presented in the form of
covariance and concentration matrices it is necessary to decide if the graph
formed by such associations is a projection of a directed acyclic graph (dag).
We show that the general problem of deciding whether such a dag exists is
NP-complete.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"Incremental computation of the value of perfect information in
  stepwise-decomposable influence diagrams","To determine the value of perfect information in an influence diagram, one
needs first to modify the diagram to reflect the change in information
availability, and then to compute the optimal expected values of both the
original diagram and the modified diagram. The value of perfect information is
the difference between the two optimal expected values. This paper is about how
to speed up the computation of the optimal expected value of the modified
diagram by making use of the intermediate computation results obtained when
computing the optimal expected value of the original diagram.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Argumentative inference in uncertain and inconsistent knowledge bases,"This paper presents and discusses several methods for reasoning from
inconsistent knowledge bases. A so-called argumentative-consequence relation
taking into account the existence of consistent arguments in favor of a
conclusion and the absence of consistent arguments in favor of its contrary, is
particularly investigated. Flat knowledge bases, i.e. without any priority
between their elements, as well as prioritized ones where some elements are
considered as more strongly entrenched than others are studied under different
consequence relations. Lastly a paraconsistent-like treatment of prioritized
knowledge bases is proposed, where both the level of entrenchment and the level
of paraconsistency attached to a formula are propagated. The priority levels
are handled in the framework of possibility theory.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Argument Calculus and Networks,"A major reason behind the success of probability calculus is that it
possesses a number of valuable tools, which are based on the notion of
probabilistic independence. In this paper, I identify a notion of logical
independence that makes some of these tools available to a class of
propositional databases, called argument databases. Specifically, I suggest a
graphical representation of argument databases, called argument networks, which
resemble Bayesian networks. I also suggest an algorithm for reasoning with
argument networks, which resembles a basic algorithm for reasoning with
Bayesian networks. Finally, I show that argument networks have several
applications: Nonmonotonic reasoning, truth maintenance, and diagnosis.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Argumentation as a General Framework for Uncertain Reasoning,"Argumentation is the process of constructing arguments about propositions,
and the assignment of statements of confidence to those propositions based on
the nature and relative strength of their supporting arguments. The process is
modelled as a labelled deductive system, in which propositions are doubly
labelled with the grounds on which they are based and a representation of the
confidence attached to the argument. Argument construction is captured by a
generalized argument consequence relation based on the ^,--fragment of minimal
logic. Arguments can be aggregated by a variety of numeric and symbolic
flattening functions. This approach appears to shed light on the common logical
structure of a variety of quantitative, qualitative and defeasible uncertainty
calculi.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
On reasoning in networks with qualitative uncertainty,"In this paper some initial work towards a new approach to qualitative
reasoning under uncertainty is presented. This method is not only applicable to
qualitative probabilistic reasoning, as is the case with other methods, but
also allows the qualitative propagation within networks of values based upon
possibility theory and Dempster-Shafer evidence theory. The method is applied
to two simple networks from which a large class of directed graphs may be
constructed. The results of this analysis are used to compare the qualitative
behaviour of the three major quantitative uncertainty handling formalisms, and
to demonstrate that the qualitative integration of the formalisms is possible
under certain assumptions.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Qualitative Measures of Ambiguity,"This paper introduces a qualitative measure of ambiguity and analyses its
relationship with other measures of uncertainty. Probability measures relative
likelihoods, while ambiguity measures vagueness surrounding those judgments.
Ambiguity is an important representation of uncertain knowledge. It deals with
a different, type of uncertainty modeled by subjective probability or belief.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
"A Bayesian Variant of Shafer's Commonalities For Modelling Unforeseen
  Events","Shafer's theory of belief and the Bayesian theory of probability are two
alternative and mutually inconsistent approaches toward modelling uncertainty
in artificial intelligence. To help reduce the conflict between these two
approaches, this paper reexamines expected utility theory-from which Bayesian
probability theory is derived. Expected utility theory requires the decision
maker to assign a utility to each decision conditioned on every possible event
that might occur. But frequently the decision maker cannot foresee all the
events that might occur, i.e., one of the possible events is the occurrence of
an unforeseen event. So once we acknowledge the existence of unforeseen events,
we need to develop some way of assigning utilities to decisions conditioned on
unforeseen events. The commonsensical solution to this problem is to assign
similar utilities to events which are similar. Implementing this commonsensical
solution is equivalent to replacing Bayesian subjective probabilities over the
space of foreseen and unforeseen events by random set theory probabilities over
the space of foreseen events. This leads to an expected utility principle in
which normalized variants of Shafer's commonalities play the role of subjective
probabilities. Hence allowing for unforeseen events in decision analysis causes
Bayesian probability theory to become much more similar to Shaferian theory.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
The Probability of a Possibility: Adding Uncertainty to Default Rules,"We present a semantics for adding uncertainty to conditional logics for
default reasoning and belief revision. We are able to treat conditional
sentences as statements of conditional probability, and express rules for
revision such as ""If A were believed, then B would be believed to degree p.""
This method of revision extends conditionalization by allowing meaningful
revision by sentences whose probability is zero. This is achieved through the
use of counterfactual probabilities. Thus, our system accounts for the best
properties of qualitative methods of update (in particular, the AGM theory of
revision) and probabilistic methods. We also show how our system can be viewed
as a unification of probability theory and possibility theory, highlighting
their orthogonality and providing a means for expressing the probability of a
possibility. We also demonstrate the connection to Lewis's method of imaging.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Possibilistic decreasing persistence,"A key issue in the handling of temporal data is the treatment of persistence;
in most approaches it consists in inferring defeasible confusions by
extrapolating from the actual knowledge of the history of the world; we propose
here a gradual modelling of persistence, following the idea that persistence is
decreasing (the further we are from the last time point where a fluent is known
to be true, the less certainly true the fluent is); it is based on possibility
theory, which has strong relations with other well-known ordering-based
approaches to nonmonotonic reasoning. We compare our approach with Dean and
Kanazawa's probabilistic projection. We give a formal modelling of the
decreasing persistence problem. Lastly, we show how to infer nonmonotonic
conclusions using the principle of decreasing persistence.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Discounting and Combination Operations in Evidential Reasoning,"Evidential reasoning is now a leading topic in Artificial Intelligence.
Evidence is represented by a variety of evidential functions. Evidential
reasoning is carried out by certain kinds of fundamental operation on these
functions. This paper discusses two of the basic operations on evidential
functions, the discount operation and the well-known orthogonal sum operation.
We show that the discount operation is not commutative with the orthogonal sum
operation, and derive expressions for the two operations applied to the various
evidential function.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Probabilistic Assumption-Based Reasoning,"The classical propositional assumption-based model is extended to incorporate
probabilities for the assumptions. Then it is placed into the framework of
evidence theory. Several authors like Laskey, Lehner (1989) and Provan (1990)
already proposed a similar point of view, but the first paper is not as much
concerned with mathematical foundations, and Provan's paper develops into a
different direction. Here we thoroughly develop and present the mathematical
foundations of this theory, together with computational methods adapted from
Reiter, De Kleer (1987) and Inoue (1992). Finally, recently proposed techniques
for computing degrees of support are presented.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Partially Specified Belief Functions,"This paper presents a procedure to determine a complete belief function from
the known values of belief for some of the subsets of the frame of discerment.
The method is based on the principle of minimum commitment and a new principle
called the focusing principle. This additional principle is based on the idea
that belief is specified for the most relevant sets: the focal elements. The
resulting procedure is compared with existing methods of building complete
belief functions: the minimum specificity principle and the least commitment
principle.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Jeffrey's rule of conditioning generalized to belief functions,"Jeffrey's rule of conditioning has been proposed in order to revise a
probability measure by another probability function. We generalize it within
the framework of the models based on belief functions. We show that several
forms of Jeffrey's conditionings can be defined that correspond to the
geometrical rule of conditioning and to Dempster's rule of conditioning,
respectively.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Inference with Possibilistic Evidence,"In this paper, the concept of possibilistic evidence which is a possibility
distribution as well as a body of evidence is proposed over an infinite
universe of discourse. The inference with possibilistic evidence is
investigated based on a unified inference framework maintaining both the
compatibility of concepts and the consistency of the probability logic.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Constructing Lower Probabilities,"An elaboration of Dempster's method of constructing belief functions suggests
a broadly applicable strategy for constructing lower probabilities under a
variety of evidentiary constraints.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Belief Revision in Probability Theory,"In a probability-based reasoning system, Bayes' theorem and its variations
are often used to revise the system's beliefs. However, if the explicit
conditions and the implicit conditions of probability assignments `me properly
distinguished, it follows that Bayes' theorem is not a generally applicable
revision rule. Upon properly distinguishing belief revision from belief
updating, we see that Jeffrey's rule and its variations are not revision rules,
either. Without these distinctions, the limitation of the Bayesian approach is
often ignored or underestimated. Revision, in its general form, cannot be done
in the Bayesian approach, because a probability distribution function alone
does not contain the information needed by the operation.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
The Assumptions Behind Dempster's Rule,"This paper examines the concept of a combination rule for belief functions.
It is shown that two fairly simple and apparently reasonable assumptions
determine Dempster's rule, giving a new justification for it.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
A Belief-Function Based Decision Support System,"In this paper, we present a decision support system based on belief functions
and the pignistic transformation. The system is an integration of an evidential
system for belief function propagation and a valuation-based system for
Bayesian decision analysis. The two subsystems are connected through the
pignistic transformation. The system takes as inputs the user's ""gut feelings""
about a situation and suggests what, if any, are to be tested and in what
order, and it does so with a user friendly interface.","Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)"
Computing as compression: the SP theory of intelligence,"This paper provides an overview of the SP theory of intelligence and its
central idea that artificial intelligence, mainstream computing, and much of
human perception and cognition, may be understood as information compression.
  The background and origins of the SP theory are described, and the main
elements of the theory, including the key concept of multiple alignment,
borrowed from bioinformatics but with important differences. Associated with
the SP theory is the idea that redundancy in information may be understood as
repetition of patterns, that compression of information may be achieved via the
matching and unification (merging) of patterns, and that computing and
information compression are both fundamentally probabilistic. It appears that
the SP system is Turing-equivalent in the sense that anything that may be
computed with a Turing machine may, in principle, also be computed with an SP
machine.
  One of the main strengths of the SP theory and the multiple alignment concept
is in modelling concepts and phenomena in artificial intelligence. Within that
area, the SP theory provides a simple but versatile means of representing
different kinds of knowledge, it can model both the parsing and production of
natural language, with potential for the understanding and translation of
natural languages, it has strengths in pattern recognition, with potential in
computer vision, it can model several kinds of reasoning, and it has
capabilities in planning, problem solving, and unsupervised learning.
  The paper includes two examples showing how alternative parsings of an
ambiguous sentence may be modelled as multiple alignments, and another example
showing how the concept of multiple alignment may be applied in medical
diagnosis.","8 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1212.0229"
"Generating extrema approximation of analytically incomputable functions
  through usage of parallel computer aided genetic algorithms","This paper presents capabilities of using genetic algorithms to find
approximations of function extrema, which cannot be found using analytic ways.
To enhance effectiveness of calculations, algorithm has been parallelized using
OpenMP library. We gained much increase in speed on platforms using
multithreaded processors with shared memory free access. During analysis we
used different modifications of genetic operator, using them we obtained varied
evolution process of potential solutions. Results allow to choose best methods
among many applied in genetic algorithms and observation of acceleration on
Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.","16 pages, 13 figures"
"Discovering Semantic Spatial and Spatio-Temporal Outliers from Moving
  Object Trajectories","Several algorithms have been proposed for discovering patterns from
trajectories of moving objects, but only a few have concentrated on outlier
detection. Existing approaches, in general, discover spatial outliers, and do
not provide any further analysis of the patterns. In this paper we introduce
semantic spatial and spatio-temporal outliers and propose a new algorithm for
trajectory outlier detection. Semantic outliers are computed between regions of
interest, where objects have similar movement intention, and there exist
standard paths which connect the regions. We show with experiments on real data
that the method finds semantic outliers from trajectory data that are not
discovered by similar approaches.",N/A
"Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus
  in Egypt","Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV
has very high mutation rate that makes it resistant to antibodies. Modeling HCV
to identify the virus mutation process is essential to its detection and
predicting its evolution. This paper presents a model based framework for
estimating mutation rate of HCV in two steps. Firstly profile hidden Markov
model (PHMM) architecture was builder to select the sequences which represents
sequence per year. Secondly mutation rate was calculated by using pair-wise
distance method between sequences. A pilot study is conducted on NS5B zone of
HCV dataset of genotype 4 subtype a (HCV4a) in Egypt.","6 pages, 5 figures"
RES - a Relative Method for Evidential Reasoning,"In this paper we describe a novel method for evidential reasoning [1]. It
involves modelling the process of evidential reasoning in three steps, namely,
evidence structure construction, evidence accumulation, and decision making.
The proposed method, called RES, is novel in that evidence strength is
associated with an evidential support relationship (an argument) between a pair
of statements and such strength is carried by comparison between arguments.
This is in contrast to the onventional approaches, where evidence strength is
represented numerically and is associated with a statement.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Optimizing Causal Orderings for Generating DAGs from Data,"An algorithm for generating the structure of a directed acyclic graph from
data using the notion of causal input lists is presented. The algorithm
manipulates the ordering of the variables with operations which very much
resemble arc reversal. Operations are only applied if the DAG after the
operation represents at least the independencies represented by the DAG before
the operation until no more arcs can be removed from the DAG. The resulting DAG
is a minimal l-map.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Modal Logics for Qualitative Possibility and Beliefs,"Possibilistic logic has been proposed as a numerical formalism for reasoning
with uncertainty. There has been interest in developing qualitative accounts of
possibility, as well as an explanation of the relationship between possibility
and modal logics. We present two modal logics that can be used to represent and
reason with qualitative statements of possibility and necessity. Within this
modal framework, we are able to identify interesting relationships between
possibilistic logic, beliefs and conditionals. In particular, the most natural
conditional definable via possibilistic means for default reasoning is
identical to Pearl's conditional for e-semantics.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Structural Controllability and Observability in Influence Diagrams,"Influence diagram is a graphical representation of belief networks with
uncertainty. This article studies the structural properties of a probabilistic
model in an influence diagram. In particular, structural controllability
theorems and structural observability theorems are developed and algorithms are
formulated. Controllability and observability are fundamental concepts in
dynamic systems (Luenberger 1979). Controllability corresponds to the ability
to control a system while observability analyzes the inferability of its
variables. Both properties can be determined by the ranks of the system
matrices. Structural controllability and observability, on the other hand,
analyze the property of a system with its structure only, without the specific
knowledge of the values of its elements (tin 1974, Shields and Pearson 1976).
The structural analysis explores the connection between the structure of a
model and the functional dependence among its elements. It is useful in
comprehending problem and formulating solution by challenging the underlying
intuitions and detecting inconsistency in a model. This type of qualitative
reasoning can sometimes provide insight even when there is insufficient
numerical information in a model.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Lattice-Based Graded Logic: a Multimodal Approach,"Experts do not always feel very, comfortable when they have to give precise
numerical estimations of certainty degrees. In this paper we present a
qualitative approach which allows for attaching partially ordered symbolic
grades to logical formulas. Uncertain information is expressed by means of
parameterized modal operators. We propose a semantics for this multimodal logic
and give a sound and complete axiomatization. We study the links with related
approaches and suggest how this framework might be used to manage both
uncertain and incomplere knowledge.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Dynamic Network Models for Forecasting,"We have developed a probabilistic forecasting methodology through a synthesis
of belief network models and classical time-series analysis. We present the
dynamic network model (DNM) and describe methods for constructing, refining,
and performing inference with this representation of temporal probabilistic
knowledge. The DNM representation extends static belief-network models to more
general dynamic forecasting models by integrating and iteratively refining
contemporaneous and time-lagged dependencies. We discuss key concepts in terms
of a model for forecasting U.S. car sales in Japan.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Reformulating Inference Problems Through Selective Conditioning,"We describe how we selectively reformulate portions of a belief network that
pose difficulties for solution with a stochastic-simulation algorithm. With
employ the selective conditioning approach to target specific nodes in a belief
network for decomposition, based on the contribution the nodes make to the
tractability of stochastic simulation. We review previous work on BNRAS
algorithms- randomized approximation algorithms for probabilistic inference. We
show how selective conditioning can be employed to reformulate a single BNRAS
problem into multiple tractable BNRAS simulation problems. We discuss how we
can use another simulation algorithm-logic sampling-to solve a component of the
inference problem that provides a means for knitting the solutions of
individual subproblems into a final result. Finally, we analyze tradeoffs among
the computational subtasks associated with the selective conditioning approach
to reformulation.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Entropy and Belief Networks,"The product expansion of conditional probabilities for belief nets is not
maximum entropy. This appears to deny a desirable kind of assurance for the
model. However, a kind of guarantee that is almost as strong as maximum entropy
can be derived. Surprisingly, a variant model also exhibits the guarantee, and
for many cases obtains a higher performance score than the product expansion.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Parallelizing Probabilistic Inference: Some Early Explorations,"We report on an experimental investigation into opportunities for parallelism
in beliefnet inference. Specifically, we report on a study performed of the
available parallelism, on hypercube style machines, of a set of randomly
generated belief nets, using factoring (SPI) style inference algorithms. Our
results indicate that substantial speedup is available, but that it is
available only through parallelization of individual conformal product
operations, and depends critically on finding an appropriate factoring. We find
negligible opportunity for parallelism at the topological, or clustering tree,
level.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Objection-Based Causal Networks,"This paper introduces the notion of objection-based causal networks which
resemble probabilistic causal networks except that they are quantified using
objections. An objection is a logical sentence and denotes a condition under
which a, causal dependency does not exist. Objection-based causal networks
enjoy almost all the properties that make probabilistic causal networks
popular, with the added advantage that objections are, arguably more intuitive
than probabilities.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A Symbolic Approach to Reasoning with Linguistic Quantifiers,"This paper investigates the possibility of performing automated reasoning in
probabilistic logic when probabilities are expressed by means of linguistic
quantifiers. Each linguistic term is expressed as a prescribed interval of
proportions. Then instead of propagating numbers, qualitative terms are
propagated in accordance with the numerical interpretation of these terms. The
quantified syllogism, modelling the chaining of probabilistic rules, is studied
in this context. It is shown that a qualitative counterpart of this syllogism
makes sense, and is relatively independent of the threshold defining the
linguistically meaningful intervals, provided that these threshold values
remain in accordance with the intuition. The inference power is less than that
of a full-fledged probabilistic con-quaint propagation device but better
corresponds to what could be thought of as commonsense probabilistic reasoning.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Possibilistic Assumption based Truth Maintenance System, Validation in a
  Data Fusion Application","Data fusion allows the elaboration and the evaluation of a situation
synthesized from low level informations provided by different kinds of sensors.
The fusion of the collected data will result in fewer and higher level
informations more easily assessed by a human operator and that will assist him
effectively in his decision process. In this paper we present the suitability
and the advantages of using a Possibilistic Assumption based Truth Maintenance
System (n-ATMS) in a data fusion military application. We first describe the
problem, the needed knowledge representation formalisms and problem solving
paradigms. Then we remind the reader of the basic concepts of ATMSs,
Possibilistic Logic and 11-ATMSs. Finally we detail the solution to the given
data fusion problem and conclude with the results and comparison with a
non-possibilistic solution.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Knowledge Integration for Conditional Probability Assessments,"In the probabilistic approach to uncertainty management the input knowledge
is usually represented by means of some probability distributions. In this
paper we assume that the input knowledge is given by two discrete conditional
probability distributions, represented by two stochastic matrices P and Q. The
consistency of the knowledge base is analyzed. Coherence conditions and
explicit formulas for the extension to marginal distributions are obtained in
some special cases.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Integrating Model Construction and Evaluation,"To date, most probabilistic reasoning systems have relied on a fixed belief
network constructed at design time. The network is used by an application
program as a representation of (in)dependencies in the domain. Probabilistic
inference algorithms operate over the network to answer queries. Recognizing
the inflexibility of fixed models has led researchers to develop automated
network construction procedures that use an expressive knowledge base to
generate a network that can answer a query. Although more flexible than fixed
model approaches, these construction procedures separate construction and
evaluation into distinct phases. In this paper we develop an approach to
combining incremental construction and evaluation of a partial probability
model. The combined method holds promise for improved methods for control of
model construction based on a trade-off between fidelity of results and cost of
construction.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Reasoning With Qualitative Probabilities Can Be Tractable,"We recently described a formalism for reasoning with if-then rules that re
expressed with different levels of firmness [18]. The formalism interprets
these rules as extreme conditional probability statements, specifying orders of
magnitude of disbelief, which impose constraints over possible rankings of
worlds. It was shown that, once we compute a priority function Z+ on the rules,
the degree to which a given query is confirmed or denied can be computed in
O(log n`) propositional satisfiability tests, where n is the number of rules in
the knowledge base. In this paper, we show that computing Z+ requires O(n2 X
log n) satisfiability tests, not an exponential number as was conjectured in
[18], which reduces to polynomial complexity in the case of Horn expressions.
We also show how reasoning with imprecise observations can be incorporated in
our formalism and how the popular notions of belief revision and epistemic
entrenchment are embodied naturally and tractably.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A computational scheme for Reasoning in Dynamic Probabilistic Networks,"A computational scheme for reasoning about dynamic systems using (causal)
probabilistic networks is presented. The scheme is based on the framework of
Lauritzen and Spiegelhalter (1988), and may be viewed as a generalization of
the inference methods of classical time-series analysis in the sense that it
allows description of non-linear, multivariate dynamic systems with complex
conditional independence structures. Further, the scheme provides a method for
efficient backward smoothing and possibilities for efficient, approximate
forecasting methods. The scheme has been implemented on top of the HUGIN shell.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"The Dynamic of Belief in the Transferable Belief Model and
  Specialization-Generalization Matrices","The fundamental updating process in the transferable belief model is related
to the concept of specialization and can be described by a specialization
matrix. The degree of belief in the truth of a proposition is a degree of
justified support. The Principle of Minimal Commitment implies that one should
never give more support to the truth of a proposition than justified. We show
that Dempster's rule of conditioning corresponds essentially to the least
committed specialization, and that Dempster's rule of combination results
essentially from commutativity requirements. The concept of generalization,
dual to thc concept of specialization, is described.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A Note on the Measure of Discord,"A new entropy-like measure as well as a new measure of total uncertainty
pertaining to the Dempster-Shafer theory are introduced. It is argued that
these measures are better justified than any of the previously proposed
candidates.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Semantics for Probabilistic Inference,"A number of writers(Joseph Halpern and Fahiem Bacchus among them) have
offered semantics for formal languages in which inferences concerning
probabilities can be made. Our concern is different. This paper provides a
formalization of nonmonotonic inferences in which the conclusion is supported
only to a certain degree. Such inferences are clearly 'invalid' since they must
allow the falsity of a conclusion even when the premises are true.
Nevertheless, such inferences can be characterized both syntactically and
semantically. The 'premises' of probabilistic arguments are sets of statements
(as in a database or knowledge base), the conclusions categorical statements in
the language. We provide standards for both this form of inference, for which
high probability is required, and for an inference in which the conclusion is
qualified by an intermediate interval of support.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Some Problems for Convex Bayesians,"We discuss problems for convex Bayesian decision making and uncertainty
representation. These include the inability to accommodate various natural and
useful constraints and the possibility of an analog of the classical Dutch Book
being made against an agent behaving in accordance with convex Bayesian
prescriptions. A more general set-based Bayesianism may be as tractable and
would avoid the difficulties we raise.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Bayesian Meta-Reasoning: Determining Model Adequacy from Within a Small
  World","This paper presents a Bayesian framework for assessing the adequacy of a
model without the necessity of explicitly enumerating a specific alternate
model. A test statistic is developed for tracking the performance of the model
across repeated problem instances. Asymptotic methods are used to derive an
approximate distribution for the test statistic. When the model is rejected,
the individual components of the test statistic can be used to guide search for
an alternate model.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
The Bounded Bayesian,"The ideal Bayesian agent reasons from a global probability model, but real
agents are restricted to simplified models which they know to be adequate only
in restricted circumstances. Very little formal theory has been developed to
help fallibly rational agents manage the process of constructing and revising
small world models. The goal of this paper is to present a theoretical
framework for analyzing model management approaches. For a probability
forecasting problem, a search process over small world models is analyzed as an
approximation to a larger-world model which the agent cannot explicitly
enumerate or compute. Conditions are given under which the sequence of
small-world models converges to the larger-world probabilities.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Representing Context-Sensitive Knowledge in a Network Formalism: A
  Preliminary Report","Automated decision making is often complicated by the complexity of the
knowledge involved. Much of this complexity arises from the context sensitive
variations of the underlying phenomena. We propose a framework for representing
descriptive, context-sensitive knowledge. Our approach attempts to integrate
categorical and uncertain knowledge in a network formalism. This paper outlines
the basic representation constructs, examines their expressiveness and
efficiency, and discusses the potential applications of the framework.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A Probabilistic Network of Predicates,"Bayesian networks are directed acyclic graphs representing independence
relationships among a set of random variables. A random variable can be
regarded as a set of exhaustive and mutually exclusive propositions. We argue
that there are several drawbacks resulting from the propositional nature and
acyclic structure of Bayesian networks. To remedy these shortcomings, we
propose a probabilistic network where nodes represent unary predicates and
which may contain directed cycles. The proposed representation allows us to
represent domain knowledge in a single static network even though we cannot
determine the instantiations of the predicates before hand. The ability to deal
with cycles also enables us to handle cyclic causal tendencies and to recognize
recursive plans.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Representing Heuristic Knowledge in D-S Theory,"The Dempster-Shafer theory of evidence has been used intensively to deal with
uncertainty in knowledge-based systems. However the representation of uncertain
relationships between evidence and hypothesis groups (heuristic knowledge) is
still a major research problem. This paper presents an approach to representing
such heuristic knowledge by evidential mappings which are defined on the basis
of mass functions. The relationships between evidential mappings and multi
valued mappings, as well as between evidential mappings and Bayesian multi-
valued causal link models in Bayesian theory are discussed. Following this the
detailed procedures for constructing evidential mappings for any set of
heuristic rules are introduced. Several situations of belief propagation are
discussed.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
The Topological Fusion of Bayes Nets,"Bayes nets are relatively recent innovations. As a result, most of their
theoretical development has focused on the simplest class of single-author
models. The introduction of more sophisticated multiple-author settings raises
a variety of interesting questions. One such question involves the nature of
compromise and consensus. Posterior compromises let each model process all data
to arrive at an independent response, and then split the difference. Prior
compromises, on the other hand, force compromise to be reached on all points
before data is observed. This paper introduces prior compromises in a Bayes net
setting. It outlines the problem and develops an efficient algorithm for fusing
two directed acyclic graphs into a single, consensus structure, which may then
be used as the basis of a prior compromise.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Calculating Uncertainty Intervals From Conditional Convex Sets of
  Probabilities","In Moral, Campos (1991) and Cano, Moral, Verdegay-Lopez (1991) a new method
of conditioning convex sets of probabilities has been proposed. The result of
it is a convex set of non-necessarily normalized probability distributions. The
normalizing factor of each probability distribution is interpreted as the
possibility assigned to it by the conditioning information. From this, it is
deduced that the natural value for the conditional probability of an event is a
possibility distribution. The aim of this paper is to study methods of
transforming this possibility distribution into a probability (or uncertainty)
interval. These methods will be based on the use of Sugeno and Choquet
integrals. Their behaviour will be compared in basis to some selected examples.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Sensor Validation Using Dynamic Belief Networks,"The trajectory of a robot is monitored in a restricted dynamic environment
using light beam sensor data. We have a Dynamic Belief Network (DBN), based on
a discrete model of the domain, which provides discrete monitoring analogous to
conventional quantitative filter techniques. Sensor observations are added to
the basic DBN in the form of specific evidence. However, sensor data is often
partially or totally incorrect. We show how the basic DBN, which infers only an
impossible combination of evidence, may be modified to handle specific types of
incorrect data which may occur in the domain. We then present an extension to
the DBN, the addition of an invalidating node, which models the status of the
sensor as working or defective. This node provides a qualitative explanation of
inconsistent data: it is caused by a defective sensor. The connection of
successive instances of the invalidating node models the status of a sensor
over time, allowing the DBN to handle both persistent and intermittent faults.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
aHUGIN: A System Creating Adaptive Causal Probabilistic Networks,"The paper describes aHUGIN, a tool for creating adaptive systems. aHUGIN is
an extension of the HUGIN shell, and is based on the methods reported by
Spiegelhalter and Lauritzen (1990a). The adaptive systems resulting from aHUGIN
are able to adjust the C011ditional probabilities in the model. A short
analysis of the adaptation task is given and the features of aHUGIN are
described. Finally a session with experiments is reported and the results are
discussed.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
MESA: Maximum Entropy by Simulated Annealing,"Probabilistic reasoning systems combine different probabilistic rules and
probabilistic facts to arrive at the desired probability values of
consequences. In this paper we describe the MESA-algorithm (Maximum Entropy by
Simulated Annealing) that derives a joint distribution of variables or
propositions. It takes into account the reliability of probability values and
can resolve conflicts between contradictory statements. The joint distribution
is represented in terms of marginal distributions and therefore allows to
process large inference networks and to determine desired probability values
with high precision. The procedure derives a maximum entropy distribution
subject to the given constraints. It can be applied to inference networks of
arbitrary topology and may be extended into a number of directions.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Decision Methods for Adaptive Task-Sharing in Associate Systems,"This paper describes some results of research on associate systems:
knowledge-based systems that flexibly and adaptively support their human users
in carrying out complex, time-dependent problem-solving tasks under
uncertainty. Based on principles derived from decision theory and decision
analysis, a problem-solving approach is presented which can overcome many of
the limitations of traditional expert-systems. This approach implements an
explicit model of the human user's problem-solving capabilities as an integral
element in the overall problem solving architecture. This integrated model,
represented as an influence diagram, is the basis for achieving adaptive task
sharing behavior between the associate system and the human user. This
associate system model has been applied toward ongoing research on a Mars Rover
Manager's Associate (MRMA). MRMA's role would be to manage a small fleet of
robotic rovers on the Martian surface. The paper describes results for a
specific scenario where MRMA examines the benefits and costs of consulting
human experts on Earth to assist a Mars rover with a complex resource
management decision.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Modeling Uncertain Temporal Evolutions in Model-Based Diagnosis,"Although the notion of diagnostic problem has been extensively investigated
in the context of static systems, in most practical applications the behavior
of the modeled system is significantly variable during time. The goal of the
paper is to propose a novel approach to the modeling of uncertainty about
temporal evolutions of time-varying systems and a characterization of
model-based temporal diagnosis. Since in most real world cases knowledge about
the temporal evolution of the system to be diagnosed is uncertain, we consider
the case when probabilistic temporal knowledge is available for each component
of the system and we choose to model it by means of Markov chains. In fact, we
aim at exploiting the statistical assumptions underlying reliability theory in
the context of the diagnosis of timevarying systems. We finally show how to
exploit Markov chain theory in order to discard, in the diagnostic process,
very unlikely diagnoses.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Guess-And-Verify Heuristics for Reducing Uncertainties in Expert
  Classification Systems","An expert classification system having statistical information about the
prior probabilities of the different classes should be able to use this
knowledge to reduce the amount of additional information that it must collect,
e.g., through questions, in order to make a correct classification. This paper
examines how best to use such prior information and additional
information-collection opportunities to reduce uncertainty about the class to
which a case belongs, thus minimizing the average cost or effort required to
correctly classify new cases.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"R&D Analyst: An Interactive Approach to Normative Decision System Model
  Construction","This paper describes the architecture of R&D Analyst, a commercial
intelligent decision system for evaluating corporate research and development
projects and portfolios. In analyzing projects, R&D Analyst interactively
guides a user in constructing an influence diagram model for an individual
research project. The system's interactive approach can be clearly explained
from a blackboard system perspective. The opportunistic reasoning emphasis of
blackboard systems satisfies the flexibility requirements of model
construction, thereby suggesting that a similar architecture would be valuable
for developing normative decision systems in other domains. Current research is
aimed at extending the system architecture to explicitly consider of sequential
decisions involving limited temporal, financial, and physical resources.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Possibilistic Constraint Satisfaction Problems or ""How to handle soft
  constraints?""","Many AI synthesis problems such as planning or scheduling may be modelized as
constraint satisfaction problems (CSP). A CSP is typically defined as the
problem of finding any consistent labeling for a fixed set of variables
satisfying all given constraints between these variables. However, for many
real tasks such as job-shop scheduling, time-table scheduling, design?, all
these constraints have not the same significance and have not to be necessarily
satisfied. A first distinction can be made between hard constraints, which
every solution should satisfy and soft constraints, whose satisfaction has not
to be certain. In this paper, we formalize the notion of possibilistic
constraint satisfaction problems that allows the modeling of uncertainly
satisfied constraints. We use a possibility distribution over labelings to
represent respective possibilities of each labeling. Necessity-valued
constraints allow a simple expression of the respective certainty degrees of
each constraint. The main advantage of our approach is its integration in the
CSP technical framework. Most classical techniques, such as Backtracking (BT),
arcconsistency enforcing (AC) or Forward Checking have been extended to handle
possibilistics CSP and are effectively implemented. The utility of our approach
is demonstrated on a simple design problem.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Decision Making Using Probabilistic Inference Methods,"The analysis of decision making under uncertainty is closely related to the
analysis of probabilistic inference. Indeed, much of the research into
efficient methods for probabilistic inference in expert systems has been
motivated by the fundamental normative arguments of decision theory. In this
paper we show how the developments underlying those efficient methods can be
applied immediately to decision problems. In addition to general approaches
which need know nothing about the actual probabilistic inference method, we
suggest some simple modifications to the clustering family of algorithms in
order to efficiently incorporate decision making capabilities.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Conditional Independence in Uncertainty Theories,"This paper introduces the notions of independence and conditional
independence in valuation-based systems (VBS). VBS is an axiomatic framework
capable of representing many different uncertainty calculi. We define
independence and conditional independence in terms of factorization of the
joint valuation. The definitions of independence and conditional independence
in VBS generalize the corresponding definitions in probability theory. Our
definitions apply not only to probability theory, but also to Dempster-Shafer's
belief-function theory, Spohn's epistemic-belief theory, and Zadeh's
possibility theory. In fact, they apply to any uncertainty calculi that fit in
the framework of valuation-based systems.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"The Nature of the Unnormalized Beliefs Encountered in the Transferable
  Belief Model","Within the transferable belief model, positive basic belief masses can be
allocated to the empty set, leading to unnormalized belief functions. The
nature of these unnormalized beliefs is analyzed.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Intuitions about Ordered Beliefs Leading to Probabilistic Models,"The general use of subjective probabilities to model belief has been
justified using many axiomatic schemes. For example, ?consistent betting
behavior' arguments are well-known. To those not already convinced of the
unique fitness and generality of probability models, such justifications are
often unconvincing. The present paper explores another rationale for
probability models. ?Qualitative probability,' which is known to provide
stringent constraints on belief representation schemes, is derived from five
simple assumptions about relationships among beliefs. While counterparts of
familiar rationality concepts such as transitivity, dominance, and consistency
are used, the betting context is avoided. The gap between qualitative
probability and probability proper can be bridged by any of several additional
assumptions. The discussion here relies on results common in the recent AI
literature, introducing a sixth simple assumption. The narrative emphasizes
models based on unique complete orderings, but the rationale extends easily to
motivate set-valued representations of partial orderings as well.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"Expressing Relational and Temporal Knowledge in Visual Probabilistic
  Networks","Bayesian networks have been used extensively in diagnostic tasks such as
medicine, where they represent the dependency relations between a set of
symptoms and a set of diseases. A criticism of this type of knowledge
representation is that it is restricted to this kind of task, and that it
cannot cope with the knowledge required in other artificial intelligence
applications. For example, in computer vision, we require the ability to model
complex knowledge, including temporal and relational factors. In this paper we
extend Bayesian networks to model relational and temporal knowledge for
high-level vision. These extended networks have a simple structure which
permits us to propagate probability efficiently. We have applied them to the
domain of endoscopy, illustrating how the general modelling principles can be
used in specific cases.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A Fuzzy Logic Approach to Target Tracking,"This paper discusses a target tracking problem in which no dynamic
mathematical model is explicitly assumed. A nonlinear filter based on the fuzzy
If-then rules is developed. A comparison with a Kalman filter is made, and
empirical results show that the performance of the fuzzy filter is better.
Intensive simulations suggest that theoretical justification of the empirical
results is possible.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Towards Precision of Probabilistic Bounds Propagation,"The DUCK-calculus presented here is a recent approach to cope with
probabilistic uncertainty in a sound and efficient way. Uncertain rules with
bounds for probabilities and explicit conditional independences can be
maintained incrementally. The basic inference mechanism relies on local bounds
propagation, implementable by deductive databases with a bottom-up fixpoint
evaluation. In situations, where no precise bounds are deducible, it can be
combined with simple operations research techniques on a local scope. In
particular, we provide new precise analytical bounds for probabilistic
entailment.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
"An Algorithm for Deciding if a Set of Observed Independencies Has a
  Causal Explanation","In a previous paper [Pearl and Verma, 1991] we presented an algorithm for
extracting causal influences from independence information, where a causal
influence was defined as the existence of a directed arc in all minimal causal
models consistent with the data. In this paper we address the question of
deciding whether there exists a causal model that explains ALL the observed
dependencies and independencies. Formally, given a list M of conditional
independence statements, it is required to decide whether there exists a
directed acyclic graph (dag) D that is perfectly consistent with M, namely,
every statement in M, and no other, is reflected via dseparation in D. We
present and analyze an effective algorithm that tests for the existence of such
a day, and produces one, if it exists.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Generalizing Jeffrey Conditionalization,"Jeffrey's rule has been generalized by Wagner to the case in which new
evidence bounds the possible revisions of a prior probability below by a
Dempsterian lower probability. Classical probability kinematics arises within
this generalization as the special case in which the evidentiary focal elements
of the bounding lower probability are pairwise disjoint. We discuss a twofold
extension of this generalization, first allowing the lower bound to be any
two-monotone capacity and then allowing the prior to be a lower envelope.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Interval Structure: A Framework for Representing Uncertain Information,"In this paper, a unified framework for representing uncertain information
based on the notion of an interval structure is proposed. It is shown that the
lower and upper approximations of the rough-set model, the lower and upper
bounds of incidence calculus, and the belief and plausibility functions all
obey the axioms of an interval structure. An interval structure can be used to
synthesize the decision rules provided by the experts. An efficient algorithm
to find the desirable set of rules is developed from a set of sound and
complete inference axioms.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Exploring Localization in Bayesian Networks for Large Expert Systems,"Current Bayesian net representations do not consider structure in the domain
and include all variables in a homogeneous network. At any time, a human
reasoner in a large domain may direct his attention to only one of a number of
natural subdomains, i.e., there is ?localization' of queries and evidence. In
such a case, propagating evidence through a homogeneous network is inefficient
since the entire network has to be updated each time. This paper presents
multiply sectioned Bayesian networks that enable a (localization preserving)
representation of natural subdomains by separate Bayesian subnets. The subnets
are transformed into a set of permanent junction trees such that evidential
reasoning takes place at only one of them at a time. Probabilities obtained are
identical to those that would be obtained from the homogeneous network. We
discuss attention shift to a different junction tree and propagation of
previously acquired evidence. Although the overall system can be large,
computational requirements are governed by the size of only one junction tree.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
A Decision Calculus for Belief Functions in Valuation-Based Systems,"Valuation-based system (VBS) provides a general framework for representing
knowledge and drawing inferences under uncertainty. Recent studies have shown
that the semantics of VBS can represent and solve Bayesian decision problems
(Shenoy, 1991a). The purpose of this paper is to propose a decision calculus
for Dempster-Shafer (D-S) theory in the framework of VBS. The proposed calculus
uses a weighting factor whose role is similar to the probabilistic
interpretation of an assumption that disambiguates decision problems
represented with belief functions (Strat 1990). It will be shown that with the
presented calculus, if the decision problems are represented in the valuation
network properly, we can solve the problems by using fusion algorithm (Shenoy
1991a). It will also be shown the presented decision calculus can be reduced to
the calculus for Bayesian probability theory when probabilities, instead of
belief functions, are given.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Sidestepping the Triangulation Problem in Bayesian Net Computations,"This paper presents a new approach for computing posterior probabilities in
Bayesian nets, which sidesteps the triangulation problem. The current state of
art is the clique tree propagation approach. When the underlying graph of a
Bayesian net is triangulated, this approach arranges its cliques into a tree
and computes posterior probabilities by appropriately passing around messages
in that tree. The computation in each clique is simply direct marginalization.
When the underlying graph is not triangulated, one has to first triangulated it
by adding edges. Referred to as the triangulation problem, the problem of
finding an optimal or even a ?good? triangulation proves to be difficult. In
this paper, we propose to first decompose a Bayesian net into smaller
components by making use of Tarjan's algorithm for decomposing an undirected
graph at all its minimal complete separators. Then, the components are arranged
into a tree and posterior probabilities are computed by appropriately passing
around messages in that tree. The computation in each component is carried out
by repeating the whole procedure from the beginning. Thus the triangulation
problem is sidestepped.","Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)"
Viterbi training in PRISM,"VT (Viterbi training), or hard EM, is an efficient way of parameter learning
for probabilistic models with hidden variables. Given an observation $y$, it
searches for a state of hidden variables $x$ that maximizes $p(x,y \mid
\theta)$ by coordinate ascent on parameters $\theta$ and $x$. In this paper we
introduce VT to PRISM, a logic-based probabilistic modeling system for
generative models. VT improves PRISM in three ways. First VT in PRISM converges
faster than EM in PRISM due to the VT's termination condition. Second,
parameters learned by VT often show good prediction performance compared to
those learned by EM. We conducted two parsing experiments with probabilistic
grammars while learning parameters by a variety of inference methods, i.e.\ VT,
EM, MAP and VB. The result is that VT achieved the best parsing accuracy among
them in both experiments. Also we conducted a similar experiment for
classification tasks where a hidden variable is not a prediction target unlike
probabilistic grammars. We found that in such a case VT does not necessarily
yield superior performance. Third since VT always deals with a single
probability of a single explanation, Viterbi explanation, the exclusiveness
condition that is imposed on PRISM programs is no more required if we learn
parameters by VT.
  Last but not least we can say that as VT in PRISM is general and applicable
to any PRISM program, it largely reduces the need for the user to develop a
specific VT algorithm for a specific model. Furthermore since VT in PRISM can
be used just by setting a PRISM flag appropriately, it makes VT easily
accessible to (probabilistic) logic programmers. To appear in Theory and
Practice of Logic Programming (TPLP).","23 pages, 1 figure"
"""Conditional Inter-Causally Independent"" Node Distributions, a Property
  of ""Noisy-Or"" Models","This paper examines the interdependence generated between two parent nodes
with a common instantiated child node, such as two hypotheses sharing common
evidence. The relation so generated has been termed ""intercausal."" It is shown
by construction that inter-causal independence is possible for binary
distributions at one state of evidence. For such ""CICI"" distributions, the two
measures of inter-causal effect, ""multiplicative synergy"" and ""additive
synergy"" are equal. The well known ""noisy-or"" model is an example of such a
distribution. This introduces novel semantics for the noisy-or, as a model of
the degree of conflict among competing hypotheses of a common observation.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Combining Multiple-Valued Logics in Modular Expert Systems,"The way experts manage uncertainty usually changes depending on the task they
are performing. This fact has lead us to consider the problem of communicating
modules (task implementations) in a large and structured knowledge based system
when modules have different uncertainty calculi. In this paper, the analysis of
the communication problem is made assuming that (i) each uncertainty calculus
is an inference mechanism defining an entailment relation, and therefore the
communication is considered to be inference-preserving, and (ii) we restrict
ourselves to the case which the different uncertainty calculi are given by a
class of truth functional Multiple-valued Logics.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Constraint Propagation with Imprecise Conditional Probabilities,"An approach to reasoning with default rules where the proportion of
exceptions, or more generally the probability of encountering an exception, can
be at least roughly assessed is presented. It is based on local uncertainty
propagation rules which provide the best bracketing of a conditional
probability of interest from the knowledge of the bracketing of some other
conditional probabilities. A procedure that uses two such propagation rules
repeatedly is proposed in order to estimate any simple conditional probability
of interest from the available knowledge. The iterative procedure, that does
not require independence assumptions, looks promising with respect to the
linear programming method. Improved bounds for conditional probabilities are
given when independence assumptions hold.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Some Properties of Plausible Reasoning,"This paper presents a plausible reasoning system to illustrate some broad
issues in knowledge representation: dualities between different reasoning
forms, the difficulty of unifying complementary reasoning styles, and the
approximate nature of plausible reasoning. These issues have a common
underlying theme: there should be an underlying belief calculus of which the
many different reasoning forms are special cases, sometimes approximate. The
system presented allows reasoning about defaults, likelihood, necessity and
possibility in a manner similar to the earlier work of Adams. The system is
based on the belief calculus of subjective Bayesian probability which itself is
based on a few simple assumptions about how belief should be manipulated.
Approximations, semantics, consistency and consequence results are presented
for the system. While this puts these often discussed plausible reasoning forms
on a probabilistic footing, useful application to practical problems remains an
issue.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Theory Refinement on Bayesian Networks,"Theory refinement is the task of updating a domain theory in the light of new
cases, to be done automatically or with some expert assistance. The problem of
theory refinement under uncertainty is reviewed here in the context of Bayesian
statistics, a theory of belief revision. The problem is reduced to an
incremental learning task as follows: the learning system is initially primed
with a partial theory supplied by a domain expert, and thereafter maintains its
own internal representation of alternative theories which is able to be
interrogated by the domain expert and able to be incrementally refined from
data. Algorithms for refinement of Bayesian networks are presented to
illustrate what is meant by ""partial theory"", ""alternative theory
representation"", etc. The algorithms are an incremental variant of batch
learning algorithms from the literature so can work well in batch and
incremental mode.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Combination of Upper and Lower Probabilities,"In this paper, we consider several types of information and methods of
combination associated with incomplete probabilistic systems. We discriminate
between 'a priori' and evidential information. The former one is a description
of the whole population, the latest is a restriction based on observations for
a particular case. Then, we propose different combination methods for each one
of them. We also consider conditioning as the heterogeneous combination of 'a
priori' and evidential information. The evidential information is represented
as a convex set of likelihood functions. These will have an associated
possibility distribution with behavior according to classical Possibility
Theory.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"A Probabilistic Analysis of Marker-Passing Techniques for
  Plan-Recognition","Useless paths are a chronic problem for marker-passing techniques. We use a
probabilistic analysis to justify a method for quickly identifying and
rejecting useless paths. Using the same analysis, we identify key conditions
and assumptions necessary for marker-passing to perform well.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Symbolic Probabilistic Inference with Continuous Variables,"Research on Symbolic Probabilistic Inference (SPI) [2, 3] has provided an
algorithm for resolving general queries in Bayesian networks. SPI applies the
concept of dependency directed backward search to probabilistic inference, and
is incremental with respect to both queries and observations. Unlike
traditional Bayesian network inferencing algorithms, SPI algorithm is goal
directed, performing only those calculations that are required to respond to
queries. Research to date on SPI applies to Bayesian networks with
discrete-valued variables and does not address variables with continuous
values. In this papers, we extend the SPI algorithm to handle Bayesian networks
made up of continuous variables where the relationships between the variables
are restricted to be ?linear gaussian?. We call this variation of the SPI
algorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI operations:
multiplication, summation, and substitution. However, SPIC retains the
framework of the SPI algorithm, namely building the search tree and recursive
query mechanism and therefore retains the goal-directed and incrementality
features of SPI.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Symbolic Probabilistic Inference with Evidence Potential,"Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2]
has focused attention on the importance of resolving general queries in
Bayesian networks. SPI applies the concept of dependency-directed backward
search to probabilistic inference, and is incremental with respect to both
queries and observations. In response to this research we have extended the
evidence potential algorithm [3] with the same features. We call the extension
symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic
queries and is incremental with respect to queries and observations. While in
SPI, operations are done on a search tree constructed from the nodes of the
original network, in SEPI, a clique-tree structure obtained from the evidence
potential algorithm [3] is the basic framework for recursive query processing.
In this paper, we describe the systematic query and caching procedure of SEPI.
SEPI begins with finding a clique tree from a Bayesian network-the standard
procedure of the evidence potential algorithm. With the clique tree, various
probability distributions are computed and stored in each clique. This is the
?pre-processing? step of SEPI. Once this step is done, the query can then be
computed. To process a query, a recursive process similar to the SPI algorithm
is used. The queries are directed to the root clique and decomposed into
queries for the clique's subtrees until a particular query can be answered at
the clique at which it is directed. The algorithm and the computation are
simple. The SEPI algorithm will be presented in this paper along with several
examples.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"A Bayesian Method for Constructing Bayesian Belief Networks from
  Databases","This paper presents a Bayesian method for constructing Bayesian belief
networks from a database of cases. Potential applications include
computer-assisted hypothesis testing, automated scientific discovery, and
automated construction of probabilistic expert systems. Results are presented
of a preliminary evaluation of an algorithm for constructing a belief network
from a database of cases. We relate the methods in this paper to previous work,
and we discuss open problems.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Local Expression Languages for Probabilistic Dependence: a Preliminary
  Report","We present a generalization of the local expression language used in the
Symbolic Probabilistic Inference (SPI) approach to inference in belief nets
[1l, [8]. The local expression language in SPI is the language in which the
dependence of a node on its antecedents is described. The original language
represented the dependence as a single monolithic conditional probability
distribution. The extended language provides a set of operators (*, +, and -)
which can be used to specify methods for combining partial conditional
distributions. As one instance of the utility of this extension, we show how
this extended language can be used to capture the semantics, representational
advantages, and inferential complexity advantages of the ""noisy or""
relationship.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Symbolic Decision Theory and Autonomous Systems,"The ability to reason under uncertainty and with incomplete information is a
fundamental requirement of decision support technology. In this paper we argue
that the concentration on theoretical techniques for the evaluation and
selection of decision options has distracted attention from many of the wider
issues in decision making. Although numerical methods of reasoning under
uncertainty have strong theoretical foundations, they are representationally
weak and only deal with a small part of the decision process. Knowledge based
systems, on the other hand, offer greater flexibility but have not been
accompanied by a clear decision theory. We describe here work which is under
way towards providing a theoretical framework for symbolic decision procedures.
A central proposal is an extended form of inference which we call
argumentation; reasoning for and against decision options from generalised
domain theories. The approach has been successfully used in several decision
support applications, but it is argued that a comprehensive decision theory
must cover autonomous decision making, where the agent can formulate questions
as well as take decisions. A major theoretical challenge for this theory is to
capture the idea of reflection to permit decision agents to reason about their
goals, what they believe and why, and what they need to know or do in order to
achieve their goals.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Reason Maintenace System Dealing with Vague Data,"A reason maintenance system which extends an ATMS through Mukaidono's fuzzy
logic is described. It supports a problem solver in situations affected by
incomplete information and vague data, by allowing nonmonotonic inferences and
the revision of previous conclusions when contradictions are detected.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Advances in Probabilistic Reasoning,"This paper discuses multiple Bayesian networks representation paradigms for
encoding asymmetric independence assertions. We offer three contributions: (1)
an inference mechanism that makes explicit use of asymmetric independence to
speed up computations, (2) a simplified definition of similarity networks and
extensions of their theory, and (3) a generalized representation scheme that
encodes more types of asymmetric independence assertions than do similarity
networks.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Probability Estimation in Face of Irrelevant Information,"In this paper, we consider one aspect of the problem of applying decision
theory to the design of agents that learn how to make decisions under
uncertainty. This aspect concerns how an agent can estimate probabilities for
the possible states of the world, given that it only makes limited observations
before committing to a decision. We show that the naive application of
statistical tools can be improved upon if the agent can determine which of his
observations are truly relevant to the estimation problem at hand. We give a
framework in which such determinations can be made, and define an estimation
procedure to use them. Our framework also suggests several extensions, which
show how additional knowledge can be used to improve tile estimation procedure
still further.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
An Approximate Nonmyopic Computation for Value of Information,"Value-of-information analyses provide a straightforward means for selecting
the best next observation to make, and for determining whether it is better to
gather additional information or to act immediately. Determining the next best
test to perform, given a state of uncertainty about the world, requires a
consideration of the value of making all possible sequences of observations. In
practice, decision analysts and expert-system designers have avoided the
intractability of exact computation of the value of information by relying on a
myopic approximation. Myopic analyses are based on the assumption that only one
additional test will be performed, even when there is an opportunity to make a
large number of observations. We present a nonmyopic approximation for value of
information that bypasses the traditional myopic analyses by exploiting the
statistical properties of large samples.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Search-based Methods to Bound Diagnostic Probabilities in Very Large
  Belief Nets","Since exact probabilistic inference is intractable in general for large
multiply connected belief nets, approximate methods are required. A promising
approach is to use heuristic search among hypotheses (instantiations of the
network) to find the most probable ones, as in the TopN algorithm. Search is
based on the relative probabilities of hypotheses which are efficient to
compute. Given upper and lower bounds on the relative probability of partial
hypotheses, it is possible to obtain bounds on the absolute probabilities of
hypotheses. Best-first search aimed at reducing the maximum error progressively
narrows the bounds as more hypotheses are examined. Here, qualitative
probabilistic analysis is employed to obtain bounds on the relative probability
of partial hypotheses for the BN20 class of networks networks and a
generalization replacing the noisy OR assumption by negative synergy. The
approach is illustrated by application to a very large belief network, QMR-BN,
which is a reformulation of the Internist-1 system for diagnosis in internal
medicine.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Time-Dependent Utility and Action Under Uncertainty,"We discuss representing and reasoning with knowledge about the time-dependent
utility of an agent's actions. Time-dependent utility plays a crucial role in
the interaction between computation and action under bounded resources. We
present a semantics for time-dependent utility and describe the use of
time-dependent information in decision contexts. We illustrate our discussion
with examples of time-pressured reasoning in Protos, a system constructed to
explore the ideal control of inference by reasoners with limit abilities.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Non-monotonic Reasoning and the Reversibility of Belief Change,"Traditional approaches to non-monotonic reasoning fail to satisfy a number of
plausible axioms for belief revision and suffer from conceptual difficulties as
well. Recent work on ranked preferential models (RPMs) promises to overcome
some of these difficulties. Here we show that RPMs are not adequate to handle
iterated belief change. Specifically, we show that RPMs do not always allow for
the reversibility of belief change. This result indicates the need for
numerical strengths of belief.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Belief and Surprise - A Belief-Function Formulation,"We motivate and describe a theory of belief in this paper. This theory is
developed with the following view of human belief in mind. Consider the belief
that an event E will occur (or has occurred or is occurring). An agent either
entertains this belief or does not entertain this belief (i.e., there is no
""grade"" in entertaining the belief). If the agent chooses to exercise ""the will
to believe"" and entertain this belief, he/she/it is entitled to a degree of
confidence c (1 > c > 0) in doing so. Adopting this view of human belief, we
conjecture that whenever an agent entertains the belief that E will occur with
c degree of confidence, the agent will be surprised (to the extent c) upon
realizing that E did not occur.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Evidential Reasoning in a Categorial Perspective: Conjunction and
  Disjunction of Belief Functions","The categorial approach to evidential reasoning can be seen as a combination
of the probability kinematics approach of Richard Jeffrey (1965) and the
maximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a
consequence of that viewpoint, it is well known that category theory provides
natural definitions for logical connectives. In particular, disjunction and
conjunction are modelled by general categorial constructions known as products
and coproducts. In this paper, I focus mainly on Dempster-Shafer theory of
belief functions for which I introduce a category I call Dempster?s category. I
prove the existence of and give explicit formulas for conjunction and
disjunction in the subcategory of separable belief functions. In Dempster?s
category, the new defined conjunction can be seen as the most cautious
conjunction of beliefs, and thus no assumption about distinctness (of the
sources) of beliefs is needed as opposed to Dempster?s rule of combination,
which calls for distinctness (of the sources) of beliefs.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Reasoning with Mass Distributions,"The concept of movable evidence masses that flow from supersets to subsets as
specified by experts represents a suitable framework for reasoning under
uncertainty. The mass flow is controlled by specialization matrices. New
evidence is integrated into the frame of discernment by conditioning or
revision (Dempster's rule of conditioning), for which special specialization
matrices exist. Even some aspects of non-monotonic reasoning can be represented
by certain specialization matrices.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Conflict and Surprise: Heuristics for Model Revision,"Any probabilistic model of a problem is based on assumptions which, if
violated, invalidate the model. Users of probability based decision aids need
to be alerted when cases arise that are not covered by the aid's model.
Diagnosis of model failure is also necessary to control dynamic model
construction and revision. This paper presents a set of decision theoretically
motivated heuristics for diagnosing situations in which a model is likely to
provide an inadequate representation of the process being modeled.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Reasoning under Uncertainty: Some Monte Carlo Results,"A series of monte carlo studies were performed to compare the behavior of
some alternative procedures for reasoning under uncertainty. The behavior of
several Bayesian, linear model and default reasoning procedures were examined
in the context of increasing levels of calibration error. The most interesting
result is that Bayesian procedures tended to output more extreme posterior
belief values (posterior beliefs near 0.0 or 1.0) than other techniques, but
the linear models were relatively less likely to output strong support for an
erroneous conclusion. Also, accounting for the probabilistic dependencies
between evidence items was important for both Bayesian and linear updating
procedures.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Representation Requirements for Supporting Decision Model Formulation,"This paper outlines a methodology for analyzing the representational support
for knowledge-based decision-modeling in a broad domain. A relevant set of
inference patterns and knowledge types are identified. By comparing the
analysis results to existing representations, some insights are gained into a
design approach for integrating categorical and uncertain knowledge in a
context sensitive manner.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Language for Planning with Statistics,"When a planner must decide whether it has enough evidence to make a decision
based on probability, it faces the sample size problem. Current planners using
probabilities need not deal with this problem because they do not generate
their probabilities from observations. This paper presents an event based
language in which the planner's probabilities are calculated from the binomial
random variable generated by the observed ratio of one type of event to
another. Such probabilities are subject to error, so the planner must
introspect about their validity. Inferences about the probability of these
events can be made using statistics. Inferences about the validity of the
approximations can be made using interval estimation. Interval estimation
allows the planner to avoid making choices that are only weakly supported by
the planner's evidence.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Modification to Evidential Probability,"Selecting the right reference class and the right interval when faced with
conflicting candidates and no possibility of establishing subset style
dominance has been a problem for Kyburg's Evidential Probability system.
Various methods have been proposed by Loui and Kyburg to solve this problem in
a way that is both intuitively appealing and justifiable within Kyburg's
framework. The scheme proposed in this paper leads to stronger statistical
assertions without sacrificing too much of the intuitive appeal of Kyburg's
latest proposal.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Investigation of Variances in Belief Networks,"The belief network is a well-known graphical structure for representing
independences in a joint probability distribution. The methods, which perform
probabilistic inference in belief networks, often treat the conditional
probabilities which are stored in the network as certain values. However, if
one takes either a subjectivistic or a limiting frequency approach to
probability, one can never be certain of probability values. An algorithm
should not only be capable of reporting the probabilities of the alternatives
of remaining nodes when other nodes are instantiated; it should also be capable
of reporting the uncertainty in these probabilities relative to the uncertainty
in the probabilities which are stored in the network. In this paper a method
for determining the variances in inferred probabilities is obtained under the
assumption that a posterior distribution on the uncertainty variables can be
approximated by the prior distribution. It is shown that this assumption is
plausible if their is a reasonable amount of confidence in the probabilities
which are stored in the network. Furthermore in this paper, a surprising upper
bound for the prior variances in the probabilities of the alternatives of all
nodes is obtained in the case where the probability distributions of the
probabilities of the alternatives are beta distributions. It is shown that the
prior variance in the probability at an alternative of a node is bounded above
by the largest variance in an element of the conditional probability
distribution for that node.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Sensitivity Analysis of Pathfinder: A Follow-up Study,"At last year?s Uncertainty in AI Conference, we reported the results of a
sensitivity analysis study of Pathfinder. Our findings were quite
unexpected-slight variations to Pathfinder?s parameters appeared to lead to
substantial degradations in system performance. A careful look at our first
analysis, together with the valuable feedback provided by the participants of
last year?s conference, led us to conduct a follow-up study. Our follow-up
differs from our initial study in two ways: (i) the probabilities 0.0 and 1.0
remained unchanged, and (ii) the variations to the probabilities that are close
to both ends (0.0 or 1.0) were less than the ones close to the middle (0.5).
The results of the follow-up study look more reasonable-slight variations to
Pathfinder?s parameters now have little effect on its performance. Taken
together, these two sets of results suggest a viable extension of a common
decision analytic sensitivity analysis to the larger, more complex settings
generally encountered in artificial intelligence.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Non-monotonic Negation in Probabilistic Deductive Databases,"In this paper we study the uses and the semantics of non-monotonic negation
in probabilistic deductive data bases. Based on the stable semantics for
classical logic programming, we introduce the notion of stable formula,
functions. We show that stable formula, functions are minimal fixpoints of
operators associated with probabilistic deductive databases with negation.
Furthermore, since a. probabilistic deductive database may not necessarily have
a stable formula function, we provide a stable class semantics for such
databases. Finally, we demonstrate that the proposed semantics can handle
default reasoning naturally in the context of probabilistic deduction.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Management of Uncertainty in the Multi-Level Monitoring and Diagnosis of
  the Time of Flight Scintillation Array","We present a general architecture for the monitoring and diagnosis of large
scale sensor-based systems with real time diagnostic constraints. This
architecture is multileveled, combining a single monitoring level based on
statistical methods with two model based diagnostic levels. At each level,
sources of uncertainty are identified, and integrated methodologies for
uncertainty management are developed. The general architecture was applied to
the monitoring and diagnosis of a specific nuclear physics detector at Lawrence
Berkeley National Laboratory that contained approximately 5000 components and
produced over 500 channels of output data. The general architecture is
scalable, and work is ongoing to apply it to detector systems one and two
orders of magnitude more complex.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Integrating Probabilistic Rules into Neural Networks: A Stochastic EM
  Learning Algorithm","The EM-algorithm is a general procedure to get maximum likelihood estimates
if part of the observations on the variables of a network are missing. In this
paper a stochastic version of the algorithm is adapted to probabilistic neural
networks describing the associative dependency of variables. These networks
have a probability distribution, which is a special case of the distribution
generated by probabilistic inference networks. Hence both types of networks can
be combined allowing to integrate probabilistic rules as well as unspecified
associations in a sound way. The resulting network may have a number of
interesting features including cycles of probabilistic rules, hidden
'unobservable' variables, and uncertain and contradictory evidence.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Representing Bayesian Networks within Probabilistic Horn Abduction,"This paper presents a simple framework for Horn clause abduction, with
probabilities associated with hypotheses. It is shown how this representation
can represent any probabilistic knowledge representable in a Bayesian belief
network. The main contributions are in finding a relationship between logical
and probabilistic notions of evidential reasoning. This can be used as a basis
for a new way to implement Bayesian Networks that allows for approximations to
the value of the posterior probabilities, and also points to a way that
Bayesian networks can be extended beyond a propositional language.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Dynamic Network Updating Techniques For Diagnostic Reasoning,"A new probabilistic network construction system, DYNASTY, is proposed for
diagnostic reasoning given variables whose probabilities change over time.
Diagnostic reasoning is formulated as a sequential stochastic process, and is
modeled using influence diagrams. Given a set O of observations, DYNASTY
creates an influence diagram in order to devise the best action given O.
Sensitivity analyses are conducted to determine if the best network has been
created, given the uncertainty in network parameters and topology. DYNASTY uses
an equivalence class approach to provide decision thresholds for the
sensitivity analysis. This equivalence-class approach to diagnostic reasoning
differentiates diagnoses only if the required actions are different. A set of
network-topology updating algorithms are proposed for dynamically updating the
network when necessary.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
High Level Path Planning with Uncertainty,"For high level path planning, environments are usually modeled as distance
graphs, and path planning problems are reduced to computing the shortest path
in distance graphs. One major drawback of this modeling is the inability to
model uncertainties, which are often encountered in practice. In this paper, a
new tool, called U-yraph, is proposed for environment modeling. A U-graph is an
extension of distance graphs with the ability to handle a kind of uncertainty.
By modeling an uncertain environment as a U-graph, and a navigation problem as
a Markovian decision process, we can precisely define a new optimality
criterion for navigation plans, and more importantly, we can come up with a
general algorithm for computing optimal plans for navigation tasks.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Formal Model of Uncertainty for Possibilistic Rules,"Given a universe of discourse X-a domain of possible outcomes-an experiment
may consist of selecting one of its elements, subject to the operation of
chance, or of observing the elements, subject to imprecision. A priori
uncertainty about the actual result of the experiment may be quantified,
representing either the likelihood of the choice of :r_X or the degree to which
any such X would be suitable as a description of the outcome. The former case
corresponds to a probability distribution, while the latter gives a possibility
assignment on X. The study of such assignments and their properties falls
within the purview of possibility theory [DP88, Y80, Z783. It, like probability
theory, assigns values between 0 and 1 to express likelihoods of outcomes.
Here, however, the similarity ends. Possibility theory uses the maximum and
minimum functions to combine uncertainties, whereas probability theory uses the
plus and times operations. This leads to very dissimilar theories in terms of
analytical framework, even though they share several semantic concepts. One of
the shared concepts consists of expressing quantitatively the uncertainty
associated with a given distribution. In probability theory its value
corresponds to the gain of information that would result from conducting an
experiment and ascertaining an actual result. This gain of information can
equally well be viewed as a decrease in uncertainty about the outcome of an
experiment. In this case the standard measure of information, and thus
uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is
characterized uniquely by a few, very natural properties, and it can be
conveniently used in decision processes. This application is based on the
principle of maximum entropy; it has become a popular method of relating
decisions to uncertainty. This paper demonstrates that an equally integrated
theory can be built on the foundation of possibility theory. We first show how
to define measures of in formation and uncertainty for possibility assignments.
Next we construct an information-based metric on the space of all possibility
distributions defined on a given domain. It allows us to capture the notion of
proximity in information content among the distributions. Lastly, we show that
all the above constructions can be carried out for continuous
distributions-possibility assignments on arbitrary measurable domains. We
consider this step very significant-finite domains of discourse are but
approximations of the real-life infinite domains. If possibility theory is to
represent real world situations, it must handle continuous distributions both
directly and through finite approximations. In the last section we discuss a
principle of maximum uncertainty for possibility distributions. We show how
such a principle could be formalized as an inference rule. We also suggest it
could be derived as a consequence of simple assumptions about combining
information. We would like to mention that possibility assignments can be
viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of
possibilities. This correspondence has far reaching consequences in logic and
in control theory. Our treatment here is independent of any special
interpretation; in particular we speak of possibility distributions and
possibility measures, defining them as measurable mappings into the interval
[0, 1]. Our presentation is intended as a self-contained, albeit terse summary.
Topics discussed were selected with care, to demonstrate both the completeness
and a certain elegance of the theory. Proofs are not included; we only offer
illustrative examples.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Deliberation and its Role in the Formation of Intentions,"Deliberation plays an important role in the design of rational agents
embedded in the real-world. In particular, deliberation leads to the formation
of intentions, i.e., plans of action that the agent is committed to achieving.
In this paper, we present a branching time possible-worlds model for
representing and reasoning about, beliefs, goals, intentions, time, actions,
probabilities, and payoffs. We compare this possible-worlds approach with the
more traditional decision tree representation and provide a transformation from
decision trees to possible worlds. Finally, we illustrate how an agent can
perform deliberation using a decision-tree representation and then use a
possible-worlds model to form and reason about his intentions.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Handling Uncertainty during Plan Recognition in Task-Oriented
  Consultation Systems","During interactions with human consultants, people are used to providing
partial and/or inaccurate information, and still be understood and assisted. We
attempt to emulate this capability of human consultants; in computer
consultation systems. In this paper, we present a mechanism for handling
uncertainty in plan recognition during task-oriented consultations. The
uncertainty arises while choosing an appropriate interpretation of a user?s
statements among many possible interpretations. Our mechanism handles this
uncertainty by using probability theory to assess the probabilities of the
interpretations, and complements this assessment by taking into account the
information content of the interpretations. The information content of an
interpretation is a measure of how well defined an interpretation is in terms
of the actions to be performed on the basis of the interpretation. This measure
is used to guide the inference process towards interpretations with a higher
information content. The information content for an interpretation depends on
the specificity and the strength of the inferences in it, where the strength of
an inference depends on the reliability of the information on which the
inference is based. Our mechanism has been developed for use in task-oriented
consultation systems. The domain that we have chosen for exploration is that of
a travel agency.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Truth as Utility: A Conceptual Synthesis,"This paper introduces conceptual relations that synthesize utilitarian and
logical concepts, extending the logics of preference of Rescher. We define
first, in the context of a possible worlds model, constraint-dependent measures
that quantify the relative quality of alternative solutions of reasoning
problems or the relative desirability of various policies in control, decision,
and planning problems. We show that these measures may be interpreted as truth
values in a multi valued logic and propose mechanisms for the representation of
complex constraints as combinations of simpler restrictions. These extended
logical operations permit also the combination and aggregation of goal-specific
quality measures into global measures of utility. We identify also relations
that represent differential preferences between alternative solutions and
relate them to the previously defined desirability measures. Extending
conventional modal logic formulations, we introduce structures for the
representation of ignorance about the utility of alternative solutions.
Finally, we examine relations between these concepts and similarity based
semantic models of fuzzy logic.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"Pulcinella: A General Tool for Propagating Uncertainty in Valuation
  Networks","We present PULCinella and its use in comparing uncertainty theories.
PULCinella is a general tool for Propagating Uncertainty based on the Local
Computation technique of Shafer and Shenoy. It may be specialized to different
uncertainty theories: at the moment, Pulcinella can propagate probabilities,
belief functions, Boolean values, and possibilities. Moreover, Pulcinella
allows the user to easily define his own specializations. To illustrate
Pulcinella, we analyze two examples by using each of the four theories above.
In the first one, we mainly focus on intrinsic differences between theories. In
the second one, we take a knowledge engineer viewpoint, and check the adequacy
of each theory to a given problem.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Structuring Bodies of Evidence,"In this article we present two ways of structuring bodies of evidence, which
allow us to reduce the complexity of the operations usually performed in the
framework of evidence theory. The first structure just partitions the focal
elements in a body of evidence by their cardinality. With this structure we are
able to reduce the complexity on the calculation of the belief functions Bel,
Pl, and Q. The other structure proposed here, the Hierarchical Trees, permits
us to reduce the complexity of the calculation of Bel, Pl, and Q, as well as of
the Dempster's rule of combination in relation to the brute-force algorithm.
Both these structures do not require the generation of all the subsets of the
reference domain.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"On the Generation of Alternative Explanations with Implications for
  Belief Revision","In general, the best explanation for a given observation makes no promises on
how good it is with respect to other alternative explanations. A major
deficiency of message-passing schemes for belief revision in Bayesian networks
is their inability to generate alternatives beyond the second best. In this
paper, we present a general approach based on linear constraint systems that
naturally generates alternative explanations in an orderly and highly efficient
manner. This approach is then applied to cost-based abduction problems as well
as belief revision in Bayesian net works.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Completing Knowledge by Competing Hierarchies,"A control strategy for expert systems is presented which is based on Shafer's
Belief theory and the combination rule of Dempster. In contrast to well known
strategies it is not sequentially and hypotheses-driven, but parallel and self
organizing, determined by the concept of information gain. The information
gain, calculated as the maximal difference between the actual evidence
distribution in the knowledge base and the potential evidence determines each
consultation step. Hierarchically structured knowledge is an important
representation form and experts even use several hierarchies in parallel for
constituting their knowledge. Hence the control strategy is applied to a
layered set of distinct hierarchies. Depending on the actual data one of these
hierarchies is chosen by the control strategy for the next step in the
reasoning process. Provided the actual data are well matched to the structure
of one hierarchy, this hierarchy remains selected for a longer consultation
time. If no good match can be achieved, a switch from the actual hierarchy to a
competing one will result, very similar to the phenomenon of restructuring in
problem solving tasks. Up to now the control strategy is restricted to multi
hierarchical knowledge bases with disjunct hierarchies. It is implemented in
the expert system IBIG (inference by information gain), being presently applied
to acquired speech disorders (aphasia).","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Graph-Based Inference Method for Conditional Independence,"The graphoid axioms for conditional independence, originally described by
Dawid [1979], are fundamental to probabilistic reasoning [Pearl, 19881. Such
axioms provide a mechanism for manipulating conditional independence assertions
without resorting to their numerical definition. This paper explores a
representation for independence statements using multiple undirected graphs and
some simple graphical transformations. The independence statements derivable in
this system are equivalent to those obtainable by the graphoid axioms.
Therefore, this is a purely graphical proof technique for conditional
independence.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Fusion Algorithm for Solving Bayesian Decision Problems,"This paper proposes a new method for solving Bayesian decision problems. The
method consists of representing a Bayesian decision problem as a
valuation-based system and applying a fusion algorithm for solving it. The
fusion algorithm is a hybrid of local computational methods for computation of
marginals of joint probability distributions and the local computational
methods for discrete optimization problems.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Algorithms for Irrelevance-Based Partial MAPs,"Irrelevance-based partial MAPs are useful constructs for domain-independent
explanation using belief networks. We look at two definitions for such partial
MAPs, and prove important properties that are useful in designing algorithms
for computing them effectively. We make use of these properties in modifying
our standard MAP best-first algorithm, so as to handle irrelevance-based
partial MAPs.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
About Updating,"Survey of several forms of updating, with a practical illustrative example.
We study several updating (conditioning) schemes that emerge naturally from a
common scenarion to provide some insights into their meaning. Updating is a
subtle operation and there is no single method, no single 'good' rule. The
choice of the appropriate rule must always be given due consideration. Planchet
(1989) presents a mathematical survey of many rules. We focus on the practical
meaning of these rules. After summarizing the several rules for conditioning,
we present an illustrative example in which the various forms of conditioning
can be explained.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Compressed Constraints in Probabilistic Logic and Their Revision,"In probabilistic logic entailments, even moderate size problems can yield
linear constraint systems with so many variables that exact methods are
impractical. This difficulty can be remedied in many cases of interest by
introducing a three valued logic (true, false, and ""don't care""). The
three-valued approach allows the construction of ""compressed"" constraint
systems which have the same solution sets as their two-valued counterparts, but
which may involve dramatically fewer variables. Techniques to calculate point
estimates for the posterior probabilities of entailed sentences are discussed.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Detecting Causal Relations in the Presence of Unmeasured Variables,"The presence of latent variables can greatly complicate inferences about
causal relations between measured variables from statistical data. In many
cases, the presence of latent variables makes it impossible to determine for
two measured variables A and B, whether A causes B, B causes A, or there is
some common cause. In this paper I present several theorems that state
conditions under which it is possible to reliably infer the causal relation
between two measured variables, regardless of whether latent variables are
acting or not.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"A Method for Integrating Utility Analysis into an Expert System for
  Design Evaluation","In mechanical design, there is often unavoidable uncertainty in estimates of
design performance. Evaluation of design alternatives requires consideration of
the impact of this uncertainty. Expert heuristics embody assumptions regarding
the designer's attitude towards risk and uncertainty that might be reasonable
in most cases but inaccurate in others. We present a technique to allow
designers to incorporate their own unique attitude towards uncertainty as
opposed to those assumed by the domain expert's rules. The general approach is
to eliminate aspects of heuristic rules which directly or indirectly include
assumptions regarding the user's attitude towards risk, and replace them with
explicit, user-specified probabilistic multi attribute utility and probability
distribution functions. We illustrate the method in a system for material
selection for automobile bumpers.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
From Relational Databases to Belief Networks,"The relationship between belief networks and relational databases is
examined. Based on this analysis, a method to construct belief networks
automatically from statistical relational data is proposed. A comparison
between our method and other methods shows that our method has several
advantages when generalization or prediction is deeded.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
A Monte-Carlo Algorithm for Dempster-Shafer Belief,"A very computationally-efficient Monte-Carlo algorithm for the calculation of
Dempster-Shafer belief is described. If Bel is the combination using Dempster's
Rule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),
Bel(b) can be calculated in time linear in 1(31 and m (given that the weight of
conflict is bounded). The algorithm can also be used to improve the complexity
of the Shenoy-Shafer algorithms on Markov trees, and be generalised to
calculate Dempster-Shafer Belief over other logics.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Compatibility of Quantitative and Qualitative Representations of Belief,"The compatibility of quantitative and qualitative representations of beliefs
was studied extensively in probability theory. It is only recently that this
important topic is considered in the context of belief functions. In this
paper, the compatibility of various quantitative belief measures and
qualitative belief structures is investigated. Four classes of belief measures
considered are: the probability function, the monotonic belief function,
Shafer's belief function, and Smets' generalized belief function. The analysis
of their individual compatibility with different belief structures not only
provides a sound b<msis for these quantitative measures, but also alleviates
some of the difficulties in the acquisition and interpretation of numeric
belief numbers. It is shown that the structure of qualitative probability is
compatible with monotonic belief functions. Moreover, a belief structure
slightly weaker than that of qualitative belief is compatible with Smets'
generalized belief functions.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
An Efficient Implementation of Belief Function Propagation,"The local computation technique (Shafer et al. 1987, Shafer and Shenoy 1988,
Shenoy and Shafer 1986) is used for propagating belief functions in so called a
Markov Tree. In this paper, we describe an efficient implementation of belief
function propagation on the basis of the local computation technique. The
presented method avoids all the redundant computations in the propagation
process, and so makes the computational complexity decrease with respect to
other existing implementations (Hsia and Shenoy 1989, Zarley et al. 1988). We
also give a combined algorithm for both propagation and re-propagation which
makes the re-propagation process more efficient when one or more of the prior
belief functions is changed.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
"A Non-Numeric Approach to Multi-Criteria/Multi-Expert Aggregation Based
  on Approximate Reasoning","We describe a technique that can be used for the fusion of multiple sources
of information as well as for the evaluation and selection of alternatives
under multi-criteria. Three important properties contribute to the uniqueness
of the technique introduced. The first is the ability to do all necessary
operations and aggregations with information that is of a nonnumeric linguistic
nature. This facility greatly reduces the burden on the providers of
information, the experts. A second characterizing feature is the ability
assign, again linguistically, differing importance to the criteria or in the
case of information fusion to the individual sources of information. A third
significant feature of the approach is its ability to be used as method to find
a consensus of the opinion of multiple experts on the issue of concern. The
techniques used in this approach are base on ideas developed from the theory of
approximate reasoning. We illustrate the approach with a problem of project
selection.","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
Why Do We Need Foundations for Modelling Uncertainties?,"Surely we want solid foundations. What kind of castle can we build on sand?
What is the point of devoting effort to balconies and minarets, if the
foundation may be so weak as to allow the structure to collapse of its own
weight? We want our foundations set on bedrock, designed to last for
generations. Who would want an architect who cannot certify the soundness of
the foundations of his buildings?","Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)"
DLOLIS-A: Description Logic based Text Ontology Learning,"Ontology Learning has been the subject of intensive study for the past
decade. Researchers in this field have been motivated by the possibility of
automatically building a knowledge base on top of text documents so as to
support reasoning based knowledge extraction. While most works in this field
have been primarily statistical (known as light-weight Ontology Learning) not
much attempt has been made in axiomatic Ontology Learning (called heavy-weight
Ontology Learning) from Natural Language text documents. Heavy-weight Ontology
Learning supports more precise formal logic-based reasoning when compared to
statistical ontology learning. In this paper we have proposed a sound Ontology
Learning tool DLOL_(IS-A) that maps English language IS-A sentences into their
equivalent Description Logic (DL) expressions in order to automatically
generate a consistent pair of T-box and A-box thereby forming both regular
(definitional form) and generalized (axiomatic form) DL ontology. The current
scope of the paper is strictly limited to IS-A sentences that exclude the
possible structures of: (i) implicative IS-A sentences, and (ii) ""Wh"" IS-A
questions. Other linguistic nuances that arise out of pragmatics and epistemic
of IS-A sentences are beyond the scope of this present work. We have adopted
Gold Standard based Ontology Learning evaluation on chosen IS-A rich Wikipedia
documents.",11 pages
Bipolar Fuzzy Soft sets and its applications in decision making problem,"In this article, we combine the concept of a bipolar fuzzy set and a soft
set. We introduce the notion of bipolar fuzzy soft set and study fundamental
properties. We study basic operations on bipolar fuzzy soft set. We define
exdended union, intersection of two bipolar fuzzy soft set. We also give an
application of bipolar fuzzy soft set into decision making problem. We give a
general algorithm to solve decision making problems by using bipolar fuzzy soft
set.",N/A
"Discrete Optimization of Statistical Sample Sizes in Simulation by Using
  the Hierarchical Bootstrap Method","The Bootstrap method application in simulation supposes that value of random
variables are not generated during the simulation process but extracted from
available sample populations. In the case of Hierarchical Bootstrap the
function of interest is calculated recurrently using the calculation tree. In
the present paper we consider the optimization of sample sizes in each vertex
of the calculation tree. The dynamic programming method is used for this aim.
Proposed method allows to decrease a variance of system characteristic
estimators.",9 pages
Design for a Darwinian Brain: Part 2. Cognitive Architecture,"The accumulation of adaptations in an open-ended manner during lifetime
learning is a holy grail in reinforcement learning, intrinsic motivation,
artificial curiosity, and developmental robotics. We present a specification
for a cognitive architecture that is capable of specifying an unlimited range
of behaviors. We then give examples of how it can stochastically explore an
interesting space of adjacent possible behaviors. There are two main novelties;
the first is a proper definition of the fitness of self-generated games such
that interesting games are expected to evolve. The second is a modular and
evolvable behavior language that has systematicity, productivity, and
compositionality, i.e. it is a physical symbol system. A part of the
architecture has already been implemented on a humanoid robot.","Submitted as Part 2 to Living Machines 2013, Natural History Museum,
  London. Code available on github as it is being developed to implement the
  cognitive architecture above, here...
  https://github.com/ctf20/DarwinianNeurodynamics"
Phase Transition and Network Structure in Realistic SAT Problems,"A fundamental question in Computer Science is understanding when a specific
class of problems go from being computationally easy to hard. Because of its
generality and applications, the problem of Boolean Satisfiability (aka SAT) is
often used as a vehicle for investigating this question. A signal result from
these studies is that the hardness of SAT problems exhibits a dramatic
easy-to-hard phase transition with respect to the problem constrainedness. Past
studies have however focused mostly on SAT instances generated using uniform
random distributions, where all constraints are independently generated, and
the problem variables are all considered of equal importance. These assumptions
are unfortunately not satisfied by most real problems. Our project aims for a
deeper understanding of hardness of SAT problems that arise in practice. We
study two key questions: (i) How does easy-to-hard transition change with more
realistic distributions that capture neighborhood sensitivity and
rich-get-richer aspects of real problems and (ii) Can these changes be
explained in terms of the network properties (such as node centrality and
small-worldness) of the clausal networks of the SAT problems. Our results,
based on extensive empirical studies and network analyses, provide important
structural and computational insights into realistic SAT problems. Our
extensive empirical studies show that SAT instances from realistic
distributions do exhibit phase transition, but the transition occurs sooner (at
lower values of constrainedness) than the instances from uniform random
distribution. We show that this behavior can be explained in terms of their
clausal network properties such as eigenvector centrality and small-worldness
(measured indirectly in terms of the clustering coefficients and average node
distance).",N/A
Disjunctive Logic Programs versus Normal Logic Programs,"This paper focuses on the expressive power of disjunctive and normal logic
programs under the stable model semantics over finite, infinite, or arbitrary
structures. A translation from disjunctive logic programs into normal logic
programs is proposed and then proved to be sound over infinite structures. The
equivalence of expressive power of two kinds of logic programs over arbitrary
structures is shown to coincide with that over finite structures, and coincide
with whether or not NP is closed under complement. Over finite structures, the
intranslatability from disjunctive logic programs to normal logic programs is
also proved if arities of auxiliary predicates and functions are bounded in a
certain way.",N/A
IFP-Intuitionistic fuzzy soft set theory and its applications,"In this work, we present definition of intuitionistic fuzzy parameterized
(IFP) intuitionistic fuzzy soft set and its operations. Then we define
IFP-aggregation operator to form IFP-intuitionistic fuzzy soft-decision-making
method which allows constructing more efficient decision processes.","This paper has been withdrawn by the author due to a crucial errors
  in the notation and some problems in the algorithm"
Duality in STRIPS planning,"We describe a duality mapping between STRIPS planning tasks. By exchanging
the initial and goal conditions, taking their respective complements, and
swapping for every action its precondition and delete list, one obtains for
every STRIPS task its dual version, which has a solution if and only if the
original does. This is proved by showing that the described transformation
essentially turns progression (forward search) into regression (backward
search) and vice versa.
  The duality sheds new light on STRIPS planning by allowing a transfer of
ideas from one search approach to the other. It can be used to construct new
algorithms from old ones, or (equivalently) to obtain new benchmarks from
existing ones. Experiments show that the dual versions of IPC benchmarks are in
general quite difficult for modern planners. This may be seen as a new
challenge. On the other hand, the cases where the dual versions are easier to
solve demonstrate that the duality can also be made useful in practice.","6 pages (two columns), 4 tables"
"Exploiting Functional Dependencies in Qualitative Probabilistic
  Reasoning","Functional dependencies restrict the potential interactions among variables
connected in a probabilistic network. This restriction can be exploited in
qualitative probabilistic reasoning by introducing deterministic variables and
modifying the inference rules to produce stronger conclusions in the presence
of functional relations. I describe how to accomplish these modifications in
qualitative probabilistic networks by exhibiting the update procedures for
graphical transformations involving probabilistic and deterministic variables
and combinations. A simple example demonstrates that the augmented scheme can
reduce qualitative ambiguity that would arise without the special treatment of
functional dependency. Analysis of qualitative synergy reveals that new
higher-order relations are required to reason effectively about synergistic
interactions among deterministic variables.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Qualitative Propagation and Scenario-based Explanation of Probabilistic
  Reasoning","Comprehensible explanations of probabilistic reasoning are a prerequisite for
wider acceptance of Bayesian methods in expert systems and decision support
systems. A study of human reasoning under uncertainty suggests two different
strategies for explaining probabilistic reasoning: The first, qualitative
belief propagation, traces the qualitative effect of evidence through a belief
network from one variable to the next. This propagation algorithm is an
alternative to the graph reduction algorithms of Wellman (1988) for inference
in qualitative probabilistic networks. It is based on a qualitative analysis of
intercausal reasoning, which is a generalization of Pearl's ""explaining away"",
and an alternative to Wellman's definition of qualitative synergy. The other,
Scenario-based reasoning, involves the generation of alternative causal
""stories"" accounting for the evidence. Comparing a few of the most probable
scenarios provides an approximate way to explain the results of probabilistic
reasoning. Both schemes employ causal as well as probabilistic knowledge.
Probabilities may be presented as phrases and/or numbers. Users can control the
style, abstraction and completeness of explanations.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Managing Uncertainty in Rule Based Cognitive Models,"An experiment replicated and extended recent findings on psychologically
realistic ways of modeling propagation of uncertainty in rule based reasoning.
Within a single production rule, the antecedent evidence can be summarized by
taking the maximum of disjunctively connected antecedents and the minimum of
conjunctively connected antecedents. The maximum certainty factor attached to
each of the rule's conclusions can be sealed down by multiplication with this
summarized antecedent certainty. Heckerman's modified certainty factor
technique can be used to combine certainties for common conclusions across
production rules.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Context-Dependent Similarity,"Attribute weighting and differential weighting, two major mechanisms for
computing context-dependent similarity or dissimilarity measures are studied
and compared. A dissimilarity measure based on subset size in the context is
proposed and its metrization and application are given. It is also shown that
while all attribute weighting dissimilarity measures are metrics differential
weighting dissimilarity measures are usually non-metric.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Similarity Networks for the Construction of Multiple-Faults Belief
  Networks","A similarity network is a tool for constructing belief networks for the
diagnosis of a single fault. In this paper, we examine modifications to the
similarity-network representation that facilitate the construction of belief
networks for the diagnosis of multiple coexisting faults.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Integrating Probabilistic, Taxonomic and Causal Knowledge in Abductive
  Diagnosis","We propose an abductive diagnosis theory that integrates probabilistic,
causal and taxonomic knowledge. Probabilistic knowledge allows us to select the
most likely explanation; causal knowledge allows us to make reasonable
independence assumptions; taxonomic knowledge allows causation to be modeled at
different levels of detail, and allows observations be described in different
levels of precision. Unlike most other approaches where a causal explanation is
a hypothesis that one or more causative events occurred, we define an
explanation of a set of observations to be an occurrence of a chain of
causation events. These causation events constitute a scenario where all the
observations are true. We show that the probabilities of the scenarios can be
computed from the conditional probabilities of the causation events. Abductive
reasoning is inherently complex even if only modest expressive power is
allowed. However, our abduction algorithm is exponential only in the number of
observations to be explained, and is polynomial in the size of the knowledge
base. This contrasts with many other abduction procedures that are exponential
in the size of the knowledge base.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
What is an Optimal Diagnosis?,"Within diagnostic reasoning there have been a number of proposed definitions
of a diagnosis, and thus of the most likely diagnosis, including most probable
posterior hypothesis, most probable interpretation, most probable covering
hypothesis, etc. Most of these approaches assume that the most likely diagnosis
must be computed, and that a definition of what should be computed can be made
a priori, independent of what the diagnosis is used for. We argue that the
diagnostic problem, as currently posed, is incomplete: it does not consider how
the diagnosis is to be used, or the utility associated with the treatment of
the abnormalities. In this paper we analyze several well-known definitions of
diagnosis, showing that the different definitions of the most likely diagnosis
have different qualitative meanings, even given the same input data. We argue
that the most appropriate definition of (optimal) diagnosis needs to take into
account the utility of outcomes and what the diagnosis is used for.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Kutato: An Entropy-Driven System for Construction of Probabilistic
  Expert Systems from Databases","Kutato is a system that takes as input a database of cases and produces a
belief network that captures many of the dependence relations represented by
those data. This system incorporates a module for determining the entropy of a
belief network and a module for constructing belief networks based on entropy
calculations. Kutato constructs an initial belief network in which all
variables in the database are assumed to be marginally independent. The entropy
of this belief network is calculated, and that arc is added that minimizes the
entropy of the resulting belief network. Conditional probabilities for an arc
are obtained directly from the database. This process continues until an
entropy-based threshold is reached. We have tested the system by generating
databases from networks using the probabilistic logic-sampling method, and then
using those databases as input to Kutato. The system consistently reproduces
the original belief networks with high fidelity.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Ideal Reformulation of Belief Networks,"The intelligent reformulation or restructuring of a belief network can
greatly increase the efficiency of inference. However, time expended for
reformulation is not available for performing inference. Thus, under time
pressure, there is a tradeoff between the time dedicated to reformulating the
network and the time applied to the implementation of a solution. We
investigate this partition of resources into time applied to reformulation and
time used for inference. We shall describe first general principles for
computing the ideal partition of resources under uncertainty. These principles
have applicability to a wide variety of problems that can be divided into
interdependent phases of problem solving. After, we shall present results of
our empirical study of the problem of determining the ideal amount of time to
devote to searching for clusters in belief networks. In this work, we acquired
and made use of probability distributions that characterize (1) the performance
of alternative heuristic search methods for reformulating a network instance
into a set of cliques, and (2) the time for executing inference procedures on
various belief networks. Given a preference model describing the value of a
solution as a function of the delay required for its computation, the system
selects an ideal time to devote to reformulation.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Computationally-Optimal Real-Resource Strategies,"This paper focuses on managing the cost of deliberation before action. In
many problems, the overall quality of the solution reflects costs incurred and
resources consumed in deliberation as well as the cost and benefit of
execution, when both the resource consumption in deliberation phase, and the
costs in deliberation and execution are uncertain and may be described by
probability distribution functions. A feasible (in terms of resource
consumption) strategy that minimizes the expected total cost is termed
computationally-optimal. For a situation with several independent,
uninterruptible methods to solve the problem, we develop a
pseudopolynomial-time algorithm to construct generate-and-test computationally
optimal strategy. We show this strategy-construction problem to be NP-complete,
and apply Bellman's Optimality Principle to solve it efficiently.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Problem Formulation as the Reduction of a Decision Model,"In this paper, we extend the QMRDT probabilistic model for the domain of
internal medicine to include decisions about treatments. In addition, we
describe how we can use the comprehensive decision model to construct a simpler
decision model for a specific patient. In so doing, we transform the task of
problem formulation to that of narrowing of a larger problem.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Dynamic Construction of Belief Networks,"We describe a method for incrementally constructing belief networks. We have
developed a network-construction language similar to a forward-chaining
language using data dependencies, but with additional features for specifying
distributions. Using this language, we can define parameterized classes of
probabilistic models. These parameterized models make it possible to apply
probabilistic reasoning to problems for which it is impractical to have a
single large static model.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A New Algorithm for Finding MAP Assignments to Belief Networks,"We present a new algorithm for finding maximum a-posterior) (MAP) assignments
of values to belief networks. The belief network is compiled into a network
consisting only of nodes with boolean (i.e. only 0 or 1) conditional
probabilities. The MAP assignment is then found using a best-first search on
the resulting network. We argue that, as one would anticipate, the algorithm is
exponential for the general case, but only linear in the size of the network
for poly trees.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Reducing Uncertainty in Navigation and Exploration,"A significant problem in designing mobile robot control systems involves
coping with the uncertainty that arises in moving about in an unknown or
partially unknown environment and relying on noisy or ambiguous sensor data to
acquire knowledge about that environment. We describe a control system that
chooses what activity to engage in next on the basis of expectations about how
the information re- turned as a result of a given activity will improve 2 its
knowledge about the spatial layout of its environment. Certain of the
higher-level components of the control system are specified in terms of
probabilistic decision models whose output is used to mediate the behavior of
lower-level control components responsible for movement and sensing.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Ergo: A Graphical Environment for Constructing Bayesian,"We describe an environment that considerably simplifies the process of
generating Bayesian belief networks. The system has been implemented on readily
available, inexpensive hardware, and provides clarity and high performance. We
present an introduction to Bayesian belief networks, discuss algorithms for
inference with these networks, and delineate the classes of problems that can
be solved with this paradigm. We then describe the hardware and software that
constitute the system, and illustrate Ergo's use with several example","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Decision Making with Interval Influence Diagrams,"In previous work (Fertig and Breese, 1989; Fertig and Breese, 1990) we
defined a mechanism for performing probabilistic reasoning in influence
diagrams using interval rather than point-valued probabilities. In this paper
we extend these procedures to incorporate decision nodes and interval-valued
value functions in the diagram. We derive the procedures for chance node
removal (calculating expected value) and decision node removal (optimization)
in influence diagrams where lower bounds on probabilities are stored at each
chance node and interval bounds are stored on the value function associated
with the diagram's value node. The output of the algorithm are a set of
admissible alternatives for each decision variable and a set of bounds on
expected value based on the imprecision in the input. The procedure can be
viewed as an approximation to a full e-dimensional sensitivity analysis where n
are the number of imprecise probability distributions in the input. We show the
transformations are optimal and sound. The performance of the algorithm on an
influence diagrams is investigated and compared to an exact algorithm.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A Randomized Approximation Algorithm of Logic Sampling,"In recent years, researchers in decision analysis and artificial intelligence
(AI) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of exact probabilistic
inference on belief networks almost certainly requires exponential computation
in the worst ease [3]. We have previously described a randomized approximation
scheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave
precise analytic bounds on the convergence of BN-RAS and showed how to trade
running time for accuracy in the evaluation of posterior marginal
probabilities. We now extend our previous results and demonstrate the
generality of our framework by applying similar mathematical techniques to the
analysis of convergence for logic sampling [7], an alternative simulation
algorithm for probabilistic inference.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Time, Chance, and Action","To operate intelligently in the world, an agent must reason about its
actions. The consequences of an action are a function of both the state of the
world and the action itself. Many aspects of the world are inherently
stochastic, so a representation for reasoning about actions must be able to
express chances of world states as well as indeterminacy in the effects of
actions and other events. This paper presents a propositional temporal
probability logic for representing and reasoning about actions. The logic can
represent the probability that facts hold and events occur at various times. It
can represent the probability that actions and other events affect the future.
It can represent concurrent actions and conditions that hold or change during
execution of an action. The model of probability relates probabilities over
time. The logical language integrates both modal and probabilistic constructs
and can thus represent and distinguish between possibility, probability, and
truth. Several examples illustrating the use of the logic are given.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A Dynamic Approach to Probabilistic Inference,"In this paper we present a framework for dynamically constructing Bayesian
networks. We introduce the notion of a background knowledge base of schemata,
which is a collection of parameterized conditional probability statements.
These schemata explicitly separate the general knowledge of properties an
individual may have from the specific knowledge of particular individuals that
may have these properties. Knowledge of individuals can be combined with this
background knowledge to create Bayesian networks, which can then be used in any
propagation scheme. We discuss the theory and assumptions necessary for the
implementation of dynamic Bayesian networks, and indicate where our approach
may be useful.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Approximations in Bayesian Belief Universe for Knowledge Based Systems,"When expert systems based on causal probabilistic networks (CPNs) reach a
certain size and complexity, the ""combinatorial explosion monster"" tends to be
present. We propose an approximation scheme that identifies rarely occurring
cases and excludes these from being processed as ordinary cases in a CPN-based
expert system. Depending on the topology and the probability distributions of
the CPN, the numbers (representing probabilities of state combinations) in the
underlying numerical representation can become very small. Annihilating these
numbers and utilizing the resulting sparseness through data structuring
techniques often results in several orders of magnitude of improvement in the
consumption of computer resources. Bounds on the errors introduced into a
CPN-based expert system through approximations are established. Finally,
reports on empirical studies of applying the approximation scheme to a
real-world CPN are given.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Robust Inference Policies,"A series of monte carlo studies were performed to assess the extent to which
different inference procedures robustly output reasonable belief values in the
context of increasing levels of judgmental imprecision. It was found that, when
compared to an equal-weights linear model, the Bayesian procedures are more
likely to deduce strong support for a hypothesis. But, the Bayesian procedures
are also more likely to strongly support the wrong hypothesis. Bayesian
techniques are more powerful, but are also more error prone.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Minimum Error Tree Decomposition,"This paper describes a generalization of previous methods for constructing
tree-structured belief network with hidden variables. The major new feature of
the described method is the ability to produce a tree decomposition even when
there are errors in the correlation data among the input variables. This is an
important extension of existing methods since the correlational coefficients
usually cannot be measured with precision. The technique involves using a
greedy search algorithm that locally minimizes an error function.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"A Polynomial Time Algorithm for Finding Bayesian Probabilities from
  Marginal Constraints","A method of calculating probability values from a system of marginal
constraints is presented. Previous systems for finding the probability of a
single attribute have either made an independence assumption concerning the
evidence or have required, in the worst case, time exponential in the number of
attributes of the system. In this paper a closed form solution to the
probability of an attribute given the evidence is found. The closed form
solution, however does not enforce the (non-linear) constraint that all terms
in the underlying distribution be positive. The equation requires O(r^3) steps
to evaluate, where r is the number of independent marginal constraints
describing the system at the time of evaluation. Furthermore, a marginal
constraint may be exchanged with a new constraint, and a new solution
calculated in O(r^2) steps. This method is appropriate for calculating
probabilities in a real time expert system","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Computation of Variances in Causal Networks,"The causal (belief) network is a well-known graphical structure for
representing independencies in a joint probability distribution. The exact
methods and the approximation methods, which perform probabilistic inference in
causal networks, often treat the conditional probabilities which are stored in
the network as certain values. However, if one takes either a subjectivistic or
a limiting frequency approach to probability, one can never be certain of
probability values. An algorithm for probabilistic inference should not only be
capable of reporting the inferred probabilities; it should also be capable of
reporting the uncertainty in these probabilities relative to the uncertainty in
the probabilities which are stored in the network. In section 2 of this paper a
method is given for determining the prior variances of the probabilities of all
the nodes. Section 3 contains an approximation method for determining the
variances in inferred probabilities.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A Sensitivity Analysis of Pathfinder,"Knowledge elicitation is one of the major bottlenecks in expert system
design. Systems based on Bayes nets require two types of information--network
structure and parameters (or probabilities). Both must be elicited from the
domain expert. In general, parameters have greater opacity than structure, and
more time is spent in their refinement than in any other phase of elicitation.
Thus, it is important to determine the point of diminishing returns, beyond
which further refinements will promise little (if any) improvement. Sensitivity
analyses address precisely this issue--the sensitivity of a model to the
precision of its parameters. In this paper, we report the results of a
sensitivity analysis of Pathfinder, a Bayes net based system for diagnosing
pathologies of the lymph system. This analysis is intended to shed some light
on the relative importance of structure and parameters to system performance,
as well as the sensitivity of a system based on a Bayes net to noise in its
assessed parameters.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
IDEAL: A Software Package for Analysis of Influence Diagrams,"IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software
environment for creation and evaluation of belief networks and influence
diagrams. IDEAL is primarily a research tool and provides an implementation of
many of the latest developments in belief network and influence diagram
evaluation in a unified framework. This paper describes IDEAL and some lessons
learned during its development.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
On the Equivalence of Causal Models,"Scientists often use directed acyclic graphs (days) to model the qualitative
structure of causal theories, allowing the parameters to be estimated from
observational data. Two causal models are equivalent if there is no experiment
which could distinguish one from the other. A canonical representation for
causal models is presented which yields an efficient graphical criterion for
deciding equivalence, and provides a theoretical basis for extracting causal
structures from empirical data. This representation is then extended to the
more general case of an embedded causal model, that is, a dag in which only a
subset of the variables are observable. The canonical representation presented
here yields an efficient algorithm for determining when two embedded causal
models reflect the same dependency information. This algorithm leads to a model
theoretic definition of causation in terms of statistical dependencies.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Application of Confidence Intervals to the Autonomous Acquisition of
  High-level Spatial Knowledge","Objects in the world usually appear in context, participating in spatial
relationships and interactions that are predictable and expected. Knowledge of
these contexts can be used in the task of using a mobile camera to search for a
specified object in a room. We call this the object search task. This paper is
concerned with representing this knowledge in a manner facilitating its
application to object search while at the same time lending itself to
autonomous learning by a robot. The ability for the robot to learn such
knowledge without supervision is crucial due to the vast number of possible
relationships that can exist for any given set of objects. Moreover, since a
robot will not have an infinite amount of time to learn, it must be able to
determine an order in which to look for possible relationships so as to
maximize the rate at which new knowledge is gained. In effect, there must be a
""focus of interest"" operator that allows the robot to choose which examples are
likely to convey the most new information and should be examined first. This
paper demonstrates how a representation based on statistical confidence
intervals allows the construction of a system that achieves the above goals. An
algorithm, based on the Highest Impact First heuristic, is presented as a means
for providing a ""focus of interest"" with which to control the learning process,
and examples are given.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Directed Reduction Algorithms and Decomposable Graphs,"In recent years, there have been intense research efforts to develop
efficient methods for probabilistic inference in probabilistic influence
diagrams or belief networks. Many people have concluded that the best methods
are those based on undirected graph structures, and that those methods are
inherently superior to those based on node reduction operations on the
influence diagram. We show here that these two approaches are essentially the
same, since they are explicitly or implicity building and operating on the same
underlying graphical structures. In this paper we examine those graphical
structures and show how this insight can lead to an improved class of directed
reduction methods.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Optimal Decomposition of Belief Networks,"In this paper, optimum decomposition of belief networks is discussed. Some
methods of decomposition are examined and a new method - the method of Minimum
Total Number of States (MTNS) - is proposed. The problem of optimum belief
network decomposition under our framework, as under all the other frameworks,
is shown to be NP-hard. According to the computational complexity analysis, an
algorithm of belief network decomposition is proposed in (Wee, 1990a) based on
simulated annealing.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Pruning Bayesian Networks for Efficient Computation,"This paper analyzes the circumstances under which Bayesian networks can be
pruned in order to reduce computational complexity without altering the
computation for variables of interest. Given a problem instance which consists
of a query and evidence for a set of nodes in the network, it is possible to
delete portions of the network which do not participate in the computation for
the query. Savings in computational complexity can be large when the original
network is not singly connected. Results analogous to those described in this
paper have been derived before [Geiger, Verma, and Pearl 89, Shachter 88] but
the implications for reducing complexity of the computations in Bayesian
networks have not been stated explicitly. We show how a preprocessing step can
be used to prune a Bayesian network prior to using standard algorithms to solve
a given problem instance. We also show how our results can be used in a
parallel distributed implementation in order to achieve greater savings. We
define a computationally equivalent subgraph of a Bayesian network. The
algorithm developed in [Geiger, Verma, and Pearl 89] is modified to construct
the subgraphs described in this paper with O(e) complexity, where e is the
number of edges in the Bayesian network. Finally, we define a minimal
computationally equivalent subgraph and prove that the subgraphs described are
minimal.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"On Heuristics for Finding Loop Cutsets in Multiply-Connected Belief
  Networks","We introduce a new heuristic algorithm for the problem of finding minimum
size loop cutsets in multiply connected belief networks. We compare this
algorithm to that proposed in [Suemmondt and Cooper, 1988]. We provide lower
bounds on the performance of these algorithms with respect to one another and
with respect to optimal. We demonstrate that no heuristic algorithm for this
problem cam be guaranteed to produce loop cutsets within a constant difference
from optimal. We discuss experimental results based on randomly generated
networks, and discuss future work and open questions.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"A Combination of Cutset Conditioning with Clique-Tree Propagation in the
  Pathfinder System","Cutset conditioning and clique-tree propagation are two popular methods for
performing exact probabilistic inference in Bayesian belief networks. Cutset
conditioning is based on decomposition of a subset of network nodes, whereas
clique-tree propagation depends on aggregation of nodes. We describe a means to
combine cutset conditioning and clique- tree propagation in an approach called
aggregation after decomposition (AD). We discuss the application of the AD
method in the Pathfinder system, a medical expert system that offers assistance
with diagnosis in hematopathology.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Possibility as Similarity: the Semantics of Fuzzy Logic,"This paper addresses fundamental issues on the nature of the concepts and
structures of fuzzy logic, focusing, in particular, on the conceptual and
functional differences that exist between probabilistic and possibilistic
approaches. A semantic model provides the basic framework to define
possibilistic structures and concepts by means of a function that quantifies
proximity, closeness, or resemblance between pairs of possible worlds. The
resulting model is a natural extension, based on multiple conceivability
relations, of the modal logic concepts of necessity and possibility. By
contrast, chance-oriented probabilistic concepts and structures rely on
measures of set extension that quantify the proportion of possible worlds where
a proposition is true. Resemblance between possible worlds is quantified by a
generalized similarity relation: a function that assigns a number between O and
1 to every pair of possible worlds. Using this similarity relation, which is a
form of numerical complement of a classic metric or distance, it is possible to
define and interpret the major constructs and methods of fuzzy logic:
conditional and unconditioned possibility and necessity distributions and the
generalized modus ponens of Zadeh.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection","Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Credibility Discounting in the Theory of Approximate Reasoning,"We are concerned with the problem of introducing credibility type information
into reasoning systems. The concept of credibility allows us to discount
information provided by agents. An important characteristic of this kind of
procedure is that a complete lack of credibility rather than resulting in the
negation of the information provided results in the nullification of the
information provided. We suggest a representational scheme for credibility
qualification in the theory of approximate reasoning. We discuss the concept of
relative credibility. By this idea we mean to indicate situations in which the
credibility of a piece of evidence is determined by its compatibility with
higher priority evidence. This situation leads to structures very much in the
spirit of nonmonotonic reasoning.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Updating with Belief Functions, Ordinal Conditioning Functions and
  Possibility Measures","This paper discusses how a measure of uncertainty representing a state of
knowledge can be updated when a new information, which may be pervaded with
uncertainty, becomes available. This problem is considered in various
framework, namely: Shafer's evidence theory, Zadeh's possibility theory,
Spohn's theory of epistemic states. In the two first cases, analogues of
Jeffrey's rule of conditioning are introduced and discussed. The relations
between Spohn's model and possibility theory are emphasized and Spohn's
updating rule is contrasted with the Jeffrey-like rule of conditioning in
possibility theory. Recent results by Shenoy on the combination of ordinal
conditional functions are reinterpreted in the language of possibility theory.
It is shown that Shenoy's combination rule has a well-known possibilistic
counterpart.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"The Transferable Belief Model and Other Interpretations of
  Dempster-Shafer's Model","Dempster-Shafer's model aims at quantifying degrees of belief But there are
so many interpretations of Dempster-Shafer's theory in the literature that it
seems useful to present the various contenders in order to clarify their
respective positions. We shall successively consider the classical probability
model, the upper and lower probabilities model, Dempster's model, the
transferable belief model, the evidentiary value model, the provability or
necessity model. None of these models has received the qualification of
Dempster-Shafer. In fact the transferable belief model is our interpretation
not of Dempster's work but of Shafer's work as presented in his book (Shafer
1976, Smets 1988). It is a ?purified' form of Dempster-Shafer's model in which
any connection with probability concept has been deleted. Any model for belief
has at least two components: one static that describes our state of belief, the
other dynamic that explains how to update our belief given new pieces of
information. We insist on the fact that both components must be considered in
order to study these models. Too many authors restrict themselves to the static
component and conclude that Dempster-Shafer theory is the same as some other
theory. But once the dynamic component is considered, these conclusions break
down. Any comparison based only on the static component is too restricted. The
dynamic component must also be considered as the originality of the models
based on belief functions lies in its dynamic component.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Valuation-Based Systems for Discrete Optimization,"This paper describes valuation-based systems for representing and solving
discrete optimization problems. In valuation-based systems, we represent
information in an optimization problem using variables, sample spaces of
variables, a set of values, and functions that map sample spaces of sets of
variables to the set of values. The functions, called valuations, represent the
factors of an objective function. Solving the optimization problem involves
using two operations called combination and marginalization. Combination tells
us how to combine the factors of the joint objective function. Marginalization
is either maximization or minimization. Solving an optimization problem can be
simply described as finding the marginal of the joint objective function for
the empty set. We state some simple axioms that combination and marginalization
need to satisfy to enable us to solve an optimization problem using local
computation. For optimization problems, the solution method of valuation-based
systems reduces to non-serial dynamic programming. Thus our solution method for
VBS can be regarded as an abstract description of dynamic programming. And our
axioms can be viewed as conditions that permit the use of dynamic programming.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Computational Aspects of the Mobius Transform,"In this paper we associate with every (directed) graph G a transformation
called the Mobius transformation of the graph G. The Mobius transformation of
the graph (O) is of major significance for Dempster-Shafer theory of evidence.
However, because it is computationally very heavy, the Mobius transformation
together with Dempster's rule of combination is a major obstacle to the use of
Dempster-Shafer theory for handling uncertainty in expert systems. The major
contribution of this paper is the discovery of the 'fast Mobius
transformations' of (O). These 'fast Mobius transformations' are the fastest
algorithms for computing the Mobius transformation of (O). As an easy but
useful application, we provide, via the commonality function, an algorithm for
computing Dempster's rule of combination which is much faster than the usual
one.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Using Dempster-Shafer Theory in Knowledge Representation,"In this paper, we suggest marrying Dempster-Shafer (DS) theory with Knowledge
Representation (KR). Born out of this marriage is the definition of
""Dempster-Shafer Belief Bases"", abstract data types representing uncertain
knowledge that use DS theory for representing strength of belief about our
knowledge, and the linguistic structures of an arbitrary KR system for
representing the knowledge itself. A formal result guarantees that both the
properties of the given KR system and of DS theory are preserved. The general
model is exemplified by defining DS Belief Bases where First Order Logic and
(an extension of) KRYPTON are used as KR systems. The implementation problem is
also touched upon.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"A Hierarchical Approach to Designing Approximate Reasoning-Based
  Controllers for Dynamic Physical Systems","This paper presents a new technique for the design of approximate reasoning
based controllers for dynamic physical systems with interacting goals. In this
approach, goals are achieved based on a hierarchy defined by a control
knowledge base and remain highly interactive during the execution of the
control task. The approach has been implemented in a rule-based computer
program which is used in conjunction with a prototype hardware system to solve
the cart-pole balancing problem in real-time. It provides a complementary
approach to the conventional analytical control methodology, and is of
substantial use where a precise mathematical model of the process being
controlled is not available.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Evidence Combination and Reasoning and Its Application to Real-World
  Problem-Solving","In this paper a new mathematical procedure is presented for combining
different pieces of evidence which are represented in the interval form to
reflect our knowledge about the truth of a hypothesis. Evidences may be
correlated to each other (dependent evidences) or conflicting in supports
(conflicting evidences). First, assuming independent evidences, we propose a
methodology to construct combination rules which obey a set of essential
properties. The method is based on a geometric model. We compare results
obtained from Dempster-Shafer's rule and the proposed combination rules with
both conflicting and non-conflicting data and show that the values generated by
proposed combining rules are in tune with our intuition in both cases.
Secondly, in the case that evidences are known to be dependent, we consider
extensions of the rules derived for handling conflicting evidence. The
performance of proposed rules are shown by different examples. The results show
that the proposed rules reasonably make decision under dependent evidences","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"On Some Equivalence Relations between Incidence Calculus and
  Dempster-Shafer Theory of Evidence","Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories
to describe agents' degrees of belief in propositions, thus being appropriate
to represent uncertainty in reasoning systems. This paper presents a
straightforward equivalence proof between some special cases of these theories.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Using Belief Functions for Uncertainty Management and Knowledge
  Acquisition: An Expert Application","This paper describes recent work on an ongoing project in medical diagnosis
at the University of Guelph. A domain on which experts are not very good at
pinpointing a single disease outcome is explored. On-line medical data is
available over a relatively short period of time. Belief Functions
(Dempster-Shafer theory) are first extracted from data and then modified with
expert opinions. Several methods for doing this are compared and results show
that one formulation statistically outperforms the others, including a method
suggested by Shafer. Expert opinions and statistically derived information
about dependencies among symptoms are also compared. The benefits of using
uncertainty management techniques as methods for knowledge acquisition from
data are discussed.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
An Architecture for Probabilistic Concept-Based Information Retrieval,"While concept-based methods for information retrieval can provide improved
performance over more conventional techniques, they require large amounts of
effort to acquire the concepts and their qualitative and quantitative
relationships. This paper discusses an architecture for probabilistic
concept-based information retrieval which addresses the knowledge acquisition
problem. The architecture makes use of the probabilistic networks technology
for representing and reasoning about concepts and includes a knowledge
acquisition component which partially automates the construction of concept
knowledge bases from data. We describe two experiments that apply the
architecture to the task of retrieving documents about terrorism from a set of
documents from the Reuters news service. The experiments provide positive
evidence that the architecture design is feasible and that there are advantages
to concept-based methods.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Amplitude-Based Approach to Evidence Accumulation,"We point out the need to use probability amplitudes rather than probabilities
to model evidence accumulation in decision processes involving real physical
sensors. Optical information processing systems are given as typical examples
of systems that naturally gather evidence in this manner. We derive a new,
amplitude-based generalization of the Hough transform technique used for object
recognition in machine vision. We argue that one should use complex Hough
accumulators and square their magnitudes to get a proper probabilistic
interpretation of the likelihood that an object is present. Finally, we suggest
that probability amplitudes may have natural applications in connectionist
models, as well as in formulating knowledge-based reasoning problems.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A Probabilistic Reasoning Environment,"A framework is presented for a computational theory of probabilistic
argument. The Probabilistic Reasoning Environment encodes knowledge at three
levels. At the deepest level are a set of schemata encoding the system's domain
knowledge. This knowledge is used to build a set of second-level arguments,
which are structured for efficient recapture of the knowledge used to construct
them. Finally, at the top level is a Bayesian network constructed from the
arguments. The system is designed to facilitate not just propagation of beliefs
and assimilation of evidence, but also the dynamic process of constructing a
belief network, evaluating its adequacy, and revising it when necessary.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
On Non-monotonic Conditional Reasoning,"This note is concerned with a formal analysis of the problem of non-monotonic
reasoning in intelligent systems, especially when the uncertainty is taken into
account in a quantitative way. A firm connection between logic and probability
is established by introducing conditioning notions by means of formal
structures that do not rely on quantitative measures. The associated
conditional logic, compatible with conditional probability evaluations, is
non-monotonic relative to additional evidence. Computational aspects of
conditional probability logic are mentioned. The importance of this development
lies on its role to provide a conceptual basis for various forms of evidence
combination and on its significance to unify multi-valued and non-monotonic
logics","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Decisions with Limited Observations over a Finite Product Space: the
  Klir Effect","Probability estimation by maximum entropy reconstruction of an initial
relative frequency estimate from its projection onto a hypergraph model of the
approximate conditional independence relations exhibited by it is investigated.
The results of this study suggest that use of this estimation technique may
improve the quality of decisions that must be made on the basis of limited
observations over a decomposable finite product space.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Fine-Grained Decision-Theoretic Search Control,"Decision-theoretic control of search has previously used as its basic unit.
of computation the generation and evaluation of a complete set of successors.
Although this simplifies analysis, it results in some lost opportunities for
pruning and satisficing. This paper therefore extends the analysis of the value
of computation to cover individual successor evaluations. The analytic
techniques used may prove useful for control of reasoning in more general
settings. A formula is developed for the expected value of a node, k of whose n
successors have been evaluated. This formula is used to estimate the value of
expanding further successors, using a general formula for the value of a
computation in game-playing developed in earlier work. We exhibit an improved
version of the MGSS* algorithm, giving empirical results for the game of
Othello.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Rules, Belief Functions and Default Logic","This paper describes a natural framework for rules, based on belief
functions, which includes a repre- sentation of numerical rules, default rules
and rules allowing and rules not allowing contraposition. In particular it
justifies the use of the Dempster-Shafer Theory for representing a particular
class of rules, Belief calculated being a lower probability given certain
independence assumptions on an underlying space. It shows how a belief function
framework can be generalised to other logics, including a general Monte-Carlo
algorithm for calculating belief, and how a version of Reiter's Default Logic
can be seen as a limiting case of a belief function formalism.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Combination of Evidence Using the Principle of Minimum Information Gain,"One of the most important aspects in any treatment of uncertain information
is the rule of combination for updating the degrees of uncertainty. The theory
of belief functions uses the Dempster rule to combine two belief functions
defined by independent bodies of evidence. However, with limited dependency
information about the accumulated belief the Dempster rule may lead to
unsatisfactory results. The present study suggests a method to determine the
accumulated belief based on the premise that the information gain from the
combination process should be minimum. This method provides a mechanism that is
equivalent to the Bayes rule when all the conditional probabilities are
available and to the Dempster rule when the normalization constant is equal to
one. The proposed principle of minimum information gain is shown to be
equivalent to the maximum entropy formalism, a special case of the principle of
minimum cross-entropy. The application of this principle results in a monotonic
increase in belief with accumulation of consistent evidence. The suggested
approach may provide a more reasonable criterion for identifying conflicts
among various bodies of evidence.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"Probabilistic Evaluation of Candidates and Symptom Clustering for
  Multidisorder Diagnosis","This paper derives a formula for computing the conditional probability of a
set of candidates, where a candidate is a set of disorders that explain a given
set of positive findings. Such candidate sets are produced by a recent method
for multidisorder diagnosis called symptom clustering. A symptom clustering
represents a set of candidates compactly as a cartesian product of differential
diagnoses. By evaluating the probability of a candidate set, then, a large set
of candidates can be validated or pruned simultaneously. The probability of a
candidate set is then specialized to obtain the probability of a single
candidate. Unlike earlier results, the equation derived here allows the
specification of positive, negative, and unknown symptoms and does not make
assumptions about disorders not in the candidate.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Extending Term Subsumption systems for Uncertainty Management,"A major difficulty in developing and maintaining very large knowledge bases
originates from the variety of forms in which knowledge is made available to
the KB builder. The objective of this research is to bring together two
complementary knowledge representation schemes: term subsumption languages,
which represent and reason about defining characteristics of concepts, and
proximate reasoning models, which deal with uncertain knowledge and data in
expert systems. Previous works in this area have primarily focused on
probabilistic inheritance. In this paper, we address two other important issues
regarding the integration of term subsumption-based systems and approximate
reasoning models. First, we outline a general architecture that specifies the
interactions between the deductive reasoner of a term subsumption system and an
approximate reasoner. Second, we generalize the semantics of terminological
language so that terminological knowledge can be used to make plausible
inferences. The architecture, combined with the generalized semantics, forms
the foundation of a synergistic tight integration of term subsumption systems
and approximate reasoning models.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Refinement and Coarsening of Bayesian Networks,"In almost all situation assessment problems, it is useful to dynamically
contract and expand the states under consideration as assessment proceeds.
Contraction is most often used to combine similar events or low probability
events together in order to reduce computation. Expansion is most often used to
make distinctions of interest which have significant probability in order to
improve the quality of the assessment. Although other uncertainty calculi,
notably Dempster-Shafer [Shafer, 1976], have addressed these operations, there
has not yet been any approach of refining and coarsening state spaces for the
Bayesian Network technology. This paper presents two operations for refining
and coarsening the state space in Bayesian Networks. We also discuss their
practical implications for knowledge acquisition.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Second Order Probabilities for Uncertain and Conflicting Evidence,"In this paper the elicitation of probabilities from human experts is
considered as a measurement process, which may be disturbed by random
'measurement noise'. Using Bayesian concepts a second order probability
distribution is derived reflecting the uncertainty of the input probabilities.
The algorithm is based on an approximate sample representation of the basic
probabilities. This sample is continuously modified by a stochastic simulation
procedure, the Metropolis algorithm, such that the sequence of successive
samples corresponds to the desired posterior distribution. The procedure is
able to combine inconsistent probabilities according to their reliability and
is applicable to general inference networks with arbitrary structure.
Dempster-Shafer probability mass functions may be included using specific
measurement distributions. The properties of the approach are demonstrated by
numerical experiments.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Computing Probability Intervals Under Independency Constraints,"Many AI researchers argue that probability theory is only capable of dealing
with uncertainty in situations where a full specification of a joint
probability distribution is available, and conclude that it is not suitable for
application in knowledge-based systems. Probability intervals, however,
constitute a means for expressing incompleteness of information. We present a
method for computing such probability intervals for probabilities of interest
from a partial specification of a joint probability distribution. Our method
improves on earlier approaches by allowing for independency relationships
between statistical variables to be exploited.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
"An Empirical Analysis of Likelihood-Weighting Simulation on a Large,
  Multiply-Connected Belief Network","We analyzed the convergence properties of likelihood- weighting algorithms on
a two-level, multiply connected, belief-network representation of the QMR
knowledge base of internal medicine. Specifically, on two difficult diagnostic
cases, we examined the effects of Markov blanket scoring, importance sampling,
demonstrating that the Markov blanket scoring and self-importance sampling
significantly improve the convergence of the simulation on our model.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Towards a Normative Theory of Scientific Evidence,"A scientific reasoning system makes decisions using objective evidence in the
form of independent experimental trials, propositional axioms, and constraints
on the probabilities of events. As a first step towards this goal, we propose a
system that derives probability intervals from objective evidence in those
forms. Our reasoning system can manage uncertainty about data and rules in a
rule based expert system. We expect that our system will be particularly
applicable to diagnosis and analysis in domains with a wealth of experimental
evidence such as medicine. We discuss limitations of this solution and propose
future directions for this research. This work can be considered a
generalization of Nilsson's ""probabilistic logic"" [Nil86] to intervals and
experimental observations.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
A Model for Non-Monotonic Reasoning Using Dempster's Rule,"Considerable attention has been given to the problem of non-monotonic
reasoning in a belief function framework. Earlier work (M. Ginsberg) proposed
solutions introducing meta-rules which recognized conditional independencies in
a probabilistic sense. More recently an e-calculus formulation of default
reasoning (J. Pearl) shows that the application of Dempster's rule to a
non-monotonic situation produces erroneous results. This paper presents a new
belief function interpretation of the problem which combines the rules in a way
which is more compatible with probabilistic results and respects conditions of
independence necessary for the application of Dempster's combination rule. A
new general framework for combining conflicting evidence is also proposed in
which the normalization factor becomes modified. This produces more intuitively
acceptable results.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Default Reasoning and the Transferable Belief Model,"Inappropriate use of Dempster's rule of combination has led some authors to
reject the Dempster-Shafer model, arguing that it leads to supposedly
unacceptable conclusions when defaults are involved. A most classic example is
about the penguin Tweety. This paper will successively present: the origin of
the miss-management of the Tweety example; two types of default; the correct
solution for both types based on the transferable belief model (our
interpretation of the Dempster-Shafer model (Shafer 1976, Smets 1988)); Except
when explicitly stated, all belief functions used in this paper are simple
support functions, i.e. belief functions for which only one proposition (the
focus) of the frame of discernment receives a positive basic belief mass with
the remaining mass being given to the tautology. Each belief function will be
described by its focus and the weight of the focus (e.g. m(A)=.9). Computation
of the basic belief masses are always performed by vacuously extending each
belief function to the product space built from all variables involved,
combining them on that space by Dempster's rule of combination, and projecting
the result to the space corresponding to each individual variable.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Separable and transitive graphoids,"We examine three probabilistic formulations of the sentence a and b are
totally unrelated with respect to a given set of variables U. First, two
variables a and b are totally independent if they are independent given any
value of any subset of the variables in U. Second, two variables are totally
uncoupled if U can be partitioned into two marginally independent sets
containing a and b respectively. Third, two variables are totally disconnected
if the corresponding nodes are disconnected in every belief network
representation. We explore the relationship between these three formulations of
unrelatedness and explain their relevance to the process of acquiring
probabilistic knowledge from human experts.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Analysis in HUGIN of Data Conflict,"After a brief introduction to causal probabilistic networks and the HUGIN
approach, the problem of conflicting data is discussed. A measure of conflict
is defined, and it is used in the medical diagnostic system MUNIN. Finally, it
is discussed how to distinguish between conflicting data and a rare case.","Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)"
Computing Datalog Rewritings beyond Horn Ontologies,"Rewriting-based approaches for answering queries over an OWL 2 DL ontology
have so far been developed mainly for Horn fragments of OWL 2 DL. In this
paper, we study the possibilities of answering queries over non-Horn ontologies
using datalog rewritings. We prove that this is impossible in general even for
very simple ontology languages, and even if PTIME = NP. Furthermore, we present
a resolution-based procedure for $\SHI$ ontologies that, in case it terminates,
produces a datalog rewriting of the ontology. Our procedure necessarily
terminates on DL-Lite_{bool}^H ontologies---an extension of OWL 2 QL with
transitive roles and Boolean connectives.",14 pages. To appear at IJCAI 2013
Lp : A Logic for Statistical Information,"This extended abstract presents a logic, called Lp, that is capable of
representing and reasoning with a wide variety of both qualitative and
quantitative statistical information. The advantage of this logical formalism
is that it offers a declarative representation of statistical knowledge;
knowledge represented in this manner can be used for a variety of reasoning
tasks. The logic differs from previous work in probability logics in that it
uses a probability distribution over the domain of discourse, whereas most
previous work (e.g., Nilsson [2], Scott et al. [3], Gaifinan [4], Fagin et al.
[5]) has investigated the attachment of probabilities to the sentences of the
logic (also, see Halpern [6] and Bacchus [7] for further discussion of the
differences). The logic Lp possesses some further important features. First, Lp
is a superset of first order logic, hence it can represent ordinary logical
assertions. This means that Lp provides a mechanism for integrating statistical
information and reasoning about uncertainty into systems based solely on logic.
Second, Lp possesses transparent semantics, based on sets and probabilities of
those sets. Hence, knowledge represented in Lp can be understood in terms of
the simple primative concepts of sets and probabilities. And finally, the there
is a sound proof theory that has wide coverage (the proof theory is complete
for certain classes of models). The proof theory captures a sufficient range of
valid inferences to subsume most previous probabilistic uncertainty reasoning
systems. For example, the linear constraints like those generated by Nilsson's
probabilistic entailment [2] can be generated by the proof theory, and the
Bayesian inference underlying belief nets [8] can be performed. In addition,
the proof theory integrates quantitative and qualitative reasoning as well as
statistical and logical reasoning. In the next section we briefly examine
previous work in probability logics, comparing it to Lp. Then we present some
of the varieties of statistical information that Lp is capable of expressing.
After this we present, briefly, the syntax, semantics, and proof theory of the
logic. We conclude with a few examples of knowledge representation and
reasoning in Lp, pointing out the advantages of the declarative representation
offered by Lp. We close with a brief discussion of probabilities as degrees of
belief, indicating how such probabilities can be generated from statistical
knowledge encoded in Lp. The reader who is interested in a more complete
treatment should consult Bacchus [7].","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Map Learning with Indistinguishable Locations,"Nearly all spatial reasoning problems involve uncertainty of one sort or
another. Uncertainty arises due to the inaccuracies of sensors used in
measuring distances and angles. We refer to this as directional uncertainty.
Uncertainty also arises in combining spatial information when one location is
mistakenly identified with another. We refer to this as recognition
uncertainty. Most problems in constructing spatial representations (maps) for
the purpose of navigation involve both directional and recognition uncertainty.
In this paper, we show that a particular class of spatial reasoning problems
involving the construction of representations of large-scale space can be
solved efficiently even in the presence of directional and recognition
uncertainty. We pay particular attention to the problems that arise due to
recognition uncertainty.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Temporal Reasoning with Probabilities,"In this paper we explore representations of temporal knowledge based upon the
formalism of Causal Probabilistic Networks (CPNs). Two different
?continuous-time? representations are proposed. In the first, the CPN includes
variables representing ?event-occurrence times?, possibly on different time
scales, and variables representing the ?state? of the system at these times. In
the second, the CPN describes the influences between random variables with
values in () representing dates, i.e. time-points associated with the
occurrence of relevant events. However, structuring a system of inter-related
dates as a network where all links commit to a single specific notion of cause
and effect is in general far from trivial and leads to severe difficulties. We
claim that we should recognize explicitly different kinds of relation between
dates, such as ?cause?, ?inhibition?, ?competition?, etc., and propose a method
whereby these relations are coherently embedded in a CPN using additional
auxiliary nodes corresponding to ""instrumental"" variables. Also discussed,
though not covered in detail, is the topic concerning how the quantitative
specifications to be inserted in a temporal CPN can be learned from specific
data.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Now that I Have a Good Theory of Uncertainty, What Else Do I Need?","Rather than discussing the isolated merits of a nominative theory of
uncertainty, this paper focuses on a class of problems, referred to as Dynamic
Classification Problem (DCP), which requires the integration of many theories,
including a prescriptive theory of uncertainty. We start by analyzing the
Dynamic Classification Problem and by defining its induced requirements on a
supporting (plausible) reasoning system. We provide a summary of the underlying
theory (based on the semantics of many-valed logics) and illustrate the
constraints imposed upon it to ensure the modularity and computational
performance required by the applications. We describe the technologies used for
knowledge engineering (such as object-based simulator to exercise requirements,
and development tools to build the Knowledge Base and functionally validate
it). We emphasize the difference between development environment and run-time
system, describe the rule cross-compiler, and the real-time inference engine
with meta-reasoning capabilities. Finally, we illustrate how our proposed
technology satisfies the pop's requirements and analyze some of the lessons
reamed from its applications to situation assessment problems for Pilot's
Associate and Submarine Commander Associate.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Uncertainty and Incompleteness,"Two major difficulties in using default logics are their intractability and
the problem of selecting among multiple extensions. We propose an approach to
these problems based on integrating nommonotonic reasoning with plausible
reasoning based on triangular norms. A previously proposed system for reasoning
with uncertainty (RUM) performs uncertain monotonic inferences on an acyclic
graph. We have extended RUM to allow nommonotonic inferences and cycles within
nonmonotonic rules. By restricting the size and complexity of the nommonotonic
cycles we can still perform efficient inferences. Uncertainty measures provide
a basis for deciding among multiple defaults. Different algorithms and
heuristics for finding the optimal defaults are discussed.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
BaRT: A Bayesian Reasoning Tool for Knowledge Based Systems,"As the technology for building knowledge based systems has matured, important
lessons have been learned about the relationship between the architecture of a
system and the nature of the problems it is intended to solve. We are
implementing a knowledge engineering tool called BART that is designed with
these lessons in mind. BART is a Bayesian reasoning tool that makes belief
networks and other probabilistic techniques available to knowledge engineers
building classificatory problem solvers. BART has already been used to develop
a decision aid for classifying ship images, and it is currently being used to
manage uncertainty in systems concerned with analyzing intelligence reports.
This paper discusses how state-of-the-art probabilistic methods fit naturally
into a knowledge based approach to classificatory problem solving, and
describes the current capabilities of BART.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Plan Recognition in Stories and in Life,"Plan recognition does not work the same way in stories and in ""real life""
(people tend to jump to conclusions more in stories). We present a theory of
this, for the particular case of how objects in stories (or in life) influence
plan recognition decisions. We provide a Bayesian network formalization of a
simple first-order theory of plans, and show how a particular network parameter
seems to govern the difference between ""life-like"" and ""story-like"" response.
We then show why this parameter would be influenced (in the desired way) by a
model of speaker (or author) topic selection which assumes that facts in
stories are typically ""relevant"".","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"An Empirical Evaluation of a Randomized Algorithm for Probabilistic
  Inference","In recent years, researchers in decision analysis and artificial intelligence
(Al) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of probabilistic inference in
belief networks is difficult and almost certainly intractable. K N ET, a
software environment for constructing knowledge-based systems within the
axiomatic framework of decision theory, contains a randomized approximation
scheme for probabilistic inference. The algorithm can, in many circumstances,
perform efficient approximate inference in large and richly interconnected
models of medical diagnosis. Unlike previously described stochastic algorithms
for probabilistic inference, the randomized approximation scheme computes a
priori bounds on running time by analyzing the structure and contents of the
belief network. In this article, we describe a randomized algorithm for
probabilistic inference and analyze its performance mathematically. Then, we
devote the major portion of the paper to a discussion of the algorithm's
empirical behavior. The results indicate that the generation of good trials
(that is, trials whose distribution closely matches the true distribution),
rather than the computation of numerous mediocre trials, dominates the
performance of stochastic simulation. Key words: probabilistic inference,
belief networks, stochastic simulation, computational complexity theory,
randomized algorithms.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Decision Making ""Biases"" and Support for Assumption-Based Higher-Order
  Reasoning","Unaided human decision making appears to systematically violate consistency
constraints imposed by normative theories; these biases in turn appear to
justify the application of formal decision-analytic models. It is argued that
both claims are wrong. In particular, we will argue that the ""confirmation
bias"" is premised on an overly narrow view of how conflicting evidence is and
ought to be handled. Effective decision aiding should focus on supporting the
contral processes by means of which knowledge is extended into novel situations
and in which assumptions are adopted, utilized, and revised. The Non- Monotonic
Probabilist represents initial work toward such an aid.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Automated Reasoning Using Possibilistic Logic: Semantics, Belief
  Revision and Variable Certainty Weights","In this paper an approach to automated deduction under uncertainty,based on
possibilistic logic, is proposed ; for that purpose we deal with clauses
weighted by a degree which is a lower bound of a necessity or a possibility
measure, according to the nature of the uncertainty. Two resolution rules are
used for coping with the different situations, and the refutation method can be
generalized. Besides the lower bounds are allowed to be functions of variables
involved in the clause, which gives hypothetical reasoning capabilities. The
relation between our approach and the idea of minimizing abnormality is briefly
discussed. In case where only lower bounds of necessity measures are involved,
a semantics is proposed, in which the completeness of the extended resolution
principle is proved. Moreover deduction from a partially inconsistent knowledge
base can be managed in this approach and displays some form of
non-monotonicity.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"How Much More Probable is ""Much More Probable""? Verbal Expressions for
  Probability Updates","Bayesian inference systems should be able to explain their reasoning to
users, translating from numerical to natural language. Previous empirical work
has investigated the correspondence between absolute probabilities and
linguistic phrases. This study extends that work to the correspondence between
changes in probabilities (updates) and relative probability phrases, such as
""much more likely"" or ""a little less likely."" Subjects selected such phrases to
best describe numerical probability updates. We examined three hypotheses about
the correspondence, and found the most descriptively accurate of these three to
be that each such phrase corresponds to a fixed difference in probability
(rather than fixed ratio of probabilities or of odds). The empirically derived
phrase selection function uses eight phrases and achieved a 72% accuracy in
correspondence with the subjects' actual usage.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Positive and Negative Explanations of Uncertain Reasoning in the
  Framework of Possibility Theory","This paper presents an approach for developing the explanation capabilities
of rule-based expert systems managing imprecise and uncertain knowledge. The
treatment of uncertainty takes place in the framework of possibility theory
where the available information concerning the value of a logical or numerical
variable is represented by a possibility distribution which restricts its more
or less possible values. We first discuss different kinds of queries asking for
explanations before focusing on the two following types : i) how, a particular
possibility distribution is obtained (emphasizing the main reasons only) ; ii)
why in a computed possibility distribution, a particular value has received a
possibility degree which is so high, so low or so contrary to the expectation.
The approach is based on the exploitation of equations in max-min algebra. This
formalism includes the limit case of certain and precise information.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Interval Influence Diagrams,"We describe a mechanism for performing probabilistic reasoning in influence
diagrams using interval rather than point valued probabilities. We derive the
procedures for node removal (corresponding to conditional expectation) and arc
reversal (corresponding to Bayesian conditioning) in influence diagrams where
lower bounds on probabilities are stored at each node. The resulting bounds for
the transformed diagram are shown to be optimal within the class of constraints
on probability distributions that can be expressed exclusively as lower bounds
on the component probabilities of the diagram. Sequences of these operations
can be performed to answer probabilistic queries with indeterminacies in the
input and for performing sensitivity analysis on an influence diagram. The
storage requirements and computational complexity of this approach are
comparable to those for point-valued probabilistic inference mechanisms, making
the approach attractive for performing sensitivity analysis and where
probability information is not available. Limited empirical data on an
implementation of the methodology are provided.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Weighing and Integrating Evidence for Stochastic Simulation in Bayesian
  Networks","Stochastic simulation approaches perform probabilistic inference in Bayesian
networks by estimating the probability of an event based on the frequency that
the event occurs in a set of simulation trials. This paper describes the
evidence weighting mechanism, for augmenting the logic sampling stochastic
simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic
sampling algorithm by weighting each simulation trial by the likelihood of a
network's evidence given the sampled state node values for that trial. We also
describe an enhancement to the basic algorithm which uses the evidential
integration technique [Chin and Cooper, 1987]. A comparison of the basic
evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987],
the logic sampling algorithm, and the evidence integration algorithm is
presented. The comparison is aided by analyzing the performance of the
algorithms in a simple example network.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
d-Separation: From Theorems to Algorithms,"An efficient algorithm is developed that identifies all independencies
implied by the topology of a Bayesian network. Its correctness and maximality
stems from the soundness and completeness of d-separation with respect to
probability theory. The algorithm runs in time O (l E l) where E is the number
of edges in the network.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"The Effects of Perfect and Sample Information on Fuzzy Utilities in
  Decision-Making","In this paper, we first consider a Bayesian framework and model the ""utility
function"" in terms of fuzzy random variables. On the basis of this model, we
define the ""prior (fuzzy) expected utility"" associated with each action, and
the corresponding ""posterior (fuzzy) expected utility given sample information
from a random experiment"". The aim of this paper is to analyze how sample
information can affect the expected utility. In this way, by using some fuzzy
preference relations, we conclude that sample information allows a decision
maker to increase the expected utility on the average. The upper bound on the
value of the expected utility is when the decision maker has perfect
information. Applications of this work to the field of artificial intelligence
are presented through two examples.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Deciding Consistency of Databases Containing Defeasible and Strict
  Information","We propose a norm of consistency for a mixed set of defeasible and strict
sentences, based on a probabilistic semantics. This norm establishes a clear
distinction between knowledge bases depicting exceptions and those containing
outright contradictions. We then define a notion of entailment based also on
probabilistic considerations and provide a characterization of the relation
between consistency and entailment. We derive necessary and sufficient
conditions for consistency, and provide a simple decision procedure for testing
consistency and deciding whether a sentence is entailed by a database. Finally,
it is shown that if al1 sentences are Horn clauses, consistency and entailment
can be tested in polynomial time.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"The Relationship between Knowledge, Belief and Certainty","We consider the relation between knowledge and certainty, where a fact is
known if it is true at all worlds an agent considers possible and is certain if
it holds with probability 1. We identify certainty with probabilistic belief.
We show that if we assume one fixed probability assignment, then the logic
KD45, which has been identified as perhaps the most appropriate for belief,
provides a complete axiomatization for reasoning about certainty. Just as an
agent may believe a fact although phi is false, he may be certain that a fact
phi, is true although phi is false. However, it is easy to see that an agent
can have such false (probabilistic) beliefs only at a set of worlds of
probability 0. If we restrict attention to structures where all worlds have
positive probability, then S5 provides a complete axiomatization. If we
consider a more general setting, where there might be a different probability
assignment at each world, then by placing appropriate conditions on the support
of the probability function (the set of worlds which have non-zero
probability), we can capture many other well-known modal logics, such as T and
S4. Finally, we consider which axioms characterize structures satisfying
Miller's principle.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Heuristic Search as Evidential Reasoning,"BPS, the Bayesian Problem Solver, applies probabilistic inference and
decision-theoretic control to flexible, resource-constrained problem-solving.
This paper focuses on the Bayesian inference mechanism in BPS, and contrasts it
with those of traditional heuristic search techniques. By performing sound
inference, BPS can outperform traditional techniques with significantly less
computational effort. Empirical tests on the Eight Puzzle show that after only
a few hundred node expansions, BPS makes better decisions than does the best
existing algorithm after several million node expansions","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
The Compilation of Decision Models,"We introduce and analyze the problem of the compilation of decision models
from a decision-theoretic perspective. The techniques described allow us to
evaluate various configurations of compiled knowledge given the nature of
evidential relationships in a domain, the utilities associated with alternative
actions, the costs of run-time delays, and the costs of memory. We describe
procedures for selecting a subset of the total observations available to be
incorporated into a compiled situation-action mapping, in the context of a
binary decision with conditional independence of evidence. The methods allow us
to incrementally select the best pieces of evidence to add to the set of
compiled knowledge in an engineering setting. After presenting several
approaches to compilation, we exercise one of the methods to provide insight
into the relationship between the distribution over weights of evidence and the
preferred degree of compilation.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
A Tractable Inference Algorithm for Diagnosing Multiple Diseases,"We examine a probabilistic model for the diagnosis of multiple diseases. In
the model, diseases and findings are represented as binary variables. Also,
diseases are marginally independent, features are conditionally independent
given disease instances, and diseases interact to produce findings via a noisy
OR-gate. An algorithm for computing the posterior probability of each disease,
given a set of observed findings, called quickscore, is presented. The time
complexity of the algorithm is O(nm-2m+), where n is the number of diseases, m+
is the number of positive findings and m- is the number of negative findings.
Although the time complexity of quickscore i5 exponential in the number of
positive findings, the algorithm is useful in practice because the number of
observed positive findings is usually far less than the number of diseases
under consideration. Performance results for quickscore applied to a
probabilistic version of Quick Medical Reference (QMR) are provided.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Bounded Conditioning: Flexible Inference for Decisions under Scarce
  Resources","We introduce a graceful approach to probabilistic inference called bounded
conditioning. Bounded conditioning monotonically refines the bounds on
posterior probabilities in a belief network with computation, and converges on
final probabilities of interest with the allocation of a complete resource
fraction. The approach allows a reasoner to exchange arbitrary quantities of
computational resource for incremental gains in inference quality. As such,
bounded conditioning holds promise as a useful inference technique for
reasoning under the general conditions of uncertain and varying reasoning
resources. The algorithm solves a probabilistic bounding problem in complex
belief networks by breaking the problem into a set of mutually exclusive,
tractable subproblems and ordering their solution by the expected effect that
each subproblem will have on the final answer. We introduce the algorithm,
discuss its characterization, and present its performance on several belief
networks, including a complex model for reasoning about problems in
intensive-care medicine.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Hierarchical Evidence Accumulation in the Pseiki System and Experiments
  in Model-Driven Mobile Robot Navigation","In this paper, we will review the process of evidence accumulation in the
PSEIKI system for expectation-driven interpretation of images of 3-D scenes.
Expectations are presented to PSEIKI as a geometrical hierarchy of
abstractions. PSEIKI's job is then to construct abstraction hierarchies in the
perceived image taking cues from the abstraction hierarchies in the
expectations. The Dempster-Shafer formalism is used for associating belief
values with the different possible labels for the constructed abstractions in
the perceived image. This system has been used successfully for autonomous
navigation of a mobile robot in indoor environments.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
A Decision-Theoretic Model for Using Scientific Data,"Many Artificial Intelligence systems depend on the agent's updating its
beliefs about the world on the basis of experience. Experiments constitute one
type of experience, so scientific methodology offers a natural environment for
examining the issues attendant to using this class of evidence. This paper
presents a framework which structures the process of using scientific data from
research reports for the purpose of making decisions, using decision analysis
as the basis for the structure and using medical research as the general
scientific domain. The structure extends the basic influence diagram for
updating belief in an object domain parameter of interest by expanding the
parameter into four parts: those of the patient, the population, the study
sample, and the effective study sample. The structure uses biases to perform
the transformation of one parameter into another, so that, for instance,
selection biases, in concert with the population parameter, yield the study
sample parameter. The influence diagram structure provides decision theoretic
justification for practices of good clinical research such as randomized
assignment and blindfolding of care providers. The model covers most research
designs used in medicine: case-control studies, cohort studies, and controlled
clinical trials, and provides an architecture to separate clearly between
statistical knowledge and domain knowledge. The proposed general model can be
the basis for clinical epidemiological advisory systems, when coupled with
heuristic pruning of irrelevant biases; of statistical workstations, when the
computational machinery for calculation of posterior distributions is added;
and of meta-analytic reviews, when multiple studies may impact on a single
population parameter.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
When Should a Decision Maker Ignore the Advice of a Decision Aid?,"This paper argues that the principal difference between decision aids and
most other types of information systems is the greater reliance of decision
aids on fallible algorithms--algorithms that sometimes generate incorrect
advice. It is shown that interactive problem solving with a decision aid that
is based on a fallible algorithm can easily result in aided performance which
is poorer than unaided performance, even if the algorithm, by itself, performs
significantly better than the unaided decision maker. This suggests that unless
certain conditions are satisfied, using a decision aid as an aid is
counterproductive. Some conditions under which a decision aid is best used as
an aid are derived.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Inference Policies,"It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Defeasible Decisions: What the Proposal is and isn't,"In two recent papers, I have proposed a description of decision analysis that
differs from the Bayesian picture painted by Savage, Jeffrey and other classic
authors. Response to this view has been either overly enthusiastic or unduly
pessimistic. In this paper I try to place the idea in its proper place, which
must be somewhere in between. Looking at decision analysis as defeasible
reasoning produces a framework in which planning and decision theory can be
integrated, but work on the details has barely begun. It also produces a
framework in which the meta-decision regress can be stopped in a reasonable
way, but it does not allow us to ignore meta-level decisions. The heuristics
for producing arguments that I have presented are only supposed to be
suggestive; but they are not open to the egregious errors about which some have
worried. And though the idea is familiar to those who have studied heuristic
search, it is somewhat richer because the control of dialectic is more
interesting than the deepening of search.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Experiments Using Belief Functions and Weights of Evidence incorporating
  Statistical Data and Expert Opinions","This paper presents some ideas and results of using uncertainty management
methods in the presence of data in preference to other statistical and machine
learning methods. A medical domain is used as a test-bed with data available
from a large hospital database system which collects symptom and outcome
information about patients. Data is often missing, of many variable types and
sample sizes for particular outcomes is not large. Uncertainty management
methods are useful for such domains and have the added advantage of allowing
for expert modification of belief values originally obtained from data.
Methodological considerations for using belief functions on statistical data
are dealt with in some detail. Expert opinions are Incorporated at various
levels of the project development and results are reported on an application to
liver disease diagnosis. Recent results contrasting the use of weights of
evidence and logistic regression on another medical domain are also presented.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Shootout-89: A Comparative Evaluation of Knowledge-based Systems that
  Forecast Severe Weather","During the summer of 1989, the Forecast Systems Laboratory of the National
Oceanic and Atmospheric Administration sponsored an evaluation of artificial
intelligence-based systems that forecast severe convective storms. The
evaluation experiment, called Shootout-89, took place in Boulder, and focussed
on storms over the northeastern Colorado foothills and plains (Moninger, et
al., 1990). Six systems participated in Shootout-89. These included traditional
expert systems, an analogy-based system, and a system developed using methods
from the cognitive science/judgment analysis tradition. Each day of the
exercise, the systems generated 2 to 9 hour forecasts of the probabilities of
occurrence of: non significant weather, significant weather, and severe
weather, in each of four regions in northeastern Colorado. A verification
coordinator working at the Denver Weather Service Forecast Office gathered
ground-truth data from a network of observers. Systems were evaluated on the
basis of several measures of forecast skill, and on other metrics such as
timeliness, ease of learning, and ease of use. Systems were generally easy to
operate, however the various systems required substantially different levels of
meteorological expertise on the part of their users--reflecting the various
operational environments for which the systems had been designed. Systems
varied in their statistical behavior, but on this difficult forecast problem,
the systems generally showed a skill approximately equal to that of persistence
forecasts and climatological (historical frequency) forecasts. The two systems
that appeared best able to discriminate significant from non significant
weather events were traditional expert systems. Both of these systems required
the operator to make relatively sophisticated meteorological judgments. We are
unable, based on only one summer's worth of data, to determine the extent to
which the greater skill of the two systems was due to the content of their
knowledge bases, or to the subjective judgments of the operator. A follow-on
experiment, Shootout-91, is currently being planned. Interested potential
participants are encouraged to contact the author at the address above.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Conditioning on Disjunctive Knowledge: Defaults and Probabilities,"Many writers have observed that default logics appear to contain the ""lottery
paradox"" of probability theory. This arises when a default ""proof by
contradiction"" lets us conclude that a typical X is not a Y where Y is an
unusual subclass of X. We show that there is a similar problem with default
""proof by cases"" and construct a setting where we might draw a different
conclusion knowing a disjunction than we would knowing any particular disjunct.
Though Reiter's original formalism is capable of representing this distinction,
other approaches are not. To represent and reason about this case, default
logicians must specify how a ""typical"" individual is selected. The problem is
closely related to Simpson's paradox of probability theory. If we accept a
simple probabilistic account of defaults based on the notion that one
proposition may favour or increase belief in another, the ""multiple extension
problem"" for both conjunctive and disjunctive knowledge vanishes.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Maximum Uncertainty Procedures for Interval-Valued Probability
  Distributions","Measures of uncertainty and divergence are introduced for interval-valued
probability distributions and are shown to have desirable mathematical
properties. A maximum uncertainty inference procedure for marginal interval
distributions is presented. A technique for reconstruction of interval
distributions from projections is developed based on this inference procedure","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"A Logical Interpretation of Dempster-Shafer Theory, with Application to
  Visual Recognition","We formulate Dempster Shafer Belief functions in terms of Propositional Logic
using the implicit notion of provability underlying Dempster Shafer Theory.
Given a set of propositional clauses, assigning weights to certain
propositional literals enables the Belief functions to be explicitly computed
using Network Reliability techniques. Also, the logical procedure corresponding
to updating Belief functions using Dempster's Rule of Combination is shown.
This analysis formalizes the implementation of Belief functions within an
Assumption-based Truth Maintenance System (ATMS). We describe the extension of
an ATMS-based visual recognition system, VICTORS, with this logical formulation
of Dempster Shafer theory. Without Dempster Shafer theory, VICTORS computes all
possible visual interpretations (i.e. all logical models) without determining
the best interpretation(s). Incorporating Dempster Shafer theory enables
optimal visual interpretations to be computed and a logical semantics to be
maintained.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Strategies for Generating Micro Explanations for Bayesian Belief
  Networks","Bayesian Belief Networks have been largely overlooked by Expert Systems
practitioners on the grounds that they do not correspond to the human inference
mechanism. In this paper, we introduce an explanation mechanism designed to
generate intuitive yet probabilistically sound explanations of inferences drawn
by a Bayesian Belief Network. In particular, our mechanism accounts for the
results obtained due to changes in the causal and the evidential support of a
node.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Evidence Absorption and Propagation through Evidence Reversals,"The arc reversal/node reduction approach to probabilistic inference is
extended to include the case of instantiated evidence by an operation called
""evidence reversal."" This not only provides a technique for computing posterior
joint distributions on general belief networks, but also provides insight into
the methods of Pearl [1986b] and Lauritzen and Spiegelhalter [1988]. Although
it is well understood that the latter two algorithms are closely related, in
fact all three algorithms are identical whenever the belief network is a
forest.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Simulation Approaches to General Probabilistic Inference on Belief
  Networks","A number of algorithms have been developed to solve probabilistic inference
problems on belief networks. These algorithms can be divided into two main
groups: exact techniques which exploit the conditional independence revealed
when the graph structure is relatively sparse, and probabilistic sampling
techniques which exploit the ""conductance"" of an embedded Markov chain when the
conditional probabilities have non-extreme values. In this paper, we
investigate a family of ""forward"" Monte Carlo sampling techniques similar to
Logic Sampling [Henrion, 1988] which appear to perform well even in some
multiply connected networks with extreme conditional probabilities, and thus
would be generally applicable. We consider several enhancements which reduce
the posterior variance using this approach and propose a framework and criteria
for choosing when to use those enhancements.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Decision under Uncertainty,"We derive axiomatically the probability function that should be used to make
decisions given any form of underlying uncertainty.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Freedom: A Measure of Second-order Uncertainty for Intervalic
  Probability Schemes","This paper discusses a new measure that is adaptable to certain intervalic
probability frameworks, possibility theory, and belief theory. As such, it has
the potential for wide use in knowledge engineering, expert systems, and
related problems in the human sciences. This measure (denoted here by F) has
been introduced in Smithson (1988) and is more formally discussed in Smithson
(1989a)o Here, I propose to outline the conceptual basis for F and compare its
properties with other measures of second-order uncertainty. I will argue that F
is an indicator of nonspecificity or alternatively, of freedom, as
distinguished from either ambiguity or vagueness.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Assessment, Criticism and Improvement of Imprecise Subjective
  Probabilities for a Medical Expert System","Three paediatric cardiologists assessed nearly 1000 imprecise subjective
conditional probabilities for a simple belief network representing congenital
heart disease, and the quality of the assessments has been measured using
prospective data on 200 babies. Quality has been assessed by a Brier scoring
rule, which decomposes into terms measuring lack of discrimination and
reliability. The results are displayed for each of 27 diseases and 24
questions, and generally the assessments are reliable although there was a
tendency for the probabilities to be too extreme. The imprecision allows the
judgements to be converted to implicit samples, and by combining with the
observed data the probabilities naturally adapt with experience. This appears
to be a practical procedure even for reasonably large expert systems.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Automated Construction of Sparse Bayesian Networks from Unstructured
  Probabilistic Models and Domain Information","An algorithm for automated construction of a sparse Bayesian network given an
unstructured probabilistic model and causal domain information from an expert
has been developed and implemented. The goal is to obtain a network that
explicitly reveals as much information regarding conditional independence as
possible. The network is built incrementally adding one node at a time. The
expert's information and a greedy heuristic that tries to keep the number of
arcs added at each step to a minimum are used to guide the search for the next
node to add. The probabilistic model is a predicate that can answer queries
about independencies in the domain. In practice the model can be implemented in
various ways. For example, the model could be a statistical independence test
operating on empirical data or a deductive prover operating on a set of
independence statements about the domain.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Making Decisions with Belief Functions,"A primary motivation for reasoning under uncertainty is to derive decisions
in the face of inconclusive evidence. However, Shafer's theory of belief
functions, which explicitly represents the underconstrained nature of many
reasoning problems, lacks a formal procedure for making decisions. Clearly,
when sufficient information is not available, no theory can prescribe actions
without making additional assumptions. Faced with this situation, some
assumption must be made if a clearly superior choice is to emerge. In this
paper we offer a probabilistic interpretation of a simple assumption that
disambiguates decision problems represented with belief functions. We prove
that it yields expected values identical to those obtained by a probabilistic
analysis that makes the same assumption. In addition, we show how the decision
analysis methodology frequently employed in probabilistic reasoning can be
extended for use with belief functions.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Efficient Parallel Estimation for Markov Random Fields,"We present a new, deterministic, distributed MAP estimation algorithm for
Markov Random Fields called Local Highest Confidence First (Local HCF). The
algorithm has been applied to segmentation problems in computer vision and its
performance compared with stochastic algorithms. The experiments show that
Local HCF finds better estimates than stochastic algorithms with much less
computation.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Comparing Expert Systems Built Using Different Uncertain Inference
  Systems","This study compares the inherent intuitiveness or usability of the most
prominent methods for managing uncertainty in expert systems, including those
of EMYCIN, PROSPECTOR, Dempster-Shafer theory, fuzzy set theory, simplified
probability theory (assuming marginal independence), and linear regression
using probability estimates. Participants in the study gained experience in a
simple, hypothetical problem domain through a series of learning trials. They
were then randomly assigned to develop an expert system using one of the six
Uncertain Inference Systems (UISs) listed above. Performance of the resulting
systems was then compared. The results indicate that the systems based on the
PROSPECTOR and EMYCIN models were significantly less accurate for certain types
of problems compared to systems based on the other UISs. Possible reasons for
these differences are discussed.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Directed Cycles in Belief Networks,"The most difficult task in probabilistic reasoning may be handling directed
cycles in belief networks. To the best knowledge of this author, there is no
serious discussion of this problem at all in the literature of probabilistic
reasoning so far.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Can Uncertainty Management be Realized in a Finite Totally Ordered
  Probability Algebra?","In this paper, the feasibility of using finite totally ordered probability
models under Alelinnas's Theory of Probabilistic Logic [Aleliunas, 1988] is
investigated. The general form of the probability algebra of these models is
derived and the number of possible algebras with given size is deduced. Based
on this analysis, we discuss problems of denominator-indifference and
ambiguity-generation that arise in reasoning by cases and abductive reasoning.
An example is given that illustrates how these problems arise. The
investigation shows that a finite probability model may be of very limited
usage.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
"Normalization and the Representation of Nonmonotonic Knowledge in the
  Theory of Evidence","We discuss the Dempster-Shafer theory of evidence. We introduce a concept of
monotonicity which is related to the diminution of the range between belief and
plausibility. We show that the accumulation of knowledge in this framework
exhibits a nonmonotonic property. We show how the belief structure can be used
to represent typical or commonsense knowledge.","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)"
Probability Aggregates in Probability Answer Set Programming,"Probability answer set programming is a declarative programming that has been
shown effective for representing and reasoning about a variety of probability
reasoning tasks. However, the lack of probability aggregates, e.g. {\em
expected values}, in the language of disjunctive hybrid probability logic
programs (DHPP) disallows the natural and concise representation of many
interesting problems. In this paper, we extend DHPP to allow arbitrary
probability aggregates. We introduce two types of probability aggregates; a
type that computes the expected value of a classical aggregate, e.g., the
expected value of the minimum, and a type that computes the probability of a
classical aggregate, e.g, the probability of sum of values. In addition, we
define a probability answer set semantics for DHPP with arbitrary probability
aggregates including monotone, antimonotone, and nonmonotone probability
aggregates. We show that the proposed probability answer set semantics of DHPP
subsumes both the original probability answer set semantics of DHPP and the
classical answer set semantics of classical disjunctive logic programs with
classical aggregates, and consequently subsumes the classical answer set
semantics of the original disjunctive logic programs. We show that the proposed
probability answer sets of DHPP with probability aggregates are minimal
probability models and hence incomparable, which is an important property for
nonmonotonic probability reasoning.",N/A
Model-based Bayesian Reinforcement Learning for Dialogue Management,"Reinforcement learning methods are increasingly used to optimise dialogue
policies from experience. Most current techniques are model-free: they directly
estimate the utility of various actions, without explicit model of the
interaction dynamics. In this paper, we investigate an alternative strategy
grounded in model-based Bayesian reinforcement learning. Bayesian inference is
used to maintain a posterior distribution over the model parameters, reflecting
the model uncertainty. This parameter distribution is gradually refined as more
data is collected and simultaneously used to plan the agent's actions. Within
this learning framework, we carried out experiments with two alternative
formalisations of the transition model, one encoded with standard multinomial
distributions, and one structured with probabilistic rules. We demonstrate the
potential of our approach with empirical results on a user simulator
constructed from Wizard-of-Oz data in a human-robot interaction scenario. The
results illustrate in particular the benefits of capturing prior domain
knowledge with high-level rules.",N/A
Fuzzy Aggregates in Fuzzy Answer Set Programming,"Fuzzy answer set programming is a declarative framework for representing and
reasoning about knowledge in fuzzy environments. However, the unavailability of
fuzzy aggregates in disjunctive fuzzy logic programs, DFLP, with fuzzy answer
set semantics prohibits the natural and concise representation of many
interesting problems. In this paper, we extend DFLP to allow arbitrary fuzzy
aggregates. We define fuzzy answer set semantics for DFLP with arbitrary fuzzy
aggregates including monotone, antimonotone, and nonmonotone fuzzy aggregates.
We show that the proposed fuzzy answer set semantics subsumes both the original
fuzzy answer set semantics of DFLP and the classical answer set semantics of
classical disjunctive logic programs with classical aggregates, and
consequently subsumes the classical answer set semantics of classical
disjunctive logic programs. We show that the proposed fuzzy answer sets of DFLP
with fuzzy aggregates are minimal fuzzy models and hence incomparable, which is
an important property for nonmonotonic fuzzy reasoning.",arXiv admin note: substantial text overlap with arXiv:1304.1684
The structure of Bayes nets for vision recognition,"This paper is part of a study whose goal is to show the effciency of using
Bayes networks to carry out model based vision calculations. [Binford et al.
1987] Recognition proceeds by drawing up a network model from the object's
geometric and functional description that predicts the appearance of an object.
Then this network is used to find the object within a photographic image. Many
existing and proposed techniques for vision recognition resemble the
uncertainty calculations of a Bayes net. In contrast, though, they lack a
derivation from first principles, and tend to rely on arbitrary parameters that
we hope to avoid by a network model. The connectedness of the network depends
on what independence considerations can be identified in the vision problem.
Greater independence leads to easier calculations, at the expense of the net's
expressiveness. Once this trade-off is made and the structure of the network is
determined, it should be possible to tailor a solution technique for it. This
paper explores the use of a network with multiply connected paths, drawing on
both techniques of belief networks [Pearl 86] and influence diagrams. We then
demonstrate how one formulation of a multiply connected network can be solved.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Summary of A New Normative Theory of Probabilistic Logic,"By probabilistic logic I mean a normative theory of belief that explains how
a body of evidence affects one's degree of belief in a possible hypothesis. A
new axiomatization of such a theory is presented which avoids a finite
additivity axiom, yet which retains many useful inference rules. Many of the
examples of this theory--its models do not use numerical probabilities. Put
another way, this article gives sharper answers to the two questions: 1.What
kinds of sets can used as the range of a probability function? 2.Under what
conditions is the range set of a probability function isomorphic to the set of
real numbers in the interval 10,1/ with the usual arithmetical operations?","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probability Distributions Over Possible Worlds,"In Probabilistic Logic Nilsson uses the device of a probability distribution
over a set of possible worlds to assign probabilities to the sentences of a
logical language. In his paper Nilsson concentrated on inference and associated
computational issues. This paper, on the other hand, examines the probabilistic
semantics in more detail, particularly for the case of first-order languages,
and attempts to explain some of the features and limitations of this form of
probability logic. It is pointed out that the device of assigning probabilities
to logical sentences has certain expressive limitations. In particular,
statistical assertions are not easily expressed by such a device. This leads to
certain difficulties with attempts to give probabilistic semantics to default
reasoning using probabilities assigned to logical sentences.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Hierarchical Evidence and Belief Functions,"Dempster/Shafer (D/S) theory has been advocated as a way of representing
incompleteness of evidence in a system's knowledge base. Methods now exist for
propagating beliefs through chains of inference. This paper discusses how rules
with attached beliefs, a common representation for knowledge in automated
reasoning systems, can be transformed into the joint belief functions required
by propagation algorithms. A rule is taken as defining a conditional belief
function on the consequent given the antecedents. It is demonstrated by example
that different joint belief functions may be consistent with a given set of
rules. Moreover, different representations of the same rules may yield
different beliefs on the consequent hypotheses.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"Decision-Theoretic Control of Problem Solving: Principles and
  Architecture","This paper presents an approach to the design of autonomous, real-time
systems operating in uncertain environments. We address issues of problem
solving and reflective control of reasoning under uncertainty in terms of two
fundamental elements: l) a set of decision-theoretic models for selecting among
alternative problem-solving methods and 2) a general computational architecture
for resource-bounded problem solving. The decisiontheoretic models provide a
set of principles for choosing among alternative problem-solving methods based
on their relative costs and benefits, where benefits are characterized in terms
of the value of information provided by the output of a reasoning activity. The
output may be an estimate of some uncertain quantity or a recommendation for
action. The computational architecture, called Schemer-ll, provides for
interleaving of and communication among various problem-solving subsystems.
These subsystems provide alternative approaches to information gathering,
belief refinement, solution construction, and solution execution. In
particular, the architecture provides a mechanism for interrupting the
subsystems in response to critical events. We provide a decision theoretic
account for scheduling problem-solving elements and for critical-event-driven
interruption of activities in an architecture such as Schemer-II.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"Induction and Uncertainty Management Techniques Applied to Veterinary
  Medical Diagnosis","This paper discusses a project undertaken between the Departments of
Computing Science, Statistics, and the College of Veterinary Medicine to design
a medical diagnostic system. On-line medical data has been collected in the
hospital database system for several years. A number of induction methods are
being used to extract knowledge from the data in an attempt to improve upon
simple diagnostic charts used by the clinicians. They also enhance the results
of classical statistical methods - finding many more significant variables. The
second part of the paper describes an essentially Bayesian method of evidence
combination using fuzzy events at an initial step. Results are presented and
comparisons are made with other methods.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
KNET: Integrating Hypermedia and Bayesian Modeling,"KNET is a general-purpose shell for constructing expert systems based on
belief networks and decision networks. Such networks serve as graphical
representations for decision models, in which the knowledge engineer must
define clearly the alternatives, states, preferences, and relationships that
constitute a decision basis. KNET contains a knowledge-engineering core written
in Object Pascal and an interface that tightly integrates HyperCard, a
hypertext authoring tool for the Apple Macintosh computer, into a novel
expert-system architecture. Hypertext and hypermedia have become increasingly
important in the storage management, and retrieval of information. In broad
terms, hypermedia deliver heterogeneous bits of information in dynamic,
extensively cross-referenced packages. The resulting KNET system features a
coherent probabilistic scheme for managing uncertainty, an objectoriented
graphics editor for drawing and manipulating decision networks, and HyperCard's
potential for quickly constructing flexible and friendly user interfaces. We
envision KNET as a useful prototyping tool for our ongoing research on a
variety of Bayesian reasoning problems, including tractable representation,
inference, and explanation.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
A Method for Using Belief Networks as Influence Diagrams,"This paper demonstrates a method for using belief-network algorithms to solve
influence diagram problems. In particular, both exact and approximation
belief-network algorithms may be applied to solve influence-diagram problems.
More generally, knowing the relationship between belief-network and
influence-diagram problems may be useful in the design and development of more
efficient influence diagram algorithms.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"Process, Structure, and Modularity in Reasoning with Uncertainty","Computational mechanisms for uncertainty management must support interactive
and incremental problem formulation, inference, hypothesis testing, and
decision making. However, most current uncertainty inference systems
concentrate primarily on inference, and provide no support for the larger
issues. We present a computational approach to uncertainty management which
provides direct support for the dynamic, incremental aspect of this task, while
at the same time permitting direct representation of the structure of
evidential relationships. At the same time, we show that this approach responds
to the modularity concerns of Heckerman and Horvitz [Heck87]. This paper
emphasizes examples of the capabilities of this approach. Another paper
[D'Am89] details the representations and algorithms involved.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probabilistic Causal Reasoning,"Predicting the future is an important component of decision making. In most
situations, however, there is not enough information to make accurate
predictions. In this paper, we develop a theory of causal reasoning for
predictive inference under uncertainty. We emphasize a common type of
prediction that involves reasoning about persistence: whether or not a
proposition once made true remains true at some later time. We provide a
decision procedure with a polynomial-time algorithm for determining the
probability of the possible consequences of a set events and initial
conditions. The integration of simple probability theory with temporal
projection enables us to circumvent problems that nonmonotonic temporal
reasoning schemes have in dealing with persistence. The ideas in this paper
have been implemented in a prototype system that refines a database of causal
rules in the course of applying those rules to construct and carry out plans in
a manufacturing domain.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"Modeling uncertain and vague knowledge in possibility and evidence
  theories","This paper advocates the usefulness of new theories of uncertainty for the
purpose of modeling some facets of uncertain knowledge, especially vagueness,
in AI. It can be viewed as a partial reply to Cheeseman's (among others)
defense of probability.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"A Temporal Logic for Uncertain Events and An Outline of A Possible
  Implementation in An Extension of PROLOG","There is uncertainty associated with the occurrence of many events in real
life. In this paper we develop a temporal logic to deal with such uncertain
events and outline a possible implementation in an extension of PROLOG. Events
are represented as fuzzy sets with the membership function giving the
possibility of occurrence of the event in a given interval of time. The
developed temporal logic is simple but powerful. It can determine effectively
the various temporal relations between uncertain events or their combinations.
PROLOG provides a uniform substrate on which to effectively implement such a
temporal logic for uncertain events","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Uncertainty Management for Fuzzy Decision Support Systems,"A new approach for uncertainty management for fuzzy, rule based decision
support systems is proposed: The domain expert's knowledge is expressed by a
set of rules that frequently refer to vague and uncertain propositions. The
certainty of propositions is represented using intervals [a, b] expressing that
the proposition's probability is at least a and at most b. Methods and
techniques for computing the overall certainty of fuzzy compound propositions
that have been defined by using logical connectives 'and', 'or' and 'not' are
introduced. Different inference schemas for applying fuzzy rules by using modus
ponens are discussed. Different algorithms for combining evidence that has been
received from different rules for the same proposition are provided. The
relationship of the approach to other approaches is analyzed and its problems
of knowledge acquisition and knowledge representation are discussed in some
detail. The basic concepts of a rule-based programming language called PICASSO,
for which the approach is a theoretical foundation, are outlined.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probability as a Modal Operator,"This paper argues for a modal view of probability. The syntax and semantics
of one particularly strong probability logic are discussed and some examples of
the use of the logic are provided. We show that it is both natural and useful
to think of probability as a modal operator. Contrary to popular belief in AI,
a probability ranging between 0 and 1 represents a continuum between
impossibility and necessity, not between simple falsity and truth. The present
work provides a clear semantics for quantification into the scope of the
probability operator and for higher-order probabilities. Probability logic is a
language for expressing both probabilistic and logical concepts.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Truth Maintenance Under Uncertainty,"This paper addresses the problem of resolving errors under uncertainty in a
rule-based system. A new approach has been developed that reformulates this
problem as a neural-network learning problem. The strength and the fundamental
limitations of this approach are explored and discussed. The main result is
that neural heuristics can be applied to solve some but not all problems in
rule-based systems.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Bayesian Assessment of a Connectionist Model for Fault Detection,"A previous paper [2] showed how to generate a linear discriminant network
(LDN) that computes likely faults for a noisy fault detection problem by using
a modification of the perceptron learning algorithm called the pocket
algorithm. Here we compare the performance of this connectionist model with
performance of the optimal Bayesian decision rule for the example that was
previously described. We find that for this particular problem the
connectionist model performs about 97% as well as the optimal Bayesian
procedure. We then define a more general class of noisy single-pattern boolean
(NSB) fault detection problems where each fault corresponds to a single
:pattern of boolean instrument readings and instruments are independently
noisy. This is equivalent to specifying that instrument readings are
probabilistic but conditionally independent given any particular fault. We
prove:
  1. The optimal Bayesian decision rule for every NSB fault detection problem
is representable by an LDN containing no intermediate nodes. (This slightly
extends a result first published by Minsky & Selfridge.) 2. Given an NSB fault
detection problem, then with arbitrarily high probability after sufficient
iterations the pocket algorithm will generate an LDN that computes an optimal
Bayesian decision rule for that problem. In practice we find that a reasonable
number of iterations of the pocket algorithm produces a network with good, but
not optimal, performance.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
On the Logic of Causal Models,"This paper explores the role of Directed Acyclic Graphs (DAGs) as a
representation of conditional independence relationships. We show that DAGs
offer polynomially sound and complete inference mechanisms for inferring
conditional independence relationships from a given causal set of such
relationships. As a consequence, d-separation, a graphical criterion for
identifying independencies in a DAG, is shown to uncover more valid
independencies then any other criterion. In addition, we employ the Armstrong
property of conditional independence to show that the dependence relationships
displayed by a DAG are inherently consistent, i.e. for every DAG D there exists
some probability distribution P that embodies all the conditional
independencies displayed in D and none other.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
The Optimality of Satisficing Solutions,"This paper addresses a prevailing assumption in single-agent heuristic search
theory- that problem-solving algorithms should guarantee shortest-path
solutions, which are typically called optimal. Optimality implies a metric for
judging solution quality, where the optimal solution is the solution with the
highest quality. When path-length is the metric, we will distinguish such
solutions as p-optimal.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
An Empirical Comparison of Three Inference Methods,"In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988). LaTex errors corrected in this version"
Parallel Belief Revision,"This paper describes a formal system of belief revision developed by Wolfgang
Spohn and shows that this system has a parallel implementation that can be
derived from an influence diagram in a manner similar to that in which Bayesian
networks are derived. The proof rests upon completeness results for an
axiomatization of the notion of conditional independence, with the Spohn system
being used as a semantics for the relation of conditional independence.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Stochastic Sensitivity Analysis Using Fuzzy Influence Diagrams,"The practice of stochastic sensitivity analysis described in the decision
analysis literature is a testimonial to the need for considering deviations
from precise point estimates of uncertainty. We propose the use of Bayesian
fuzzy probabilities within an influence diagram computational scheme for
performing sensitivity analysis during the solution of probabilistic inference
and decision problems. Unlike other parametric approaches, the proposed scheme
does not require resolving the problem for the varying probability point
estimates. We claim that the solution to fuzzy influence diagrams provides as
much information as the classical point estimate approach plus additional
information concerning stochastic sensitivity. An example based on diagnostic
decision making in microcomputer assembly is used to illustrate this idea. We
claim that the solution to fuzzy influence diagrams provides as much
information as the classical point estimate approach plus additional interval
information that is useful for stochastic sensitivity analysis.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
A Representation of Uncertainty to Aid Insight into Decision Models,"Many real world models can be characterized as weak, meaning that there is
significant uncertainty in both the data input and inferences. This lack of
determinism makes it especially difficult for users of computer decision aids
to understand and have confidence in the models. This paper presents a
representation for uncertainty and utilities that serves as a framework for
graphical summary and computer-generated explanation of decision models. The
application described that tests the methodology is a computer decision aid
designed to enhance the clinician-patient consultation process for patients
with angina (chest pain due to lack of blood flow to the heart muscle). The
angina model is represented as a Bayesian decision network. Additionally, the
probabilities and utilities are treated as random variables with probability
distributions on their range of possible values. The initial distributions
represent information on all patients with anginal symptoms, and the approach
allows for rapid tailoring to more patientspecific distributions. This
framework provides a metric for judging the importance of each variable in the
model dynamically.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Rational Nonmonotonic Reasoning,"Nonmonotonic reasoning is a pattern of reasoning that allows an agent to make
and retract (tentative) conclusions from inconclusive evidence. This paper
gives a possible-worlds interpretation of the nonmonotonic reasoning problem
based on standard decision theory and the emerging probability logic. The
system's central principle is that a tentative conclusion is a decision to make
a bet, not an assertion of fact. The system is rational, and as sound as the
proof theory of its underlying probability log.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"A Comparison of Decision Analysis and Expert Rules for Sequential
  Diagnosis","There has long been debate about the relative merits of decision theoretic
methods and heuristic rule-based approaches for reasoning under uncertainty. We
report an experimental comparison of the performance of the two approaches to
troubleshooting, specifically to test selection for fault diagnosis. We use as
experimental testbed the problem of diagnosing motorcycle engines. The first
approach employs heuristic test selection rules obtained from expert mechanics.
We compare it with the optimal decision analytic algorithm for test selection
which employs estimated component failure probabilities and test costs. The
decision analytic algorithm was found to reduce the expected cost (i.e. time)
to arrive at a diagnosis by an average of 14% relative to the expert rules.
Sensitivity analysis shows the results are quite robust to inaccuracy in the
probability and cost estimates. This difference suggests some interesting
implications for knowledge acquisition.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probabilistic Inference and Probabilistic Reasoning,"Uncertainty enters into human reasoning and inference in at least two ways.
It is reasonable to suppose that there will be roles for these distinct uses of
uncertainty also in automated reasoning.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probabilistic and Non-Monotonic Inference,"(l) I have enough evidence to render the sentence S probable. (la) So,
relative to what I know, it is rational of me to believe S. (2) Now that I have
more evidence, S may no longer be probable. (2a) So now, relative to what I
know, it is not rational of me to believe S. These seem a perfectly ordinary,
common sense, pair of situations. Generally and vaguely, I take them to embody
what I shall call probabilistic inference. This form of inference is clearly
non-monotonic. Relatively few people have taken this form of inference, based
on high probability, to serve as a foundation for non-monotonic logic or for a
logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks
that sometimes probability has something to do with non-monotonic reasoning.
Judea Pearl [ 17] has recently been exploring the possibility. There are any
number of people whom one might call probability enthusiasts who feel that
probability provides all the answers by itself, with no need of help from
logic. Cheeseman [1], Henrion [5] and others think it useful to look at a
distribution of probabilities over a whole algebra of statements, to update
that distribution in the light of new evidence, and to use the latest updated
distribution of probability over the algebra as a basis for planning and
decision making. A slightly weaker form of this approach is captured by Nilsson
[15], where one assumes certain probabilities for certain statements, and
infers the probabilities, or constraints on the probabilities of other
statement. None of this corresponds to what I call probabilistic inference. All
of the inference that is taking place, either in Bayesian updating, or in
probabilistic logic, is strictly deductive. Deductive inference, particularly
that concerned with the distribution of classical probabilities or chances, is
of great importance. But this is not to say that there is no important role for
what earlier logicians have called ""ampliative"" or ""inductive"" or ""scientific""
inference, in which the conclusion goes beyond the premises, asserts more than
do the premises. This depends on what David Israel [6] has called ""real rules
of inference"". It is characteristic of any such logic or inference procedure
that it can go wrong: that statements accepted at one point may be rejected at
a later point. Research underlying the results reported here has been partially
supported by the Signals Warfare Center of the United States Army.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Epistemological Relevance and Statistical Knowledge,"For many years, at least since McCarthy and Hayes (1969), writers have
lamented, and attempted to compensate for, the alleged fact that we often do
not have adequate statistical knowledge for governing the uncertainty of
belief, for making uncertain inferences, and the like. It is hardly ever
spelled out what ""adequate statistical knowledge"" would be, if we had it, and
how adequate statistical knowledge could be used to control and regulate
epistemic uncertainty.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Evidential Reasoning in a Network Usage Prediction Testbed,"This paper reports on empirical work aimed at comparing evidential reasoning
techniques. While there is prima facie evidence for some conclusions, this i6
work in progress; the present focus is methodology, with the goal that
subsequent results be meaningful. The domain is a network of UNIX* cycle
servers, and the task is to predict properties of the state of the network from
partial descriptions of the state. Actual data from the network are taken and
used for blindfold testing in a betting game that allows abstention. The focal
technique has been Kyburg's method for reasoning with data of varying relevance
to a particular query, though the aim is to be able eventually to compare
various uncertainty calculi. The conclusions are not novel, but are
instructive. 1. All of the calculi performed better than human subjects, so
unbiased access to sample experience is apparently of value. 2. Performance
depends on metric: (a) when trials are repeated, net = gains - losses favors
methods that place many bets, if the probability of placing a correct bet is
sufficiently high; that is, it favors point-valued formalisms; (b) yield =
gains/(gains + lossee) favors methods that bet only when sure to bet correctly;
that is, it favors interval-valued formalisms. 3. Among the calculi, there were
no clear winners or losers. Methods are identified for eliminating the bias of
the net as a performance criterion and for separating the calculi effectively:
in both cases by posting odds for the betting game in the appropriate way.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Justifying the Principle of Interval Constraints,"When knowledge is obtained from a database, it is only possible to deduce
confidence intervals for probability values. With confidence intervals
replacing point values, the results in the set covering model include interval
constraints for the probabilities of mutually exclusive and exhaustive
explanations. The Principle of Interval Constraints ranks these explanations by
determining the expected values of the probabilities based on distributions
determined from the interval, constraints. This principle was developed using
the Classical Approach to probability. This paper justifies the Principle of
Interval Constraints with a more rigorous statement of the Classical Approach
and by defending the concept of probabilities of probabilities.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Probabilistic Semantics and Defaults,"There is much interest in providing probabilistic semantics for defaults but
most approaches seem to suffer from one of two problems: either they require
numbers, a problem defaults were intended to avoid, or they generate peculiar
side effects. Rather than provide semantics for defaults, we address the
problem defaults were intended to solve: that of reasoning under uncertainty
where numeric probability distributions are not available. We describe a
non-numeric formalism called an inference graph based on standard probability
theory, conditional independence and sentences of favouring where a favours b -
favours(a, b) - p(a|b) > p(a). The formalism seems to handle the examples from
the nonmonotonic literature. Most importantly, the sentences of our system can
be verified by performing an appropriate experiment in the semantic domain.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Decision Making with Linear Constraints on Probabilities,"Techniques for decision making with knowledge of linear constraints on
condition probabilities are examined. These constraints arise naturally in many
situations: upper and lower condition probabilities are known; an ordering
among the probabilities is determined; marginal probabilities or bounds on such
probabilities are known, e.g., data are available in the form of a
probabilistic database (Cavallo and Pittarelli, 1987a); etc. Standard
situations of decision making under risk and uncertainty may also be
characterized by linear constraints. Each of these types of information may be
represented by a convex polyhedron of numerically determinate condition
probabilities. A uniform approach to decision making under risk, uncertainty,
and partial uncertainty based on a generalized version of a criterion of
Hurwicz is proposed, Methods for processing marginal probabilities to improve
decision making using any of the criteria discussed are presented.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Maintenance in Probabilistic Knowledge-Based Systems,"Recent developments using directed acyclical graphs (i.e., influence diagrams
and Bayesian networks) for knowledge representation have lessened the problems
of using probability in knowledge-based systems (KBS). Most current research
involves the efficient propagation of new evidence, but little has been done
concerning the maintenance of domain-specific knowledge, which includes the
probabilistic information about the problem domain. By making use of
conditional independencies represented in she graphs, however, probability
assessments are required only for certain variables when the knowledge base is
updated. The purpose of this study was to investigate, for those variables
which require probability assessments, ways to reduce the amount of new
knowledge required from the expert when updating probabilistic information in a
probabilistic knowledge-based system. Three special cases (ignored outcome,
split outcome, and assumed constraint outcome) were identified under which many
of the original probabilities (those already in the knowledge-base) do not need
to be reassessed when maintenance is required.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
A Linear Approximation Method for Probabilistic Inference,"An approximation method is presented for probabilistic inference with
continuous random variables. These problems can arise in many practical
problems, in particular where there are ""second order"" probabilities. The
approximation, based on the Gaussian influence diagram, iterates over linear
approximations to the inference problem.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
An Axiomatic Framework for Bayesian and Belief-function Propagation,"In this paper, we describe an abstract framework and axioms under which exact
local computation of marginals is possible. The primitive objects of the
framework are variables and valuations. The primitive operators of the
framework are combination and marginalization. These operate on valuations. We
state three axioms for these operators and we derive the possibility of local
computation from the axioms. Next, we describe a propagation scheme for
computing marginals of a valuation when we have a factorization of the
valuation on a hypertree. Finally we show how the problem of computing
marginals of joint probability distributions and joint belief functions fits
the general framework.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
A General Non-Probabilistic Theory of Inductive Reasoning,"Probability theory, epistemically interpreted, provides an excellent, if not
the best available account of inductive reasoning. This is so because there are
general and definite rules for the change of subjective probabilities through
information or experience; induction and belief change are one and same topic,
after all. The most basic of these rules is simply to conditionalize with
respect to the information received; and there are similar and more general
rules. 1 Hence, a fundamental reason for the epistemological success of
probability theory is that there at all exists a well-behaved concept of
conditional probability. Still, people have, and have reasons for, various
concerns over probability theory. One of these is my starting point:
Intuitively, we have the notion of plain belief; we believe propositions2 to be
true (or to be false or neither). Probability theory, however, offers no formal
counterpart to this notion. Believing A is not the same as having probability 1
for A, because probability 1 is incorrigible3; but plain belief is clearly
corrigible. And believing A is not the same as giving A a probability larger
than some 1 - c, because believing A and believing B is usually taken to be
equivalent to believing A & B.4 Thus, it seems that the formal representation
of plain belief has to take a non-probabilistic route. Indeed, representing
plain belief seems easy enough: simply represent an epistemic state by the set
of all propositions believed true in it or, since I make the common assumption
that plain belief is deductively closed, by the conjunction of all propositions
believed true in it. But this does not yet provide a theory of induction, i.e.
an answer to the question how epistemic states so represented are changed
tbrough information or experience. There is a convincing partial answer: if the
new information is compatible with the old epistemic state, then the new
epistemic state is simply represented by the conjunction of the new information
and the old beliefs. This answer is partial because it does not cover the quite
common case where the new information is incompatible with the old beliefs. It
is, however, important to complete the answer and to cover this case, too;
otherwise, we would not represent plain belief as conigible. The crucial
problem is that there is no good completion. When epistemic states are
represented simply by the conjunction of all propositions believed true in it,
the answer cannot be completed; and though there is a lot of fruitful work, no
other representation of epistemic states has been proposed, as far as I know,
which provides a complete solution to this problem. In this paper, I want to
suggest such a solution. In [4], I have more fully argued that this is the only
solution, if certain plausible desiderata are to be satisfied. Here, in section
2, I will be content with formally defining and intuitively explaining my
proposal. I will compare my proposal with probability theory in section 3. It
will turn out that the theory I am proposing is structurally homomorphic to
probability theory in important respects and that it is thus equally easily
implementable, but moreover computationally simpler. Section 4 contains a very
brief comparison with various kinds of logics, in particular conditional logic,
with Shackle's functions of potential surprise and related theories, and with
the Dempster - Shafer theory of belief functions.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
"Generating Decision Structures and Causal Explanations for Decision
  Making","This paper examines two related problems that are central to developing an
autonomous decision-making agent, such as a robot. Both problems require
generating structured representafions from a database of unstructured
declarative knowledge that includes many facts and rules that are irrelevant in
the problem context. The first problem is how to generate a well structured
decision problem from such a database. The second problem is how to generate,
from the same database, a well-structured explanation of why some possible
world occurred. In this paper it is shown that the problem of generating the
appropriate decision structure or explanation is intractable without
introducing further constraints on the knowledge in the database. The paper
proposes that the problem search space can be constrained by adding knowledge
to the database about causal relafions between events. In order to determine
the causal knowledge that would be most useful, causal theories for
deterministic and indeterministic universes are proposed. A program that uses
some of these causal constraints has been used to generate explanations about
faulty plans. The program shows the expected increase in efficiency as the
causal constraints are introduced.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Updating Probabilities in Multiply-Connected Belief Networks,"This paper focuses on probability updates in multiply-connected belief
networks. Pearl has designed the method of conditioning, which enables us to
apply his algorithm for belief updates in singly-connected networks to
multiply-connected belief networks by selecting a loop-cutset for the network
and instantiating these loop-cutset nodes. We discuss conditions that need to
be satisfied by the selected nodes. We present a heuristic algorithm for
finding a loop-cutset that satisfies these conditions.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Handling uncertainty in a system for text-symbol context analysis,"In pattern analysis, information regarding an object can often be drawn from
its surroundings. This paper presents a method for handling uncertainty when
using context of symbols and texts for analyzing technical drawings. The method
is based on Dempster-Shafer theory and possibility theory.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Causal Networks: Semantics and Expressiveness,"Dependency knowledge of the form ""x is independent of y once z is known""
invariably obeys the four graphoid axioms, examples include probabilistic and
database dependencies. Often, such knowledge can be represented efficiently
with graphical structures such as undirected graphs and directed acyclic graphs
(DAGs). In this paper we show that the graphical criterion called d-separation
is a sound rule for reading independencies from any DAG based on a causal input
list drawn from a graphoid. The rule may be extended to cover DAGs that
represent functional dependencies as well as conditional dependencies.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
MCE Reasoning in Recursive Causal Networks,"A probabilistic method of reasoning under uncertainty is proposed based on
the principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal
Model (RCM). The dependency and correlations among the variables are described
in a special language BNDL (Belief Networks Description Language). Beliefs are
propagated among the clauses of the BNDL programs representing the underlying
probabilistic distributions. BNDL interpreters in both Prolog and C has been
developed and the performance of the method is compared with those of the
others.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Nonmonotonic Reasoning via Possibility Theory,"We introduce the operation of possibility qualification and show how. this
modal-like operator can be used to represent ""typical"" or default knowledge in
a theory of nonmonotonic reasoning. We investigate the representational power
of this approach by looking at a number of prototypical problems from the
nonmonotonic reasoning literature. In particular we look at the so called Yale
shooting problem and its relation to priority in default reasoning.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Generalizing the Dempster-Shafer Theory to Fuzzy Sets,"With the desire to apply the Dempster-Shafer theory to complex real world
problems where the evidential strength is often imprecise and vague, several
attempts have been made to generalize the theory. However, the important
concept in the D-S theory that the belief and plausibility functions are lower
and upper probabilities is no longer preserved in these generalizations. In
this paper, we describe a generalized theory of evidence where the degree of
belief in a fuzzy set is obtained by minimizing the probability of the fuzzy
set under the constraints imposed by a basic probability assignment. To
formulate the probabilistic constraint of a fuzzy focal element, we decompose
it into a set of consonant non-fuzzy focal elements. By generalizing the
compatibility relation to a possibility theory, we are able to justify our
generalization to Dempster's rule based on possibility distribution. Our
generalization not only extends the application of the D-S theory but also
illustrates a way that probability theory and fuzzy set theory can be combined
to deal with different kinds of uncertain information in AI systems.","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)"
Logical Fuzzy Optimization,"We present a logical framework to represent and reason about fuzzy
optimization problems based on fuzzy answer set optimization programming. This
is accomplished by allowing fuzzy optimization aggregates, e.g., minimum and
maximum in the language of fuzzy answer set optimization programming to allow
minimization or maximization of some desired criteria under fuzzy environments.
We show the application of the proposed logical fuzzy optimization framework
under the fuzzy answer set optimization programming to the fuzzy water
allocation optimization problem.",N/A
Modèle flou d'expression des préférences basé sur les CP-Nets,"This article addresses the problem of expressing preferences in flexible
queries while basing on a combination of the fuzzy logic theory and Conditional
Preference Networks or CP-Nets.","2 pages, EGC 2013"
Symmetry-Aware Marginal Density Estimation,"The Rao-Blackwell theorem is utilized to analyze and improve the scalability
of inference in large probabilistic models that exhibit symmetries. A novel
marginal density estimator is introduced and shown both analytically and
empirically to outperform standard estimators by several orders of magnitude.
The developed theory and algorithms apply to a broad class of probabilistic
models including statistical relational models considered not susceptible to
lifted probabilistic inference.",To appear in proceedings of AAAI 2013
Is Shafer General Bayes?,"This paper examines the relationship between Shafer's belief functions and
convex sets of probability distributions. Kyburg's (1986) result showed that
belief function models form a subset of the class of closed convex probability
distributions. This paper emphasizes the importance of Kyburg's result by
looking at simple examples involving Bernoulli trials. Furthermore, it is shown
that many convex sets of probability distributions generate the same belief
function in the sense that they support the same lower and upper values. This
has implications for a decision theoretic extension. Dempster's rule of
combination is also compared with Bayes' rule of conditioning.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Modifiable Combining Functions,"Modifiable combining functions are a synthesis of two common approaches to
combining evidence. They offer many of the advantages of these approaches and
avoid some disadvantages. Because they facilitate the acquisition,
representation, explanation, and modification of knowledge about combinations
of evidence, they are proposed as a tool for knowledge engineers who build
systems that reason under uncertainty, not as a normative theory of evidence.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Dempster-Shafer vs. Probabilistic Logic,"The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Higher Order Probabilities,"A number of writers have supposed that for the full specification of belief,
higher order probabilities are required. Some have even supposed that there may
be an unending sequence of higher order probabilities of probabilities of
probabilities.... In the present paper we show that higher order probabilities
can always be replaced by the marginal distributions of joint probability
distributions. We consider both the case in which higher order probabilities
are of the same sort as lower order probabilities and that in which higher
order probabilities are distinct in character, as when lower order
probabilities are construed as frequencies and higher order probabilities are
construed as subjective degrees of belief. In neither case do higher order
probabilities appear to offer any advantages, either conceptually or
computationally.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Belief in Belief Functions: An Examination of Shafer's Canonical
  Examples","In the canonical examples underlying Shafer-Dempster theory, beliefs over the
hypotheses of interest are derived from a probability model for a set of
auxiliary hypotheses. Beliefs are derived via a compatibility relation
connecting the auxiliary hypotheses to subsets of the primary hypotheses. A
belief function differs from a Bayesian probability model in that one does not
condition on those parts of the evidence for which no probabilities are
specified. The significance of this difference in conditioning assumptions is
illustrated with two examples giving rise to identical belief functions but
different Bayesian probability distributions.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Do We Need Higher-Order Probabilities and, If So, What Do They Mean?","The apparent failure of individual probabilistic expressions to distinguish
uncertainty about truths from uncertainty about probabilistic assessments have
prompted researchers to seek formalisms where the two types of uncertainties
are given notational distinction. This paper demonstrates that the desired
distinction is already a built-in feature of classical probabilistic models,
thus, specialized notations are unnecessary.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Bayesian Prediction for Artificial Intelligence,"This paper shows that the common method used for making predictions under
uncertainty in A1 and science is in error. This method is to use currently
available data to select the best model from a given class of models-this
process is called abduction-and then to use this model to make predictions
about future data. The correct method requires averaging over all the models to
make a prediction-we call this method transduction. Using transduction, an AI
system will not give misleading results when basing predictions on small
amounts of data, when no model is clearly best. For common classes of models we
show that the optimal solution can be given in closed form.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Can Evidence Be Combined in the Dempster-Shafer Theory,"Dempster's rule of combination has been the most controversial part of the
Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on
the noncombinability of evidence from a relational model of the D-S theory. In
this paper, we will describe another relational model where D-S masses are
represented as conditional granular distributions. By comparing it with Zadeh's
relational model, we will show how Zadeh's conjecture on combinability does not
affect the applicability of Dempster's rule in our model.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"An Interesting Uncertainty-Based Combinatoric Problem in Spare Parts
  Forecasting: The FRED System","The domain of spare parts forecasting is examined, and is found to present
unique uncertainty based problems in the architectural design of a
knowledge-based system. A mixture of different uncertainty paradigms is
required for the solution, with an intriguing combinatoric problem arising from
an uncertain choice of inference engines. Thus, uncertainty in the system is
manifested in two different meta-levels. The different uncertainty paradigms
and meta-levels must be integrated into a functioning whole. FRED is an example
of a difficult real-world domain to which no existing uncertainty approach is
completely appropriate. This paper discusses the architecture of FRED,
highlighting: the points of uncertainty and other interesting features of the
domain, the specific implications of those features on the system design
(including the combinatoric explosions), their current implementation & future
plans,and other problems and issues with the architecture.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Bayesian Inference in Model-Based Machine Vision,"This is a preliminary version of visual interpretation integrating multiple
sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a
thorough integration of hierarchical Bayesian inference with comprehensive
physical representation of objects and their relations in a system for
reasoning with geometry, surface materials and sensor models in machine vision.
Bayesian inference provides a framework for accruing_ probabilities to rank
order hypotheses.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Using the Dempster-Shafer Scheme in a Diagnostic Expert System Shell,"This paper discusses an expert system shell that integrates rule-based
reasoning and the Dempster-Shafer evidence combination scheme. Domain knowledge
is stored as rules with associated belief functions. The reasoning component
uses a combination of forward and backward inferencing mechanisms to allow
interaction with users in a mixed-initiative format.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Stochastic Simulation of Bayesian Belief Networks,"This paper examines Bayesian belief network inference using simulation as a
method for computing the posterior probabilities of network variables.
Specifically, it examines the use of a method described by Henrion, called
logic sampling, and a method described by Pearl, called stochastic simulation.
We first review the conditions under which logic sampling is computationally
infeasible. Such cases motivated the development of the Pearl's stochastic
simulation algorithm. We have found that this stochastic simulation algorithm,
when applied to certain networks, leads to much slower than expected
convergence to the true posterior probabilities. This behavior is a result of
the tendency for local areas in the network to become fixed through many
simulation cycles. The time required to obtain significant convergence can be
made arbitrarily long by strengthening the probabilistic dependency between
nodes. We propose the use of several forms of graph modification, such as graph
pruning, arc reversal, and node reduction, in order to convert some networks
into formats that are computationally more efficient for simulation.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Temporal Reasoning About Uncertain Worlds,"We present a program that manages a database of temporally scoped beliefs.
The basic functionality of the system includes maintaining a network of
constraints among time points, supporting a variety of fetches, mediating the
application of causal rules, monitoring intervals of time for the addition of
new facts, and managing data dependencies that keep the database consistent. At
this level the system operates independent of any measure of belief or belief
calculus. We provide an example of how an application program mi9ght use this
functionality to implement a belief calculus.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"A Perspective on Confidence and Its Use in Focusing Attention During
  Knowledge Acquisition","We present a representation of partial confidence in belief and preference
that is consistent with the tenets of decision-theory. The fundamental insight
underlying the representation is that if a person is not completely confident
in a probability or utility assessment, additional modeling of the assessment
may improve decisions to which it is relevant. We show how a traditional
decision-analytic approach can be used to balance the benefits of additional
modeling with associated costs. The approach can be used during knowledge
acquisition to focus the attention of a knowledge engineer or expert on parts
of a decision model that deserve additional refinement.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Practical Issues in Constructing a Bayes' Belief Network,"Bayes belief networks and influence diagrams are tools for constructing
coherent probabilistic representations of uncertain knowledge. The process of
constructing such a network to represent an expert's knowledge is used to
illustrate a variety of techniques which can facilitate the process of
structuring and quantifying uncertain relationships. These include some
generalizations of the ""noisy OR gate"" concept. Sensitivity analysis of generic
elements of Bayes' networks provides insight into when rough probability
assessments are sufficient and when greater precision may be important.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"NAIVE: A Method for Representing Uncertainty and Temporal Relationships
  in an Automated Reasoner","This paper describes NAIVE, a low-level knowledge representation language and
inferencing process. NAIVE has been designed for reasoning about
nondeterministic dynamic systems like those found in medicine. Knowledge is
represented in a graph structure consisting of nodes, which correspond to the
variables describing the system of interest, and arcs, which correspond to the
procedures used to infer the value of a variable from the values of other
variables. The value of a variable can be determined at an instant in time,
over a time interval or for a series of times. Information about the value of a
variable is expressed as a probability density function which quantifies the
likelihood of each possible value. The inferencing process uses these
probability density functions to propagate uncertainty. NAIVE has been used to
develop medical knowledge bases including over 100 variables.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Objective Probability,"A distinction is sometimes made between ""statistical"" and ""subjective""
probabilities. This is based on a distinction between ""unique"" events and
""repeatable"" events. We argue that this distinction is untenable, since all
events are ""unique"" and all events belong to ""kinds"", and offer a conception of
probability for A1 in which (1) all probabilities are based on -- possibly
vague -- statistical knowledge, and (2) every statement in the language has a
probability. This conception of probability can be applied to very rich
languages.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Coefficients of Relations for Probabilistic Reasoning,"Definitions and notations with historical references are given for some
numerical coefficients commonly used to quantify relations among collections of
objects for the purpose of expressing approximate knowledge and probabilistic
reasoning.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Satisfaction of Assumptions is a Weak Predictor of Performance,"This paper demonstrates a methodology for examining the accuracy of uncertain
inference systems (UIS), after their parameters have been optimized, and does
so for several common UIS's. This methodology may be used to test the accuracy
when either the prior assumptions or updating formulae are not exactly
satisfied. Surprisingly, these UIS's were revealed to be no more accurate on
the average than a simple linear regression. Moreover, even on prior
distributions which were deliberately biased so as give very good accuracy,
they were less accurate than the simple probabilistic model which assumes
marginal independence between inputs. This demonstrates that the importance of
updating formulae can outweigh that of prior assumptions. Thus, when UIS's are
judged by their final accuracy after optimization, we get completely different
results than when they are judged by whether or not their prior assumptions are
perfectly satisfied.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Structuring Causal Tree Models with Continuous Variables,"This paper considers the problem of invoking auxiliary, unobservable
variables to facilitate the structuring of causal tree models for a given set
of continuous variables. Paralleling the treatment of bi-valued variables in
[Pearl 1986], we show that if a collection of coupled variables are governed by
a joint normal distribution and a tree-structured representation exists, then
both the topology and all internal relationships of the tree can be uncovered
by observing pairwise dependencies among the observed variables (i.e., the
leaves of the tree). Furthermore, the conditions for normally distributed
variables are less restrictive than those governing bi-valued variables. The
result extends the applications of causal tree models which were found useful
in evidential reasoning tasks.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Implementing Evidential Reasoning in Expert Systems,"The Dempster-Shafer theory has been extended recently for its application to
expert systems. However, implementing the extended D-S reasoning model in
rule-based systems greatly complicates the task of generating informative
explanations. By implementing GERTIS, a prototype system for diagnosing
rheumatoid arthritis, we show that two kinds of knowledge are essential for
explanation generation: (l) taxonomic class relationships between hypotheses
and (2) pointers to the rules that significantly contribute to belief in the
hypothesis. As a result, the knowledge represented in GERTIS is richer and more
complex than that of conventional rule-based systems. GERTIS not only
demonstrates the feasibility of rule-based evidential-reasoning systems, but
also suggests ways to generate better explanations, and to explicitly represent
various useful relationships among hypotheses and rules.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Decision Tree Induction Systems: A Bayesian Analysis,"Decision tree induction systems are being used for knowledge acquisition in
noisy domains. This paper develops a subjective Bayesian interpretation of the
task tackled by these systems and the heuristic methods they use. It is argued
that decision tree systems implicitly incorporate a prior belief that the
simpler (in terms of decision tree complexity) of two hypotheses be preferred,
all else being equal, and that they perform a greedy search of the space of
decision rules to find one in which there is strong posterior belief. A number
of improvements to these systems are then suggested.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"The Automatic Training of Rule Bases that Use Numerical Uncertainty
  Representations","The use of numerical uncertainty representations allows better modeling of
some aspects of human evidential reasoning. It also makes knowledge acquisition
and system development, test, and modification more difficult. We propose that
where possible, the assignment and/or refinement of rule weights should be
performed automatically. We present one approach to performing this training -
numerical optimization - and report on the results of some preliminary tests in
training rule bases. We also show that truth maintenance can be used to make
training more efficient and ask some epistemological questions raised by
training rule weights.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
The Inductive Logic of Information Systems,"An inductive logic can be formulated in which the elements are not
propositions or probability distributions, but information systems. The logic
is complete for information systems with binary hypotheses, i.e., it applies to
all such systems. It is not complete for information systems with more than two
hypotheses, but applies to a subset of such systems. The logic is inductive in
that conclusions are more informative than premises. Inferences using the
formalism have a strong justification in terms of the expected value of the
derived information system.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Automated Generation of Connectionist Expert Systems for Problems
  Involving Noise and Redundancy","When creating an expert system, the most difficult and expensive task is
constructing a knowledge base. This is particularly true if the problem
involves noisy data and redundant measurements. This paper shows how to modify
the MACIE process for generating connectionist expert systems from training
examples so that it can accommodate noisy and redundant data. The basic idea is
to dynamically generate appropriate training examples by constructing both a
'deep' model and a noise model for the underlying problem. The use of
winner-take-all groups of variables is also discussed. These techniques are
illustrated with a small example that would be very difficult for standard
expert system approaches.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
The Recovery of Causal Poly-Trees from Statistical Data,"Poly-trees are singly connected causal networks in which variables may arise
from multiple causes. This paper develops a method of recovering ply-trees from
empirically measured probability distributions of pairs of variables. The
method guarantees that, if the measured distributions are generated by a causal
process structured as a ply-tree then the topological structure of such tree
can be recovered precisely and, in addition, the causal directionality of the
branches can be determined up to the maximum extent possible. The method also
pinpoints the minimum (if any) external semantics required to determine the
causal relationships among the variables considered.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"A Heuristic Bayesian Approach to Knowledge Acquisition: Application to
  Analysis of Tissue-Type Plasminogen Activator","This paper describes a heuristic Bayesian method for computing probability
distributions from experimental data, based upon the multivariate normal form
of the influence diagram. An example illustrates its use in medical technology
assessment. This approach facilitates the integration of results from different
studies, and permits a medical expert to make proper assessments without
considerable statistical training.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Theory-Based Inductive Learning: An Integration of Symbolic and
  Quantitative Methods","The objective of this paper is to propose a method that will generate a
causal explanation of observed events in an uncertain world and then make
decisions based on that explanation. Feedback can cause the explanation and
decisions to be modified. I call the method Theory-Based Inductive Learning
(T-BIL). T-BIL integrates deductive learning, based on a technique called
Explanation-Based Generalization (EBG) from the field of machine learning, with
inductive learning methods from Bayesian decision theory. T-BIL takes as inputs
(1) a decision problem involving a sequence of related decisions over time, (2)
a training example of a solution to the decision problem in one period, and (3)
the domain theory relevant to the decision problem. T-BIL uses these inputs to
construct a probabilistic explanation of why the training example is an
instance of a solution to one stage of the sequential decision problem. This
explanation is then generalized to cover a more general class of instances and
is used as the basis for making the next-stage decisions. As the outcomes of
each decision are observed, the explanation is revised, which in turn affects
the subsequent decisions. A detailed example is presented that uses T-BIL to
solve a very general stochastic adaptive control problem for an autonomous
mobile robot.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Using T-Norm Based Uncertainty Calculi in a Naval Situation Assessment
  Application","RUM (Reasoning with Uncertainty Module), is an integrated software tool based
on a KEE, a frame system implemented in an object oriented language. RUM's
architecture is composed of three layers: representation, inference, and
control. The representation layer is based on frame-like data structures that
capture the uncertainty information used in the inference layer and the
uncertainty meta-information used in the control layer. The inference layer
provides a selection of five T-norm based uncertainty calculi with which to
perform the intersection, detachment, union, and pooling of information. The
control layer uses the meta-information to select the appropriate calculus for
each context and to resolve eventual ignorance or conflict in the information.
This layer also provides a context mechanism that allows the system to focus on
the relevant portion of the knowledge base, and an uncertain-belief revision
system that incrementally updates the certainty values of well-formed formulae
(wffs) in an acyclic directed deduction graph. RUM has been tested and
validated in a sequence of experiments in both naval and aerial situation
assessment (SA), consisting of correlating reports and tracks, locating and
classifying platforms, and identifying intents and threats. An example of naval
situation assessment is illustrated. The testbed environment for developing
these experiments has been provided by LOTTA, a symbolic simulator implemented
in Flavors. This simulator maintains time-varying situations in a multi-player
antagonistic game where players must make decisions in light of uncertain and
incomplete data. RUM has been used to assist one of the LOTTA players to
perform the SA task.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
A Study of Associative Evidential Reasoning,"Evidential reasoning is cast as the problem of simplifying the
evidence-hypothesis relation and constructing combination formulas that possess
certain testable properties. Important classes of evidence as identifiers,
annihilators, and idempotents and their roles in determining binary operations
on intervals of reals are discussed. The appropriate way of constructing
formulas for combining evidence and their limitations, for instance, in
robustness, are presented.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
A Measure-Free Approach to Conditioning,"In an earlier paper, a new theory of measurefree ""conditional"" objects was
presented. In this paper, emphasis is placed upon the motivation of the theory.
The central part of this motivation is established through an example involving
a knowledge-based system. In order to evaluate combination of evidence for this
system, using observed data, auxiliary at tribute and diagnosis variables, and
inference rules connecting them, one must first choose an appropriate algebraic
logic description pair (ALDP): a formal language or syntax followed by a
compatible logic or semantic evaluation (or model). Three common choices- for
this highly non-unique choice - are briefly discussed, the logics being
Classical Logic, Fuzzy Logic, and Probability Logic. In all three,the key
operator representing implication for the inference rules is interpreted as the
often-used disjunction of a negation (b => a) = (b'v a), for any events a,b.
  However, another reasonable interpretation of the implication operator is
through the familiar form of probabilistic conditioning. But, it can be shown -
quite surprisingly - that the ALDP corresponding to Probability Logic cannot be
used as a rigorous basis for this interpretation! To fill this gap, a new ALDP
is constructed consisting of ""conditional objects"", extending ordinary
Probability Logic, and compatible with the desired conditional probability
interpretation of inference rules. It is shown also that this choice of ALDP
leads to feasible computations for the combination of evidence evaluation in
the example. In addition, a number of basic properties of conditional objects
and the resulting Conditional Probability Logic are given, including a
characterization property and a developed calculus of relations.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Convergent Deduction for Probabilistic Logic,"This paper discusses the semantics and proof theory of Nilsson's
probabilistic logic, outlining both the benefits of its well-defined model
theory and the drawbacks of its proof theory. Within Nilsson's semantic
framework, we derive a set of inference rules which are provably sound. The
resulting proof system, in contrast to Nilsson's approach, has the important
feature of convergence - that is, the inference process proceeds by computing
increasingly narrow probability intervals which converge from above and below
on the smallest entailed probability interval. Thus the procedure can be
stopped at any time to yield partial information concerning the smallest
entailed interval.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
A Knowledge Engineer's Comparison of Three Evidence Aggregation Methods,"The comparisons of uncertainty calculi from the last two Uncertainty
Workshops have all used theoretical probabilistic accuracy as the sole metric.
While mathematical correctness is important, there are other factors which
should be considered when developing reasoning systems. These other factors
include, among other things, the error in uncertainty measures obtainable for
the problem and the effect of this error on the performance of the resulting
system.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Towards Solving the Multiple Extension Problem: Combining Defaults and
  Probabilities","The multiple extension problem arises frequently in diagnostic and default
inference. That is, we can often use any of a number of sets of defaults or
possible hypotheses to explain observations or make Predictions. In default
inference, some extensions seem to be simply wrong and we use qualitative
techniques to weed out the unwanted ones. In the area of diagnosis, however,
the multiple explanations may all seem reasonable, however improbable. Choosing
among them is a matter of quantitative preference. Quantitative preference
works well in diagnosis when knowledge is modelled causally. Here we suggest a
framework that combines probabilities and defaults in a single unified
framework that retains the semantics of diagnosis as construction of
explanations from a fixed set of possible hypotheses. We can then compute
probabilities incrementally as we construct explanations. Here we describe a
branch and bound algorithm that maintains a set of all partial explanations
while exploring a most promising one first. A most probable explanation is
found first if explanations are partially ordered.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Problem Structure and Evidential Reasoning,"In our previous series of studies to investigate the role of evidential
reasoning in the RUBRIC system for full-text document retrieval (Tong et al.,
1985; Tong and Shapiro, 1985; Tong and Appelbaum, 1987), we identified the
important role that problem structure plays in the overall performance of the
system. In this paper, we focus on these structural elements (which we now call
""semantic structure"") and show how explicit consideration of their properties
reduces what previously were seen as difficult evidential reasoning problems to
more tractable questions.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
The Role of Calculi in Uncertain Inference Systems,"Much of the controversy about methods for automated decision making has
focused on specific calculi for combining beliefs or propagating uncertainty.
We broaden the debate by (1) exploring the constellation of secondary tasks
surrounding any primary decision problem, and (2) identifying knowledge
engineering concerns that present additional representational tradeoffs. We
argue on pragmatic grounds that the attempt to support all of these tasks
within a single calculus is misguided. In the process, we note several
uncertain reasoning objectives that conflict with the Bayesian ideal of
complete specification of probabilities and utilities. In response, we advocate
treating the uncertainty calculus as an object language for reasoning
mechanisms that support the secondary tasks. Arguments against Bayesian
decision theory are weakened when the calculus is relegated to this role.
Architectures for uncertainty handling that take statements in the calculus as
objects to be reasoned about offer the prospect of retaining normative status
with respect to decision making while supporting the other tasks in uncertain
reasoning.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
The Role of Tuning Uncertain Inference Systems,"This study examined the effects of ""tuning"" the parameters of the incremental
function of MYCIN, the independent function of PROSPECTOR, a probability model
that assumes independence, and a simple additive linear equation. me parameters
of each of these models were optimized to provide solutions which most nearly
approximated those from a full probability model for a large set of simple
networks. Surprisingly, MYCIN, PROSPECTOR, and the linear equation performed
equivalently; the independence model was clearly more accurate on the networks
studied.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Implementing a Bayesian Scheme for Revising Belief Commitments,"Our previous work on classifying complex ship images [1,2] has evolved into
an effort to develop software tools for building and solving generic
classification problems. Managing the uncertainty associated with feature data
and other evidence is an important issue in this endeavor. Bayesian techniques
for managing uncertainty [7,12,13] have proven to be useful for managing
several of the belief maintenance requirements of classification problem
solving. One such requirement is the need to give qualitative explanations of
what is believed. Pearl [11] addresses this need by computing what he calls a
belief commitment-the most probable instantiation of all hypothesis variables
given the evidence available. Before belief commitments can be computed, the
straightforward implementation of Pearl's procedure involves finding an
analytical solution to some often difficult optimization problems. We describe
an efficient implementation of this procedure using tensor products that solves
these problems enumeratively and avoids the need for case by case analysis. The
procedure is thereby made more practical to use in the general case.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Integrating Logical and Probabilistic Reasoning for Decision Making,"We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Compiling Fuzzy Logic Control Rules to Hardware Implementations,"A major aspect of human reasoning involves the use of approximations.
Particularly in situations where the decision-making process is under stringent
time constraints, decisions are based largely on approximate, qualitative
assessments of the situations. Our work is concerned with the application of
approximate reasoning to real-time control. Because of the stringent processing
speed requirements in such applications, hardware implementations of fuzzy
logic inferencing are being pursued. We describe a programming environment for
translating fuzzy control rules into hardware realizations. Two methods of
hardware realizations are possible. The First is based on a special purpose
chip for fuzzy inferencing. The second is based on a simple memory chip. The
ability to directly translate a set of decision rules into hardware
implementations is expected to make fuzzy control an increasingly practical
approach to the control of complex systems.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Steps Towards Programs that Manage Uncertainty,"Reasoning under uncertainty in Al hats come to mean assessing the credibility
of hypotheses inferred from evidence. But techniques for assessing credibility
do not tell a problem solver what to do when it is uncertain. This is the focus
of our current research. We have developed a medical expert system called MUM,
for Managing Uncertainty in Medicine, that plans diagnostic sequences of
questions, tests, and treatments. This paper describes the kinds of problems
that MUM was designed to solve and gives a brief description of its
architecture. More recently, we have built an empty version of MUM called MU,
and used it to reimplement MUM and a small diagnostic system for plant
pathology. The latter part of the paper describes the features of MU that make
it appropriate for building expert systems that manage uncertainty.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
An Algorithm for Computing Probabilistic Propositions,"A method for computing probabilistic propositions is presented. It assumes
the availability of a single external routine for computing the probability of
one instantiated variable, given a conjunction of other instantiated variables.
In particular, the method allows belief network algorithms to calculate general
probabilistic propositions over nodes in the network. Although in the worst
case the time complexity of the method is exponential in the size of a query,
it is polynomial in the size of a number of common types of queries.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Combining Symbolic and Numeric Approaches to Uncertainty Management,"A complete approach to reasoning under uncertainty requires support for
incremental and interactive formulation and revision of, as well as reasoning
with, models of the problem domain capable of representing our uncertainty. We
present a hybrid reasoning scheme which combines symbolic and numeric methods
for uncertainty management to provide efficient and effective support for each
of these tasks. The hybrid is based on symbolic techniques adapted from
Assumption-based Truth Maintenance systems (ATMS), combined with numeric
methods adapted from the Dempster/Shafer theory of evidence, as extended in
Baldwin's Support Logic Programming system. The hybridization is achieved by
viewing an ATMS as a symbolic algebra system for uncertainty calculations. This
technique has several major advantages over conventional methods for performing
inference with numeric certainty estimates in addition to the ability to
dynamically determine hypothesis spaces, including improved management of
dependent and partially independent evidence, faster run-time evaluation of
propositional certainties, the ability to query the certainty value of a
proposition from multiple perspectives, and the ability to incrementally extend
or revise domain models.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Explanation of Probabilistic Inference for Decision Support Systems,"An automated explanation facility for Bayesian conditioning aimed at
improving user acceptance of probability-based decision support systems has
been developed. The domain-independent facility is based on an information
processing perspective on reasoning about conditional evidence that accounts
both for biased and normative inferences. Experimental results indicate that
the facility is both acceptable to naive users and effective in improving
understanding.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Efficient Inference on Generalized Fault Diagrams,"The generalized fault diagram, a data structure for failure analysis based on
the influence diagram, is defined. Unlike the fault tree, this structure allows
for dependence among the basic events and replicated logical elements. A
heuristic procedure is developed for efficient processing of these structures.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
"Reasoning About Beliefs and Actions Under Computational Resource
  Constraints","Although many investigators affirm a desire to build reasoning systems that
behave consistently with the axiomatic basis defined by probability theory and
utility theory, limited resources for engineering and computation can make a
complete normative analysis impossible. We attempt to move discussion beyond
the debate over the scope of problems that can be handled effectively to cases
where it is clear that there are insufficient computational resources to
perform an analysis deemed as complete. Under these conditions, we stress the
importance of considering the expected costs and benefits of applying
alternative approximation procedures and heuristics for computation and
knowledge acquisition. We discuss how knowledge about the structure of user
utility can be used to control value tradeoffs for tailoring inference to
alternative contexts. We address the notion of real-time rationality, focusing
on the application of knowledge about the expected timewise-refinement
abilities of reasoning strategies to balance the benefits of additional
computation with the costs of acting with a partial result. We discuss the
benefits of applying decision theory to control the solution of difficult
problems given limitations and uncertainty in reasoning resources.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Advantages and a Limitation of Using LEG Nets in a Real-TIme Problem,"After experimenting with a number of non-probabilistic methods for dealing
with uncertainty many researchers reaffirm a preference for probability methods
[1] [2], although this remains controversial. The importance of being able to
form decisions from incomplete data in diagnostic problems has highlighted
probabilistic methods [5] which compute posterior probabilities from prior
distributions in a way similar to Bayes Rule, and thus are called Bayesian
methods. This paper documents the use of a Bayesian method in a real time
problem which is similar to medical diagnosis in that there is a need to form
decisions and take some action without complete knowledge of conditions in the
problem domain. This particular method has a limitation which is discussed.","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)"
Logical Fuzzy Preferences,"We present a unified logical framework for representing and reasoning about
both quantitative and qualitative preferences in fuzzy answer set programming,
called fuzzy answer set optimization programs. The proposed framework is vital
to allow defining quantitative preferences over the possible outcomes of
qualitative preferences. We show the application of fuzzy answer set
optimization programs to the course scheduling with fuzzy preferences problem.
To the best of our knowledge, this development is the first to consider a
logical framework for reasoning about quantitative preferences, in general, and
reasoning about both quantitative and qualitative preferences in particular.",arXiv admin note: substantial text overlap with arXiv:1304.2384
"Nested Aggregates in Answer Sets: An Application to a Priori
  Optimization","We allow representing and reasoning in the presence of nested multiple
aggregates over multiple variables and nested multiple aggregates over
functions involving multiple variables in answer sets, precisely, in answer set
optimization programming and in answer set programming. We show the
applicability of the answer set optimization programming with nested multiple
aggregates and the answer set programming with nested multiple aggregates to
the Probabilistic Traveling Salesman Problem, a fundamental a priori
optimization problem in Operation Research.",arXiv admin note: text overlap with arXiv:1304.2384
Knowledge Engineering Within A Generalized Bayesian Framework,"During the ongoing debate over the representation of uncertainty in
Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that
probability theory, and in particular the Bayesian theory, should be used as
the basis for the inference mechanisms of Expert Systems dealing with
uncertainty. In order to pursue the issue in a practical setting, sophisticated
tools for knowledge engineering are needed that allow flexible and
understandable interaction with the underlying knowledge representation
schemes. This paper describes a Generalized Bayesian framework for building
expert systems which function in uncertain domains, using algorithms proposed
by Lemmer. It is neither rule-based nor frame-based, and requires a new system
of knowledge engineering tools. The framework we describe provides a
knowledge-based system architecture with an inference engine, explanation
capability, and a unique aid for building consistent knowledge bases.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"Taxonomy, Structure, and Implementation of Evidential Reasoning","The fundamental elements of evidential reasoning problems are described,
followed by a discussion of the structure of various types of problems.
Bayesian inference networks and state space formalism are used as the tool for
problem representation.
  A human-oriented decision making cycle for solving evidential reasoning
problems is described and illustrated for a military situation assessment
problem. The implementation of this cycle may serve as the basis for an expert
system shell for evidential reasoning; i.e. a situation assessment processor.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Probabilistic Reasoning About Ship Images,"One of the most important aspects of current expert systems technology is the
ability to make causal inferences about the impact of new evidence. When the
domain knowledge and problem knowledge are uncertain and incomplete Bayesian
reasoning has proven to be an effective way of forming such inferences [3,4,8].
While several reasoning schemes have been developed based on Bayes Rule, there
has been very little work examining the comparative effectiveness of these
schemes in a real application. This paper describes a knowledge based system
for ship classification [1], originally developed using the PROSPECTOR updating
method [2], that has been reimplemented to use the inference procedure
developed by Pearl and Kim [4,5]. We discuss our reasons for making this
change, the implementation of the new inference engine, and the comparative
performance of the two versions of the system.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Towards The Inductive Acquisition of Temporal Knowledge,"The ability to predict the future in a given domain can be acquired by
discovering empirically from experience certain temporal patterns that tend to
repeat unerringly. Previous works in time series analysis allow one to make
quantitative predictions on the likely values of certain linear variables.
Since certain types of knowledge are better expressed in symbolic forms, making
qualitative predictions based on symbolic representations require a different
approach. A domain independent methodology called TIM (Time based Inductive
Machine) for discovering potentially uncertain temporal patterns from real time
observations using the technique of inductive inference is described here.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Some Extensions of Probabilistic Logic,"In [12], Nilsson proposed the probabilistic logic in which the truth values
of logical propositions are probability values between 0 and 1. It is
applicable to any logical system for which the consistency of a finite set of
propositions can be established. The probabilistic inference scheme reduces to
the ordinary logical inference when the probabilities of all propositions are
either 0 or 1. This logic has the same limitations of other probabilistic
reasoning systems of the Bayesian approach. For common sense reasoning,
consistency is not a very natural assumption. We have some well known examples:
{Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick
is a Republican}and {Tweety is a bird, birds can fly, Tweety is a penguin}. In
this paper, we shall propose some extensions of the probabilistic logic. In the
second section, we shall consider the space of all interpretations, consistent
or not. In terms of frames of discernment, the basic probability assignment
(bpa) and belief function can be defined. Dempster's combination rule is
applicable. This extension of probabilistic logic is called the evidential
logic in [ 1]. For each proposition s, its belief function is represented by an
interval [Spt(s), Pls(s)]. When all such intervals collapse to single points,
the evidential logic reduces to probabilistic logic (in the generalized version
of not necessarily consistent interpretations). Certainly, we get Nilsson's
probabilistic logic by further restricting to consistent interpretations. In
the third section, we shall give a probabilistic interpretation of
probabilistic logic in terms of multi-dimensional random variables. This
interpretation brings the probabilistic logic into the framework of probability
theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical
propositions. Each proposition may have true or false values; and may be
considered as a random variable. We have a probability distribution for each
proposition. The e-dimensional random variable (sl,..., Sn) may take values in
the space of all interpretations of 2n binary vectors. We may compute absolute
(marginal), conditional and joint probability distributions. It turns out that
the permissible probabilistic interpretation vector of Nilsson [12] consists of
the joint probabilities of S. Inconsistent interpretations will not appear, by
setting their joint probabilities to be zeros. By summing appropriate joint
probabilities, we get probabilities of individual propositions or subsets of
propositions. Since the Bayes formula and other techniques are valid for
e-dimensional random variables, the probabilistic logic is actually very close
to the Bayesian inference schemes. In the last section, we shall consider a
relaxation scheme for probabilistic logic. In this system, not only new
evidences will update the belief measures of a collection of propositions, but
also constraint satisfaction among these propositions in the relational network
will revise these measures. This mechanism is similar to human reasoning which
is an evaluative process converging to the most satisfactory result. The main
idea arises from the consistent labeling problem in computer vision. This
method is originally applied to scene analysis of line drawings. Later, it is
applied to matching, constraint satisfaction and multi sensor fusion by several
authors [8], [16] (and see references cited there). Recently, this method is
used in knowledge aggregation by Landy and Hummel [9].","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Predicting The Performance of Minimax and Product in Game-Tree,"The discovery that the minimax decision rule performs poorly in some games
has sparked interest in possible alternatives to minimax. Until recently, the
only games in which minimax was known to perform poorly were games which were
mainly of theoretical interest. However, this paper reports results showing
poor performance of minimax in a more common game called kalah. For the kalah
games tested, a non-minimax decision rule called the product rule performs
significantly better than minimax.
  This paper also discusses a possible way to predict whether or not minimax
will perform well in a game when compared to product. A parameter called the
rate of heuristic flaw (rhf) has been found to correlate positively with the.
performance of product against minimax. Both analytical and experimental
results are given that appear to support the predictive power of rhf.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Reasoning With Uncertain Knowledge,"A model of knowledge representation is described in which propositional facts
and the relationships among them can be supported by other facts. The set of
knowledge which can be supported is called the set of cognitive units, each
having associated descriptions of their explicit and implicit support
structures, summarizing belief and reliability of belief. This summary is
precise enough to be useful in a computational model while remaining
descriptive of the underlying symbolic support structure. When a fact supports
another supportive relationship between facts we call this meta-support. This
facilitates reasoning about both the propositional knowledge. and the support
structures underlying it.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Models vs. Inductive Inference for Dealing With Probabilistic Knowledge,"Two different approaches to dealing with probabilistic knowledge are examined
-models and inductive inference. Examples of the first are: influence diagrams
[1], Bayesian networks [2], log-linear models [3, 4]. Examples of the second
are: games-against nature [5, 6] varieties of maximum-entropy methods [7, 8,
9], and the author's min-score induction [10]. In the modeling approach, the
basic issue is manageability, with respect to data elicitation and computation.
Thus, it is assumed that the pertinent set of users in some sense knows the
relevant probabilities, and the problem is to format that knowledge in a way
that is convenient to input and store and that allows computation of the
answers to current questions in an expeditious fashion. The basic issue for the
inductive approach appears at first sight to be very different. In this
approach it is presumed that the relevant probabilities are only partially
known, and the problem is to extend that incomplete information in a reasonable
way to answer current questions. Clearly, this approach requires that some form
of induction be invoked. Of course, manageability is an important additional
concern. Despite their seeming differences, the two approaches have a fair
amount in common, especially with respect to the structural framework they
employ. Roughly speaking, this framework involves identifying clusters of
variables which strongly interact, establishing marginal probability
distributions on the clusters, and extending the subdistributions to a more
complete distribution, usually via a product formalism. The product extension
is justified on the modeling approach in terms of assumed conditional
independence; in the inductive approach the product form arises from an
inductive rule.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Towards a General-Purpose Belief Maintenance System,"There currently exists a gap between the theories proposed by the probability
and uncertainty and the needs of Artificial Intelligence research. These
theories primarily address the needs of expert systems, using knowledge
structures which must be pre-compiled and remain static in structure during
runtime. Many Al systems require the ability to dynamically add and remove
parts of the current knowledge structure (e.g., in order to examine what the
world would be like for different causal theories). This requires more
flexibility than existing uncertainty systems display. In addition, many Al
researchers are only interested in using ""probabilities"" as a means of
obtaining an ordering, rather than attempting to derive an accurate
probabilistic account of a situation. This indicates the need for systems which
stress ease of use and don't require extensive probability information when one
cannot (or doesn't wish to) provide such information. This paper attempts to
help reconcile the gap between approaches to uncertainty and the needs of many
AI systems by examining the control issues which arise, independent of a
particular uncertainty calculus. when one tries to satisfy these needs. Truth
Maintenance Systems have been used extensively in problem solving tasks to help
organize a set of facts and detect inconsistencies in the believed state of the
world. These systems maintain a set of true/false propositions and their
associated dependencies. However, situations often arise in which we are unsure
of certain facts or in which the conclusions we can draw from available
information are somewhat uncertain. The non-monotonic TMS 12] was an attempt at
reasoning when all the facts are not known, but it fails to take into account
degrees of belief and how available evidence can combine to strengthen a
particular belief. This paper addresses the problem of probabilistic reasoning
as it applies to Truth Maintenance Systems. It describes a belief Maintenance
System that manages a current set of beliefs in much the same way that a TMS
manages a set of true/false propositions. If the system knows that belief in
fact is dependent in some way upon belief in fact2, then it automatically
modifies its belief in facts when new information causes a change in belief of
fact2. It models the behavior of a TMS, replacing its 3-valued logic (true,
false, unknown) with an infinite valued logic, in such a way as to reduce to a
standard TMS if all statements are given in absolute true/false terms. Belief
Maintenance Systems can, therefore, be thought of as a generalization of Truth
Maintenance Systems, whose possible reasoning tasks are a superset of those for
a TMS.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"Planning, Scheduling, and Uncertainty in the Sequence of Future Events","Scheduling in the factory setting is compounded by computational complexity
and temporal uncertainty. Together, these two factors guarantee that the
process of constructing an optimal schedule will be costly and the chances of
executing that schedule will be slight. Temporal uncertainty in the task
execution time can be offset by several methods: eliminate uncertainty by
careful engineering, restore certainty whenever it is lost, reduce the
uncertainty by using more accurate sensors, and quantify and circumscribe the
remaining uncertainty. Unfortunately, these methods focus exclusively on the
sources of uncertainty and fail to apply knowledge of the tasks which are to be
scheduled. A complete solution must adapt the schedule of activities to be
performed according to the evolving state of the production world. The example
of vision-directed assembly is presented to illustrate that the principle of
least commitment, in the creation of a plan, in the representation of a
schedule, and in the execution of a schedule, enables a robot to operate
intelligently and efficiently, even in the presence of considerable uncertainty
in the sequence of future events.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"Deriving And Combining Continuous Possibility Functions in the Framework
  of Evidential Reasoning","To develop an approach to utilizing continuous statistical information within
the Dempster- Shafer framework, we combine methods proposed by Strat and by
Shafero We first derive continuous possibility and mass functions from
probability-density functions. Then we propose a rule for combining such
evidence that is simpler and more efficiently computed than Dempster's rule. We
discuss the relationship between Dempster's rule and our proposed rule for
combining evidence over continuous frames.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Non-Monotonicity in Probabilistic Reasoning,"We start by defining an approach to non-monotonic probabilistic reasoning in
terms of non-monotonic categorical (true-false) reasoning. We identify a type
of non-monotonic probabilistic reasoning, akin to default inheritance, that is
commonly found in practice, especially in ""evidential"" and ""Bayesian""
reasoning. We formulate this in terms of the Maximization of Conditional
Independence (MCI), and identify a variety of applications for this sort of
default. We propose a formalization using Pointwise Circumscription. We compare
MCI to Maximum Entropy, another kind of non-monotonic principle, and conclude
by raising a number of open questions","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"Flexible Interpretations: A Computational Model for Dynamic Uncertainty
  Assessment","The investigations reported in this paper center on the process of dynamic
uncertainty assessment during interpretation tasks in real domain. In
particular, we are interested here in the nature of the control structure of
computer programs that can support multiple interpretation and smooth
transitions between them, in real time. Each step of the processing involves
the interpretation of one input item and the appropriate re-establishment of
the system's confidence of the correctness of its interpretation(s).","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
The Myth of Modularity in Rule-Based Systems,"In this paper, we examine the concept of modularity, an often cited advantage
of the ruled-based representation methodology. We argue that the notion of
modularity consists of two distinct concepts which we call syntactic modularity
and semantic modularity. We argue that when reasoning under certainty, it is
reasonable to regard the rule-based approach as both syntactically and
semantically modular. However, we argue that in the case of plausible
reasoning, rules are syntactically modular but are rarely semantically modular.
To illustrate this point, we examine a particular approach for managing
uncertainty in rule-based systems called the MYCIN certainty factor model. We
formally define the concept of semantic modularity with respect to the
certainty factor model and discuss logical consequences of the definition. We
show that the assumption of semantic modularity imposes strong restrictions on
rules in a knowledge base. We argue that such restrictions are rarely valid in
practical applications. Finally, we suggest how the concept of semantic
modularity can be relaxed in a manner that makes it appropriate for plausible
reasoning.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
An Axiomatic Framework for Belief Updates,"In the 1940's, a physicist named Cox provided the first formal justification
for the axioms of probability based on the subjective or Bayesian
interpretation. He showed that if a measure of belief satisfies several
fundamental properties, then the measure must be some monotonic transformation
of a probability. In this paper, measures of change in belief or belief updates
are examined. In the spirit of Cox, properties for a measure of change in
belief are enumerated. It is shown that if a measure satisfies these
properties, it must satisfy other restrictive conditions. For example, it is
shown that belief updates in a probabilistic context must be equal to some
monotonic transformation of a likelihood ratio. It is hoped that this formal
explication of the belief update paradigm will facilitate critical discussion
and useful extensions of the approach.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Evidence as Opinions of Experts,"We describe a viewpoint on the Dempster/Shafer 'Theory of Evidence', and
provide an interpretation which regards the combination formulas as statistics
of the opinions of ""experts"". This is done by introducing spaces with binary
operations that are simpler to interpret or simpler to implement than the
standard combination formula, and showing that these spaces can be mapped
homomorphically onto the Dempster/Shafer theory of evidence space. The experts
in the space of ""opinions of experts"" combine information in a Bayesian
fashion. We present alternative spaces for the combination of evidence
suggested by this viewpoint.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Decision Under Uncertainty in Diagnosis,"This paper describes the incorporation of uncertainty in diagnostic reasoning
based on the set covering model of Reggia et. al. extended to what in the
Artificial Intelligence dichotomy between deep and compiled (shallow, surface)
knowledge based diagnosis may be viewed as the generic form at the compiled end
of the spectrum. A major undercurrent in this is advocating the need for a
strong underlying model and an integrated set of support tools for carrying
such a model in order to deal with uncertainty.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Knowledge and Uncertainty,"One purpose -- quite a few thinkers would say the main purpose -- of seeking
knowledge about the world is to enhance our ability to make good decisions. An
item of knowledge that can make no conceivable difference with regard to
anything we might do would strike many as frivolous. Whether or not we want to
be philosophical pragmatists in this strong sense with regard to everything we
might want to enquire about, it seems a perfectly appropriate attitude to adopt
toward artificial knowledge systems. If is granted that we are ultimately
concerned with decisions, then some constraints are imposed on our measures of
uncertainty at the level of decision making. If our measure of uncertainty is
real-valued, then it isn't hard to show that it must satisfy the classical
probability axioms. For example, if an act has a real-valued utility U(E) if
the event E obtains, and the same real-valued utility if the denial of E
obtains, so that U(E) = U(-E), then the expected utility of that act must be
U(E), and that must be the same as the uncertainty-weighted average of the
returns of the act, p-U(E) + q-U('E), where p and q represent the uncertainty
of E and-E respectively. But then we must have p + q = 1.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"An Application of Non-Monotonic Probabilistic Reasoning to Air Force
  Threat Correlation","Current approaches to expert systems' reasoning under uncertainty fail to
capture the iterative revision process characteristic of intelligent human
reasoning. This paper reports on a system, called the Non-monotonic
Probabilist, or NMP (Cohen, et al., 1985). When its inferences result in
substantial conflict, NMP examines and revises the assumptions underlying the
inferences until conflict is reduced to acceptable levels. NMP has been
implemented in a demonstration computer-based system, described below, which
supports threat correlation and in-flight route replanning by Air Force pilots.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Bayesian Inference for Radar Imagery Based Surveillance,"We are interested in creating an automated or semi-automated system with the
capability of taking a set of radar imagery, collection parameters and a priori
map and other tactical data, and producing likely interpretations of the
possible military situations given the available evidence. This paper is
concerned with the problem of the interpretation and computation of certainty
or belief in the conclusions reached by such a system.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Computing Reference Classes,"For any system with limited statistical knowledge, the combination of
evidence and the interpretation of sampling information require the
determination of the right reference class (or of an adequate one). The present
note (1) discusses the use of reference classes in evidential reasoning, and
(2) discusses implementations of Kyburg's rules for reference classes. This
paper contributes the first frank discussion of how much of Kyburg's system is
needed to be powerful, how much can be computed effectively, and how much is
philosophical fat.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"An Uncertainty Management Calculus for Ordering Searches in Distributed
  Dynamic Databases","MINDS is a distributed system of cooperating query engines that customize,
document retrieval for each user in a dynamic environment. It improves its
performance and adapts to changing patterns of document distribution by
observing system-user interactions and modifying the appropriate certainty
factors, which act as search control parameters. It argued here that the
uncertainty management calculus must account for temporal precedence,
reliability of evidence, degree of support for a proposition, and saturation
effects. The calculus presented here possesses these features. Some results
obtained with this scheme are discussed.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
An Explanation Mechanism for Bayesian Inferencing Systems,"Explanation facilities are a particularly important feature of expert system
frameworks. It is an area in which traditional rule-based expert system
frameworks have had mixed results. While explanations about control are well
handled, facilities are needed for generating better explanations concerning
knowledge base content. This paper approaches the explanation problem by
examining the effect an event has on a variable of interest within a symmetric
Bayesian inferencing system. We argue that any effect measure operating in this
context must satisfy certain properties. Such a measure is proposed. It forms
the basis for an explanation facility which allows the user of the Generalized
Bayesian Inferencing System to question the meaning of the knowledge base. That
facility is described in detail.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"Distributed Revision of Belief Commitment in Multi-Hypothesis
  Interpretations","This paper extends the applications of belief-networks to include the
revision of belief commitments, i.e., the categorical acceptance of a subset of
hypotheses which, together, constitute the most satisfactory explanation of the
evidence at hand. A coherent model of non-monotonic reasoning is established
and distributed algorithms for belief revision are presented. We show that, in
singly connected networks, the most satisfactory explanation can be found in
linear time by a message-passing algorithm similar to the one used in belief
updating. In multiply-connected networks, the problem may be exponentially hard
but, if the network is sparse, topological considerations can be used to render
the interpretation task tractable. In general, finding the most probable
combination of hypotheses is no more complex than computing the degree of
belief for any individual hypothesis. Applications to medical diagnosis are
illustrated.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Learning Link-Probabilities in Causal Trees,"A learning algorithm is presented which given the structure of a causal tree,
will estimate its link probabilities by sequential measurements on the leaves
only. Internal nodes of the tree represent conceptual (hidden) variables
inaccessible to observation. The method described is incremental, local,
efficient, and remains robust to measurement imprecisions.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Approximate Deduction in Single Evidential Bodies,"Results on approximate deduction in the context of the calculus of evidence
of Dempster-Shafer and the theory of interval probabilities are reported.
Approximate conditional knowledge about the truth of conditional propositions
was assumed available and expressed as sets of possible values (actually
numeric intervals) of conditional probabilities. Under different
interpretations of this conditional knowledge, several formulas were produced
to integrate unconditioned estimates (assumed given as sets of possible values
of unconditioned probabilities) with conditional estimates. These formulas are
discussed together with the computational characteristics of the methods
derived from them. Of particular importance is one such evidence integration
formulation, produced under a belief oriented interpretation, which
incorporates both modus ponens and modus tollens inferential mechanisms, allows
integration of conditioned and unconditioned knowledge without resorting to
iterative or sequential approximations, and produces elementary mass
distributions as outputs using similar distributions as inputs.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
"The Rational and Computational Scope of Probabilistic Rule-Based Expert
  Systems","Belief updating schemes in artificial intelligence may be viewed as three
dimensional languages, consisting of a syntax (e.g. probabilities or certainty
factors), a calculus (e.g. Bayesian or CF combination rules), and a semantics
(i.e. cognitive interpretations of competing formalisms). This paper studies
the rational scope of those languages on the syntax and calculus grounds. In
particular, the paper presents an endomorphism theorem which highlights the
limitations imposed by the conditional independence assumptions implicit in the
CF calculus. Implications of the theorem to the relationship between the CF and
the Bayesian languages and the Dempster-Shafer theory of evidence are
presented. The paper concludes with a discussion of some implications on
rule-based knowledge engineering in uncertain domains.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
A Causal Bayesian Model for the Diagnosis of Appendicitis,"The causal Bayesian approach is based on the assumption that effects (e.g.,
symptoms) that are not conditionally independent with respect to some causal
agent (e.g., a disease) are conditionally independent with respect to some
intermediate state caused by the agent, (e.g., a pathological condition). This
paper describes the development of a causal Bayesian model for the diagnosis of
appendicitis. The paper begins with a description of the standard Bayesian
approach to reasoning about uncertainty and the major critiques it faces. The
paper then lays the theoretical groundwork for the causal extension of the
Bayesian approach, and details specific improvements we have developed. The
paper then goes on to describe our knowledge engineering and implementation and
the results of a test of the system. The paper concludes with a discussion of
how the causal Bayesian approach deals with the criticisms of the standard
Bayesian model and why it is superior to alternative approaches to reasoning
about uncertainty popular in the Al community.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
A Backwards View for Assessment,"Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
DAVID: Influence Diagram Processing System for the Macintosh,"Influence diagrams are a directed graph representation for uncertainties as
probabilities. The graph distinguishes between those variables which are under
the control of a decision maker (decisions, shown as rectangles) and those
which are not (chances, shown as ovals), as well as explicitly denoting a goal
for solution (value, shown as a rounded rectangle.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Propagation of Belief Functions: A Distributed Approach,"In this paper, we describe a scheme for propagating belief functions in
certain kinds of trees using only local computations. This scheme generalizes
the computational scheme proposed by Shafer and Logan1 for diagnostic trees of
the type studied by Gordon and Shortliffe, and the slightly more general scheme
given by Shafer for hierarchical evidence. It also generalizes the scheme
proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's
causal trees and Gordon and Shortliffe's diagnostic trees are both ways of
breaking the evidence that bears on a large problem down into smaller items of
evidence that bear on smaller parts of the problem so that these smaller
problems can be dealt with one at a time. This localization of effort is often
essential in order to make the process of probability judgment feasible, both
for the person who is making probability judgments and for the machine that is
combining them. The basic structure for our scheme is a type of tree that
generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this
general type permit localized computation in Pearl's sense. They are based on
qualitative judgments of conditional independence. We believe that the scheme
we describe here will prove useful in expert systems. It is now clear that the
successful propagation of probabilities or certainty factors in expert systems
requires much more structure than can be provided in a pure production-system
framework. Bayesian schemes, on the other hand, often make unrealistic demands
for structure. The propagation of belief functions in trees and more general
networks stands on a middle ground where some sensible and useful things can be
done. We would like to emphasize that the basic idea of local computation for
propagating probabilities is due to Judea Pearl. It is a very innovative idea;
we do not believe that it can be found in the Bayesian literature prior to
Pearl's work. We see our contribution as extending the usefulness of Pearl's
idea by generalizing it from Bayesian probabilities to belief functions. In the
next section, we give a brief introduction to belief functions. The notions of
qualitative independence for partitions and a qualitative Markov tree are
introduced in Section III. Finally, in Section IV, we describe a scheme for
propagating belief functions in qualitative Markov trees.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Appropriate and Inappropriate Estimation Techniques,"Mode {also called MAP} estimation, mean estimation and median estimation are
examined here to determine when they can be safely used to derive {posterior)
cost minimizing estimates. (These are all Bayes procedures, using the mode.
mean. or median of the posterior distribution). It is found that modal
estimation only returns cost minimizing estimates when the cost function is
0-t. If the cost function is a function of distance then mean estimation only
returns cost minimizing estimates when the cost function is squared distance
from the true value and median estimation only returns cost minimizing
estimates when the cost function ts the distance from the true value. Results
are presented on the goodness or modal estimation with non 0-t cost functions","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Estimating Uncertain Spatial Relationships in Robotics,"In this paper, we describe a representation for spatial information, called
the stochastic map, and associated procedures for building it, reading
information from it, and revising it incrementally as new information is
obtained. The map contains the estimates of relationships among objects in the
map, and their uncertainties, given all the available information. The
procedures provide a general solution to the problem of estimating uncertain
relative spatial relationships. The estimates are probabilistic in nature, an
advance over the previous, very conservative, worst-case approaches to the
problem. Finally, the procedures are developed in the context of
state-estimation and filtering theory, which provides a solid basis for
numerous extensions.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
A VLSI Design and Implementation for a Real-Time Approximate Reasoning,"The role of inferencing with uncertainty is becoming more important in
rule-based expert systems (ES), since knowledge given by a human expert is
often uncertain or imprecise. We have succeeded in designing a VLSI chip which
can perform an entire inference process based on fuzzy logic. The design of the
VLSI fuzzy inference engine emphasizes simplicity, extensibility, and
efficiency (operational speed and layout area). It is fabricated in 2.5 um CMOS
technology. The inference engine consists of three major components; a rule set
memory, an inference processor, and a controller. In this implementation, a
rule set memory is realized by a read only memory (ROM). The controller
consists of two counters. In the inference processor, one data path is laid out
for each rule. The number of the inference rule can be increased adding more
data paths to the inference processor. All rules are executed in parallel, but
each rule is processed serially. The logical structure of fuzzy inference
proposed in the current paper maps nicely onto the VLSI structure. A two-phase
nonoverlapping clocking scheme is used. Timing tests indicate that the
inference engine can operate at approximately 20.8 MHz. This translates to an
execution speed of approximately 80,000 Fuzzy Logical Inferences Per Second
(FLIPS), and indicates that the inference engine is suitable for a demanding
real-time application. The potential applications include decision-making in
the area of command and control for intelligent robot systems, process control,
missile and aircraft guidance, and other high performance machines.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
A General Purpose Inference Engine for Evidential Reasoning Research,"The purpose of this paper is to report on the most recent developments in our
ongoing investigation of the representation and manipulation of uncertainty in
automated reasoning systems. In our earlier studies (Tong and Shapiro, 1985) we
described a series of experiments with RUBRIC (Tong et al., 1985), a system for
full-text document retrieval, that generated some interesting insights into the
effects of choosing among a class of scalar valued uncertainty calculi. [n
order to extend these results we have begun a new series of experiments with a
larger class of representations and calculi, and to help perform these
experiments we have developed a general purpose inference engine.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Generalizing Fuzzy Logic Probabilistic Inferences,"Linear representations for a subclass of boolean symmetric functions selected
by a parity condition are shown to constitute a generalization of the linear
constraints on probabilities introduced by Boole. These linear constraints are
necessary to compute probabilities of events with relations between the.
arbitrarily specified with propositional calculus boolean formulas.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Qualitative Probabilistic Networks for Planning Under Uncertainty,"Bayesian networks provide a probabilistic semantics for qualitative
assertions about likelihood. A qualitative reasoner based on an algebra over
these assertions can derive further conclusions about the influence of actions.
While the conclusions are much weaker than those computed from complete
probability distributions, they are still valuable for suggesting potential
actions, eliminating obviously inferior plans, identifying important tradeoffs,
and explaining probabilistic models.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Experimentally Comparing Uncertain Inference Systems to Probability,"This paper examines the biases and performance of several uncertain inference
systems: Mycin, a variant of Mycin. and a simplified version of probability
using conditional independence assumptions. We present axiomatic arguments for
using Minimum Cross Entropy inference as the best way to do uncertain
inference. For Mycin and its variant we found special situations where its
performance was very good, but also situations where performance was worse than
random guessing, or where data was interpreted as having the opposite of its
true import We have found that all three of these systems usually gave accurate
results, and that the conditional independence assumptions gave the most robust
results. We illustrate how the Importance of biases may be quantitatively
assessed and ranked. Considerations of robustness might be a critical factor is
selecting UlS's for a given application.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Evaluation of Uncertain Inference Models I: PROSPECTOR,"This paper examines the accuracy of the PROSPECTOR model for uncertain
reasoning. PROSPECTOR's solutions for a large number of computer-generated
inference networks were compared to those obtained from probability theory and
minimum cross-entropy calculations. PROSPECTOR's answers were generally
accurate for a restricted subset of problems that are consistent with its
assumptions. However, even within this subset, we identified conditions under
which PROSPECTOR's performance deteriorates.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
On Implementing Usual Values,"In many cases commonsense knowledge consists of knowledge of what is usual.
In this paper we develop a system for reasoning with usual information. This
system is based upon the fact that these pieces of commonsense information
involve both a probabilistic aspect and a granular aspect. We implement this
system with the aid of possibility-probability granules.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
On the Combinality of Evidence in the Dempster-Shafer Theory,"In the current versions of the Dempster-Shafer theory, the only essential
restriction on the validity of the rule of combination is that the sources of
evidence must be statistically independent. Under this assumption, it is
permissible to apply the Dempster-Shafer rule to two or mere distinct
probability distributions.","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)"
Logical Probability Preferences,"We present a unified logical framework for representing and reasoning about
both probability quantitative and qualitative preferences in probability answer
set programming, called probability answer set optimization programs. The
proposed framework is vital to allow defining probability quantitative
preferences over the possible outcomes of qualitative preferences. We show the
application of probability answer set optimization programs to a variant of the
well-known nurse restoring problem, called the nurse restoring with probability
preferences problem. To the best of our knowledge, this development is the
first to consider a logical framework for reasoning about probability
quantitative preferences, in general, and reasoning about both probability
quantitative and qualitative preferences in particular.","arXiv admin note: substantial text overlap with arXiv:1304.2384,
  arXiv:1304.2797"
"From Constraints to Resolution Rules, Part I: Conceptual Framework","Many real world problems naturally appear as constraints satisfaction
problems (CSP), for which very efficient algorithms are known. Most of these
involve the combination of two techniques: some direct propagation of
constraints between variables (with the goal of reducing their sets of possible
values) and some kind of structured search (depth-first, breadth-first,...).
But when such blind search is not possible or not allowed or when one wants a
'constructive' or a 'pattern-based' solution, one must devise more complex
propagation rules instead. In this case, one can introduce the notion of a
candidate (a 'still possible' value for a variable). Here, we give this
intuitive notion a well defined logical status, from which we can define the
concepts of a resolution rule and a resolution theory. In order to keep our
analysis as concrete as possible, we illustrate each definition with the well
known Sudoku example. Part I proposes a general conceptual framework based on
first order logic; with the introduction of chains and braids, Part II will
give much deeper results.","International Joint Conferences on Computer, Information, Systems
  Sciences and Engineering (CISSE 08), December 5-13, 2008, Springer. Also a
  chapter of the book ""Advanced Techniques in Computing Sciences and Software
  Engineering"", Khaled Elleithy Editor, pp. 165-170, Springer, 2010, ISBN
  9789094136599"
"From Constraints to Resolution Rules, Part II: chains, braids,
  confluence and T&E","In this Part II, we apply the general theory developed in Part I to a
detailed analysis of the Constraint Satisfaction Problem (CSP). We show how
specific types of resolution rules can be defined. In particular, we introduce
the general notions of a chain and a braid. As in Part I, these notions are
illustrated in detail with the Sudoku example - a problem known to be
NP-complete and which is therefore typical of a broad class of hard problems.
For Sudoku, we also show how far one can go in 'approximating' a CSP with a
resolution theory and we give an empirical statistical analysis of how the
various puzzles, corresponding to different sets of entries, can be classified
along a natural scale of complexity. For any CSP, we also prove the confluence
property of some Resolution Theories based on braids and we show how it can be
used to define different resolution strategies. Finally, we prove that, in any
CSP, braids have the same solving capacity as Trial-and-Error (T&E) with no
guessing and we comment this result in the Sudoku case.","International Joint Conferences on Computer, Information, Systems
  Sciences and Engineering (CISSE 08), December 5-13, 2008, Springer. Also a
  chapter of the book 'Advanced Techniques in Computing Sciences and Software
  Engineering', Khaled Elleithy Editor, pp. 171-176, Springer, 2010, ISBN
  9789094136599"
An Inequality Paradigm for Probabilistic Knowledge,"We propose an inequality paradigm for probabilistic reasoning based on a
logic of upper and lower bounds on conditional probabilities. We investigate a
family of probabilistic logics, generalizing the work of Nilsson [14]. We
develop a variety of logical notions for probabilistic reasoning, including
soundness, completeness justification; and convergence: reduction of a theory
to a simpler logical class. We argue that a bound view is especially useful for
describing the semantics of probabilistic knowledge representation and for
describing intermediate states of probabilistic inference and updating. We show
that the Dempster-Shafer theory of evidence is formally identical to a special
case of our generalized probabilistic logic. Our paradigm thus incorporates
both Bayesian ""rule-based"" approaches and avowedly non-Bayesian ""evidential""
approaches such as MYCIN and DempsterShafer. We suggest how to integrate the
two ""schools"", and explore some possibilities for novel synthesis of a variety
of ideas in probabilistic reasoning.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Probabilistic Interpretations for MYCIN's Certainty Factors,"This paper examines the quantities used by MYCIN to reason with uncertainty,
called certainty factors. It is shown that the original definition of certainty
factors is inconsistent with the functions used in MYCIN to combine the
quantities. This inconsistency is used to argue for a redefinition of certainty
factors in terms of the intuitively appealing desiderata associated with the
combining functions. It is shown that this redefinition accommodates an
unlimited number of probabilistic interpretations. These interpretations are
shown to be monotonic transformations of the likelihood ratio p(EIH)/p(El H).
The construction of these interpretations provides insight into the assumptions
implicit in the certainty factor model. In particular, it is shown that if
uncertainty is to be propagated through an inference network in accordance with
the desiderata, evidence must be conditionally independent given the hypothesis
and its negation and the inference network must have a tree structure. It is
emphasized that assumptions implicit in the model are rarely true in practical
applications. Methods for relaxing the assumptions are suggested.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Uncertain Reasoning Using Maximum Entropy Inference,"The use of maximum entropy inference in reasoning with uncertain information
is commonly justified by an information-theoretic argument. This paper
discusses a possible objection to this information-theoretic justification and
shows how it can be met. I then compare maximum entropy inference with certain
other currently popular methods for uncertain reasoning. In making such a
comparison, one must distinguish between static and dynamic theories of degrees
of belief: a static theory concerns the consistency conditions for degrees of
belief at a given time; whereas a dynamic theory concerns how one's degrees of
belief should change in the light of new information. It is argued that maximum
entropy is a dynamic theory and that a complete theory of uncertain reasoning
can be gotten by combining maximum entropy inference with probability theory,
which is a static theory. This total theory, I argue, is much better grounded
than are other theories of uncertain reasoning.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Independence and Bayesian Updating Methods,"Duda, Hart, and Nilsson have set forth a method for rule-based inference
systems to use in updating the probabilities of hypotheses on the basis of
multiple items of new evidence. Pednault, Zucker, and Muresan claimed to give
conditions under which independence assumptions made by Duda et al. preclude
updating-that is, prevent the evidence from altering the probabilities of the
hypotheses. Glymour refutes Pednault et al.'s claim with a counterexample of a
rather special form (one item of evidence is incompatible with all but one of
the hypotheses); he raises, but leaves open, the question whether their result
would be true with an added assumption to rule out such special cases. We show
that their result does not hold even with the added assumption, but that it can
nevertheless be largely salvaged. Namely, under the conditions assumed by
Pednault et al., at most one of the items of evidence can alter the probability
of any given hypothesis; thus, although updating is possible, multiple updating
for any of the hypotheses is precluded.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
A Constraint Propagation Approach to Probabilistic Reasoning,"The paper demonstrates that strict adherence to probability theory does not
preclude the use of concurrent, self-activated constraint-propagation
mechanisms for managing uncertainty. Maintaining local records of
sources-of-belief allows both predictive and diagnostic inferences to be
activated simultaneously and propagate harmoniously towards a stable
equilibrium.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Relative Entropy, Probabilistic Inference and AI","Various properties of relative entropy have led to its widespread use in
information theory. These properties suggest that relative entropy has a role
to play in systems that attempt to perform inference in terms of probability
distributions. In this paper, I will review some basic properties of relative
entropy as well as its role in probabilistic inference. I will also mention
briefly a few existing and potential applications of relative entropy to
so-called artificial intelligence (AI).","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Foundations of Probability Theory for AI - The Application of
  Algorithmic Probability to Problems in Artificial Intelligence","This paper covers two topics: first an introduction to Algorithmic Complexity
Theory: how it defines probability, some of its characteristic properties and
past successful applications. Second, we apply it to problems in A.I. - where
it promises to give near optimum search procedures for two very broad classes
of problems.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Selecting Uncertainty Calculi and Granularity: An Experiment in
  Trading-Off Precision and Complexity","The management of uncertainty in expert systems has usually been left to ad
hoc representations and rules of combinations lacking either a sound theory or
clear semantics. The objective of this paper is to establish a theoretical
basis for defining the syntax and semantics of a small subset of calculi of
uncertainty operating on a given term set of linguistic statements of
likelihood. Each calculus is defined by specifying a negation, a conjunction
and a disjunction operator. Families of Triangular norms and conorms constitute
the most general representations of conjunction and disjunction operators.
These families provide us with a formalism for defining an infinite number of
different calculi of uncertainty. The term set will define the uncertainty
granularity, i.e. the finest level of distinction among different
quantifications of uncertainty. This granularity will limit the ability to
differentiate between two similar operators. Therefore, only a small finite
subset of the infinite number of calculi will produce notably different
results. This result is illustrated by two experiments where nine and eleven
different calculi of uncertainty are used with three term sets containing five,
nine, and thirteen elements, respectively. Finally, the use of context
dependent rule set is proposed to select the most appropriate calculus for any
given situation. Such a rule set will be relatively small since it must only
describe the selection policies for a small number of calculi (resulting from
the analyzed trade-off between complexity and precision).","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
A Framework for Non-Monotonic Reasoning About Probabilistic Assumptions,"Attempts to replicate probabilistic reasoning in expert systems have
typically overlooked a critical ingredient of that process. Probabilistic
analysis typically requires extensive judgments regarding interdependencies
among hypotheses and data, and regarding the appropriateness of various
alternative models. The application of such models is often an iterative
process, in which the plausibility of the results confirms or disconfirms the
validity of assumptions made in building the model. In current expert systems,
by contrast, probabilistic information is encapsulated within modular rules
(involving, for example, ""certainty factors""), and there is no mechanism for
reviewing the overall form of the probability argument or the validity of the
judgments entering into it.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Metaprobability and Dempster-Shafer in Evidential Reasoning,"Evidential reasoning in expert systems has often used ad-hoc uncertainty
calculi. Although it is generally accepted that probability theory provides a
firm theoretical foundation, researchers have found some problems with its use
as a workable uncertainty calculus. Among these problems are representation of
ignorance, consistency of probabilistic judgements, and adjustment of a priori
judgements with experience. The application of metaprobability theory to
evidential reasoning is a new approach to solving these problems.
Metaprobability theory can be viewed as a way to provide soft or hard
constraints on beliefs in much the same manner as the Dempster-Shafer theory
provides constraints on probability masses on subsets of the state space. Thus,
we use the Dempster-Shafer theory, an alternative theory of evidential
reasoning to illuminate metaprobability theory as a theory of evidential
reasoning. The goal of this paper is to compare how metaprobability theory and
Dempster-Shafer theory handle the adjustment of beliefs with evidence with
respect to a particular thought experiment. Sections 2 and 3 give brief
descriptions of the metaprobability and Dempster-Shafer theories.
Metaprobability theory deals with higher order probabilities applied to
evidential reasoning. Dempster-Shafer theory is a generalization of probability
theory which has evolved from a theory of upper and lower probabilities.
Section 4 describes a thought experiment and the metaprobability and
DempsterShafer analysis of the experiment. The thought experiment focuses on
forming beliefs about a population with 6 types of members {1, 2, 3, 4, 5, 6}.
A type is uniquely defined by the values of three features: A, B, C. That is,
if the three features of one member of the population were known then its type
could be ascertained. Each of the three features has two possible values, (e.g.
A can be either ""a0"" or ""al""). Beliefs are formed from evidence accrued from
two sensors: sensor A, and sensor B. Each sensor senses the corresponding
defining feature. Sensor A reports that half of its observations are ""a0"" and
half the observations are 'al'. Sensor B reports that half of its observations
are ``b0,' and half are ""bl"". Based on these two pieces of evidence, what
should be the beliefs on the distribution of types in the population? Note that
the third feature is not observed by any sensor.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Implementing Probabilistic Reasoning,"General problems in analyzing information in a probabilistic database are
considered. The practical difficulties (and occasional advantages) of storing
uncertain data, of using it conventional forward- or backward-chaining
inference engines, and of working with a probabilistic version of resolution
are discussed. The background for this paper is the incorporation of uncertain
reasoning facilities in MRS, a general-purpose expert system building tool.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Probability Judgement in Artificial Intelligence,"This paper is concerned with two theories of probability judgment: the
Bayesian theory and the theory of belief functions. It illustrates these
theories with some simple examples and discusses some of the issues that arise
when we try to implement them in expert systems. The Bayesian theory is well
known; its main ideas go back to the work of Thomas Bayes (1702-1761). The
theory of belief functions, often called the Dempster-Shafer theory in the
artificial intelligence community, is less well known, but it has even older
antecedents; belief-function arguments appear in the work of George Hooper
(16401723) and James Bernoulli (1654-1705). For elementary expositions of the
theory of belief functions, see Shafer (1976, 1985).","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
A Framework for Comparing Uncertain Inference Systems to Probability,"Several different uncertain inference systems (UISs) have been developed for
representing uncertainty in rule-based expert systems. Some of these, such as
Mycin's Certainty Factors, Prospector, and Bayes' Networks were designed as
approximations to probability, and others, such as Fuzzy Set Theory and
DempsterShafer Belief Functions were not. How different are these UISs in
practice, and does it matter which you use? When combining and propagating
uncertain information, each UIS must, at least by implication, make certain
assumptions about correlations not explicily specified. The maximum entropy
principle with minimum cross-entropy updating, provides a way of making
assumptions about the missing specification that minimizes the additional
information assumed, and thus offers a standard against which the other UISs
can be compared. We describe a framework for the experimental comparison of the
performance of different UISs, and provide some illustrative results.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Inductive Inference and the Representation of Uncertainty,"The form and justification of inductive inference rules depend strongly on
the representation of uncertainty. This paper examines one generic
representation, namely, incomplete information. The notion can be formalized by
presuming that the relevant probabilities in a decision problem are known only
to the extent that they belong to a class K of probability distributions. The
concept is a generalization of a frequent suggestion that uncertainty be
represented by intervals or ranges on probabilities. To make the representation
useful for decision making, an inductive rule can be formulated which
determines, in a well-defined manner, a best approximation to the unknown
probability, given the set K. In addition, the knowledge set notion entails a
natural procedure for updating -- modifying the set K given new evidence.
Several non-intuitive consequences of updating emphasize the differences
between inference with complete and inference with incomplete information.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Induction, of and by Probability","This paper examines some methods and ideas underlying the author's successful
probabilistic learning systems(PLS), which have proven uniquely effective and
efficient in generalization learning or induction. While the emerging
principles are generally applicable, this paper illustrates them in heuristic
search, which demands noise management and incremental learning. In our
approach, both task performance and learning are guided by probability.
Probabilities are incrementally normalized and revised, and their errors are
located and corrected.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
An Odds Ratio Based Inference Engine,"Expert systems applications that involve uncertain inference can be
represented by a multidimensional contingency table. These tables offer a
general approach to inferring with uncertain evidence, because they can embody
any form of association between any number of pieces of evidence and
conclusions. (Simpler models may be required, however, if the number of pieces
of evidence bearing on a conclusion is large.) This paper presents a method of
using these tables to make uncertain inferences without assumptions of
conditional independence among pieces of evidence or heuristic combining rules.
As evidence is accumulated, new joint probabilities are calculated so as to
maintain any dependencies among the pieces of evidence that are found in the
contingency table. The new conditional probability of the conclusion is then
calculated directly from these new joint probabilities and the conditional
probabilities in the contingency table.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
A Framework for Control Strategies in Uncertain Inference Networks,"Control Strategies for hierarchical tree-like probabilistic inference
networks are formulated and investigated. Strategies that utilize staged
look-ahead and temporary focus on subgoals are formalized and refined using the
Depth Vector concept that serves as a tool for defining the 'virtual tree'
regarded by the control strategy. The concept is illustrated by four types of
control strategies for three-level trees that are characterized according to
their Depth Vector, and according to the way they consider intermediate nodes
and the role that they let these nodes play. INFERENTI is a computerized
inference system written in Prolog, which provides tools for exercising a
variety of control strategies. The system also provides tools for simulating
test data and for comparing the relative average performance under different
strategies.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Combining Uncertain Estimates,"In a real expert system, one may have unreliable, unconfident, conflicting
estimates of the value for a particular parameter. It is important for decision
making that the information present in this aggregate somehow find its way into
use. We cast the problem of representing and combining uncertain estimates as
selection of two kinds of functions, one to determine an estimate, the other
its uncertainty. The paper includes a long list of properties that such
functions should satisfy, and it presents one method that satisfies them.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Confidence Factors, Empiricism and the Dempster-Shafer Theory of
  Evidence","The issue of confidence factors in Knowledge Based Systems has become
increasingly important and Dempster-Shafer (DS) theory has become increasingly
popular as a basis for these factors. This paper discusses the need for an
empirical lnterpretatlon of any theory of confidence factors applied to
Knowledge Based Systems and describes an empirical lnterpretatlon of DS theory
suggesting that the theory has been extensively misinterpreted. For the
essentially syntactic DS theory, a model is developed based on sample spaces,
the traditional semantic model of probability theory. This model is used to
show that, if belief functions are based on reasonably accurate sampling or
observation of a sample space, then the beliefs and upper probabilities as
computed according to DS theory cannot be interpreted as frequency ratios.
Since many proposed applications of DS theory use belief functions in
situations with statistically derived evidence (Wesley [1]) and seem to appeal
to statistical intuition to provide an lnterpretatlon of the results as has
Garvey [2], it may be argued that DS theory has often been misapplied.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Incidence Calculus: A Mechanism for Probabilistic Reasoning,"Mechanisms for the automation of uncertainty are required for expert systems.
Sometimes these mechanisms need to obey the properties of probabilistic
reasoning. A purely numeric mechanism, like those proposed so far, cannot
provide a probabilistic logic with truth functional connectives. We propose an
alternative mechanism, Incidence Calculus, which is based on a representation
of uncertainty using sets of points, which might represent situations, models
or possible worlds. Incidence Calculus does provide a probabilistic logic with
truth functional connectives.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Evidential Confirmation as Transformed Probability,"A considerable body of work in AI has been concerned with aggregating
measures of confirmatory and disconfirmatory evidence for a common set of
propositions. Claiming classical probability to be inadequate or inappropriate,
several researchers have gone so far as to invent new formalisms and methods.
We show how to represent two major such alternative approaches to evidential
confirmation not only in terms of transformed (Bayesian) probability, but also
in terms of each other. This unifies two of the leading approaches to
confirmation theory, by showing that a revised MYCIN Certainty Factor method
[12] is equivalent to a special case of Dempster-Shafer theory. It yields a
well-understood axiomatic basis, i.e. conditional independence, to interpret
previous work on quantitative confirmation theory. It substantially resolves
the ""taxe-them-or-leave-them"" problem of priors: MYCIN had to leave them out,
while PROSPECTOR had to have them in. It recasts some of confirmation theory's
advantages in terms of the psychological accessibility of probabilistic
information in different (transformed) formats. Finally, it helps to unify the
representation of uncertain reasoning (see also [11]).","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Interval-Based Decisions for Reasoning Systems,"This essay looks at decision-making with interval-valued probability
measures. Existing decision methods have either supplemented expected utility
methods with additional criteria of optimality, or have attempted to supplement
the interval-valued measures. We advocate a new approach, which makes the
following questions moot: 1. which additional criteria to use, and 2. how wide
intervals should be. In order to implement the approach, we need more
epistemological information. Such information can be generated by a rule of
acceptance with a parameter that allows various attitudes toward error, or can
simply be declared. In sketch, the argument is: 1. probability intervals are
useful and natural in All. systems; 2. wide intervals avoid error, but are
useless in some risk sensitive decision-making; 3. one may obtain narrower
intervals if one is less cautious; 4. if bodies of knowledge can be ordered by
their caution, one should perform the decision analysis with the acceptable
body of knowledge that is the most cautious, of those that are useful. The
resulting behavior differs from that of a behavioral probabilist (a Bayesian)
because in the proposal, 5. intervals based on successive bodies of knowledge
are not always nested; 6. if the agent uses a probability for a particular
decision, she need not commit to that probability for credence or future
decision; and 7. there may be no acceptable body of knowledge that is useful;
hence, sometimes no decision is mandated.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"Machine Generalization and Human Categorization: An
  Information-Theoretic View","In designing an intelligent system that must be able to explain its reasoning
to a human user, or to provide generalizations that the human user finds
reasonable, it may be useful to take into consideration psychological data on
what types of concepts and categories people naturally use. The psychological
literature on concept learning and categorization provides strong evidence that
certain categories are more easily learned, recalled, and recognized than
others. We show here how a measure of the informational value of a category
predicts the results of several important categorization experiments better
than standard alternative explanations. This suggests that information-based
approaches to machine generalization may prove particularly useful and natural
for human users of the systems.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
Exact Reasoning Under Uncertainty,"This paper focuses on designing expert systems to support decision making in
complex, uncertain environments. In this context, our research indicates that
strictly probabilistic representations, which enable the use of
decision-theoretic reasoning, are highly preferable to recently proposed
alternatives (e.g., fuzzy set theory and Dempster-Shafer theory). Furthermore,
we discuss the language of influence diagrams and a corresponding methodology
-decision analysis -- that allows decision theory to be used effectively and
efficiently as a decision-making aid. Finally, we use RACHEL, a system that
helps infertile couples select medical treatments, to illustrate the
methodology of decision analysis as basis for expert decision systems.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
"The Estimation of Subjective Probabilities via Categorical Judgments of
  Uncertainty","Theoretically as well as experimentally it is investigated how people
represent their knowledge in order to make decisions or to share their
knowledge with others. Experiment 1 probes into the ways how people 6ather
information about the frequencies of events and how the requested response
mode, that is, numerical vs. verbal estimates interferes with this knowledge.
The least interference occurs if the subjects are allowed to give verbal
responses. From this it is concluded that processing knowledge about
uncertainty categorically, that is, by means of verbal expressions, imposes
less mental work load on the decision matter than numerical processing.
Possibility theory is used as a framework for modeling the individual usage of
verbal categories for grades of uncertainty. The 'elastic' constraints on the
verbal expressions for every sing1e subject are determined in Experiment 2 by
means of sequential calibration. In further experiments it is shown that the
superiority of the verbal processing of knowledge about uncertainty guise
generally reduces persistent biases reported in the literature: conservatism
(Experiment 3) and neg1igence of regression (Experiment 4). The reanalysis of
Hormann's data reveal that in verbal Judgments people exhibit sensitivity for
base rates and are not prone to the conjunction fallacy. In a final experiment
(5) about predictions in a real-life situation it turns out that in a numerical
forecasting task subjects restricted themselves to those parts of their
knowledge which are numerical. On the other hand subjects in a verbal
forecasting task accessed verbally as well as numerically stated knowledge.
Forecasting is structurally related to the estimation of probabilities for rare
events insofar as supporting and contradicting arguments have to be evaluated
and the choice of the final Judgment has to be Justified according to the
evidence brought forward. In order to assist people in such choice situations a
formal model for the interactive checking of arguments has been developed. The
model transforms the normal-language quantifiers used in the arguments into
fuzzy numbers and evaluates the given train of arguments by means of fuzzy
numerica1 operations. Ambiguities in the meanings of quantifiers are resolved
interactively.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
A Cure for Pathological Behavior in Games that Use Minimax,"The traditional approach to choosing moves in game-playing programs is the
minimax procedure. The general belief underlying its use is that increasing
search depth improves play. Recent research has shown that given certain
simplifying assumptions about a game tree's structure, this belief is
erroneous: searching deeper decreases the probability of making a correct move.
This phenomenon is called game tree pathology. Among these simplifying
assumptions is uniform depth of win/loss (terminal) nodes, a condition which is
not true for most real games. Analytic studies in [10] have shown that if every
node in a pathological game tree is made terminal with probability exceeding a
certain threshold, the resulting tree is nonpathological. This paper considers
a new evaluation function which recognizes increasing densities of forced wins
at deeper levels in the tree. This property raises two points that strengthen
the hypothesis that uniform win depth causes pathology. First, it proves
mathematically that as search deepens, an evaluation function that does not
explicitly check for certain forced win patterns becomes decreasingly likely to
force wins. This failing predicts the pathological behavior of the original
evaluation function. Second, it shows empirically that despite recognizing
fewer mid-game wins than the theoretically predicted minimum, the new function
is nonpathological.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
An Evaluation of Two Alternatives to Minimax,"In the field of Artificial Intelligence, traditional approaches to choosing
moves in games involve the we of the minimax algorithm. However, recent
research results indicate that minimizing may not always be the best approach.
In this paper we summarize the results of some measurements on several model
games with several different evaluation functions. These measurements, which
are presented in detail in [NPT], show that there are some new algorithms that
can make significantly better use of evaluation function values than the
minimax algorithm does.","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)"
