{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/pclinux/spiced/data_science/capstone_project/convergence_oracle/0_scraping_alp.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/capstone_project/convergence_oracle/0_scraping_alp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/capstone_project/convergence_oracle/0_scraping_alp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pclinux/spiced/data_science/capstone_project/convergence_oracle/0_scraping_alp.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers in this function because google scholar webpage required login\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "url = 'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=object+detection+in+aerial+image+&btnG=&oq=ob'\n",
    "response=requests.get(url,headers=headers)\n",
    "doc = BeautifulSoup(page_contents,'html.parser')\n",
    "\n",
    "# this function for the getting inforamtion of the web page\n",
    "def get_paperinfo(paper_url):\n",
    "\n",
    "  #download the page\n",
    "  response=requests.get(url,headers=headers)\n",
    "\n",
    "  # check successful response\n",
    "  if response.status_code != 200:\n",
    "    print('Status code:', response.status_code)\n",
    "    raise Exception('Failed to fetch web page ')\n",
    "\n",
    "  #parse using beautiful soup\n",
    "  paper_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "  return paper_doc\n",
    "\n",
    "# this function for the extracting information of the tags\n",
    "def get_tags(doc):\n",
    "  paper_tag = doc.select('[data-lid]')\n",
    "  cite_tag = doc.select('[title=Cite] + a')\n",
    "  link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
    "  author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "\n",
    "  return paper_tag,cite_tag,link_tag,author_tag\n",
    "\n",
    "# it will return the title of the paper\n",
    "def get_papertitle(paper_tag):\n",
    "  \n",
    "  paper_names = []\n",
    "  \n",
    "  for tag in paper_tag:\n",
    "    paper_names.append(tag.select('h3')[0].get_text())\n",
    "\n",
    "  return paper_names\n",
    "\n",
    "# it will return the number of citation of the paper\n",
    "def get_citecount(cite_tag):\n",
    "  cite_count = []\n",
    "  for i in cite_tag:\n",
    "    cite = i.text\n",
    "    if i is None or cite is None:  # if paper has no citatation then consider 0\n",
    "      cite_count.append(0)\n",
    "    else:\n",
    "      tmp = re.search(r'\\d+', cite) # its handle the None type object error and re use to remove the string \" cited by \" and return only integer value\n",
    "      if tmp is None :\n",
    "        cite_count.append(0)\n",
    "      else :\n",
    "        cite_count.append(int(tmp.group()))\n",
    "\n",
    "  return cite_count\n",
    "\n",
    "# function for the getting link information\n",
    "def get_link(link_tag):\n",
    "\n",
    "  links = []\n",
    "\n",
    "  for i in range(len(link_tag)) :\n",
    "    links.append(link_tag[i].a['href']) \n",
    "\n",
    "  return links \n",
    "\n",
    "# function for the getting autho , year and publication information\n",
    "def get_author_year_publi_info(authors_tag):\n",
    "  years = []\n",
    "  publication = []\n",
    "  authors = []\n",
    "  for i in range(len(authors_tag)):\n",
    "      authortag_text = (authors_tag[i].text).split()\n",
    "      year = int(re.search(r'\\d+', authors_tag[i].text).group())\n",
    "      years.append(year)\n",
    "      publication.append(authortag_text[-1])\n",
    "      author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])\n",
    "      authors.append(author)\n",
    "  \n",
    "  return years , publication, authors\n",
    "\n",
    "# creating final repository\n",
    "paper_repos_dict = {\n",
    "                    'Paper Title' : [],\n",
    "                    'Year' : [],\n",
    "                    'Author' : [],\n",
    "                    'Citation' : [],\n",
    "                    'Publication' : [],\n",
    "                    'Url of paper' : [] }\n",
    "\n",
    "# adding information in repository\n",
    "def add_in_paper_repo(papername,year,author,cite,publi,link):\n",
    "  paper_repos_dict['Paper Title'].extend(papername)\n",
    "  paper_repos_dict['Year'].extend(year)\n",
    "  paper_repos_dict['Author'].extend(author)\n",
    "  paper_repos_dict['Citation'].extend(cite)\n",
    "  paper_repos_dict['Publication'].extend(publi)\n",
    "  paper_repos_dict['Url of paper'].extend(link)\n",
    "\n",
    "  return pd.DataFrame(paper_repos_dict)\n",
    "\n",
    "for i in range (0,110,10):\n",
    "\n",
    "  # get url for the each page\n",
    "  url = \"https://scholar.google.com/scholar?start={}&q=object+detection+in+aerial+image+&hl=en&as_sdt=0,5\".format(i)\n",
    "\n",
    "  # function for the get content of each page\n",
    "  doc = get_paperinfo(url)\n",
    "\n",
    "  # function for the collecting tags\n",
    "  paper_tag,cite_tag,link_tag,author_tag = get_tags(doc)\n",
    "  \n",
    "  # paper title from each page\n",
    "  papername = get_papertitle(paper_tag)\n",
    "\n",
    "  # year , author , publication of the paper\n",
    "  year , publication , author = get_author_year_publi_info(author_tag)\n",
    "\n",
    "  # cite count of the paper \n",
    "  cite = get_citecount(cite_tag)\n",
    "\n",
    "  # url of the paper\n",
    "  link = get_link(link_tag)\n",
    "\n",
    "  # add in paper repo dict\n",
    "  final = add_in_paper_repo(papername,year,author,cite,publication,link)\n",
    "  \n",
    "  # use sleep to avoid status code 429\n",
    "  sleep(30)\n",
    "  \n",
    "final.to_csv('aerial_image_reserachpapers.csv', sep=',', index=False,header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
