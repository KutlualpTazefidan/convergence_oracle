{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fields_to_retrieve = [\n",
    "    'paperId', 'title', 'abstract', 'year', 'citationCount', 'url'\n",
    "]\n",
    "\n",
    "# Convert the list of fields to a comma-separated string\n",
    "fields_param = ','.join(fields_to_retrieve)\n",
    "\n",
    "# API URL with the specific fields\n",
    "api_url = f'http://api.semanticscholar.org/graph/v1/paper/search?query=law&offset=0&limit=100&fields={fields_param}'\n",
    "\n",
    "# Number of records to retrieve\n",
    "total_records = 500000  # You can adjust this number as needed\n",
    "\n",
    "# Calculate the number of iterations needed\n",
    "iterations = total_records // 100\n",
    "\n",
    "# Initialize a list to store the retrieved data\n",
    "all_data = []\n",
    "\n",
    "# Send requests with rate limiting\n",
    "for _ in range(iterations):\n",
    "    # Send an HTTP GET request to the API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract and append the information for each paper\n",
    "        all_data.extend(data['data'])\n",
    "\n",
    "        # Wait for 1 minutes (60 seconds) before the next iteration\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the Semantic Scholar API. Status code:\", response.status_code)\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_file_name = \"semantic_scholar_data4.csv\"\n",
    "\n",
    "# Define the CSV header\n",
    "csv_header = fields_to_retrieve\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_header)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for paper in all_data:\n",
    "        writer.writerow({\n",
    "            field: paper.get(field, '') for field in fields_to_retrieve\n",
    "        })\n",
    "\n",
    "print(\"Data has been saved to\", csv_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # Define the API URL\n",
    "# api_url = 'http://api.semanticscholar.org/graph/v1/paper/search'\n",
    "\n",
    "# # Send an HTTP GET request to the API\n",
    "# response = requests.get(api_url)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     # Parse the JSON response\n",
    "#     data = response.json()\n",
    "\n",
    "#     # Extract the first paper's fields\n",
    "#     first_paper = data['data'][0] if 'data' in data and len(data['data']) > 0 else None\n",
    "\n",
    "#     if first_paper:\n",
    "#         # Get the keys (field names) of the first paper\n",
    "#         field_names = first_paper.keys()\n",
    "\n",
    "#         # Print the field names\n",
    "#         print(\"Available Fields:\")\n",
    "#         for field in field_names:\n",
    "#             print(field)\n",
    "#     else:\n",
    "#         print(\"No data found in the API response.\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve data from the Semantic Scholar API. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('semantic_scholar_data.csv')\n",
    "df2 = pd.read_csv('semantic_scholar_data2.csv')\n",
    "df3 = pd.read_csv('semantic_scholar_data3.csv')\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Remove duplicates based on all columns\n",
    "#merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "dff=pd.read_csv('merged_data.csv')\n",
    "dff = dff.drop_duplicates(subset='abstract')\n",
    "\n",
    "print(\"Number of remaining rows after dropping duplicates:\", len(df))\n",
    "\n",
    "dff\n",
    "# Find and count duplicated rows\n",
    "duplicated_rows = merged_df[merged_df.duplicated()]\n",
    "num_duplicates = len(duplicated_rows)\n",
    "\n",
    "# Print the number of duplicated rows\n",
    "print(\"Number of duplicated rows:\", num_duplicates)\n",
    "\n",
    "# Print a sample of duplicated rows\n",
    "sample_of_duplicates = duplicated_rows.sample(n=10)  # Change 'n' to the number of samples you want to print\n",
    "print(\"Sample of duplicated rows:\")\n",
    "print(sample_of_duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
