{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elsevier API with generated key with university account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API request failed with status code 405\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "api_key = '02d0c4174287f162c45c7853a4ccd18e'\n",
    "\n",
    "\n",
    "# Define the API endpoint and query parameters\n",
    "api_url = 'https://api.elsevier.com/content/search/sciencedirect'\n",
    "query_params = {\n",
    "    # 'query': 'pub-date(2020-2022) AND random(1000)',\n",
    "    'apiKey': api_key,\n",
    "    'view': 'COMPLETE',\n",
    "    'content': 'metadata',\n",
    "    'start': 0,  # Start with the first result\n",
    "    'count': 1000,  # Retrieve 1000 results\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(api_url, params=query_params) \n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    publications = []\n",
    "    for entry in data['search-results']['entry']:\n",
    "        title = entry['dc:title']\n",
    "        year = entry['prism:coverDisplayDate']\n",
    "        abstract = entry['dc:description']\n",
    "        citations = entry.get('citedby-count', 'N/A')\n",
    "        publications.append([title, year, abstract, citations])\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    with open('publications.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write the header row\n",
    "        csv_writer.writerow(['Title', 'Year', 'Abstract', 'Citations'])\n",
    "        # Write the publication data\n",
    "        csv_writer.writerows(publications)\n",
    "\n",
    "    print(\"Data saved to 'publications.csv'\")\n",
    "else:\n",
    "    print(f\"API request failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work:\n",
    "\"The 405 error happens when a client attempts to use an HTTP method that is not allowed by the server. For example, this issue can occur when a user tries to use a method such as PUT or DELETE on a resource that only allows GET or POST requests.\"\n",
    "\n",
    "Not sure how to resolve this\n",
    "\n",
    "\n",
    "Also 'query': 'pub-date(2020-2022) AND random(1000) does not work (that's why I commented it out), it gives a 401 error:\n",
    "\"The HyperText Transfer Protocol (HTTP) 401 Unauthorized response status code indicates that the client request has not been completed because it lacks valid authentication credentials for the requested resource.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib3\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myKeyList = [\"7d2b7b28f29e2c6841427ef6dcb7aee0\",\n",
    "             \"76b4641054b6e47c3d7f2703fc64035f\",\n",
    "             ]\n",
    "\n",
    "class Citations:\n",
    "    def __init__(self, pii):\n",
    "        self.MY_API_KEY = myKeyList[0]\n",
    "        self.pii = pii\n",
    "        \n",
    "        self.attrs = {\n",
    "                    0:[]\n",
    "                    }\n",
    "        \n",
    "        sciDir = self.getSciDir(pii)\n",
    "        referList = self.getReferenceList(sciDir)\n",
    "        print (referList)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.attrs:\n",
    "            return self.attrs[key]\n",
    "        \n",
    "    def __setitem__(self, key, item):\n",
    "        if key in self.attrs:\n",
    "            self.attrs[key] = item\n",
    "            \n",
    "    def getSciDir(self, pii):\n",
    "        resource = \"http://api.elsevier.com/content/article/pii/\"\n",
    "        url = resource + pii\n",
    "        \n",
    "        webPage = requests.get(url)\n",
    "        soup = BeautifulSoup(webPage.content.decode('utf-8'), \"xml\")\n",
    "        \n",
    "        try:\n",
    "            sciDir = soup.find('link',{'rel':'scidir'}).get('href')\n",
    "        except:\n",
    "            sciDir = 'N/A'\n",
    "        return sciDir\n",
    "        \n",
    "    def getReferenceList(self, sciDir):\n",
    "        driver = webdriver.Firefox(executable_path=r'geckodriver.exe')\n",
    "        driver.get(sciDir)\n",
    "        driver.maximize_window()\n",
    "        \n",
    "        url = sciDir\n",
    "        literatureList = []\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'references')))\n",
    "            print (\"Page Ready\")\n",
    "        except(TimeoutException):\n",
    "            print (\"Fail to load\")\n",
    "        else:\n",
    "            soup = BeautifulSoup(driver.page_source.decode('utf-8'),\"lxml\")\n",
    "            driver.quit()\n",
    "            \n",
    "            references = soup.find('dl', {'class':'references'})\n",
    "            reference = references.findAll('dd', {'class':'reference'})\n",
    "            \n",
    "            \n",
    "            for refer in reference:\n",
    "#                 print refer\n",
    "                try:\n",
    "                    contribution = refer.find('div', {'class':'contribution'})\n",
    "                    authors = contribution.getText().encode('utf-8', 'ignore')\n",
    "                    try:\n",
    "                        title = contribution.find('strong', {'class':'title'}).getText().encode('utf-8', 'ignore')\n",
    "                    except:\n",
    "                        title = ''\n",
    "                    host = refer.find('div', {'class':'host'}).getText().encode('utf-8', 'ignore')                     \n",
    "                except:\n",
    "                    literature = refer.find('span').getText().encode('utf-8', 'ignore')\n",
    "                else:\n",
    "                    literature = authors + ', ' + title + ', ' + host\n",
    "                \n",
    "                print (literature)\n",
    "                literatureList.append(literature)\n",
    "        return literatureList\n",
    "\n",
    "        \n",
    "class Reference:\n",
    "    #Science Direct Search\n",
    "    def __init__(self, title):\n",
    "        MY_API_KEY = myKeyList[1]\n",
    "        self.title = title\n",
    "        resource = \"http://api.elsevier.com/content/search/scidir\"\n",
    "        \n",
    "        url = (resource\n",
    "          + \"?query=\"\n",
    "          + title)\n",
    "#           + \"&apiKey=\"\n",
    "#           + self.myKeyList[0])\n",
    "        \n",
    "        resp = requests.get(url,\n",
    "                    headers={'Accept':'application/json',\n",
    "                             'X-ELS-APIKey': MY_API_KEY})\n",
    "        \n",
    "        print (('Input title') + str(title))\n",
    "        \n",
    "        results = json.loads(resp.text.encode('utf-8'))\n",
    "        \n",
    "        self.attrs = {\n",
    "                      0: [None,  'identifier',               0],\n",
    "                      1: [None,  'url',                      1],\n",
    "                      2: [None,  'title',                    2],\n",
    "                      3: [None,  'publicationName',          3],\n",
    "                      4: [None,  'issueName',                4],\n",
    "                      5: [None,  'issn',                     5],\n",
    "                      6: [None,  'volume',                   6],\n",
    "                      7: [None,  'issueIdentifier',          7],\n",
    "                      8: [None,  'coverDisplayDate',         8],\n",
    "                      9: [None,  'startingPage',             9],\n",
    "                      10: [None,  'endingPage',              10],\n",
    "                      11: [None,  'doi',                     11],\n",
    "                      12: [None,  'pii',                     12],\n",
    "                      13: [None,  'eid',                     13],\n",
    "                      14: [None,  'authors',                 14]}\n",
    "            \n",
    "        refer = self.matchCheck(results)\n",
    "        \n",
    "        if refer != 'N/A':\n",
    "            self.setAttr(refer)\n",
    "                \n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if key in self.attrs:\n",
    "            return self.attrs[key][0]\n",
    "        \n",
    "    def __setitem__(self, key, item):\n",
    "        if key in self.attrs:\n",
    "            self.attrs[key][0] = item\n",
    "            \n",
    "    def matchCheck(self, results):\n",
    "        matchRefer = 'N/A'\n",
    "        try:\n",
    "            resultList = results['search-results']['entry']\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        else:\n",
    "            for refer in resultList:\n",
    "                similarity = self.similar(refer['dc:title'], self.title)\n",
    "                print ('Similarity score = ' + str(similarity) + '(/1.0)')\n",
    "                \n",
    "                if  similarity> 0.8:\n",
    "                    matchRefer = refer\n",
    "                    print (\"(matched to) \"+str(matchRefer['dc:title'].encode('utf-8', 'ignore')))\n",
    "                    break\n",
    "        if matchRefer == 'N/A':\n",
    "            print (\"No matched reference\")\n",
    "        return matchRefer\n",
    "    \n",
    "    \n",
    "    def similar(self, a, b):\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    \n",
    "    def setAuthors(self, refer):\n",
    "        authorsRaw = refer['authors']['author']\n",
    "        authors = ''\n",
    "        for indx, auth in enumerate(authorsRaw):\n",
    "            authors += (auth['given-name']+' '\n",
    "                        +auth['surname'])\n",
    "            if indx != len(authorsRaw)-1:\n",
    "                authors += ', '\n",
    "        return authors\n",
    "        \n",
    "    \n",
    "    def setAttr(self, refer):\n",
    "        attrList = ['dc:identifier', 'prism:url', 'dc:title', 'prism:publicationName', 'prism:issueName', 'prism:issn', 'prism:volume', 'prism:issueIdentifier', 'prism:coverDisplayDate', 'prism:startingPage', 'prism:endingPage', 'prism:doi', 'pii', 'eid']\n",
    "        \n",
    "        for indx, attr in enumerate(attrList):\n",
    "            try:\n",
    "                refer[attr]\n",
    "            except(KeyError):\n",
    "                pass\n",
    "            else:\n",
    "                self.attrs[indx][0] = str(refer[attr]).encode('utf-8', 'ignore')\n",
    "\n",
    "        try:\n",
    "            author = self.setAuthors(refer)\n",
    "        except(KeyError):\n",
    "            pass\n",
    "        else:\n",
    "            self.attrs[14][0] = str(author).encode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\49176\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class Citations:\n",
    "    def __init__(self, pii):\n",
    "        self.MY_API_KEY = myKeyList[0]\n",
    "        self.pii = pii\n",
    "        self.attrs = {0: []}\n",
    "        sciDir = self.getSciDir(pii)\n",
    "        referList = self.getReferenceList(sciDir)\n",
    "        print(referList)\n",
    "\n",
    "    # Rest of your Citations class code...\n",
    "\n",
    "class Reference:\n",
    "    # Science Direct Search\n",
    "    def __init__(self, title):\n",
    "        MY_API_KEY = myKeyList[1]\n",
    "        self.title = title\n",
    "        resource = \"http://api.elsevier.com/content/search/scidir\"\n",
    "\n",
    "        url = (resource + \"?query=\" + title)\n",
    "\n",
    "        resp = requests.get(url, headers={'Accept': 'application/json', 'X-ELS-APIKey': MY_API_KEY})\n",
    "\n",
    "        print(('Input title') + str(title))\n",
    "\n",
    "        results = json.loads(resp.text.encode('utf-8'))\n",
    "\n",
    "        self.attrs = {\n",
    "            0: [None, 'identifier', 0],\n",
    "            1: [None, 'url', 1],\n",
    "            2: [None, 'title', 2],\n",
    "            3: [None, 'publicationName', 3],\n",
    "            4: [None, 'issueName', 4],\n",
    "            5: [None, 'issn', 5],\n",
    "            6: [None, 'volume', 6],\n",
    "            7: [None, 'issueIdentifier', 7],\n",
    "            8: [None, 'coverDisplayDate', 8],\n",
    "            9: [None, 'startingPage', 9],\n",
    "            10: [None, 'endingPage', 10],\n",
    "            11: [None, 'doi', 11],\n",
    "            12: [None, 'pii', 12],\n",
    "            13: [None, 'eid', 13],\n",
    "            14: [None, 'authors', 14]\n",
    "        }\n",
    "\n",
    "        refer = self.matchCheck(results)\n",
    "\n",
    "        if refer != 'N/A':\n",
    "            self.setAttr(refer)\n",
    "\n",
    "    # Rest of your Reference class code...\n",
    "\n",
    "def get_arxiv_publications():\n",
    "    # Define the API endpoint and query parameters\n",
    "    api_url = 'http://export.arxiv.org/api/query'\n",
    "    query_params = {\n",
    "        'search_query': 'cat:cs.AI',\n",
    "        'start': 0,\n",
    "        'max_results': 100,  # Get 100 publications\n",
    "    }\n",
    "\n",
    "    # Initialize a list to store publication data\n",
    "    publications_2 = []\n",
    "\n",
    "    # Make API requests in batches\n",
    "    while query_params['start'] < query_params['max_results']:\n",
    "        # Make the API request\n",
    "        response = requests.get(api_url, params=query_params)\n",
    "\n",
    "        # Check the response status\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML response using BeautifulSoup with 'html.parser'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract publication information\n",
    "            for entry in soup.find_all('entry'):\n",
    "                title = entry.find('title').text.strip()\n",
    "                abstract = entry.find('summary').text.strip()\n",
    "\n",
    "                # Check if 'arxiv:comment' element exists before accessing 'text'\n",
    "                arxiv_comment = entry.find('arxiv:comment')\n",
    "                if arxiv_comment:\n",
    "                    citations = arxiv_comment.text.strip()\n",
    "                else:\n",
    "                    citations = 'N/A'\n",
    "\n",
    "                # Append publication data to the list\n",
    "                publications_2.append([title, abstract, citations])\n",
    "\n",
    "            # Update the 'start' parameter for the next batch\n",
    "            query_params['start'] += len(soup.find_all('entry'))\n",
    "        else:\n",
    "            print(f\"API request failed with status code {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return publications_2\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # Specify the CSV file name\n",
    "    csv_filename = 'publications_2.csv'\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write the header row\n",
    "        csv_writer.writerow(['Title', 'Abstract', 'Citations'])\n",
    "        # Write the publication data\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "    print(f\"Data saved to '{csv_filename}'\")\n",
    "\n",
    "def main():\n",
    "    publications_2 = get_arxiv_publications()\n",
    "    save_to_csv(publications_2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
