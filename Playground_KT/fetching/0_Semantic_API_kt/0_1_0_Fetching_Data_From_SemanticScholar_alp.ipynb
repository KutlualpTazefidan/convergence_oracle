{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar API\n",
    "100 requests per 5 minutes\n",
    "The API allows up to 100 requests per 5 minutes. \n",
    "\n",
    "To access a higher rate limit, complete the form to request authentication for your project.\n",
    "\n",
    "Max limit for each request is 100, so every 5 min we can gather (100*100)* amount of people\n",
    "\n",
    "Issue/Drawback of semantic scholar: it needs a search term, meaning, the topics/industry/niche needs to be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for retrieving data\n",
    "amount_of_data = 100  # Total amount of data to retrieve\n",
    "start_date = 2019  # Year of publication\n",
    "end_date = 2023  # Year of publication\n",
    "journal = [\"Nat. Chem. Biol\"]\n",
    "# research_field = ['Computer Science'] # Research field(s) to query (as a list)\n",
    "alphabet_letters = ['A','E','I','U','O'] #  list(string.ascii_uppercase) # Research field(s) to query (as a list)\n",
    "initial_offset = 0  # Initial start offset\n",
    "number_of_publications_per_request = 100 # Dont put more than a 100\n",
    "\n",
    "# Gentle fetching\n",
    "sleep_duration = 1  # Sleep duration in seconds between requests\n",
    "retry_sleep_duration = 1  # Sleep duration in seconds between requests\n",
    "max_retries = 100  # Maximum number of retries for failed requests\n",
    "\n",
    "# Define the API endpoint\n",
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "length_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving single publication for debugging purposes\n",
    "params = {\n",
    "    'query': \"A\",  # Use the first research field from the list\n",
    "    'venue': journal[0],\n",
    "    'year': 2013,\n",
    "    'limit': 1,  # Number of results per request\n",
    "    'offset': 0,  # Offset for pagination\n",
    "    # 'fields': 'externalIds,title,abstract,citationCount,year',\n",
    "    'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,publicationVenue,year,tldr,embedding.specter_v2,citations,references',\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(api_url, params=params,headers={'X-API-KEY':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.text)\n",
    "else:\n",
    "    print(f\"Error: Request failed with status code {response.status_code}\")\n",
    "    print(f\"Error message: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>externalIds_MAG</th>\n",
       "      <th>externalIds_DOI</th>\n",
       "      <th>externalIds_CorpusId</th>\n",
       "      <th>externalIds_PubMed</th>\n",
       "      <th>publicationVenue_id</th>\n",
       "      <th>publicationVenue_name</th>\n",
       "      <th>publicationVenue_type</th>\n",
       "      <th>publicationVenue_alternate_names</th>\n",
       "      <th>publicationVenue_issn</th>\n",
       "      <th>...</th>\n",
       "      <th>s2FieldsOfStudy</th>\n",
       "      <th>tldr_model</th>\n",
       "      <th>tldr_text</th>\n",
       "      <th>publicationTypes</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>embedding_vector</th>\n",
       "      <th>authors</th>\n",
       "      <th>citations</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08e50d1cc59a1d9c1a0d21dfe9667e243aa1094d</td>\n",
       "      <td>1984615306</td>\n",
       "      <td>10.1016/S0140-6736(13)61611-6</td>\n",
       "      <td>41940846</td>\n",
       "      <td>23993280</td>\n",
       "      <td>ba627f0e-abf9-4f01-8146-73118f4ebaf9</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>journal</td>\n",
       "      <td>[Lancet]</td>\n",
       "      <td>0923-7577</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'category': 'Medicine', 'source': 'external'...</td>\n",
       "      <td>tldr@v2.0.0</td>\n",
       "      <td>The findings show the striking and growing cha...</td>\n",
       "      <td>[Review, JournalArticle]</td>\n",
       "      <td>2013-11-09</td>\n",
       "      <td>specter_v2</td>\n",
       "      <td>[0.24848510324954987, 0.40732088685035706, -0....</td>\n",
       "      <td>[{'authorId': '5260255', 'name': 'H. Whiteford...</td>\n",
       "      <td>[{'paperId': '53840ae196ecf4d46c5f02ed535beafe...</td>\n",
       "      <td>[{'paperId': '00f63e8997831bbe70cd2bb949c44dbd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId externalIds_MAG   \n",
       "0  08e50d1cc59a1d9c1a0d21dfe9667e243aa1094d      1984615306  \\\n",
       "\n",
       "                 externalIds_DOI  externalIds_CorpusId externalIds_PubMed   \n",
       "0  10.1016/S0140-6736(13)61611-6              41940846           23993280  \\\n",
       "\n",
       "                    publicationVenue_id publicationVenue_name   \n",
       "0  ba627f0e-abf9-4f01-8146-73118f4ebaf9            The Lancet  \\\n",
       "\n",
       "  publicationVenue_type publicationVenue_alternate_names   \n",
       "0               journal                         [Lancet]  \\\n",
       "\n",
       "  publicationVenue_issn  ...   \n",
       "0             0923-7577  ...  \\\n",
       "\n",
       "                                     s2FieldsOfStudy   tldr_model   \n",
       "0  [{'category': 'Medicine', 'source': 'external'...  tldr@v2.0.0  \\\n",
       "\n",
       "                                           tldr_text   \n",
       "0  The findings show the striking and growing cha...  \\\n",
       "\n",
       "           publicationTypes publicationDate  embedding_model   \n",
       "0  [Review, JournalArticle]      2013-11-09       specter_v2  \\\n",
       "\n",
       "                                    embedding_vector   \n",
       "0  [0.24848510324954987, 0.40732088685035706, -0....  \\\n",
       "\n",
       "                                             authors   \n",
       "0  [{'authorId': '5260255', 'name': 'H. Whiteford...  \\\n",
       "\n",
       "                                           citations   \n",
       "0  [{'paperId': '53840ae196ecf4d46c5f02ed535beafe...  \\\n",
       "\n",
       "                                          references  \n",
       "0  [{'paperId': '00f63e8997831bbe70cd2bb949c44dbd...  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten_dict(d, parent_key='', separator='_'):\n",
    "    flattened = {}\n",
    "    for key, value in d.items():\n",
    "        new_key = parent_key + separator + key if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                sub_new_key = new_key + separator + sub_key\n",
    "                flattened[sub_new_key] = sub_value\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "flattened_data = []  # A list to store flattened dictionaries\n",
    "original_data = pd.DataFrame(data)  # Original DataFrame\n",
    "num_records = len(data['data'])  # Number of records in the 'data' list\n",
    "\n",
    "# Loop through each record in 'data' and flatten it\n",
    "for i in range(num_records):\n",
    "    flattened_data.append(flatten_dict(data['data'][i]))\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "flattened_df = pd.DataFrame(flattened_data)\n",
    "flattened_df.to_csv(\"./temp_data/all_features.csv\",index=False)\n",
    "# Display the first few rows of the flattened DataFrame\n",
    "display(flattened_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2023\n",
      "For the letter:  A  batches:  [100, 100, 50]\n",
      "Letter:  A  Batch:  100  Iteration:  0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 89\u001b[0m\n\u001b[0;32m     83\u001b[0m length_df \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df)  \u001b[39m# Update the total length of unfiltered data\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m# Save each iteration into a separate file - to keep some of the data in case of an error\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39m# The filename format includes the research field, offset, and is converted to lowercase\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[39m# The saving is maybe not necc.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(year) \u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49m journal[\u001b[39m0\u001b[39;49m]\u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m alphabet_letters[k]\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mlower() \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(offset) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Sleep for a specified duration between requests\u001b[39;00m\n\u001b[0;32m     92\u001b[0m time\u001b[39m.\u001b[39msleep(sleep_duration)\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\erick\\gitrepositories\\convergence_oracle\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '..\\data'"
     ]
    }
   ],
   "source": [
    "# Fetching Data\n",
    "year_list = list(range(start_date, end_date + 1))\n",
    "retries_retry_exhausted = False\n",
    "year_list.reverse()\n",
    "for j, year in enumerate(year_list):\n",
    "    print(\"Year\",year)\n",
    "    for k,letter in enumerate(alphabet_letters):\n",
    "        # First request to get the number of total papers to limit the loop\n",
    "        params = {\n",
    "            'query': alphabet_letters[k],  # Use the first research field from the list\n",
    "            'venue': journal[0],\n",
    "            'year': year,\n",
    "            'limit': 1,  # Number of results per request\n",
    "            'offset': 0,  # Offset for pagination\n",
    "            'fields': 'title,authors,abstract,citationCount,year',\n",
    "            # 'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,citations,references,publicationVenue,year',\n",
    "\n",
    "        }\n",
    "\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, params=params,headers={'x-api-key':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "        # response = requests.get(api_url, params=params)\n",
    "        # sleep time is one second because you can request every second based on the Semantic Scholar Regulations\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "        # calculate the batches required to get every publication for that topic\n",
    "        num_batches, remainder = divmod(data['total'], number_of_publications_per_request)\n",
    "        batches = [number_of_publications_per_request] * num_batches\n",
    "        if remainder > 0:\n",
    "            batches.append(remainder)\n",
    "            \n",
    "        print(\"For the letter: \", letter ,\" batches: \",batches)\n",
    "\n",
    "        for i,batch in enumerate(batches):\n",
    "            offset = initial_offset + number_of_publications_per_request * i  # Calculate the current offset\n",
    "            retries = 0\n",
    "\n",
    "            print(\"Letter: \", alphabet_letters[k],\" Batch: \",str(batches[i]), \" Iteration: \", i)\n",
    "            while retries < max_retries:\n",
    "                # Define query parameters\n",
    "                params = {\n",
    "                    'query': alphabet_letters[k],  # Use the first research field from the list\n",
    "                    'venue': journal[0],\n",
    "                    'year': year,\n",
    "                    'limit': batch,  # Number of results per request\n",
    "                    'offset': offset,  # Offset for pagination\n",
    "                    # 'fields': 'externalIds,title,authors,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,citations,references,publicationVenue,year',\n",
    "                    'fields': 'externalIds,title,abstract,citationCount,referenceCount,influentialCitationCount,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,publicationVenue,year',\n",
    "                }\n",
    "\n",
    "                # Make the GET request\n",
    "                response = requests.get(api_url, params=params,headers={'x-api-key':os.getenv(\"SEMANTICSCHOLAR_API_KEY\")})\n",
    "                # response = requests.get(api_url, params=params)\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    data = json.loads(response.text)\n",
    "\n",
    "                    # Create a DataFrame for unfiltered results\n",
    "                    df = pd.DataFrame(data['data'])\n",
    "\n",
    "                    break  # Successful request, exit the retry loop\n",
    "                else:\n",
    "                    print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "                    retries += 1\n",
    "                    if retries < max_retries:\n",
    "                        print(\"Retrying after sleep: \"+str(retry_sleep_duration*1)+\"min\")\n",
    "                        time.sleep(retry_sleep_duration*1)\n",
    "                    if retries == max_retries:\n",
    "                        print(\"Fetch Failed\")\n",
    "                        retries_retry_exhausted = True\n",
    "                        break\n",
    "                if retries_retry_exhausted == True:\n",
    "                    print(\"Fetch Failed\")\n",
    "                    break\n",
    "            if retries_retry_exhausted == True:\n",
    "                print(\"Fetch Failed\")\n",
    "                break\n",
    "            # print(\"total amount\",response.json())\n",
    "            length_df += len(df)  # Update the total length of unfiltered data\n",
    "\n",
    "            # Save each iteration into a separate file - to keep some of the data in case of an error\n",
    "            # The filename format includes the research field, offset, and is converted to lowercase\n",
    "            \n",
    "            # The saving is maybe not necc.\n",
    "            df.to_csv('../data/'+ str(year) +\"_\"+ journal[0]+ \"_\" + alphabet_letters[k].replace(\" \", \"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "\n",
    "            # Sleep for a specified duration between requests\n",
    "            time.sleep(sleep_duration)\n",
    "    if retries_retry_exhausted == True:\n",
    "        print(\"Fetch Failed\")\n",
    "        break\n",
    "    # Create an empty DataFrame to store the merged data\n",
    "    merged_data = pd.DataFrame()\n",
    "    # Iterate through CSV files in the directory\n",
    "    for filename in os.listdir(\"../data/\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Extract the year from the filename (assuming the filename follows the \"year_\" format)\n",
    "            file_year = int(filename.split(\"_\")[0])\n",
    "\n",
    "            if file_year == year:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                temp_df = pd.read_csv(\"../data/\"+ filename)\n",
    "                merged_data = pd.concat([merged_data, temp_df], ignore_index=True)\n",
    "                # Remove the old CSV file\n",
    "                os.remove(\"../data/\"+ filename)\n",
    "    # Calculate the number of rows dropped\n",
    "    dropped_count = len(merged_data) - len(merged_data.drop_duplicates(subset='paperId', keep='first'))\n",
    "\n",
    "    # Remove rows with duplicate 'externalIdsIDs' values\n",
    "    merged_data = merged_data.drop_duplicates(subset='paperId', keep='first')\n",
    "\n",
    "    # Calculate the number of rows remaining\n",
    "    remaining_count = len(merged_data)\n",
    "\n",
    "    # Print the counts\n",
    "    print(f\"Removed {dropped_count} rows. Rows remaining: {remaining_count}\")\n",
    "    merged_data.to_csv('../data/'+ str(year) +\"_\"+ journal[0]+\"_\"+str(remaining_count)+'no_cit_no_ref_no_auth.csv', index=False)\n",
    "    time.sleep(sleep_duration)\n",
    " \n",
    "\n",
    "\n",
    "# Print the total length of unfiltered data after all iterations\n",
    "print(\"Length of the data\", length_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Query - Using the Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "# https://api.semanticscholar.org/graph/v1/paper/search?query=computer+science&year=2015&fields=title,year,authors,citationCount&limit=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of the code is not required but it provides example how to filter the data before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetching and filtering at the same time if required\n",
    "# # Fetching Data\n",
    "# length_filtered_df = 0\n",
    "# length_unfiltered_df = 0\n",
    "\n",
    "# for i in range(int(amount_of_data/100)):\n",
    "#     offset = initial_offset + 100 * i\n",
    "#     retries = 0\n",
    "    \n",
    "#     while retries < max_retries:\n",
    "#         # Define query parameters\n",
    "#         params = {\n",
    "#             'query': research_field,\n",
    "#             'year': year_of_publication,\n",
    "#             'limit': 100,\n",
    "#             'offset': offset,\n",
    "#             'fields': 'title,authors,abstract,citationCount,year',\n",
    "#         }\n",
    "\n",
    "#         # Make the GET request\n",
    "#         response = requests.get(api_url, params=params)\n",
    "\n",
    "#         # Check if the request was successful\n",
    "#         if response.status_code == 200:\n",
    "#             data = json.loads(response.text)\n",
    "\n",
    "#             # Create a DataFrame for unfiltered results\n",
    "#             df_unfiltered = pd.DataFrame(data['data'])\n",
    "\n",
    "#             # Filter the results based on 'citationCount' > 10\n",
    "#             filtered_results = [paper for paper in data['data'] if paper.get('citationCount', 0) > 10]\n",
    "\n",
    "#             # Create a DataFrame for filtered results\n",
    "#             df_filtered = pd.DataFrame(filtered_results)\n",
    "\n",
    "\n",
    "#             break  # Successful request, exit the retry loop\n",
    "#         else:\n",
    "#             print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "#             retries += 1\n",
    "#             if retries < max_retries:\n",
    "#                 print(\"Retrying after sleep...\")\n",
    "#                 time.sleep(sleep_duration)\n",
    "    \n",
    "#     # length_filtered_df = len(df_filtered)\n",
    "#     length_unfiltered_df += len(df_unfiltered)\n",
    "\n",
    "#     # Save each iteration into separate file - to keep some of the data in case of an error\n",
    "#     df_unfiltered.to_csv('./data/'+research_field.replace(\" \",\"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "#     df_filtered.to_csv('./data/df_filtered' + str(offset) + '.csv', index=False)\n",
    "\n",
    "#     # Sleep for a specified duration between requests\n",
    "#     print(\"Research field\",research_field,\"Iteration: \", i)\n",
    "#     time.sleep(sleep_duration)\n",
    "# print(\"Length of the filtered data\",length_unfiltered_df)\n",
    "# print(\"Length of the unfiltered data\",length_filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
