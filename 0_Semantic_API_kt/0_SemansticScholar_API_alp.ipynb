{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar API\n",
    "100 requests per 5 minutes\n",
    "The API allows up to 100 requests per 5 minutes. \n",
    "\n",
    "To access a higher rate limit, complete the form to request authentication for your project.\n",
    "\n",
    "Max limit for each request is 100, so every 5 min we can gather (100*100)* amount of people\n",
    "\n",
    "Issue/Drawback of semantic scholar: it needs a search term, meaning, the topics/industry/niche needs to be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for retrieving data\n",
    "amount_of_data = 1000  # Total amount of data to retrieve\n",
    "year_of_publication = 2015  # Year of publication\n",
    "research_field = ['computer science']  # Research field(s) to query (as a list)\n",
    "initial_offset = 0  # Initial start offset\n",
    "\n",
    "# Gentle fetching\n",
    "sleep_duration = 30  # Sleep duration in seconds between requests\n",
    "max_retries = 3  # Maximum number of retries for failed requests\n",
    "\n",
    "# Define the API endpoint\n",
    "api_url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "length_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Data\n",
    "\n",
    "for i in range(int(amount_of_data / 100)):\n",
    "    offset = initial_offset + 100 * i  # Calculate the current offset\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        # Define query parameters\n",
    "        params = {\n",
    "            'query': research_field[0],  # Use the first research field from the list\n",
    "            'year': year_of_publication,\n",
    "            'limit': 100,  # Number of results per request\n",
    "            'offset': offset,  # Offset for pagination\n",
    "            'fields': 'title,authors,abstract,citationCount,year',\n",
    "        }\n",
    "\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "            # Create a DataFrame for unfiltered results\n",
    "            df = pd.DataFrame(data['data'])\n",
    "\n",
    "            break  # Successful request, exit the retry loop\n",
    "        else:\n",
    "            print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(\"Retrying after sleep...\")\n",
    "                time.sleep(sleep_duration)\n",
    "\n",
    "    length_df += len(df)  # Update the total length of unfiltered data\n",
    "\n",
    "    # Save each iteration into a separate file - to keep some of the data in case of an error\n",
    "    # The filename format includes the research field, offset, and is converted to lowercase\n",
    "    df.to_csv('./data/' + research_field[0].replace(\" \", \"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "\n",
    "    # Sleep for a specified duration between requests\n",
    "    print(\"Research field\", research_field[0], \"Iteration: \", i)\n",
    "    time.sleep(sleep_duration)\n",
    "\n",
    "# Print the total length of unfiltered data after all iterations\n",
    "print(\"Length of the unfiltered data\", length_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of the code is not required but it provides example how to filter the data before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetching and filtering at the same time if required\n",
    "# # Fetching Data\n",
    "# length_filtered_df = 0\n",
    "# length_unfiltered_df = 0\n",
    "\n",
    "# for i in range(int(amount_of_data/100)):\n",
    "#     offset = initial_offset + 100 * i\n",
    "#     retries = 0\n",
    "    \n",
    "#     while retries < max_retries:\n",
    "#         # Define query parameters\n",
    "#         params = {\n",
    "#             'query': research_field,\n",
    "#             'year': year_of_publication,\n",
    "#             'limit': 100,\n",
    "#             'offset': offset,\n",
    "#             'fields': 'title,authors,abstract,citationCount,year',\n",
    "#         }\n",
    "\n",
    "#         # Make the GET request\n",
    "#         response = requests.get(api_url, params=params)\n",
    "\n",
    "#         # Check if the request was successful\n",
    "#         if response.status_code == 200:\n",
    "#             data = json.loads(response.text)\n",
    "\n",
    "#             # Create a DataFrame for unfiltered results\n",
    "#             df_unfiltered = pd.DataFrame(data['data'])\n",
    "\n",
    "#             # Filter the results based on 'citationCount' > 10\n",
    "#             filtered_results = [paper for paper in data['data'] if paper.get('citationCount', 0) > 10]\n",
    "\n",
    "#             # Create a DataFrame for filtered results\n",
    "#             df_filtered = pd.DataFrame(filtered_results)\n",
    "\n",
    "\n",
    "#             break  # Successful request, exit the retry loop\n",
    "#         else:\n",
    "#             print(f\"Error (Attempt {retries + 1}):\", response.status_code)\n",
    "#             retries += 1\n",
    "#             if retries < max_retries:\n",
    "#                 print(\"Retrying after sleep...\")\n",
    "#                 time.sleep(sleep_duration)\n",
    "    \n",
    "#     # length_filtered_df = len(df_filtered)\n",
    "#     length_unfiltered_df += len(df_unfiltered)\n",
    "\n",
    "#     # Save each iteration into separate file - to keep some of the data in case of an error\n",
    "#     df_unfiltered.to_csv('./data/'+research_field.replace(\" \",\"_\").lower() + \"_\" + str(offset) + '.csv', index=False)\n",
    "#     df_filtered.to_csv('./data/df_filtered' + str(offset) + '.csv', index=False)\n",
    "\n",
    "#     # Sleep for a specified duration between requests\n",
    "#     print(\"Research field\",research_field,\"Iteration: \", i)\n",
    "#     time.sleep(sleep_duration)\n",
    "# print(\"Length of the filtered data\",length_unfiltered_df)\n",
    "# print(\"Length of the unfiltered data\",length_filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
