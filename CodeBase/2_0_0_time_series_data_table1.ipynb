{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/7: Loading the dataset ...\n"
     ]
    }
   ],
   "source": [
    "## PULL Functions from custom functions\n",
    "from custom_functions import bertopic_addons as cfc\n",
    "\n",
    "## you will need to change the name below\n",
    "currenttable = 'topified_vectorized_Science1900_2023'\n",
    "aws_dfs = cfc.pull_aws_sql_database(currenttable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "from custom_functions import time_series_addons as tsa\n",
    "tsa.final_tableau_table(final_df= aws_dfs, final_forecasting = 5, final_moving_ave = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_or_moving_average(infun_df, cumlative):\n",
    "    # convert target topic to a moving average\n",
    "    list_of_topic_codes = list(infun_df.columns.drop('date'))\n",
    "    if cumlative == True:\n",
    "        ## now predicting the moving average\n",
    "        infun_df[list_of_topic_codes] = infun_df[list_of_topic_codes].cumsum()\n",
    "    else:    \n",
    "        ## now predicting the moving average\n",
    "        infun_df[list_of_topic_codes] = infun_df[list_of_topic_codes].rolling(moving_average_years).mean()\n",
    "    return infun_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_pivot_dataset(infun_df, infun_type_of_metric, infun_timeframe):\n",
    "    \n",
    "## you can change this to all topics \n",
    "    aws1_dfs= infun_df\n",
    "    aws1_dfs['topic_code'] = pd.to_numeric(aws1_dfs['topic_code'], downcast='integer')\n",
    "    aws1_dfs[infun_type_of_metric] = pd.to_numeric(aws1_dfs[infun_type_of_metric], downcast='integer')\n",
    "    aws1_dfs = aws1_dfs[aws1_dfs['publicationDate'] != 'None'].reset_index()\n",
    "    aws1_dfs['publicationDate'] = pd.to_datetime(aws1_dfs['publicationDate'], format='%Y-%m-%d')\n",
    "    aws1_dfs['date'] = aws1_dfs['publicationDate'].dt.to_period(infun_timeframe)\n",
    "    aws1_dfs['date']=aws1_dfs['date'].astype(str)\n",
    "    aws1_dfs['date']=pd.to_datetime(aws1_dfs['date'])\n",
    "    # ==== LEGACY NO NEED :)) aws1_dfs= aws1_dfs[aws1_dfs['topic_code']!=-1]\n",
    "\n",
    "    ## code to change the date time, for the moment use year\n",
    "    grouped_df = aws1_dfs.groupby(['topic_code', 'date'])[infun_type_of_metric].sum().reset_index()\n",
    "\n",
    "    df = grouped_df.pivot(index='date', columns='topic_code', values=infun_type_of_metric)\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bertopic_data_into_future(dataframe, topic_number, future_years, moving_average, type_of_metric, timeframe, cumlative):\n",
    "    \n",
    "    ##### ============================ CLEANING UP DATAFRAME \n",
    "    # ## number of years into the future you can forecast\n",
    "    if timeframe == 'M':\n",
    "        n = future_years*12\n",
    "        moving_average = moving_average*12\n",
    "    else:\n",
    "        n = future_years\n",
    "        \n",
    "    # Define the target column (Topic you want to forecast)\n",
    "    target_topic = topic_number  # Change this to the Topic you want to forecast\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## pivot df and clean the dataset\n",
    "    pivot_df = clean_and_pivot_dataset(dataframe, type_of_metric, timeframe)\n",
    "    # convert target topic to a moving average or cumalative \n",
    "    pivot_df = cumsum_or_moving_average(pivot_df, cumlative=cumlative)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ================================  forecasting rearrangement\n",
    "\n",
    "    # Dropping last n rows using drop\n",
    "    target_column = pivot_df[target_topic]\n",
    "    \n",
    "    # forget about the lost values\n",
    "    ## dropped the oldest columns\n",
    "    target_column.drop(target_column.head(n).index, inplace = True)\n",
    "\n",
    "    ## pivot_df['year'] + pd.offsets.DateOffset(years=5)\n",
    "    untarget_columns = pivot_df.drop(target_topic, axis=1)\n",
    "\n",
    "    ## keep this for later -- these are your forecasting columns \n",
    "    X_forecasting_data = untarget_columns.tail(n)\n",
    "    X_forecasting_years = untarget_columns['date'].tail(n) + pd.offsets.DateOffset(years=future_years)\n",
    "    X_forecasting_data['date'] = X_forecasting_years\n",
    "    X_forecasting = X_forecasting_data.drop(columns=['date'])\n",
    "\n",
    "    ##  drop the columns you keep for forecasting\n",
    "    untarget_columns.drop(target_column.tail(n).index, inplace = True)\n",
    "\n",
    "    df = pd.concat([untarget_columns, target_column.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    ## shift the prediction of each year\n",
    "    df['date'] = df['date'] + pd.offsets.DateOffset(years=future_years)\n",
    "\n",
    "    pivot_df = df    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ================== TEST TRAIN SPLIT \n",
    "    # Separating the training set and testing set\n",
    "    train_data=pivot_df[pivot_df['date'].dt.year<2011].reset_index(drop = True)\n",
    "    test_data=pivot_df[pivot_df['date'].dt.year>2010].reset_index(drop = True)\n",
    "\n",
    "    # Prepare the training and testing data\n",
    "    X_train = train_data.drop(target_topic, axis=1)\n",
    "    X_test = test_data.drop(target_topic, axis=1)\n",
    "\n",
    "    # Shift the target column to align with next year's frequency\n",
    "    y_train = train_data[target_topic].shift(-1).dropna()\n",
    "    y_test = test_data[target_topic].shift(-1).dropna()\n",
    "\n",
    "    # Exclude the 'YearMonth' column from the training and testing data\n",
    "    X_train = train_data.drop(columns=['date', target_topic]).iloc[:-1]\n",
    "    X_test = test_data.drop(columns=['date', target_topic]).iloc[:-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ======================= MODEL \n",
    "\n",
    "    # Create an XGBoost regressor\n",
    "    if cumlative == True: \n",
    "        model = Lasso()\n",
    "    else:    \n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=3)\n",
    "    \n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_forecasting = model.predict(X_forecasting)\n",
    "\n",
    "    # Create a DataFrame with YearMonth and the predicted values\n",
    "    y_pred_df = pd.DataFrame({'date': test_data['date'].iloc[:-1], 'Predicted': y_pred})\n",
    "    y_fore_df = pd.DataFrame({'date': X_forecasting_years, 'Forecasted': y_forecasting})\n",
    "\n",
    "    # Merge the predicted DataFrame with the original test_data DataFrame\n",
    "    merged_data = pd.merge(y_pred_df, test_data, on='date')\n",
    "    merged_forecasted_data = pd.merge(y_fore_df, X_forecasting_data, on='date')\n",
    "\n",
    "    #print(merged_forecasted_data)\n",
    "    # Calculate the root mean squared error (RMSE)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    #print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## =============================   BUILDING DATA TABLE \n",
    "    \n",
    "    y_actual_test_df = test_data[['date',target_topic]]\n",
    "    y_actual_train_df = train_data[['date',target_topic]]\n",
    "    y_fore_df =y_fore_df.rename(columns={'Forecasted': target_topic})\n",
    "    y_pred_df =y_pred_df.rename(columns={'Predicted': target_topic})\n",
    "\n",
    "    y_actual_test_df['value'] = 'Actual_Test'\n",
    "    y_actual_train_df['value'] = 'Actual_Train'\n",
    "    y_fore_df['value'] = 'Forecasted'\n",
    "    y_pred_df['value'] = 'Predicted_Test'\n",
    "\n",
    "    final_data = pd.concat([y_actual_test_df,y_actual_train_df,y_fore_df,y_pred_df], axis=0)\n",
    "    final_data =final_data.rename(columns={target_topic: 'quantity'})\n",
    "    final_data['topic_code'] = target_topic\n",
    "    final_data['RMSE'] = rmse\n",
    "    final_data['metric'] = type_of_metric\n",
    "    \n",
    "    \n",
    "    if cumlative == True: \n",
    "        final_data['cumsum_or_moving_average'] = 'cumsum'\n",
    "    else: \n",
    "        final_data['cumsum_or_moving_average'] = 'moving_average'\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### add moving average or cum sum\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"# Plot the predicted vs. actual values along with y_train\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(merged_data['year'], merged_data['Predicted'], label='Predicted', marker='x')\n",
    "    plt.plot(merged_data['year'], merged_data[target_topic], label='Actual', marker='o')\n",
    "    plt.plot(train_data['year'].iloc[:-1], y_train, label='Train', linestyle='--', color='gray')\n",
    "    plt.plot(merged_forecasted_data['year'], merged_forecasted_data['Forecasted'], label='Forecasted', marker='+')\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(type_of_metric)\n",
    "    plt.title(f'Topic {target_topic} Citation Forecast (RMSE: {rmse:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Feature importance plot (optional)\n",
    "    plot_importance(model)\n",
    "    plt.show()\"\"\"\n",
    "    \n",
    "    return final_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## create an ordered list of topic codes \\ntopic_code_list = list(pd.Series(dataframe['topic_code'].sort_values()).unique())\\n\\n## create a dataframe to concat later \\ngrowing_df = pd.DataFrame()\\n\\n## for loop through the topic code list \\nfor metric in ['count', 'citationCount', 'influentialCitationCount']:\\n    for topic_num in topic_code_list:\\n        current_df = plot_bertopic_data_into_future(dataframe, topic_num, 5, 4, metric, timeframe, cumlative=False)\\n        growing_df = pd.concat([current_df, growing_df]) \\n    \\n## reset the index and drop the inxiex column \\ngrowing_df = growing_df.reset_index().drop('index',axis=1)\\n\\n## match the topic code to the topic description \\ntopic_codes = dataframe[['topic_code','topic_list']].drop_duplicates().reset_index().drop('index',axis=1)\\n\\n## merge the topics descriptions to the data set\\nfull_datase= growing_df.merge(topic_codes, on='topic_code')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## create an ordered list of topic codes \n",
    "topic_code_list = list(pd.Series(dataframe['topic_code'].sort_values()).unique())\n",
    "\n",
    "## create a dataframe to concat later \n",
    "growing_df = pd.DataFrame()\n",
    "\n",
    "## for loop through the topic code list \n",
    "for metric in ['count', 'citationCount', 'influentialCitationCount']:\n",
    "    for topic_num in topic_code_list:\n",
    "        current_df = plot_bertopic_data_into_future(dataframe, topic_num, 5, 4, metric, timeframe, cumlative=False)\n",
    "        growing_df = pd.concat([current_df, growing_df]) \n",
    "    \n",
    "## reset the index and drop the inxiex column \n",
    "growing_df = growing_df.reset_index().drop('index',axis=1)\n",
    "\n",
    "## match the topic code to the topic description \n",
    "topic_codes = dataframe[['topic_code','topic_list']].drop_duplicates().reset_index().drop('index',axis=1)\n",
    "\n",
    "## merge the topics descriptions to the data set\n",
    "full_datase= growing_df.merge(topic_codes, on='topic_code')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableau_tables(tab_df, tab_forecasting, tab_moving_ave, tab_cumsum):\n",
    "    ## variables\n",
    "    dataframe = tab_df\n",
    "    forecasting_years = tab_forecasting\n",
    "    moving_average_years = tab_moving_ave\n",
    "    cumlative_boolean = tab_cumsum\n",
    "    \n",
    "    ## True goes with Monthly forecasting\n",
    "    if cumlative_boolean == True: \n",
    "        timeframe = 'M'\n",
    "    else: \n",
    "        timeframe = 'Y'\n",
    "    \n",
    "    ## \n",
    "    dataframe['count'] = 1\n",
    "\n",
    "    ## create an ordered list of topic codes \n",
    "    topic_code_list = list(pd.Series(dataframe['topic_code'].sort_values()).unique())\n",
    "\n",
    "    ## create a dataframe to concat later \n",
    "    growing_df = pd.DataFrame()\n",
    "\n",
    "    ## for loop through the topic code list \n",
    "    for metric in ['count', 'citationCount', 'influentialCitationCount']:\n",
    "        for topic_num in topic_code_list:\n",
    "            print(topic_num)\n",
    "            current_df = plot_bertopic_data_into_future(dataframe, topic_num, forecasting_years, moving_average_years, metric, timeframe, cumlative=cumlative_boolean)\n",
    "            growing_df = pd.concat([current_df, growing_df]) \n",
    "        \n",
    "    ## reset the index and drop the inxiex column \n",
    "    growing_df = growing_df.reset_index().drop('index',axis=1)\n",
    "\n",
    "    ## match the topic code to the topic description \n",
    "    topic_codes = dataframe[['topic_code','topic_list']].drop_duplicates().reset_index().drop('index',axis=1)\n",
    "\n",
    "    ## merge the topics descriptions to the data set\n",
    "    full_dataset = growing_df.merge(topic_codes, on='topic_code')\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_tableau_table(final_df, final_forecasting, final_moving_ave):\n",
    "    cumsum_df = tableau_tables(tab_df= final_df, tab_forecasting= final_forecasting, tab_moving_ave= final_moving_ave, tab_cumsum= True)\n",
    "    other_df = tableau_tables(tab_df= final_df, tab_forecasting= final_forecasting, tab_moving_ave= final_moving_ave, tab_cumsum= False)\n",
    "    final_df = pd.concat([other_df, cumsum_df]) \n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = aws_dfs\n",
    "forecasting_years = 5\n",
    "moving_average_years = 1\n",
    "## True goes with \n",
    "cumlative_boolean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>value</th>\n",
       "      <th>topic_code</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>metric</th>\n",
       "      <th>cumsum_or_moving_average</th>\n",
       "      <th>topic_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>Actual_Test</td>\n",
       "      <td>117</td>\n",
       "      <td>26.053664</td>\n",
       "      <td>influentialCitationCount</td>\n",
       "      <td>1</td>\n",
       "      <td>117_multiple sclerosis_sclerosis_multiple_axonal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>Actual_Test</td>\n",
       "      <td>117</td>\n",
       "      <td>26.053664</td>\n",
       "      <td>influentialCitationCount</td>\n",
       "      <td>1</td>\n",
       "      <td>117_multiple sclerosis_sclerosis_multiple_axonal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Actual_Test</td>\n",
       "      <td>117</td>\n",
       "      <td>26.053664</td>\n",
       "      <td>influentialCitationCount</td>\n",
       "      <td>1</td>\n",
       "      <td>117_multiple sclerosis_sclerosis_multiple_axonal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>Actual_Test</td>\n",
       "      <td>117</td>\n",
       "      <td>26.053664</td>\n",
       "      <td>influentialCitationCount</td>\n",
       "      <td>1</td>\n",
       "      <td>117_multiple sclerosis_sclerosis_multiple_axonal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Actual_Test</td>\n",
       "      <td>117</td>\n",
       "      <td>26.053664</td>\n",
       "      <td>influentialCitationCount</td>\n",
       "      <td>1</td>\n",
       "      <td>117_multiple sclerosis_sclerosis_multiple_axonal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583333</th>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>19357.702272</td>\n",
       "      <td>Predicted_Test</td>\n",
       "      <td>-1</td>\n",
       "      <td>715.215259</td>\n",
       "      <td>count</td>\n",
       "      <td>cumsum</td>\n",
       "      <td>-1_patients_study_results_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583334</th>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>19411.822625</td>\n",
       "      <td>Predicted_Test</td>\n",
       "      <td>-1</td>\n",
       "      <td>715.215259</td>\n",
       "      <td>count</td>\n",
       "      <td>cumsum</td>\n",
       "      <td>-1_patients_study_results_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583335</th>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>19567.054142</td>\n",
       "      <td>Predicted_Test</td>\n",
       "      <td>-1</td>\n",
       "      <td>715.215259</td>\n",
       "      <td>count</td>\n",
       "      <td>cumsum</td>\n",
       "      <td>-1_patients_study_results_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583336</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>19626.748011</td>\n",
       "      <td>Predicted_Test</td>\n",
       "      <td>-1</td>\n",
       "      <td>715.215259</td>\n",
       "      <td>count</td>\n",
       "      <td>cumsum</td>\n",
       "      <td>-1_patients_study_results_disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583337</th>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>19772.521503</td>\n",
       "      <td>Predicted_Test</td>\n",
       "      <td>-1</td>\n",
       "      <td>715.215259</td>\n",
       "      <td>count</td>\n",
       "      <td>cumsum</td>\n",
       "      <td>-1_patients_study_results_disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>631890 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      quantity           value  topic_code        RMSE   \n",
       "0      2011-01-01     81.000000     Actual_Test         117   26.053664  \\\n",
       "1      2012-01-01     18.000000     Actual_Test         117   26.053664   \n",
       "2      2013-01-01      5.000000     Actual_Test         117   26.053664   \n",
       "3      2014-01-01      2.000000     Actual_Test         117   26.053664   \n",
       "4      2015-01-01      0.000000     Actual_Test         117   26.053664   \n",
       "...           ...           ...             ...         ...         ...   \n",
       "583333 2023-04-01  19357.702272  Predicted_Test          -1  715.215259   \n",
       "583334 2023-05-01  19411.822625  Predicted_Test          -1  715.215259   \n",
       "583335 2023-06-01  19567.054142  Predicted_Test          -1  715.215259   \n",
       "583336 2023-07-01  19626.748011  Predicted_Test          -1  715.215259   \n",
       "583337 2023-08-01  19772.521503  Predicted_Test          -1  715.215259   \n",
       "\n",
       "                          metric cumsum_or_moving_average   \n",
       "0       influentialCitationCount                        1  \\\n",
       "1       influentialCitationCount                        1   \n",
       "2       influentialCitationCount                        1   \n",
       "3       influentialCitationCount                        1   \n",
       "4       influentialCitationCount                        1   \n",
       "...                          ...                      ...   \n",
       "583333                     count                   cumsum   \n",
       "583334                     count                   cumsum   \n",
       "583335                     count                   cumsum   \n",
       "583336                     count                   cumsum   \n",
       "583337                     count                   cumsum   \n",
       "\n",
       "                                              topic_list  \n",
       "0       117_multiple sclerosis_sclerosis_multiple_axonal  \n",
       "1       117_multiple sclerosis_sclerosis_multiple_axonal  \n",
       "2       117_multiple sclerosis_sclerosis_multiple_axonal  \n",
       "3       117_multiple sclerosis_sclerosis_multiple_axonal  \n",
       "4       117_multiple sclerosis_sclerosis_multiple_axonal  \n",
       "...                                                  ...  \n",
       "583333                 -1_patients_study_results_disease  \n",
       "583334                 -1_patients_study_results_disease  \n",
       "583335                 -1_patients_study_results_disease  \n",
       "583336                 -1_patients_study_results_disease  \n",
       "583337                 -1_patients_study_results_disease  \n",
       "\n",
       "[631890 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tableau_table(final_df= aws_dfs, final_forecasting = 5, final_moving_ave = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_functions import bertopic_addons as cfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_datase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m name_for_dataframe \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mforecasted_predicted_actual_Science_1900_2023\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m cfc\u001b[39m.\u001b[39mdataframe_to_aws_sql(full_datase,name_for_dataframe)\n\u001b[0;32m      3\u001b[0m cfc\u001b[39m.\u001b[39mpull_aws_sql_database(name_for_dataframe)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_datase' is not defined"
     ]
    }
   ],
   "source": [
    "name_for_dataframe = 'forecasted_predicted_actual_Science_1900_2023'\n",
    "cfc.dataframe_to_aws_sql(the_df,name_for_dataframe)\n",
    "cfc.pull_aws_sql_database(name_for_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
